2.5D Visual Sound

Ruohan Gao*
The University of Texas at Austin

rhgao@cs.utexas.edu

Kristen Grauman
Facebook AI Research

†
grauman@fb.com

Abstract

Binaural audio provides a listener with 3D sound sen-
sation, allowing a rich perceptual experience of the scene.
However, binaural recordings are scarcely available and
require nontrivial expertise and equipment to obtain. We
propose to convert common monaural audio into binau-
ral audio by leveraging video. The key idea is that vi-
sual frames reveal signiﬁcant spatial cues that, while ex-
plicitly lacking in the accompanying single-channel au-
dio, are strongly linked to it. Our multi-modal approach
recovers this link from unlabeled video. We devise a
deep convolutional neural network that learns to decode
the monaural (single-channel) soundtrack into its binau-
ral counterpart by injecting visual information about ob-
ject and scene conﬁgurations. We call the resulting output
2.5D visual sound—the visual stream helps “lift” the ﬂat
single channel audio into spatialized sound. In addition to
sound generation, we show the self-supervised representa-
tion learned by our network beneﬁts audio-visual source
separation. Our video results: http://vision.cs.
utexas.edu/projects/2.5D_visual_sound/

1. Introduction

Multi-modal perception is essential to capture the rich-
ness of real-world sensory data and environments. People
perceive the world by combining a number of simultaneous
sensory streams, among which the visual and audio streams
are paramount.
In particular, both audio and visual data
convey signiﬁcant spatial information. We see where ob-
jects are and how the room is laid out. We also hear them:
sound-emitting objects indicate their location, and sound re-
verberations reveal the room’s main surfaces, materials, and
dimensions. Similarly, as in the famous cocktail party sce-
nario, while having a conversation at a noisy party, one can
hear another voice calling out and turn to face it. The two
senses naturally work in concert to interpret spatial signals.

*Work done during an internship at Facebook AI Research.
†On
leave
from The University
of
Texas
at Austin
(grauman@cs.utexas.edu).

Figure 1: Binaural audio creates a 3D soundscape for lis-
teners, but such recordings remain rare. The proposed ap-
proach infers 2.5D visual sound by injecting the spatial in-
formation contained in the video frames accompanying a
typical monaural audio stream.

The human auditory system uses two ears to extract indi-
vidual sound sources from a complex mixture. The duplex
theory proposed by Lord Rayleigh says that sound source
locations are mainly determined by time differences be-
tween the sounds reaching each ear (Interaural Time Dif-
ference, ITD) and differences in sound level entering the
ears (Interaural Level Difference, ILD) [37]. Accordingly,
to mimic human hearing, binaural audio is usually recorded
using two microphones attached to the two ears of a dummy
head (see Fig. 2). The rig’s two microphones, their spacing,
and the physical shape of the ears are all signiﬁcant for ap-
proximating how humans receive sound signals. As a result,
when playing binaural audio through headphones, listeners
feel the 3D sound sensation of being in the place where the
recording was made and can easily localize the sounds. The
immersive spatial sound is valuable for audiophiles, AR/VR
applications, and social video sharers alike.

However, binaural recordings are difﬁcult to obtain in
daily life due to the high price of the recording device and
the required expertise. Consumer-level cameras typically
only record monaural audio with a single microphone, or
stereo audio recorded using two microphones with arbi-
trary arrangement and without physical representation of
the pinna (outer ear). We contend that for both machines

1324

and people, monaural or even stereo auditory input has very
limited dimension. Monaural audio collapses all indepen-
dent audio streams to the same spatial point, and the listener
cannot sense the spatial locations of the sound sources.

Our key insight is that video accompanying monaural au-
dio has the potential to unlock spatial sound, lifting a ﬂat au-
dio signal into what we call “2.5D visual sound”. Although
a single channel audio track alone does not encode any spa-
tial information, its accompanying visual frames do contain
object and scene conﬁgurations. For example, as shown in
Fig. 1, we observe from the video frame that a man is play-
ing the piano on the left and a man is playing the cello on the
right. Although we cannot sense the locations of the sound
sources by listening to the mono recording, we can nonethe-
less anticipate what we would hear if we were personally in
the scene by inference from the visual frames.

We introduce an approach to realize this intuition. Given
unlabeled training video, we devise a MONO2B INAURA L
deep convolutional neural network to convert monaural au-
dio to binaural audio by injecting the spatial cues embed-
ded in the visual frames. Our encoder-decoder style net-
work takes a mixed single-channel audio and its accompa-
nying visual frames as input to perform joint audio-visual
analysis, and attempts to predict a two-channel binaural au-
dio that agrees with the spatial conﬁgurations in the video.
When listening to the predicted binaural audio—the 2.5D
visual sound—listeners can then feel the locations of the
sound sources as they are displayed in the video.

Moreover, we show that apart from binaural audio gen-
eration, the MONO2B INAURA L conversion process can also
beneﬁt audio-visual source separation, a key challenge in
audio-visual analysis. State-of-the-art systems [13, 52, 31,
1, 10] aim to separate a mixed monaural audio record-
ing into its component sound sources, and thus far they
rely solely on the spatial cues evident in the visual stream.
We show that the proposed audio-visual binauralization can
self-supervise representation learning to elicit spatial sig-
nals relevant to separation from the audio stream as well.
Critically, gaining this new learning signal requires nei-
ther semantic annotations nor single-source data prepara-
tion, only the same unlabeled binaural training video.

Our main contributions are threefold: Firstly, we propose
to convert monaural audio to binaural audio by leverag-
ing video frames, and we design a MONO2B INAURA L deep
network to achieve that goal; Secondly, we collect FAIR-
Play, a 5.2 hour video dataset with binaural audio—the ﬁrst
dataset of its kind to facilitate research in both the audio
and vision communities; Thirdly, we propose to perform
audio-visual source separation on predicted binaural audio,
and show that it provides a useful self-supervised represen-
tation for the separation task. We validate our approach
on four challenging datasets spanning a variety of sound
sources (e.g., instruments, street scenes, travel, sports).

2. Related Work

Generating Sounds from Video Recent work explores
ways to generate audio conditioned on “silent” video. Ma-
terial properties are revealed by the sounds objects make
when hit with a drumstick, and can be used to syn-
thesize new sounds from silent videos [32]. Recurrent
networks [53] or conditional generative adversarial net-
works [7] can generate audio for input video frames, while
powerful simulators can synthesize audio-visual data for 3D
shapes [51]. Rather than generate audio from scratch, our
task entails converting an input one-channel audio to two-
channel binaural audio guided by the visual frames.

Only limited prior work considers video-based audio
spatialization [26, 28]. The system of [26] synthesizes
sound from a speaker in a room as a function of viewing an-
gle, but assumes access to an acoustic impulse recorded in
the speciﬁc room of interest, which restricts practical use,
e.g., for novel “off-the-shelf” videos. Concurrent work to
ours [28] generates ambisonics (audio for the full viewing
sphere) given 360◦ video and its mono audio. In contrast,
we focus on normal ﬁeld of view (NFOV) video and bin-
aural audio. We show that directly predicting binaural au-
dio creates better 3D sound sensations for listeners without
being restricted to 360◦ videos. Moreover, while the end
goal of [28] is audio spatialization, we also demonstrate
that our MONO2B INAURA L conversion process aids audio-
visual source separation.

Audio(-Visual) Source Separation Audio-only source
separation has been extensively studied in the signal pro-
cessing literature. “Blind” separation tackles the case where
only a single channel is available [42, 43, 46, 19]. Separa-
tion becomes easier when multiple channels are observed
using multiple microphones [29, 49, 9] or binaural au-
dio [48, 8, 50].
Inspired by this, we transform mono to
binaural by observing video, and then leverage the result-
ing representation to improve audio-visual separation.

Audio-visual source separation also has a rich history,
with methods exploring mutual information [11], subspace
analysis [41, 35], matrix factorization [34, 39, 13], and cor-
related onsets [6, 25]. Recent methods leverage deep learn-
ing for audio-visual separation of speech [10, 31, 1, 12],
musical instruments [52], and other objects [13]. New tasks
are also emerging, such as learning to separate on- and off-
screen sounds [31], learning object sound models from un-
labeled video [13], or predicting sounds per pixel [52]. All
these methods exploit mono audio cues to perform audio-
visual source separation, whereas we propose to predict bin-
aural cues to enhance separation. Furthermore, different
from the task of localizing pixels responsible for a given
sound [21, 18, 54, 3, 52, 40, 44], our goal is to perform
binaural audio synthesis.

325

Self-Supervised Learning Self-supervised learning ex-
ploits labels freely available in the structure of the data,
and audio-visual data offers a wealth of such tasks. Re-
cent work explores self-supervision for visual [33, 2]
and audio [4] feature learning, cross-modal representa-
tions [5], and audio-visual alignment [31, 23, 16]. Our
MONO2B INAURA L formulation is also self-supervised, but
unlike any of the above, we use visual frames to supervise
audio spatialization, while also learning better sound repre-
sentations for audio-visual source separation.

3. Approach

Our approach learns to map monaural audio to binaural
audio via video. In the following, we ﬁrst describe our bin-
aural audio video dataset (Sec. 3.1). Then we present our
MONO2B INAURA L formulation (Sec. 3.2), and our network
and training procedure to solve it (Sec. 3.3). Finally we in-
troduce our approach to leverage inferred binaural sound to
perform audio-visual source separation (Sec. 3.4).

3.1. FAIR(cid:173)Play Data Collection

Training our method requires binaural audio and accom-
panying video. Since no large public video datasets contain
binaural audio, we collect a new dataset we call FAIR-Play
with a custom rig. As shown in Fig. 2, we assembled a
rig consisting of a 3Dio Free Space XLR binaural micro-
phone, a GoPro HERO6 Black camera, and a Tascam DR-
60D recorder as the audio pre-ampliﬁer. We mounted the
GoPro camera on top of the 3Dio binaural microphone to
mimic a person’s embodiment for seeing and hearing, re-
spectively. The 3Dio binaural microphone records binaural
audio, and the GoPro camera records videos at 30fps with
stereo audio. We simultaneously record from both devices
so the streams are roughly aligned.
Note that both the ear shaped housing (pinnae) for the
microphones and their spatial separation are signiﬁcant;
professional binaural mics like 3Dio simulate the physical
manner in which humans receive sound. In contrast, stereo
sound is captured by two mics with an arbitrary separation
that varies across capture devices (phones, cameras), and so
lacks the spatial nuances of binaural. The limit of binaural
capture, however, is that a single rig inherently assumes a
single head-related transfer function, whereas individuals
have slight variations due to inter-person anatomical differ-
ences. Personalizing head-related transfer functions is an
area of active research [20, 45].
We captured videos with our custom rig in a large music
room (about 1,000 square feet). Our intent was to capture a
variety of sound making objects in a variety of spatial con-
texts, by assembling different combinations of instruments
and people in the room. The room contains various instru-
ments including cello, guitar, drum, ukelele, harp, piano,
trumpet, upright bass, and banjo. We recruited 20 volun-

Figure 2: Binaural rig and data collection in a music room.

teers to play and recorded them in solo, duet, and multi-
player performances. We post-process the raw data into 10s
clips. In the end, our FAIR-Play1 dataset consists of 1,871
short clips of musical performances, totaling 5.2 hours. In
experiments we use both the music data as well as ambison-
ics datasets [28] for street scenes and YouTube videos of
sports, travel, etc. (cf. Sec. 4).

3.2. Mono2Binaural Formulation

Binaural cues let us infer the location of sound sources.
The interaural time difference (ITD) and the interaural level
difference (ILD) play an essential role. ITD is caused by the
difference in travel distances between the two ears. When
a sound source is closer to one ear than the other, there is
a time delay between the signals’ arrival at the two ears.
ILD is caused by a “shadowing” effect—a listener’s head is
large relative to certain wavelengths of sound, so it serves
as a barrier, creating a shadow. The particular shape of the
head, pinnae, and torso also act as a ﬁlter depending on the
locations of the sound sources (distance, azimuth, and ele-
vation). All these cues are missing in monaural audio, thus
we cannot sense any spatial effect by listening to single-
channel audio.
We denote the signal received at the left and right ears
by xL (t ) and xR (t ), respectively. If we mix the two channels
into a single channel xM (t ) = xL (t ) + xR (t ), then all spatial
information collapses. We can formulate a self-supervised
task to take the mixed monaural signal xM (t ) as input and
split it into two separate channels ˜xL (t ) and ˜xR (t ), using the
original xL (t ), xR (t ) as ground-truth during training. How-
ever, this is a highly under-constrained problem, as xM (t )

1 https://github.com/facebookresearch/FAIR- Play

326

Figure 3: Our MONO2B INAURA L deep network takes a mixed monaural audio and its accompanying visual frame as input,
and predicts a two-channel binaural audio output that satisﬁes the visual spatial conﬁgurations. An ImageNet pre-trained
ResNet-18 network is used to extract visual features, and a U-NET is used to extract audio features and perform joint audio-
visual analysis. We predict a complex mask for the audio difference signal, then combine it with the input mono audio to
restore the left and right channels, respectively. At test time, the input is single-channel monaural audio.

lacks the necessary information to recover both channels.
Our key idea is to guide the MONO2B INAURA L process with
the accompanying video frames, from which visual spatial
information can serve as supervision.
Instead of directly predicting the two channels, we pre-
dict the difference of the two channels:

xD (t ) = xL (t ) − xR (t ).

(1)

More speciﬁcally, we operate on the frequency domain
and perform short-time Fourier transform (STFT) [15] on
xM (t ) to obtain the complex-valued spectrogram XM , and
the objective is to predict the complex-valued spectrogram
XD for xD (t ):

XM = {XM

t , f }T ,F

t=1, f =1 , XD = {XD

t , f }T ,F

t=1, f =1 ,

(2)

where t and f are the time frame and frequency bin indices,
respectively, and T and F are the numbers of bins. Then we
obtain the predicted difference signal ˜xD (t ) by the inverse
short-time Fourier transform (ISTFT) [15] of XD . Finally,
we recover both channels—the binaural audio output:

˜xL (t ) =

xM (t ) + ˜xD (t )
2

,

˜xR (t ) =

xM (t ) − ˜xD (t )
2

.

(3)

3.3. Mono2Binaural Network

Next we present our MONO2B INAURA L deep network to
perform audio spatialization. The network takes the mono
audio xM (t ) and visual frames as input and predicts xD (t ).
As shown in Fig. 3, we extract visual features from the
center frame of the audio segment using ResNet-18 [17],
which is pre-trained on ImageNet. The ResNet-18 network
extracts per-frame features after the 4t h ResNet block with
size (H /32) × (W /32) ×C, where H ,W ,C denote the frame
and channel dimensions. We then pass the visual feature

through a 1 × 1 convolution layer to reduce the channel di-
mension, and ﬂatten it into a single visual feature vector.
On the audio side, we adopt a U-NET [38] style architec-
ture. The U-NET encoder-decoder network adopted here is
ideal for our dense prediction task where the input and out-
put have the same dimension. We mix the left and right
channels of the binaural audio, and extract a sequence of
STFT frames to generate an audio spectrogram XM . We use
the complex spectrogram: each time-frequency bin contains
the real and imaginary part of the corresponding complex
spectrogram value. Then it is passed through a series of
convolution layers to extract an audio feature of dimension
(T /32) × (F /32) ×C. We replicate the visual feature vector
(T /32) × (F /32) times, tile them to match the audio feature
dimension, and then concatenate the audio and visual fea-
ture maps along the channel dimension. Through the series
of operations, each audio feature dimension is injected with
the visual feature to perform joint audio-visual analysis.
Finally, we perform up-convolutions on the concatenated
audio-visual feature map to generate a complex multiplica-
tive spectrogram mask M. In source separation tasks, spec-
trogram masks have proven better than alternatives such as
direct prediction of spectrograms or raw waveforms [47].
Similarly, here we also adopt the idea of masking, but our
goal is to mask the spectrogram of the mixed mono au-
dio and predict the spectrogram of the difference signal,
rather than perform separation. The real and imaginary
components of the complex mask are separately estimated
in the real domain. We add a sigmoid layer after the up-
convolution layers to bound the complex mask values to [-
1, 1], similar to [10]. The series of convolutions and up-
convolutions maps the input mono spectrogram to a com-
plex mask that encodes the predicted binaural audio.
Initially, we attempted to directly predict the left and
right channels. However, we found that direct prediction

327

makes the network fall back on a “safe” but useless solu-
tion of copying and pasting the input audio, without reason-
ing with the visual features. Instead, predicting the differ-
ence signal forces the deep network to analyze the visual
information and learn the subtle difference between the two
channels, as required by the binaural audio target.
The spectrogram of the difference signal is then obtained
by complex multiplying the input spectrogram with the pre-
dicted complex mask:

(4)

D = M · XM .

˜X
We train our MONO2B INAURA L network using L2 loss to
minimize the distance between the ground-truth complex
spectrogram and the predicted one. Finally, using ISTFT,
we obtain the predicted difference signal ˜xD (t ), through
which we recover the two channels ˜xL (t ) and ˜xR (t ) as de-
ﬁned in Eq. 3. See supp. for network details.
At test time, the network is presented with monaural au-
dio and a video frame and infers the binaural output, i.e.,
the 2.5D visual sound. To process a full video stream,
each video is decomposed into many short audio segments.
Video frames usually do not change much within such a
short segment. We use a sliding window to perform spa-
tialization segment by segment with a small hop size, and
average predictions on overlapping parts. Thus, our method
is able to handle moving sound sources and cameras.
Our approach expects a similar ﬁeld of view (FoV) be-
tween training and testing, and assumes the microphone is
near the camera. Our experiments demonstrate we can learn
MONO2B INAURA L for both normal FoV and 360◦ video,
and furthermore the same system can cope with mono in-
puts from variable hardware (e.g., YouTube videos).

3.4. Audio(cid:173)Visual Source Separation

So far we have deﬁned our MONO2B INAURA L approach
to convert monaural audio to binaural audio by introducing
visual spatial cues from video. Recall that we have two
goals: to predict binaural audio for sound generation itself,
and to explore its utility for audio-visual source separation.
Audio source separation is the problem of obtaining an
estimate for each of the J sources s j from the observed lin-
ear mixture x(t ) = ∑J
j=1 s j (t ). For binaural audio source
separation, the problem is to obtain an estimate for each of
the J sources s j from the observed binaural mixture xL (t )
and xR (t ):

xL (t ) =

J

∑

j=1

sL

j (t ),

xR (t ) =

sR

j (t ),

(5)

J

∑

j=1

where sL
j (t ) and sR
j (t ) are time-discrete signals received at
the left ear and the right ear for each source, respectively.
Interfering sound sources are often located at different
spatial positions in the physical space. Human listeners
exploit the spatial information from the coordination of
both ears to resolve sound ambiguity caused by multiple

Figure 4: Mix-and-Separate [52, 31, 10]-inspired frame-
work for audio-visual source separation. During training,
we mix the binaural audio tracks for a pair of videos to gen-
erate a mixed audio input. The network learns to separate
the sound for each video conditioned on their visual frames.

sources. This ability is greatly diminished when listen-
ing with only one ear, especially in reverberant environ-
ments [22]. Audio source separation by machine listeners is
similarly handicapped, typically lacking access to binaural
audio [52, 13, 31, 10]. However, we hypothesize that our
MONO2B INAURA L predicted binaural audio can aid sepa-
ration. Intuitively, by forcing the network to learn how to
lift mono audio to binaural, its representation is encouraged
to expose the very spatial cues that are valuable for source
separation. Thus, even though the MONO2B INAURA L fea-
tures see the same video as any other audio-visual separa-
tion method, they may better decode the latent spatial cues
because of their binauralization “pre-training” task.

In particular, we expect two main effects. First, binau-
ral audio embeds information about the spatial distribution
of sound sources, which can act as a regularizer for sep-
aration. Second, binaural cues may be especially helpful
in cases where sound sources have similar acoustic char-
acteristics, since the spatial organization can reduce source
ambiguities. Related regularization effects are observed in
other vision tasks. For example, hallucinating motion en-
hances static-image action recognition [14], or predicting
semantic segmentation informs depth estimation [27].

To implement a testbed for audio-visual source separa-
tion, we adopt the Mix-and-Separate idea [52, 31, 10]. We
use the same base architecture as our MONO2B INAURA L
network except that now the input to the network is a pair of
training video clips. Fig. 4 illustrates the separation frame-
work. We mix the sounds of the predicted binaural audio
for the two videos to generate a complex audio input sig-
nal, and the learning objective is to separate the binaural
audio for each video conditioned on their corresponding vi-
sual frames. Following [52], we only use spectrogram mag-
nitude and predict a ratio mask for separation. Per-pixel L1
loss is used for training. See supp. for details.

328

4. Experiments

4.3. Mono2Binaural Generation Accuracy

We validate our approach for generation and separation.

4.1. Datasets

We use four challenging datasets spanning a wide vari-
ety of sound sources, including musical instruments, street
scenes, travel, and sports.

FAIR-Play Our new dataset consists of 1,871 10s clips
of videos recorded in a music room (Fig. 2). The videos
are paired with binaural audios of high quality recorded
by a professional binaural microphone. We create 10 ran-
dom splits by splitting the data into train/val/test splits of
1,497/187/187 clips, respectively.

REC-STREET A dataset collected by [28] using a Theta
V 360◦ camera with TA-1 spatial audio microphone. It con-
sists of 43 videos (3.5 hours) of outdoor street scenes.

YT-CLEAN This dataset contains
in-the-wild 360◦
videos from YouTube crawled by [28] using queries related
to spatial audio. It consists of 496 videos of a small num-
ber of super-imposed sources, such as people talking in a
meeting room, outdoor sports, etc.

YT-MUSIC A dataset
that consists of 397 YouTube
videos of music performances collected by [28]. It is their
most challenging dataset due to the large number of mixed
sources (voices and instruments).
To our knowledge, FAIR-Play is the ﬁrst dataset of its
kind that contains videos of professional recorded binaural
audio. For REC-STREET, YT-CLEAN and YT-MUSIC,
we split the videos into 10s clips and divide them into
train/val/test splits based on the provided split1. These
datasets only contain ambisonics, so we use a binaural de-
coder to convert them to binaural audio. Speciﬁcally, we use
the head related transfer function (HRTF) from NH2 sub-
ject in the ARI HRTF Dataset2 to perform decoding. For
our FAIR-Play dataset, half of the training data is used to
train the MONO2B INAURA L network, and the other half is
reserved for audio-visual source separation experiments.

4.2. Implementation Details

Both our MONO2B INAURA L and separation networks are
in PyTorch. For all experiments, we resample the audio at
16kHz and STFT is computed using a Hann window of
length 25ms, hop length of 10ms, and FFT size of 512.
For MONO2B INAURA L training, we randomly sample audio
segments of length 0.63s from each 10s audio clip. During
testing, we use a sliding window with hop size 0.05s to bin-
auralize 10s audio clips for both our method and baselines.
For source separation experiments, we use similar network
design and training/testing strategies. See supp. for details.

2 http://www.kfs.oeaw.ac.at/hrtf

We evaluate the quality of our predicted binaural audio
by using common metrics as well as two user studies. We
compare to the following baselines:

• Ambisonics [28]: We use the pre-trained models pro-
vided by [28] to predict ambisonics. The models are
trained on the same data as our method. Then we use
the binaural decoder to convert the predicted ambison-
ics to binaural audio. This baseline is not available for
the BINAURAL-MUSIC-ROOM dataset.

• Audio-Only: To determine if visual information is es-
sential to perform MONO2B INAURA L conversion, we
remove the visual stream and implement a baseline us-
ing only audio as input. All other settings are the same
except that only audio features are passed to the up-
convolution layers for binaural audio prediction.

• Flipped-Visual: During testing, we ﬂip the accompa-
nying visual frames of the mono audios to perform pre-
diction using the wrong visual information.

• Mono-Mono: A straightforward baseline that copies
the mixed monaural audio onto both channels to create
a fake binaural audio.

We report two metrics: 1) STFT Distance: The eu-
clidean distance between the ground-truth and predicted
complex spectrograms of the left and right channels:
D{STFT} = ||XL − ˜X

L ||2 + ||XR − ˜X
R ||2 .

2) Envelope (ENV) Distance: Direct comparison of raw
waveforms may not capture perceptual similarity well. Fol-
lowing [28], we take the envelope of the signals, and mea-
sure the euclidean distance between the envelopes of the
ground-truth left and right channels and the predicted sig-
nals. Let E [x(t )] denote the envelope of signal x(t ). The
envelope distance is deﬁned as:

D{ENV} = ||E [xL (t )] − E [ ˜xL (t )||2 + ||E [xR (t )] − E [ ˜xR (t )||2 .

Results. Table 1 shows the binaural generation results.
Our method outperforms all baselines consistently on all
four datasets. Our MONO2B INAURA L approach performs
better than the Audio-Only baseline, indicating the visual
stream is essential to guide conversion. Note that the Audio-
Only baseline uses the same network design as our method,
so it has reasonably good performance. Still, we ﬁnd our
method outperforms it most when object(s) are not simply
located in the center. Flipped-Visual performs much worse,
demonstrating that our network properly learns to localize
sound sources to predict binaural audio correctly.
The Ambisonics [28] approach does not do as well. We
hypothesize several reasons. The method predicts four
channel ambisonics directly, which must be converted to

329

Ambisonics [28]
Audio-Only
Flipped-Visual
Mono-Mono

MONO2B INAURA L (Ours)

FAIR-Play
STFT
ENV

REC-STREET
STFT
ENV

YT-CLEAN
YT-MUSIC
STFT
ENV STFT
ENV

-
0.966
1.145
1.155
0.836

-
0.141
0.149
0.153
0.132

0.744
0.590
0.658
0.774
0.565

0.126
0.114
0.123
0.136
0.109

1.435
1.065
1.095
1.369
1.027

0.155
0.131
0.132
0.153
0.130

1.885
1.553
1.590
1.853
1.451

0.183
0.167
0.165
0.184
0.156

Table 1: Quantitative results of binaural audio prediction on four diverse datasets. We report the STFT distance and the
envelope distance; lower is better. For FAIR-Play, we report the average results across 10 random splits. The results have a
standard error of approximately 5 × 10−2 for STFT distance and 3 × 10−3 for ENV distance on average.

User studies. Having quantiﬁed the advantage of our
method in Table 1, we now report real user studies. To test
how well the predicted binaural audio makes a listener feel
the 3D sensation, we conduct two user studies.
For the ﬁrst study, the participants listen to a 10s ground-
truth binaural audio and see the visual frame. Then they
listen to two predicted binaural audios generated by our
method and a baseline (Ambisonics, Audio-Only, or Mono-
Mono). After listening to each pair, participants are asked
which of the two creates a better 3D sensation that matches
the ground-truth binaural audio. We recruited 18 partici-
pants with normal hearing. Each listened to 45 pairs span-
ning all the datasets. Fig. 5a shows the results. We report
the percentage of times each method is chosen as the pre-
ferred one. We can see that the binaural audio generated by
our method creates a more realistic 3D sensation.
For the second user study, we ask participants to name
the direction they hear a particular sound coming from.
Using the FAIR-Play data, we randomly select 10 in-
strument video clips where some player is located in the
left/center/right of the visual frames. We ask every partic-
ipant to only listen to the ground-truth or predicted binau-
ral audio from our method or a baseline, and then choose
the direction the sound of a speciﬁed instrument is coming
from. Note that for this study, we input real mono audio
recorded by the GoPro mic for binaural audio prediction.
Fig. 5b shows the results from the 18 participants. The true
recorded binaural audio is of high quality, and the listeners
can often easily perceive the correct direction. However,
our predicted binaural audio also clearly conveys direction-
ality. Compared to the baselines, ours presents listeners a
much more accurate spatial audio experience.

4.4. Localizing the Sound Sources

Does the network attend to the locations of the sound
sources when performing binauralization? As a byproduct
of our MONO2B INAURA L training, we can use the network
to perform sound source localization. We use a mask of size
32 × 32 to replace image regions with image mean values,
and forward the masked frame through the network to pre-
dict binaural audio. Then we compute the loss, and repeat

330

(a) User study 1

(b) User study 2

Figure 5: User studies to test how listeners perceive the pre-
dicted binaural audio.

binaural audio. While ambisonics have the advantage of be-
ing a more general audio representation that is ideal for 360◦
video, predicting ambisonics ﬁrst and then decoding to bin-
aural audio for deployment can introduce artifacts that make
the binaural audio less realistic. Better head-related transfer
functions could help to render more realistic binaural audio
from ambisonics, but this remains active research [30, 24].3
Furthermore, manually inspecting the results, we ﬁnd that
the decoded binaural audio by [28] conveys spatial sensa-
tion, but it is less accurate and stable than our method. Our
approach directly formulates the audio spatialization prob-
lem in terms of the two-channel binaural audio that listeners
ultimately hear, which yields better accuracy.
Our video results4 show qualitative results including fail-
ure cases. Our system can fail when there are multiple
objects of similar appearance, e.g. multiple human speak-
ers. Our model incorrectly spatializes the audio, because
the people are too visually similar. However, when there
is only one human speaker amidst other sounds, it can suc-
cessfully perform audio spatialization. Future work incor-
porating motion may beneﬁt instance-level spatialization.

3We experimented with multiple ambisonics-binaural decoding solu-
tions and report the best results for [28] in Table 1.
4 http://vision.cs.utexas.edu/projects/2.5D_
visual_sound/

Figure 6: Visualizing the key regions the visual network focuses on when performing MONO2B INAURA L conversion. Each
pair of images shows the frame accompanying the monaural audio (left) and the heatmap of the key regions overlaid (right).

Mono
Mono-Mono
Predicted Binaural (Ours)
GT Binaural (upper bound)

SDR

SIR

SAR

2.57
2.43
3.01
3.25

4.25
4.01
5.03
5.32

10.12
10.15
10.24
10.60

Table 2: Audio-visual source separation results. SDR, SIR,
SAR are reported in dB; higher is better.

by placing the mask at different locations of the frame. Fi-
nally, we highlight the regions which, when replaced, lead
to the largest losses. They are considered the most impor-
tant regions for MONO2B INAURA L conversion, and are ex-
pected to align with sound sources.
Fig. 6 shows examples. The highlighted key regions cor-
relate quite well with sound sources. They are usually the
instruments playing in the music room, the moving cars in
street scenes, the place where an activity is going on, etc.
The ﬁnal row shows some failure cases. The model can
be confused when there are multiple similar instruments in
view, or silent or noisy scenes. Sound sources in YT-Clean
and YT-Music are especially difﬁcult to spatialize and lo-
calize due to diverse and/or large number of sound sources.

4.5. Audio(cid:173)Visual Source Separation

Having demonstrated our predicted binaural audio cre-
ates a better 3D sensation, we now examine its impact on
audio-visual source separation using the FAIR-Play dataset.
The dataset contains object-level sounds of diverse sound
making objects (instruments), which is well-suited for the
Mix-and-Separate audio-visual source separation approach
we adopt. We train on the held-out data of FAIR-Play, and
test on 10 typical single-instrument video clips from the
val/test set, with each representing one unique instrument
in our dataset. We pairwise mix each video clip and per-
form separation, for a total of 45 test videos.

In addition to the ground truth binaural (upper bound)
and the Mono-Mono baseline deﬁned above, we compare
to a Mono baseline that takes monaural audio as input and
separates monaural audios for each source. Mono repre-
sents the current norm of performing audio-visual source
separation using only single-channel audio [52, 13, 31]. We
stress that all other aspects of the networks are the same, so
that any differences in performance can be attributed to our
binauralization self-supervision. To evaluate source sepa-
ration quality, we use the widely used mir eval library [36],
and the standard metrics: Signal-to-Distortion Ratio (SDR),
Signal-to-Interference Ratio (SIR), and Signal-to-Artifact
Ratio (SAR). Table 2 shows the results. We obtain large
gains by inferring binaural audio. The inferred binaural
audio offers a more informative audio representation com-
pared to the original monaural audio, leading to cleaner sep-
aration. See supp. video4 for examples.

5. Conclusion

We presented an approach to convert single channel au-
dio into binaural audio by leveraging object/scene conﬁg-
urations in the visual frames. The predicted 2.5D visual
sound offers a more immersive audio experience. Our
MONO2B INAURA L framework achieves state-of-the-art per-
formance on audio spatialization. Moreover, using the pre-
dicted binaural audio as a better audio representation, we
boost a modern model for audio-visual source separation.
Generating binaural audio for off-the-shelf video can poten-
tially close the gap between transporting audio and visual
experiences, enabling new applications in VR/AR. As fu-
ture work, we plan to explore ways to incorporate object lo-
calization and motion, and explicitly model scene sounds.

Acknowledgements: Thanks to Tony Miller, Jacob Donley, Pablo
Hoffmann, Vladimir Tourbabin, Vamsi Ithapu, Varun Nair, Abesh Thakur,
Jaime Morales, Chetan Gupta from Facebook, Xinying Hao, Dongguang
You, and the UT Austin vision group for helpful discussions.

331

References

[1] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisser-
man. The conversation: Deep audio-visual speech enhance-
ment. In Interspeech, 2018. 2
[2] Relja Arandjelovic and Andrew Zisserman. Look, listen and
learn. In ICCV, 2017. 3
[3] Relja Arandjelovi ´c and Andrew Zisserman. Objects that
sound. In ECCV, 2018. 2
[4] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Sound-
net: Learning sound representations from unlabeled video.
In NIPS, 2016. 3
[5] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. See,
hear, and read: Deep aligned representations. arXiv preprint
arXiv:1706.00932, 2017. 3
[6] Zohar Barzelay and Yoav Y Schechner. Harmony in motion.
In CVPR, 2007. 2
[7] Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, and Chen-
liang Xu. Deep cross-modal audio-visual generation. In on
Thematic Workshops of ACM Multimedia, 2017. 2
[8] Antoine Deleforge and Radu Horaud. The cocktail party
robot: Sound source separation and localisation with an ac-
tive binaural head.
In Proceedings of the seventh annual
ACM/IEEE international conference on Human-Robot Inter-
action, 2012. 2
[9] Ngoc QK Duong, Emmanuel Vincent, and R ´emi Gribonval.
Under-determined reverberant audio source separation using
a full-rank spatial covariance model. IEEE Transactions on
Audio, Speech, and Language Processing, 2010. 2
[10] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin
Wilson, Avinatan Hassidim, William T Freeman, and
Michael Rubinstein. Looking to listen at the cocktail party:
A speaker-independent audio-visual model for speech sepa-
ration. In SIGGRAPH, 2018. 2, 4, 5
[11] John W Fisher III, Trevor Darrell, William T Freeman, and
Paul A Viola. Learning joint statistical models for audio-
visual fusion and segregation. In NIPS, 2001. 2
[12] Aviv Gabbay, Asaph Shamir, and Shmuel Peleg. Visual
speech enhancement. In Interspeech, 2018. 2
[13] Ruohan Gao, Rogerio Feris, and Kristen Grauman. Learning
to separate object sounds by watching unlabeled video. In
ECCV, 2018. 2, 5, 8
[14] Ruohan Gao, Bo Xiong, and Kristen Grauman.
Im2ﬂow:
Motion hallucination from static images for action recogni-
tion. In CVPR, 2018. 5
[15] Daniel Grifﬁn and Jae Lim. Signal estimation from modiﬁed
short-time fourier transform. IEEE Transactions on Acous-
tics, Speech, and Signal Processing, 1984. 4
[16] David Harwath, Adri `a Recasens, D´ıdac Sur´ıs, Galen
Chuang, Antonio Torralba, and James Glass.
Jointly dis-
covering visual objects and spoken words from raw sensory
input. In ECCV, 2018. 3
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
2016. 4
[18] John R Hershey and Javier R Movellan. Audio vision: Using
audio-visual synchrony to locate sounds. In NIPS, 2000. 2

[19] Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, and
Paris Smaragdis. Deep learning for monaural speech separa-
tion. In ICASSP, 2014. 2
[20] Kazuhiro Iida, Yohji Ishii, and Shinsuke Nishioka. Personal-
ization of head-related transfer functions in the median plane
based on the anthropometry of the listener’s pinnae. The
Journal of the Acoustical Society of America, 2014. 3
[21] Einat Kidron, Yoav Y Schechner, and Michael Elad. Pixels
that sound. In CVPR, 2005. 2
[22] W Koenig. Subjective effects in binaural hearing. The Jour-
nal of the Acoustical Society of America, 1950. 5
[23] Bruno Korbar, Du Tran, and Lorenzo Torresani. Co-training
of audio and video representations from self-supervised tem-
poral synchronization. In NIPS, 2018. 3
[24] Matthias Kronlachner. Spatial transformations for the alter-
ation of ambisonic recordings. M. Thesis, University of Mu-
sic and Performing Arts, Graz, Institute of Electronic Music
and Acoustics, 2014. 7
[25] Bochen Li, Karthik Dinesh, Zhiyao Duan, and Gaurav
Sharma. See and listen: Score-informed association of sound
tracks to players in chamber music performance videos. In
ICASSP, 2017. 2
[26] Dingzeyu Li, Timothy R. Langlois, and Changxi Zheng.
Scene-aware audio for 360° videos. SIGGRAPH, 2018. 2
[27] Beyang Liu, Stephen Gould, and Daphne Koller. Single
image depth estimation from predicted semantic labels. In
CVPR, 2010. 5
[28] Pedro Morgado, Nono Vasconcelos, Timothy Langlois, and
Oliver Wang. Self-supervised generation of spatial audio for
360◦ video. arXiv preprint arXiv:1809.02587, 2018. 2, 3, 6,
7
[29] Kazuhiro Nakadai, Ken-ichi Hidai, Hiroshi G Okuno, and
Hiroaki Kitano. Real-time speaker localization and speech
separation by audio-visual integration. In IEEE International
Conference on Robotics and Automation, 2002. 2
[30] Markus Noisternig, Alois Sontacchi, Thomas Musil, and
Robert Holdrich. A 3d ambisonic based binaural sound re-
production system.
In Audio Engineering Society Confer-
ence: 24th International Conference: Multichannel Audio,
The New Reality. Audio Engineering Society, 2003. 7
[31] Andrew Owens and Alexei A Efros. Audio-visual scene
analysis with self-supervised multisensory features.
In
ECCV, 2018. 2, 3, 5, 8
[32] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Tor-
ralba, Edward H Adelson, and William T Freeman. Visually
indicated sounds. In CVPR, 2016. 2
[33] Andrew Owens, Jiajun Wu, Josh H McDermott, William T
Freeman, and Antonio Torralba. Ambient sound provides
supervision for visual learning. In ECCV, 2016. 3
[34] Sanjeel Parekh, Slim Essid, Alexey Ozerov, Ngoc QK
Duong, Patrick P ´erez, and Ga ¨el Richard. Motion informed
audio source separation. In ICASSP, 2017. 2
[35] Jie Pu, Yannis Panagakis, Stavros Petridis, and Maja Pantic.
Audio-visual object localization and separation using low-
rank and sparsity. In ICASSP, 2017. 2
[36] Colin Raffel, Brian McFee, Eric J Humphrey, Justin Sala-
mon, Oriol Nieto, Dawen Liang, Daniel PW Ellis, and

332

C Colin Raffel. mir eval: A transparent implementation of
common mir metrics. In ISMIR, 2014. 8

[37] Lord Rayleigh. On our perception of the direction of a source
of sound. Proceedings of the Musical Association, 1875. 1

[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention, 2015. 4

[39] Farnaz Sedighin, Massoud Babaie-Zadeh, Bertrand Rivet,
and Christian Jutten. Two multimodal approaches for sin-
gle microphone source separation. In 24th European Signal
Processing Conference, 2016. 2

[40] Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan
Yang, and In So Kweon. Learning to localize sound source
in visual scenes. In CVPR, 2018. 2

[41] Paris Smaragdis and Michael Casey. Audio/visual indepen-
dent components. In International Conference on Indepen-
dent Component Analysis and Signal Separation, 2003. 2

[42] Paris Smaragdis, Bhiksha Raj, and Madhusudana Shashanka.
Supervised and semi-supervised separation of sounds from
single-channel mixtures.
In International Conference on
Independent Component Analysis and Signal Separation,
2007. 2

[43] Martin Spiertz and Volker Gnann. Source-ﬁlter based clus-
tering for monaural blind source separation. In 12th Interna-
tional Conference on Digital Audio Effects, 2009. 2

[44] Y. Tian, J. Shi, B. Li, Z. Duan, and C. Xu. Audio-visual event
localization in unconstrained videos. In ECCV, 2018. 2

[45] Edgar A Torres-Gallegos, Felipe Orduna-Bustamante, and
Fernando Ar ´ambula-Cos´ıo.
Personalization of head-
related transfer functions (hrtf) based on automatic photo-
anthropometry and inference from a database.
Applied
Acoustics, 2015. 3

[46] Tuomas Virtanen. Monaural sound source separation by non-
negative matrix factorization with temporal continuity and
sparseness criteria. IEEE transactions on audio, speech, and
language processing, 2007. 2

[47] DeLiang Wang and Jitong Chen. Supervised speech sep-
aration based on deep learning: An overview.
IEEE/ACM
Transactions on Audio, Speech, and Language Processing,
2018. 4

[48] Ron J Weiss, Michael I Mandel, and Daniel PW Ellis. Source
separation based on binaural cues and source model con-
straints.
In Ninth Annual Conference of the International
Speech Communication Association, 2008. 2

[49] Ozgur Yilmaz and Scott Rickard. Blind separation of speech
mixtures via time-frequency masking. IEEE Transactions on
signal processing, 2004. 2

[50] Xueliang Zhang and DeLiang Wang. Deep learning based
binaural speech separation in reverberant environments.
IEEE/ACM transactions on audio, speech, and language
processing, 2017. 2

[51] Zhoutong Zhang, Jiajun Wu, Qiujia Li, Zhengjia Huang,
James Traer, Josh H. McDermott, Joshua B. Tenenbaum, and
William T. Freeman. Generative modeling of audible shapes
for object perception. In ICCV, 2017. 2

[52] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Von-
drick, Josh McDermott, and Antonio Torralba. The sound of
pixels. In ECCV, 2018. 2, 5, 8
[53] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and
Tamara L Berg. Visual to sound: Generating natural sound
for videos in the wild. In CVPR, 2018. 2
[54] A. Zunino, M. Crocco, S. Martelli, A. Trucco, A. Bue, and
V. Murino. Seeing the sound: a new multimodal imaging
device for computer vision. In ICCV workshops, 2015. 2

333

A General and Adaptive Robust Loss Function

Jonathan T. Barron
Google Research

Abstract

We present a generalization of the Cauchy/Lorentzian,
Geman-McClure, Welsch/Leclerc, generalized Charbon-
nier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss func-
tions. By introducing robustness as a continuous param-
eter, our loss function allows algorithms built around ro-
bust loss minimization to be generalized, which improves
performance on basic vision tasks such as registration and
clustering.
Interpreting our loss as the negative log of a
univariate density yields a general probability distribution
that includes normal and Cauchy distributions as special
cases. This probabilistic interpretation enables the training
of neural networks in which the robustness of the loss auto-
matically adapts itself during training, which improves per-
formance on learning-based tasks such as generative im-
age synthesis and unsupervised monocular depth estima-
tion, without requiring any manual parameter tuning.

Many problems in statistics and optimization require ro-
bustness — that a model be less inﬂuenced by outliers than
by inliers [17, 19]. This idea is common in parameter es-
timation and learning tasks, where a robust loss (say, ab-
solute error) may be preferred over a non-robust loss (say,
squared error) due to its reduced sensitivity to large errors.
Researchers have developed various robust penalties with
particular properties, many of which are summarized well
in [3, 39]. In gradient descent or M-estimation [16] these
losses are often interchangeable, so researchers may exper-
iment with different losses when designing a system. This
ﬂexibility in shaping a loss function may be useful because
of non-Gaussian noise, or simply because the loss that is
minimized during learning or parameter estimation is dif-
ferent from how the resulting learned model or estimated
parameters will be evaluated. For example, one might train
a neural network by minimizing the difference between the
network’s output and a set of images, but evaluate that net-
work in terms of how well it hallucinates random images.

In this paper we present a single loss function that is a
superset of many common robust loss functions. A single
continuous-valued parameter in our general loss function
can be set such that it is equal to several traditional losses,

and can be adjusted to model a wider family of functions.
This allows us to generalize algorithms built around a ﬁxed
robust loss with a new “robustness” hyperparameter that can
be tuned or annealed to improve performance.
Though new hyperparameters may be valuable to a prac-
titioner, they complicate experimentation by requiring man-
ual tuning or time-consuming cross-validation. However,
by viewing our general loss function as the negative log-
likelihood of a probability distribution, and by treating the
robustness of that distribution as a latent variable, we show
that maximizing the likelihood of that distribution allows
gradient-based optimization frameworks to automatically
determine how robust the loss should be without any manual
parameter tuning. This “adaptive” form of our loss is par-
ticularly effective in models with multivariate output spaces
(say, image generation or depth estimation) as we can intro-
duce independent robustness variables for each dimension
in the output and thereby allow the model to independently
adapt the robustness of its loss in each dimension.
The rest of the paper is as follows: In Section 1 we de-
ﬁne our general loss function, relate it to existing losses,
and enumerate some of its useful properties.
In Sec-
tion 2 we use our loss to construct a probability distri-
bution, which requires deriving a partition function and a
sampling procedure. Section 3 discusses four representa-
tive experiments:
In Sections 3.1 and 3.2 we take two

Figure 1. Our general loss function (left) and its gradient (right)
for different values of its shape parameter α. Several values of α
reproduce existing loss functions: L2 loss (α = 2), Charbonnier
loss (α = 1), Cauchy loss (α = 0), Geman-McClure loss (α =
−2), and Welsch loss (α = −∞).

14331

vision-oriented deep learning models (variational autoen-
coders for image synthesis and self-supervised monocular
depth estimation), replace their losses with the negative log-
likelihood of our general distribution, and demonstrate that
allowing our distribution to automatically determine its own
robustness can improve performance without introducing
any additional manually-tuned hyperparameters.
In Sec-
tions 3.3 and 3.4 we use our loss function to generalize
algorithms for the classic vision tasks of registration and
clustering, and demonstrate the performance improvement
that can be achieved by introducing robustness as a hyper-
parameter that is annealed or manually tuned.

1. Loss Function

The simplest form of our loss function is:

+ 1!

α/2

− 1

f (x, α, c) = |α − 2|

α 

 (x/c)2

|α − 2|

 (1)
Here α ∈ R is a shape parameter that controls the robust-
ness of the loss and c > 0 is a scale parameter that controls
the size of the loss’s quadratic bowl near x = 0.
Though our loss is undeﬁned when α = 2, it approaches
L2 loss (squared error) in the limit:

lim

α→2

f (x, α, c) =

(x/c)2

1
2

When α = 1 our loss is a smoothed form of L1 loss:

f (x, 1, c) = p(x/c)2 + 1 − 1

This is often referred to as Charbonnier loss [5], pseudo-
Huber loss (as it resembles Huber loss [18]), or L1-L2 loss
[39] (as it behaves like L2 loss near the origin and like L1
loss elsewhere).
Our loss’s ability to express L2 and smoothed L1 losses
is shared by the “generalized Charbonnier” loss [34], which
has been used in ﬂow and depth estimation tasks that require
robustness [6, 23] and is commonly deﬁned as:

α/2

(4)

(cid:0)x2 + ǫ2 (cid:1)

Our loss has signiﬁcantly more expressive power than the
generalized Charbonnier loss, which we can see by set-
ting our shape parameter α to nonpositive values. Though
f (x, 0, c) is undeﬁned, we can take the limit of f (x, α, c)
as α approaches zero:

lim

α→0

f (x, α, c) = log (cid:18) 1
2

(x/c)2 + 1(cid:19)

(5)

This yields Cauchy (aka Lorentzian) loss [2]. By setting
α = −2, our loss reproduces Geman-McClure loss [13]:

(2)

(3)

f (x, −2, c) =

2 (x/c)2
(x/c)2 + 4

(6)

4332

In the limit as α approaches negative inﬁnity, our loss be-
comes Welsch [20] (aka Leclerc [25]) loss:

lim

α→−∞

f (x, α, c) = 1 − exp (cid:18)−

1
2

(x/c)2(cid:19)

(7)

With this analysis we can present our ﬁnal loss function,
which is simply f (·) with special cases for its removable
singularities at α = 0 and α = 2 and its limit at α = −∞.

ρ (x, α, c) =

if α = 2

if α = 0
if α = −∞

1
2 (x/c)2

log (cid:16) 1
2 (x/c)2 + 1(cid:17)
1 − exp (cid:16)− 1
2 (x/c)2(cid:17)




α/2

|α−2|
α (cid:18)(cid:16) (x/c)2
|α−2| + 1(cid:17)

− 1(cid:19) otherwise
(8)
As we have shown,
this loss function is a superset of
the Welsch/Leclerc, Geman-McClure, Cauchy/Lorentzian,
generalized Charbonnier, Charbonnier/pseudo-Huber/L1-
L2, and L2 loss functions.
To enable gradient-based optimization we can derive the
derivative of ρ (x, α, c) with respect to x:

∂ ρ
∂x

(x, α, c) =

x
c2




c2 exp (cid:16)− 1
2 (x/c)2(cid:17)

2x
x2+2c2
x
|α−2| + 1(cid:17)(α/2−1)
c2 (cid:16) (x/c)2
x

if α = 2
if α = 0
if α = −∞

otherwise

(9)
Our loss and its derivative are visualized for different values
of α in Figure 1.
The shape of the derivative gives some intuition as to
how α affects behavior when our loss is being minimized by
gradient descent or some related method. For all values of α
the derivative is approximately linear when |x| < c, so the
effect of a small residual is always linearly proportional to
that residual’s magnitude. If α = 2, the derivative’s magni-
tude stays linearly proportional to the residual’s magnitude
— a larger residual has a correspondingly larger effect. If
α = 1 the derivative’s magnitude saturates to a constant 1/c
as |x| grows larger than c, so as a residual increases its ef-
fect never decreases but never exceeds a ﬁxed amount. If
α < 1 the derivative’s magnitude begins to decrease as |x|
grows larger than c (in the language of M-estimation [16],
the derivative, aka “inﬂuence”, is “redescending”) so as the
residual of an outlier increases, that outlier has less effect
during gradient descent. The effect of an outlier diminishes
as α becomes more negative, and as α approaches −∞ an
outlier whose residual magnitude is larger than 3c is almost
completely ignored.
We can also reason about α in terms of averages. Be-
cause the empirical mean of a set of values minimizes total
squared error between the mean and the set, and the empir-
ical median similarly minimizes absolute error, minimizing

our loss with α = 2 is equivalent to estimating a mean, and
with α = 1 is similar to estimating a median. Minimizing
our loss with α = −∞ is equivalent to local mode-ﬁnding
[35]. Values of α between these extents can be thought of
as smoothly interpolating between these three kinds of av-
erages during estimation.
Our loss function has several useful properties that we
will take advantage of. The loss is smooth (i.e., in C∞ )
with respect to x, α, and c > 0, and is therefore well-suited
to gradient-based optimization over its input and its param-
eters. The loss is zero at the origin, and increases monoton-
ically with respect to |x|:

ρ (0, α, c) = 0

∂ ρ

∂ |x|

(x, α, c) ≥ 0

(10)

The loss is invariant to a simultaneous scaling of c and x:

∀k>0 ρ(kx, α, kc) = ρ(x, α, c)

The loss increases monotonically with respect to α:

∂ ρ
∂α

(x, α, c) ≥ 0

(11)

(12)

This is convenient for graduated non-convexity [4]: we can
initialize α such that our loss is convex and then gradually
reduce α (and therefore reduce convexity and increase ro-
bustness) during optimization, thereby enabling robust esti-
mation that (often) avoids local minima.
We can take the limit of the loss as α approaches inﬁnity,
which due to Eq. 12 must be the upper bound of the loss:

ρ (x, α, c) ≤ lim

α→+∞

ρ (x, α, c) = exp (cid:18) 1
2

(x/c)2(cid:19) − 1

(13)
We can bound the magnitude of the gradient of the loss,
which allows us to better reason about exploding gradients:

∂ ρ
∂x

(cid:12)(cid:12)(cid:12)(cid:12)

2 )

≤ 


c (cid:16) α−2
α−1 (cid:17)( α−1
1
|x|
c2

(x, α, c)(cid:12)(cid:12)(cid:12)(cid:12)
f (x, 1, c) ≈ |x|
c − 1

≤ 1

c

if α ≤ 1
if α ≤ 2

(14)

if c ≪ x

(15)

L1 loss is not expressible by our loss, but if c is much
smaller than x we can approximate it with α = 1:

See the supplement for other potentially-useful properties
that are not used in our experiments.

2. Probability Density Function

With our loss function we can construct a general prob-
ability distribution, such that the negative log-likelihood
(NLL) of its PDF is a shifted version of our loss function:

p (x | µ, α, c) =

1
cZ (α)

exp (−ρ (x − µ, α, c))

(16)

Z (α) = Z ∞

−∞

exp (−ρ (x, α, 1))

(17)

where p (x | µ, α, c) is only deﬁned if α ≥ 0, as Z (α) is
divergent when α < 0. For some values of α the partition
function is relatively straightforward:

Z (0) = π√2
Z (2) = √2π

Z (1) = 2eK1 (1)
Z (4) = e1/4K1/4 (1/4)

(18)

where Kn (·) is the modiﬁed Bessel function of the second
kind. For any rational positive α (excluding a singularity at
α = 2) where α = n/d with n, d ∈ N, we see that

e| 2d

n −1|q(cid:12)(cid:12)

(2π)(d−1)

2d

p,q ap
G 0,0

n − 1(cid:12)(cid:12)
2 (cid:27) ∪ (cid:26) i

3

bq (cid:12)(cid:12)(cid:12)(cid:12)
2d (cid:12)(cid:12)(cid:12)(cid:12)

1
i = −
, ..., n −
2
i = 1, ..., n − 1(cid:27)

Z (cid:16) n
d (cid:17) =
bq = (cid:26) i
ap = (cid:26) i

n (cid:12)(cid:12)(cid:12)(cid:12)
n (cid:12)(cid:12)(cid:12)(cid:12)

n −

(cid:18) 1

2d (cid:19)2d!
1
i = 1, ..., 2d − 1(cid:27)

(19)

where G(·) is the Meijer G-function and bq is a multiset
(items may occur twice). Because the partition function is
difﬁcult to evaluate or differentiate, in our experiments we
approximate log(Z (α)) with a cubic hermite spline (see the
supplement for details).
Just as our loss function includes several common loss
function as special cases, our distribution includes several
common distributions as special cases. When α = 2 our
distribution becomes a normal (Gaussian) distribution, and
when α = 0 our distribution becomes a Cauchy distri-
bution. These are also both special cases of Student’s t-
distribution (ν = ∞ and ν = 1, respectively), though these
are the only two points where these two families of distribu-
tions intersect. Our distribution resembles the generalized
Gaussian distribution [28, 33], except that it is “smoothed”
so as to approach a Gaussian distribution near the origin re-
gardless of the shape parameter α. The PDF and NLL of our
distribution for different values of α can be seen in Figure 2.
In later experiments we will use the NLL of our general
distribution − log(p(·|α, c)) as the loss for training our neu-
ral networks, not our general loss ρ (·, α, c). Critically, us-
ing the NLL allows us to treat α as a free parameter, thereby
allowing optimization to automatically determine the de-
gree of robustness that should be imposed by the loss be-
ing used during training. To understand why the NLL must
be used for this, consider a training procedure in which we
simply minimize ρ (·, α, c) with respect to α and our model
weights. In this scenario, the monotonicity of our general
loss with respect to α (Eq. 12) means that optimization can
trivially minimize the cost of outliers by setting α to be as
small as possible. Now consider that same training pro-
cedure in which we minimize the NLL of our distribution

4333

3. Experiments

We will now demonstrate the utility of our loss function
and distribution with four experiments. None of these re-
sults are intended to represent the state-of-the-art for any
particular task — our goal is to demonstrate the value of our
loss and distribution as useful tools in isolation. We will
show that across a variety of tasks, just replacing the loss
function of an existing model with our general loss function
can enable signiﬁcant performance improvements.
In Sections 3.1 and 3.2 we focus on learning based vi-
sion tasks in which training involves minimizing the differ-
ence between images: variational autoencoders for image
synthesis and self-supervised monocular depth estimation.
We will generalize and improve models for both tasks by
using our general distribution (either as a conditional dis-
tribution in a generative model or by using its NLL as an
adaptive loss) and allowing the distribution to automatically
determine its own degree of robustness. Because robustness
is automatic and requires no manually-tuned hyperparame-
ters, we can even allow for the robustness of our loss to
be adapted individually for each dimension of our output
space — we can have a different degree of robustness at
each pixel in an image, for example. As we will show, this
approach is particularly effective when combined with im-
age representations such as wavelets, in which we expect to
see non-Gaussian, heavy-tailed distributions.
In Sections 3.3 and 3.4 we will build upon existing al-
gorithms for two classic vision tasks (registration and clus-
tering) that both work by minimizing a robust loss that is
subsumed by our general loss. We will then replace each
algorithm’s ﬁxed robust loss with our loss, thereby intro-
ducing a continuous tunable robustness parameter α. This
generalization allows us to introduce new models in which
α is manually tuned or annealed, thereby improving per-
formance. These results demonstrate the value of our loss
function when designing classic vision algorithms, by al-
lowing model robustness to be introduced into the algorithm
design space as a continuous hyperparameter.

3.1. Variational Autoencoders

Variational autoencoders [22, 30] are a landmark tech-
nique for training autoencoders as generative models, which
can then be used to draw random samples that resemble
training data. We will demonstrate that our general distribu-
tion can be used to improve the log-likelihood performance
of VAEs for image synthesis on the CelebA dataset [26]. A
common design decision for VAEs is to model images us-
ing an independent normal distribution on a vector of RGB
pixel values [22], and we use this design as our baseline
model. Recent work has improved upon this model by us-
ing deep, learned, and adversarial loss functions [8, 15, 24].
Though it’s possible that our general loss or distribution
can add value in these circumstances, to more precisely iso-

4334

Figure 2. The negative log-likelihoods (left) and probability den-
sities (right) of the distribution corresponding to our loss function
when it is deﬁned (α ≥ 0). NLLs are simply losses (Fig. 1) shifted
by a log partition function. Densities are bounded by a scaled
Cauchy distribution.

instead of our loss. As can be observed in Figure 2, reduc-
ing α will decrease the NLL of outliers but will increase
the NLL of inliers. During training, optimization will have
to choose between reducing α, thereby getting “discount”
on large errors at the cost of paying a penalty for small er-
rors, or increasing α, thereby incurring a higher cost for
outliers but a lower cost for inliers. This tradeoff forces op-
timization to judiciously adapt the robustness of the NLL
being minimized. As we will demonstrate later, allowing
the NLL to adapt in this way can increase performance on
a variety of learning tasks, in addition to obviating the need
for manually tuning α as a ﬁxed hyperparameter.

Sampling from our distribution is straightforward given
the observation that − log (p (x | 0, α, 1)) is bounded from
below by ρ(x, 0, 1) + log(Z (α)) (shifted Cauchy loss). See
Figure 2 for visualizations of this bound when α = ∞,
which also bounds the NLL for all values of α. This lets
us perform rejection sampling using a Cauchy as the pro-
posal distribution. Because our distribution is a location-
scale family, we sample from p (x | 0, α, 1) and then scale
and shift that sample by c and µ respectively. This sam-
pling approach is efﬁcient, with an acceptance rate between
∼ 45% (α = ∞) and 100% (α = 0). Pseudocode for sam-
pling is shown in Algorithm 1.

Algorithm 1 Sampling from our general distribution
Input: Parameters for the distribution to sample {µ, α, c}
Output: A sample drawn from p (x | µ, α, c).
1: while True:

x ∼ Cauchy(x0 = 0, γ = √2)
u ∼ Uniform(0, 1)
if u <

p(x | 0,α,1)
exp(−ρ(x,0,1)−log(Z (α))) :

2:

3:

4:

5:

return cx + µ

late our contribution we will explore the hypothesis that the
baseline model of normal distributions placed on a per-pixel
image representation can be improved signiﬁcantly with the
small change of just modeling a linear transformation of a
VAE’s output with our general distribution. Again, our goal
is not to advance the state of the art for any particular im-
age synthesis task, but is instead to explore the value of our
distribution in an experimentally controlled setting.
In our baseline model we give each pixel’s normal distri-
bution a variable scale parameter σ (i) that will be optimized
over during training, thereby allowing the VAE to adjust the
scale of its distribution for each output dimension. We can
straightforwardly replace this per-pixel normal distribution
with a per-pixel general distribution, in which each output
dimension is given a distinct shape parameter α(i) in ad-
dition to its scale parameter c(i) (i.e., σ (i) ). By letting the
α(i) parameters be free variables alongside the scale param-
eters, training is able to adaptively select both the scale and
robustness of the VAE’s posterior distribution over pixel
values. We restrict all α(i) to be in (0, 3), which allows
our distribution to generalize Cauchy (α = 0) and Normal
(α = 2) distributions and anything in between, as well as
more platykurtic distributions (α > 2) which helps for this
task. We limit α to be less than 3 because of the increased
risk of numerical instability during training as α increases.
We also compare against a Cauchy distribution as an ex-
ample of a ﬁxed heavy-tailed distribution, and against Stu-
dent’s t-distribution as an example of a distribution that can
adjust its own robustness similarly to ours.
Regarding implementation, for each output dimension
i we construct unconstrained TensorFlow variables {α(i)
and {c(i)
ℓ } and deﬁne

ℓ }

α(i) = (αmax − αmin ) sigmoid (cid:16)α(i)
c(i) = softplus (cid:16)c(i)
αmin = 0, αmax = 3, cmin = 10−8

ℓ (cid:17) + cmin

ℓ (cid:17) + αmin

(20)

(21)

(22)

The cmin offset avoids degenerate optima where likelihood
is maximized by having c(i) approach 0, while αmin and
αmax determine the range of values that α(i) can take. Vari-
ables are initialized such that initially all α(i) = 1 and
c(i) = 0.01, and are optimized simultaneously with the au-
toencoder’s weights using the same Adam [21] optimizer
instance.
Though modeling images using independent distribu-
tions on pixel intensities is a popular choice due to its sim-
plicity, classic work in natural image statistics suggest that
images are better modeled with heavy-tailed distributions
on wavelet-like image decompositions [9, 27]. We there-
fore train additional models in which our decoded RGB per-
pixel images are linearly transformed into spaces that bet-
ter model natural images before computing the NLL of our

Normal Cauchy
Pixels + RGB
8,662
9,602
DCT + YUV
31,837
31,295
Wavelets + YUV 31,505
35,779

t-dist.
10,177
32,804
36,373

Ours
10,240
32,806
36,316

Table 1. Validation set ELBOs (higher is better) for our varia-
tional autoencoders. Models using our general distribution better
maximize the likelihood of unseen data than those using normal
or Cauchy distributions (both special cases of our model) for all
three image representations, and perform similarly to Student’s t-
distribution (a different generalization of normal and Cauchy dis-
tributions). The best and second best performing techniques for
each representation are colored orange and yellow respectively.

Normal

Cauchy

t-distribution

Ours

B

G

R

+

s

l

e

x

i

P

V
U
Y

+
T

C

D

V
U
Y

+

s

t

e

l

e

v

a

W

Figure 3. Random samples from our variational autoencoders. We
use either normal, Cauchy, Student’s t, or our general distributions
(columns) to model the coefﬁcients of three different image rep-
resentations (rows). Because our distribution can adaptively inter-
polate between Cauchy-like or normal-like behavior for each co-
efﬁcient individually, using it results in sharper and higher-quality
samples (particularly when using DCT or wavelet representations)
and does a better job of capturing low-frequency image content
than Student’s t-distribution.

distribution. For this we use the DCT [1] and the CDF 9/7
wavelet decomposition [7], both with a YUV colorspace.
These representations resemble the JPEG and JPEG 2000
compression standards, respectively.
Our results can be seen in Table 1, where we report the
validation set evidence lower bound (ELBO) for all com-
binations of our four distributions and three image repre-
sentations, and in Figure 3, where we visualize samples
from these models. We see that our general distribution per-

4335

forms similarly to a Student’s t-distribution, with both pro-
ducing higher ELBOs than any ﬁxed distribution across all
representations. These two adaptive distributions appear to
have complementary strengths: ours can be more platykur-
tic (α > 2) while a t-distribution can be more leptokurtic
(ν < 1), which may explain why neither model consistently
outperforms the other across representations. Note that the
t-distribution’s NLL does not generalize the Charbonnier,
L1, Geman-McClure, or Welsch losses, so unlike ours it
will not generalize the losses used in the other tasks we will
address. For all representations, VAEs trained with our gen-
eral distribution produce sharper and more detailed samples
than those trained with normal distributions. Models trained
with Cauchy and t-distributions preserve high-frequency
detail and work well on pixel representations, but systemat-
ically fail to synthesize low-frequency image content when
given non-pixel representations, as evidenced by the gray
backgrounds of those samples. Comparing performance
across image representations shows that the “Wavelets +
YUV” representation best maximizes validation set ELBO
— though if we were to limit our model to only normal dis-
tributions the “DCT + YUV” model would appear superior,
suggesting that there is value in reasoning jointly about dis-
tributions and image representations. After training we see
shape parameters {α(i) } that span (0, 2.5), suggesting that
an adaptive mixture of normal-like and Cauchy-like distri-
butions is useful in modeling natural images, as has been
observed previously [29]. Note that this adaptive robustness
is just a consequence of allowing {α(i)
ℓ } to be free variables
during training, and requires no manual parameter tuning.
See the supplement for more samples and reconstructions
from these models, and a review of our experimental proce-
dure.

3.2. Unsupervised Monocular Depth Estimation

Due to the difﬁculty of acquiring ground-truth direct
depth observations, there has been recent interest in “unsu-
pervised” monocular depth estimation, in which stereo pairs
and geometric constraints are used to directly train a neural
network [10, 11, 14, 41]. We use [41] as a representative
model from this literature, which is notable for its estima-
tion of depth and camera pose. This model is trained by
minimizing the differences between two images in a stereo
pair, where one image has been warped to match the other
according to the depth and pose predictions of a neural net-
work. In [41] that difference between images is deﬁned as
the absolute difference between RGB values. We will re-
place that loss with different varieties of our general loss,
and demonstrate that using annealed or adaptive forms of
our loss can improve performance.
The absolute loss in [41] is equivalent to maximizing the
likelihood of a Laplacian distribution with a ﬁxed scale on
RGB pixel values. We replace that ﬁxed Laplacian distri-

lower is better
higher is better
Avg AbsRel SqRel RMS logRMS < 1.25 < 1.252 < 1.253
Baseline [41] as reported
0.407
0.221 2.226 7.527
0.294
0.676 0.885
0.954
Baseline [41] reproduced
0.398
0.208 2.773 7.085
0.286
0.726 0.895
0.953
Ours, ﬁxed α = 1
0.356
0.194 2.138 6.743
0.268
0.738 0.906
0.960
Ours, ﬁxed α = 0
0.350 0.187
2.407 6.649
0.261
0.766 0.911
0.960
Ours, ﬁxed α = 2
0.349
0.190 1.922 6.648
0.267
0.737 0.904
0.961
Ours, annealing α = 2 → 0 0.341
0.184 2.063 6.697
0.260
0.756 0.911
0.963
Ours, adaptive α ∈ (0, 2)
0.332
0.181
2.144 6.454
0.254
0.766 0.916
0.965

Table 2. Results on unsupervised monocular depth estimation us-
ing the KITTI dataset [12], building upon the model from [41]
(“Baseline”). By replacing the per-pixel loss used by [41] with
several variants of our own per-wavelet general loss function in
which our loss’s shape parameters are ﬁxed, annealed, or adap-
tive, we see a signiﬁcant performance improvement. The top three
techniques are colored red, orange, and yellow for each metric.

t

u
p
n

I

e

n

i
l

e

s

a

B

s

r

u

O

h

t

u

r

T

Figure 4. Monocular depth estimation results on the KITTI bench-
mark using the “Baseline” network of [41]. Replacing only the
network’s loss function with our “adaptive” loss over wavelet co-
efﬁcients results in signiﬁcantly improved depth estimates.

bution with our general distribution, keeping our scale ﬁxed
but allowing the shape parameter α to vary. Following our
observation from Section 3.1 that YUV wavelet representa-
tions work well when modeling images with our loss func-
tion, we impose our loss on a YUV wavelet decomposition
instead of the RGB pixel representation of [41]. The only
changes we made to the code from [41] were to replace its
loss function with our own and to remove the model compo-
nents that stopped yielding any improvement after the loss
function was replaced (see the supplement for details). All
training and evaluation was performed on the KITTI dataset
[12] using the same training/test split as [41].
Results can be seen in Table 2. We present the error
and accuracy metrics used in [41] and our own “average”
error measure: the geometric mean of the four errors and
one minus the three accuracies. The “Baseline“ models use
the loss function of [41], and we present both the numbers
in [41] (“as reported”) and our own numbers from running

4336

the code from [41] ourselves (“reproduced”). The “Ours”
entries all use our general loss imposed on wavelet coefﬁ-
cients, but for each entry we use a different strategy for set-
ting the shape parameter or parameters. We keep our loss’s
scale c ﬁxed to 0.01, thereby matching the ﬁxed scale as-
sumption of the baseline model and roughly matching the
shape of its L1 loss (Eq. 15). To avoid exploding gradients
we multiply the loss being minimized by c, thereby bound-
ing gradient magnitudes by residual magnitudes (Eq. 14).
For the “ﬁxed” models we use a constant value for α for all
wavelet coefﬁcients, and observe that though performance
is improved relative to the baseline, no single value of α is
optimal. The α = 1 entry is simply a smoothed version
of the L1 loss used by the baseline model, suggesting that
just using a wavelet representation improves performance.
In the “annealing α = 2 → 0” model we linearly inter-
polate α from 2 (L2) to 0 (Cauchy) as a function of train-
ing iteration, which outperforms all “ﬁxed” models. In the
“adaptive α ∈ (0, 2)” model we assign each wavelet co-
efﬁcient its own shape parameter as a free variable and we
allow those variables to be optimized alongside our network
weights during training as was done in Section 3.1, but with
αmin = 0 and αmax = 2. This “adaptive” strategy out-
performs the “annealing” and all “ﬁxed” strategies, thereby
demonstrating the value of allowing the model to adaptively
determine the robustness of its loss during training. Note
that though the “ﬁxed” and “annealed” strategies only re-
quire our general loss, the “adaptive” strategy requires that
we use the NLL of our general distribution as our loss —
otherwise training would simply drive α to be as small as
possible due to the monotonicity of our loss with respect
to α, causing performance to degrade to the “ﬁxed α = 0”
model. Comparing the “adaptive” model’s performance to
that of the “ﬁxed” models suggests that, as in Section 3.1,
no single setting of α is optimal for all wavelet coefﬁcients.
Overall, we see that just replacing the loss function of [41]
with our adaptive loss on wavelet coefﬁcients reduces aver-
age error by ∼ 17%.
In Figure 4 we compare our “adaptive” model’s output to
the baseline model and the ground-truth depth, and demon-
strate a substantial qualitative improvement. See the sup-
plement for many more results, and for visualizations of the
per-coefﬁcient robustness selected by our model.

3.3. Fast Global Registration

Robustness is often a core component of geometric regis-
tration [37]. The Fast Global Registration (FGR) algorithm
of [40] ﬁnds the rigid transformation T that aligns point sets
{p} and {q} by minimizing the following loss:

ρgm (kp − Tqk, c)

(23)

X(p,q)

σ =

Mean RMSE ×100

0.0025

0

0.005

Max RMSE ×100

0.0025

0

0.005

FGR [40]
0.373
shape-annealed gFGR 0.374
gFGR*
0.370

0.518
0.510
0.509

0.821
0.802
0.806

0.591
0.590
0.545

1.040
0.997
0.961

1.767
1.670
1.669

Table 3. Results on the registration task of [40], in which we
compare their “FGR” algorithm to two versions of our “gFGR”
generalization.

Figure 5. Performance (lower is better) of our gFGR algorithm
on the task of [40] as we vary our shape parameter α, with the
lowest-error point indicated by a circle. FGR (equivalent to gFGR
with α = −2) is shown as a dashed line and a square, and shape-
annealed gFGR for each noise level is shown as a dotted line.

where ρgm (·) is Geman-McClure loss. By using the Black
and Rangarajan duality between robust estimation and line
processes [3] FGR is capable of producing high-quality reg-
istrations at high speeds. Because Geman-McClure loss is a
special case of our loss, and because we can formulate our
loss as an outlier process (see supplement), we can gener-
alize FGR to an arbitrary shape parameter α by replacing
ρgm (·, c) with our ρ(·, α, c) (where setting α = −2 repro-
duces FGR).
This generalized FGR (gFGR) enables algorithmic im-
provements. FGR iteratively solves a linear system while
annealing its scale parameter c, which has the effect of grad-
ually introducing nonconvexity. gFGR enables an alterna-
tive strategy in which we directly manipulate convexity by
annealing α instead of c. This “shape-annealed gFGR” fol-
lows the same procedure as [40]: 64 iterations in which a
parameter is annealed every 4 iterations. Instead of anneal-
ing c, we set it to its terminal value and instead anneal α
over the following values:

2, 1, 1/2, 1/4, 0, −1/4, −1/2, −1, −2, −4, −8, −16, −32

Table 3 shows results for the 3D point cloud registration
task of [40] (Table 1 in that paper), which shows that an-
nealing shape produces moderately improved performance
over FGR for high-noise inputs, and behaves equivalently
in low-noise inputs. This suggests that performing gradu-
ated non-convexity by directly adjusting a shape parameter
that controls non-convexity — a procedure that is enabled
by our general loss – is preferable to indirectly controlling
non-convexity by annealing a scale parameter.

4337

]

2
3

[

s

t

u

-

C
N

0.928
0.871

-

0.752
0.813
0.536
0.545
0.000
0.140

]

6
3

[

I

G

M

D

L

0.945
0.888
0.761
0.518
0.775
0.527
0.523

0.591

0.382

W

-

C
A

0.767
0.853
0.679
0.801
0.728
0.525
0.471
0.291
0.364

]

1
3

[

-

R
D
C
C
R

0.974
0.957
0.828
0.874
0.854
0.638
0.553
0.513

]

8
3

[

C

I

P

0.941

0.965
-

0.676
0.467
0.394
0.057

-

0.015

0.442

]

1
3

[

C
C
R

0.975
0.957
0.893
0.836
0.848
0.649
0.556
0.488
0.138

*

C
C
R

g

0.975

0.962

0.901
0.888
0.871
0.650
0.561

0.493
0.338

.

r

p

m

I

.
l

e

R

0.4%
11.6%
7.9%
31.9%
15.1%
0.2%
1.1%
0.9%
23.2%

Dataset
YaleB
COIL-100
MNIST
YTF
Pendigits
Mice Protein
Reuters
Shuttle
RCV1

Table 4. Results on the clustering task of [31] where we compare
their “RCC” algorithm to our “gRCC*” generalization in terms
of AMI on several datasets. We also report the AMI increase of
“gRCC*” with respect to “RCC”. Baselines are taken from [31].

Another generalization is to continue using the c-
annealing strategy of [40], but treat α as a hyperparameter
and tune it independently for each noise level in this task.
In Figure 5 we set α to a wide range of values and report
errors for each setting, using the same evaluation of [40].
We see that for high-noise inputs more negative values of
α are preferable, but for low-noise inputs values closer to
0 are optimal. We report the lowest-error entry for each
noise level as “gFGR*” in Table 3 where we see a signiﬁ-
cant reduction in error, thereby demonstrating the improve-
ment that can be achieved from treating robustness as a hy-
perparameter.

3.4. Robust Continuous Clustering

In [31] robust losses are used for unsupervised cluster-
ing, by minimizing:

Xi

kxi − ui k2

2 + λ X(p,q)∈E

wp,q ρgm (kup − uq k2 )

(24)

where {xi } is a set of input datapoints, {ui } is a set of “rep-
resentatives” (cluster centers), and E is a mutual k-nearest
neighbors (m-kNN) graph. As in Section 3.3, ρgm (·) is
Geman-McClure loss, which means that our loss can be
used to generalize this algorithm. Using the RCC code
provided by the authors (and keeping all hyperparameters
ﬁxed to their default values) we replace Geman-McClure
loss with our general loss and then sweep over values of α.
In Figure 6 we show the adjusted mutual information (AMI,
the metric used by [31]) of the resulting clustering for each
value of α on the datasets used in [31], and in Table 4 we
report the AMI for the best-performing value of α for each
dataset as “gRCC*”. On some datasets performance is in-
sensitive to α, but on others adjusting α improves perfor-
mance by as much as 32%. This improvement demonstrates
the gains that can be achieved by introducing robustness as
a hyperparameter and tuning it accordingly.

Figure 6. Performance (higher is better) of our gRCC algorithm
on the clustering task of [31], for different values of our shape
parameter α, with the highest-accuracy point indicated by a dot.
Because the baseline RCC algorithm is equivalent to gRCC with
α = −2, we highlight that α value with a dashed line and a square.

4. Conclusion

We have presented a two-parameter
loss function
that
generalizes many
existing
one-parameter
ro-
bust
loss functions:
the Cauchy/Lorentzian, Geman-
McClure, Welsch/Leclerc,
generalized Charbonnier,
Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions.
By reducing a family of discrete single-parameter losses
to a single function with two continuous parameters, our
loss enables the convenient exploration and comparison
of different robust penalties. This allows us to generalize
and improve algorithms designed around the minimiza-
tion of some ﬁxed robust loss function, which we have
demonstrated for registration and clustering. When used
as a negative log-likelihood,
this loss gives a general
probability distribution that includes normal and Cauchy
distributions as special cases. This distribution lets us train
neural networks in which the loss has an adaptive degree
of robustness for each output dimension, which allows
training to automatically determine how much robustness
should be imposed by the loss without any manual param-
eter tuning. When this adaptive loss is paired with image
representations in which variable degrees of heavy-tailed
behavior occurs, such as wavelets, this adaptive training ap-
proach allows us to improve the performance of variational
autoencoders for image synthesis and of neural networks
for unsupervised monocular depth estimation.

Acknowledgements: Thanks to Rob Anderson, Jesse En-
gel, David Gallup, Ross Girshick, Jaesik Park, Ben Poole,
Vivek Rathod, and Tinghui Zhou.

4338

References

[1] Nasir Ahmed, T Natarajan, and Kamisetty R Rao. Discrete
cosine transform. IEEE Transactions on Computers, 1974.

[2] Michael J Black and Paul Anandan. The robust estimation
of multiple motions: Parametric and piecewise-smooth ﬂow
ﬁelds. CVIU, 1996.

[3] Michael J. Black and Anand Rangarajan. On the uniﬁcation
of line processes, outlier rejection, and robust statistics with
applications in early vision. IJCV, 1996.

[4] Andrew Blake and Andrew Zisserman. Visual Reconstruc-
tion. MIT Press, 1987.

[5] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and
Michel Barlaud. Two deterministic half-quadratic regular-
ization algorithms for computed imaging. ICIP, 1994.

[6] Qifeng Chen and Vladlen Koltun. Fast mrf optimization with
application to depth reconstruction. CVPR, 2014.

[7] Albert Cohen,
Ingrid Daubechies, and J-C Feauveau.
Biorthogonal bases of compactly supported wavelets. Com-
munications on pure and applied mathematics, 1992.

[8] Alexey Dosovitskiy and Thomas Brox. Generating images
with perceptual similarity metrics based on deep networks.
NIPS, 2016.

[9] David J. Field. Relations between the statistics of natural
images and the response properties of cortical cells. JOSA A,
1987.

[10] John Flynn, Ivan Neulander, James Philbin, and Noah
Snavely. Deepstereo: Learning to predict new views from
the world’s imagery. CVPR, 2016.

[11] Ravi Garg, BG Vijay Kumar, Gustavo Carneiro, and Ian
Reid. Unsupervised cnn for single view depth estimation:
Geometry to the rescue. ECCV, 2016.

[12] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. CVPR, 2012.

[13] Stuart Geman and Donald E. McClure. Bayesian image anal-
ysis: An application to single photon emission tomography.
Proceedings of the American Statistical Association, 1985.

[14] Cl ´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. CVPR, 2017.

[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. NIPS, 2014.

[16] Frank R. Hampel, Elvezio M. Ronchetti, Peter J. Rousseeuw,
and Werner A. Stahel. Robust Statistics: The Approach
Based on Inﬂuence Functions. Wiley, 1986.

[17] Trevor Hastie, Robert Tibshirani, and Martin Wainwright.
Statistical Learning with Sparsity: The Lasso and General-
izations. Chapman and Hall/CRC, 2015.

[18] Peter J. Huber. Robust estimation of a location parameter.
Annals of Mathematical Statistics, 1964.

[19] Peter J. Huber. Robust Statistics. Wiley, 1981.

[20] John E. Dennis Jr. and Roy E. Welsch. Techniques for non-
linear least squares and robust regression. Communications
in Statistics-simulation and Computation, 1978.

[21] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. ICLR, 2015.
[22] Diederik P. Kingma and Max Welling. Auto-encoding vari-
ational bayes. ICLR, 2014.
[23] Philipp Kr ¨ahenb ¨uhl and Vladlen Koltun. Efﬁcient nonlocal
regularization for optical ﬂow. ECCV, 2012.
[24] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo
Larochelle, and Ole Winther. Autoencoding beyond pixels
using a learned similarity metric. ICML, 2016.
[25] Yvan G Leclerc. Constructing simple stable descriptions for
image partitioning. IJCV, 1989.
[26] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. ICCV, 2015.
[27] St ´ephane Mallat. A theory for multiresolution signal decom-
position: The wavelet representation. TPAMI, 1989.
[28] Saralees Nadarajah. A generalized normal distribution. Jour-
nal of Applied Statistics, 2005.
[29] Javier Portilla, Vasily Strela, Martin J. Wainwright, and
Eero P. Simoncelli.
Image denoising using scale mixtures
of gaussians in the wavelet domain. IEEE TIP, 2003.
[30] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-
stra. Stochastic backpropagation and approximate inference
in deep generative models. ICML, 2014.
[31] Sohil Atul Shah and Vladlen Koltun. Robust continuous
clustering. PNAS, 2017.
[32] Jianbo Shi and Jitendra Malik. Normalized cuts and image
segmentation. TPAMI, 2000.
[33] M Th Subbotin. On the law of frequency of error. Matem-
aticheskii Sbornik, 1923.
[34] Deqing Sun, Stefan Roth, and Michael J. Black. Secrets of
optical ﬂow estimation and their principles. CVPR, 2010.
[35] Rein van den Boomgaard and Joost van de Weijer. On
the equivalence of local-mode ﬁnding, robust estimation and
mean-shift analysis as used in early vision tasks. ICPR, 2002.
[36] Yi Yang, Dong Xu, Feiping Nie, Shuicheng Yan, and Yueting
Zhuang. Image clustering using local discriminant models
and global integration. TIP, 2010.
[37] Christopher Zach.
Robust bundle adjustment revisited.
ECCV, 2014.
[38] Wei Zhang, Deli Zhao, and Xiaogang Wang. Agglomerative
clustering via maximum incremental path integral. Pattern
Recognition, 2013.
[39] Zhengyou Zhang. Parameter estimation techniques: A tuto-
rial with application to conic ﬁtting, 1995.
[40] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Fast global
registration. ECCV, 2016.
[41] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G.
Lowe. Unsupervised learning of depth and ego-motion from
video. CVPR, 2017.

4339

A Skeleton-bridged Deep Learning Approach for Generating Meshes
of Complex Topologies from Single RGB Images

Jiapeng Tang ∗ 1 , Xiaoguang Han ∗ 2 , Junyi Pan1 , Kui Jia † 1 , and Xin Tong3

1School of Electronic and Information Engineering, South China University of Technology
2Shenzhen Research Institute of Big Data, the Chinese University of Hong Kong (Shenzhen)
3Microsoft Research Asia

msjptang@mail.scut.edu.cn, hanxiaoguang@cuhk.edu.cn, eejypan@mail.scut.edu.cn,

kuijia@scut.edu.cn, xtong@microsoft.com

(cid:1043)

Abstract

This paper focuses on the challenging task of learning
3D object surface reconstructions from single RGB images.
Existing methods achieve varying degrees of success by us-
ing different geometric representations. However, they all
have their own drawbacks, and cannot well reconstruct
those surfaces of complex topologies. To this end, we pro-
pose in this paper a skeleton-bridged, stage-wise learning
approach to address the challenge. Our use of skeleton is
due to its nice property of topology preservation, while be-
ing of lower complexity to learn. To learn skeleton from an
input image, we design a deep architecture whose decoder
is based on a novel design of parallel streams respectively
for synthesis of curve- and surface-like skeleton points. We
use different shape representations of point cloud, volume,
and mesh in our stage-wise learning, in order to take their
respective advantages. We also propose multi-stage use of
the input image to correct prediction errors that are possi-
bly accumulated in each stage. We conduct intensive exper-
iments to investigate the efﬁcacy of our proposed approach.
Qualitative and quantitative results on representative object
categories of both simple and complex topologies demon-
strate the superiority of our approach over existing ones.
We will make our ShapeNet-Skeleton dataset publicly avail-
able.

1. Introduction

Learning 3D surface reconstructions of objects from sin-
gle RGB images is an important topic from both the aca-

∗ Equal contributions
†Corresponding author

cient, discrete approximation of the continuous manifold of
an object surface, a few recent methods [6, 16] attempt to di-
rectly learn mesh reconstructions from single input images.
These methods are inherently of mesh deformation, since
they assume that an initial meshing over point cloud is avail-
able; for example, they typically assume unit square/sphere
as the initial mesh. In spite of the success achieved by these
recent methods, they still suffer from generating surface
meshes of complex topologies, e.g., those with thin struc-
tures as shown in Fig 1.
To this end, we propose in this paper a skeleton-bridged,
stage-wise deep learning approach for generating mesh re-
constructions of object surfaces from single RGB images.
We particularly focus on those object surfaces with com-
plex topologies, e.g., chairs or tables that have local, long
and thin structures. Our choice of the meso-skeleton 1 is
due to its nice property of topology preservation, while be-
ing of lower complexity to learn when compared with learn-
ing the surface meshes directly. Our proposed approach is
composed of three stages. The ﬁrst stage learns to generate
skeleton points from the input image, for which we design a
deep architecture whose decoder is based on a novel, paral-
lel design of CurSkeNet and SurSkeNet, which are respec-
tively responsible for the synthesis of curve- and surface-
like skeleton points. To train CurSkeNet and SurSkeNet,
we compute skeletal shape representations for instances of
ShapeNet [3]. We will make our ShapeNet-Skeleton dataset
publicly available. In the second stage, we produce a base
mesh by ﬁrstly converting the obtained skeleton to its coarse
volumetric representation, and then reﬁning the coarse vol-
ume using a learned 3D CNN, where we adopt a strategy
of independent sub-volume synthesis with regularization of
global structure, in order to reduce the complexity of pro-
ducing high-resolution volumes. In the last stage, we gener-
ate our ﬁnal mesh result by extracting a base mesh from the
obtained volume [17], and deforming vertices of the base
mesh using a learned Graph CNN (GCNN) [12, 5, 1, 22].
Learning and inference in three stages of our approach are
based on different shape representations, which take the
respective advantages of point cloud, volume, and mesh.
We also propose multi-stage use of the input image to cor-
rect prediction errors that are possibly accumulated in each
stage. We conduct intensive ablation studies which show
the efﬁcacy of stage-wise designs of our proposed approach.
We summarize our main contributions as follows.

• Our approach is based on an integrated stage-wise
learning, where learning and inference in different
stages are based on different shape representations by

1 Skeletal shape representation is a kind of medial axis transform
(MAT). While the MAT of a 2D shape is a 1D skeleton, for a 3D model, the
MAT is generally composed of 2D surface sheets. The skeleton composed
of skeletal curves and skeletal sheets (i.e., medial axes) is generally called
meso-skeleton.

taking the respective advantages of point cloud, vol-
ume, and mesh. We also propose multi-stage use of the
input image to correct prediction errors that are possi-
bly accumulated in each stage.

• We propose in this paper a skeleton-bridged approach
for learning object surface meshes of complex topolo-
gies from single RGB images. Our use of skeleton
is due to its nice property of topology preservation,
while being of lower complexity to learn. We design a
deep architecture for skeleton learning, whose decoder
is based on a novel design of parallel streams respec-
tively for the synthesis of curve- and surface-like skele-
ton points. To train the network, we prepare ShapeNet-
Skeleton dataset and will make it publicly available.

• We conduct intensive ablation studies to investigate the
efﬁcacy of our proposed approach. Qualitative and
quantitative results on representative object categories
of both simple and complex topologies demonstrate
the superiority of our approach over existing ones, es-
pecially for those objects with local thin structures.

2. Related Works

In this section, we only focus on the related works about
deepnets-based algorithms for fully object reconstruction.
The literature reviews are studied in the following three as-
pects.
Volume-based Generator Voxels, extended from pixels,
are usually used in the form of binary values or signed dis-
tances to represent a 3D shape. Because of its regularity,
most of existing deepnets-based shape analysis [32, 2] or
shape generation [4, 7, 30] methods adopt it as the primary
representation. For example, the work of [4] combines 3D
convolutions with long short-term memory (LSTM) units
to achieve volumetric grid reconstruction from single-view
or multi-view RGB images. These methods tend to predict
a low-resolution volumetric grid due to the high computa-
tional cost of 3D convolution operators. Based on the ob-
servation that only a small portion of regions around the
boundary surface contain the shape information, the Oc-
tree representation has been adopted in recent shape anal-
ysis works
[20, 29]. A convolutional Octree decoder is
also designed in [27] to support high-resolution reconstruc-
tion with a limited time and memory cost. In our work, we
aim to generate the surface mesh of the object instead of its
solid volume. As its efﬁciency and topology-insensitivity,
we also leverage volumetric-based generator to convert the
inferred skeletal point cloud to a solid volume, effectively
bridging the gap between the skeleton and the surface mesh.
Surface-based Generator Point cloud, sampled from the
object’ surface, is one of the most popular representations
of 3D shapes. Fan et al.
[6] proposes the ﬁrst point could

4542

Figure 2. Our overall pipeline. Given an input image I , we employ two parallel MLPs to infer skeletal points K in stage one. After
converting K to a coarse volume Vk , we reﬁne Vk to get V by 3D CNN and extract base mesh Mb from V in stage two. We further
optimize vertices of Mb using GCNN to acquire a ﬁnal mesh M . The operation a means voxelization and the operation b stands for
Marching Cubes.

generation neural network, which is built upon a deep re-
gression model trained with the loss functions that evaluate
the similarity of two unordered point set, such as chamfer
distance. Although the rough shape can be captured, the
generated points are placed sparse and scattered. Mesh, as
the most natural discretization of a manifold surface, has
been widely used in many graphics applications. Due to
its irregular structure, CNN is difﬁcult to be directly ap-
plied to mesh generation. To alleviate this challenge, the
methods of
[11, 28] take an extra template mesh as in-
put and attempt to learn the deformations to approximate
the target surfaces. Limited to the requirement of an initial
mesh, they cannot deal with topology-free reconstruction.
Another recent method, called Atlasnet [8], proposes to de-
form multiple 2D planar patches to cover the surface of the
object. Residual prediction and progressive deformation are
adopted in [19], which decrease the complexity of learning
and make more details added. It is free of complex topology
yet causes severe patch overlaps and holes. In our work, we
aim not only to generate a clean mesh but also to capture
the correct topology. To do so, we ﬁrstly borrow the idea in
[8] to infer the meso-skeleton points, which are then con-
verted to a base mesh. Finally, the method of [28] is further
adopted for generating geometric details.

Structure Inference Instead of estimating geometric
shapes, many recent works attempt to recover the 3D struc-
tures of objects. From a single image, Zou et al.
[33]
presents a primitive recurrent neural network to sequen-
tially predict a set of cuboid primitives to approximate the
shape structure. A recursive decoder is proposed in [15] to
generate shape parts and infer reasonable high-level struc-
ture information including part connectivity, adjacency and
symmetry relation. This is further exploited in [18] for
image-based structure inference. However, the cuboids are
hard to be used for ﬁtting curved shapes. In addition, these
methods also require a large human-labeled dataset. We use
meso-skeleton, a point cloud, to represent the shape struc-

ture which is easier to be obtained from the ground truth.
The usage of parametric line and square elements also eases
the approximation of the diverse local structures.

3. The Proposed Approach

We ﬁrst overview our proposed skeleton-bridged ap-
proach for generating a surface mesh from an input RGB
image, before explaining the details of stage-wise learning.
Given an input image I of an object, our goal is to recover a
surface mesh M that ideally captures the possibly complex
topology of 3D shape of the object. This is an extremely
challenging inverse task; existing methods [11, 28, 8] may
only achieve partial success for objects with relatively sim-
ple topologies. To address the challenge, our key idea in this
work is to bridge the mesh generation of object surface via
learning of meso-skeleton. As discussed in Section 1, the
rationale is that skeletal shape representation preserves the
main topological structure of a shape, while being of lower
complexity to learn.
More speciﬁcally, our mesh generation process is com-
posed of the following three stages. In the ﬁrst stage, we
learn an encoder-decoder architecture that maps I to its
meso-skeleton K , represented as a compact point cloud. In
the second stage, we produce a volume V from K by ﬁrstly
converting K to its coarse volumetric representation Vk , and
then reﬁning Vk using a learned 3D CNN (e.g., of the style
[9]). In the last stage, we generate the ﬁnal output mesh M
by extracting a base mesh Mb from V , and further optimiz-
ing vertices of Mb using a learned graph CNN [22]. Each
stage owns its own image encoder, and thus inferences in
all the three stages are guided by the input image I . Fig 2
illustrates the whole pipeline of our approach.

3.1. Learning of Meso(cid:173)Skeleton

As deﬁned in Section 1, the meso-skeleton of a shape
is represented as its medial axis, and the medial axis of a
3D model is made up of curve skeletons and median sheets,

4543

which are adaptively generated from local regions of the
shape. In this work, we utilize the skeleton representation
introduced in [31], i.e., a compact point cloud. Fig 8 shows
an example of skeleton that we aim to recover.
The ShapeNet-Skeleton dataset Training skeletons are
necessary in order to learn to generate a skeleton from an
input image. In this work, we prepare training data of skele-
ton for ShapeNet [3] as follows: 1) for each 3D polygo-
nal model in ShapeNet, we convert it into a point cloud; 2)
we extract meso-skeleton points using the method of [31];
3) we classify each skeleton point as either curve-like or
surface-like categories, based on principle component anal-
ysis of its neighbor points. We will make our ShapeNet-
Skeleton dataset publicly available.
CurSkeNet and SurSkeNet Given the training skeleton
points for the object in each image, we design an encoder-
decoder architecture for skeleton learning, where the input
I is ﬁrstly encoded to a latent vector that is then decoded to
a point cloud of skeleton. Our encoder is similar to those
in existing methods of point set generation, such as [6, 8].
In this work, we use ResNet-18 [10] as our image encoder.
Our key contribution is a novel design of decoder architec-
ture that will be presented shortly. We note that one may
think of using existing methods [6, 8] to generate K from
I ; however, they tend to fail due to the complex, especially
thin, structures of skeletons, as shown in Fig 8. Our de-
coder is based on two parallel streams of CurSkeNet and
SurSkeNet, which are designed to synthesize the points at
curve-shaped and surface-shaped regions respectively. Both
CurSkeNet and SurSkeNet are based on multilayer percep-
trons (MLPs) with the same settings as in AtlasNet [8], in-
cluding 4 fully-connected layers with the respective sizes of
1024, 512, 256, and 3, where the non-linear activation func-
tions are ReLU for the ﬁrst 3 layers and tanh for the last
layer. Our SurSkeNet learns to deform a set of 2D primi-
tives deﬁned on the open unit square [0, 1]2 , producing a lo-
cal approximation of the desired sheet skeleton points. Our
CurSkeNet learns to deform a set of 1D primitives deﬁned
on the open unit line [0, 1]; it thus conducts afﬁne trans-
formations on them to form curves, and learns to assem-
ble generated curves to approximate the curve-like skeleton
part. In our current implementation, we use 20 line primi-
tives in CurSkeNet and 20 square primitives in SurSkeNet.
In Section 4.3, we conduct ablation studies that verify the
efﬁcacy of our design of CurSkeNet and SurSkeNet.
Network Training We use training data of curve-like
and surface-like skeleton points to train CurSkeNet and
SurSkeNet. The learning task is essentially of point set gen-
eration. Similar to [8], we use the Chamfer Distance (CD)
as one of our loss functions. The CD loss is deﬁned as:

Lcd = X

x∈K

min

y∈K ∗

kx − yk2
2 + X

y∈K ∗

min

x∈K

kx − yk2

2 ,

(1)

Figure 3. The pipeline of our high-resolution skeletal volume syn-
thesis method. We convert the inferred skeletal points K to low-
resolution volume V64 and high-resolution volume V128 in paral-
lel. Given V64 , V128 paired with the input image I , a global-guided
sub-volume synthesis network is proposed to output a reﬁned vol-
ume of V128 . It consists of two subnetworks: one network gen-
erates a coarse skeletal volume from I and V64 while the other
enhances V128 locally patch by patch under the guidance of the
output from the ﬁrst network.

where {x ∈ K } and {y ∈ K ∗ } are respectively the sets of
predicted and training points. Besides, to ensure local con-
sistency, regularizer of Laplacian smoothness is also used
for generation of both curve- and surface-like points. It is
deﬁned as:

Llap = X

x∈K

x −

(cid:13)(cid:13)(cid:13)

1
|N (x)| X

p∈N (x)

,

p(cid:13)(cid:13)(cid:13)2

(2)

where N (x) is the neighbor of point x.

3.2. From Skeleton to Base Mesh

We present in this section how to generate a base mesh
Mb from the obtained skeleton K . To do so, a straightfor-
ward approach is to coarsen K to a volume directly with
hand-crafted methods, and then to produce the base mesh
using the method of Marching Cubes [17]. However, such
an approach may accumulate stage-wise prediction errors.
Instead, we rely on the original input I to correct the pos-
sible stage-wise errors, by ﬁrstly converting K to its volu-
metric representation Vk , and then using a trained 3D CNN
for a ﬁner and more accurate volumetric shape synthesis, re-
sulting in a volume V . Base mesh Mb can then be obtained
by applying Marching Cubes to the ﬁner V .
Sub-volume Synthesis with Global Guidance To preserve
the topology captured by K , a high-resolution volume rep-
resentation is required. However, this is not easy to satisfy
due to the expensive computational cost of 3D convolution
operations. OctNet
[20] may alleviate the computational
burden, it is however complex and difﬁcult to implement.
We instead partition the volume space into overlapped sub-
volumes, and conduct reﬁnement on them in parallel. We

4544

k | = 1283

also follow [9] and employ a global guidance to preserve
spatial consistency across sub-volumes. More speciﬁcally,
we ﬁrstly convert K to two volumes of varying scales, de-
noted as V l
k and V h
k . We set |V l
k | = 643 and |V h
in this work. We use two networks of 3D CNNs for global
and local synthesis of skeletal volumes. The global net-
work is trained to reﬁne V l
k and generate a skeletal volume
V l of the size 643 . The local network takes as inputs sub-
volumes of the size 643 , which are uniformly cropped from
V h
k , and then conduct their reﬁnement individually. Both of
our global and local reﬁnement networks are based on 3D
U-Net architecture [21]. When reﬁning each sub-volume of
V h
k , the corresponding 323 -sized sub-volume of V l is con-
catenated to provide structural regularization. The overall
pipeline of our method is shown in Fig 3. As seen in Fig 4,
our method not only supports high-resolution synthesis but
also preserves global structure.

Figure 4. (a)Input images; (b)Inferred skeletal points; (c)sub-
volume synthesis only; (d) adding global guidance; (e) adding im-
age guidance.

Image-guided Volume Correction To correct the possi-
blely accumulated prediction errors from the stage of skele-
ton generation, we reuse the original input I by learning an
independent encoder-decoder network, which is trained to
map I to a 323 -sized volume. We use ResNet-18 as the en-
coder and several 3D de-convolution layers as the decoder.
The output of the decoder is incorporated into the aforemen-
tioned global synthesis network, aiming for a more accurate
V l , which ultimately contributes to the generation of a bet-
ter V . From the perspective of learning task for generating
3D volumes from single images [4, 7, 30, 27], our method is
superior to existing ones by augmenting with an additional
path of skeleton inference. As shown in Fig 4, our usage of
I for error correction greatly improves the synthesis results.
Base Mesh extraction Given V , we use Marching Cubes
[17] to produce the base mesh Mb , which ideally preserves
the same topology as that of the skeleton K . Because V
is in high resolution, Mb would contain a large number of
vertices and faces. To reduce the computational burden of
the last stage, we apply QEM algorithm [13] on Mb to get a
simpliﬁed mesh for subsequent processing.

3.3. Mesh Reﬁnement

Figure 5. Our mesh reﬁnement network. Given an image I and
an initial mesh Mb , we concatenate pixel-wise features of I (ex-
tracted by VGG-16) to vertices’ coordinates and form vertex-wise
features which are followed by a graph-CNN to generate the geo-
metric details.

We have up to now the base mesh Mb that captures the
topology of the underlying object surface, but may lack sur-
face details. To compensate Mb with surface details, we
take the approach of mesh deformation using graph CNNs
[12, 5, 1, 22].
Mesh Deformation using Graph CNNs Take Mb as the
input, our graph CNN is simply composed of a few graph
convolutional layers, each of which apply spatial ﬁltering
operation to local neighborhood associated with each vertex
point of Mb . The graph-based covolutional layer is deﬁned
as:

hl+1

p = w0hl

p + X

w1hl
q ,

(3)

q∈N (p)

p

p , hl+1

where hl
are the feature vectors on the vertex p be-
fore and after applying a convolution operation, and N (p)
is the neighbor of p. w0 and w1 are the learnable parameter
matrices that are applied to all vertices.
Similar to [28], we also concatenate pixel-wise VGG
features extracted from I with coordinates of the corre-
sponding vertices to enhance learning. We again use CD
loss to train our graph CNN. Several smoothness terms are
also added to regularize the mesh deformation. One is
edge regularization, used to avoid large deformations, by
restricting the length of output edges. Another one is nor-
mal loss, used to guarantee the smoothness of the output
surface. The geometric details commonly exist at the re-
gions where the normals are changed obviously. Regarding
this fact, to guide the GCNN to better learn the surface in
those areas, we accordingly construct weighted loss func-
tions. Fig 5 shows the efﬁcacy of this weighting strategy,
where the sharp edges are better synthesized.

4. Experiments

Dataset To support the training and testing of our proposed
approach, we collect 17705 3D shapes from ﬁve categories
in ShapeNet
[3]: plane(1000), bench(1816), chair(5380),

4545

Figure 6. (a)Input images; (b)R2N2; (c)PSG; (d)AtlasNet; (e)Pixel2Mesh; (f)Ours; (g)Ground truth

table(8509), ﬁrearm(1000). The dataset is split into two
parts, 80% shapes are used for training and the other for
testing. We take as the inputs the rendered images provided
by [4], where each 3D model is rendered into 24 RGB im-
ages. Each shape in the dataset is converted to a point cloud
(10, 000 points are sampled on the surface) as the ground
truth for mesh reﬁnement network.

Implementation details The input images are all in the size
of 224*224. We train CurSkeNet and SurSkeNet using a
batch size of 32 with a learning rate of 1e-3 (dropped to
3e-4 after 80 epochs) for 120 epochs. The skeletal volume
reﬁnement network is trained in three steps: 1) the global
volume inference network is trained alone with learning rate
1e-4 for 50 epochs(dropped to 1e-5 after 35 epochs); 2) we
train the sub-volume synthesis network with learning rate
1e-5 for 10 epochs; 3) the entire network is ﬁne-tuned. The
mesh reﬁnement network is trained with learning rate 3e-
5 for 50 epochs(dropped to 1e-5 after 20 epochs) using a
batch size of 1.

4.1. Comparisons against State(cid:173)of(cid:173)the(cid:173)Arts

We ﬁrst evaluated our overall pipeline against exist-
ing methods on singe-view reconstruction. 3D-R2N2 [4],
PSG [6], AtlasNet [8], Pixel2Mesh [28] are chosen for their
popularity: 3D-R2N2 is one of the most famous volumet-

ric shape generators, PSG is the ﬁrst point set generator
based on a deep regression model, and both AtlasNet and
Pixel2Mesh are current state-of-the-art mesh generator. For
fair comparison, these models are retrained under our pre-
processed dataset.

Qualitative results The visual comparisons are shown in
Fig 6. As seen, 3D-R2N2 always produces low-resolution
volumes which cause broken structures. Their results show
no surface details either. The point sets regressed by PSG
are sparse and scattered, leading to the difﬁculty of extract-
ing triangular meshes from them. AtlasNet is capable of
generating mesh representations without a strong restric-
tion on the shape’s topology. Yet, the outputs are of non-
closed and suffer from surface self-penetration, which also
gives rise to a challenge to convert it to a manifold mesh.
Limited to the requirement of a genus-0 template mesh in-
put, Pixel2Mesh is difﬁcult to reach an accurate reconstruc-
tion for the objects with complex topologies, as the chairs
shown. Our method shows great superiority than the others
from the visual appearances, as it generates closed meshes
with accurate topologies and more details. For the exam-
ples of ﬁrearm as shown, our approach also outperforms
Pixel2Mesh, which in another aspect, indicates the pro-
posed approach is also good at recovering the shapes with
complex structures no mention to topology.

4546

Category

CD

EMD

R2N2

PSG AtlasNet Pixel2Mesh Ours R2N2

PSG AtlasNet Pixel2Mesh Ours

plane
10.434 3.824
bench
10.511 3.504
chair
4.723
2.553
ﬁrearm 10.176 1.473
table
12.230 5.466

1.529
2.264
1.342
2.276
1.751

mean

9.615

3.364

1.832

1.890
1.774
1.923
1.793
2.109

1.898

1.364 11.060 13.945
1.639 10.555
8.053
1.002
7.762
10.222
1.784
9.760
12.555
1.321 11.160
9.561

8.981
9.143
7.866
9.825
9.053

1.422 10.059 10.867

8.974

7.728
7.083
8.312
6.887
7.442

7.490

6.026
6.059
5.484
6.413
5.688

5.934

Table 1. Quantitative comparisons of our method against state-of-the-arts. The Chamfer distance(× 103 ) and Earth Mover’s distance(×
0.01) are used. The lower is better on all metrics.

Figure 7. From real photographs and object masks (top row), our
method successfully reconstructs 3D object meshes. The results
of AtlasNet (left of bottom row) v.s ours (right of bottom row).

Quantitative results Similar to Pixel2Mesh [28], we adopt
Chamfer Distance(CD) and Earth Mover’s Distance(EMD)
to evaluate the reconstruction quality. Both of them are cal-
culated between the point set (10, 000 points) sampled on
the predicted mesh surface and the ground truth point cloud.
The quantitative comparison results are reported in Tab 1.
Notably, on both metrics, our approach outperforms all the
other methods across almost all listed categories, especial
on the models with complex topologies like chairs and ta-
bles.
Generalization on real images Fig 7 illustrates 3D shapes
reconstructed by our method on three real photographs from
Pix3D [25], where the chairs and tables in the images are
manually segmented. The results’ quality is similar to the
results obtained from synthetic images. As seen in Fig 7 (a),
the real-world images has no relation with ShapeNet, while
the chair rod can still be well reconstructed. This validates
the generalization ability of our method.

4.2. Ablation Studies on Mesh Generation

Our whole framework contains multiple stages. In this
section, we conduct the ablation studies by alternatively re-
moving one of them, to verify the necessity of each stage.
w/o skeleton inference Based on our pipeline, an alterna-
tive solution without using skeleton inference is ﬁrstly gen-
erating a volume directly from the image and then applying
our mesh reﬁnement model to output the ﬁnal result. Then,
we implement this approach by using OGN [27] as the
image-based volume generator, for high-resolution(1283 )

reconstruction. This method is compared with ours visu-
ally in Fig 9. As seen, the OGN-based mesh generation
method fails to capture the thin structures which causes in-
correct topologies. In contrast, our approach gives rise to
much better performance.
w/o voxel-based correction After inferring skeleton from
our ﬁrst stage, it is a straightforward approach to acquire a
base mesh by directly applying the corrosion technique for
volume generation, and the base mesh can be extracted. The
visual comparisons of this method against ours are shown in
Fig 10. It can be seen, without volume correction, the wrong
predictions caused by skeleton inference will be transferred
to the mesh reﬁnement stage, affecting the ﬁnal output. Our
proposed voxel-based correction network addresses this is-
sue effectively.

4.3. Evaluation on Skeleton Inference

In this section, we conduct comparisons with several
variants of our skeleton inference approach, to verify our
ﬁnal model is the optimal choice. These variants include:
"Point-only ﬁtting" method directly adopts PSG [6] to
regress the skeletal points; "Line-only ﬁtting" method re-
moves the square stream of our model and only deforms
multiple lines to approximate the skeleton; "Square-only
ﬁtting" removes the line stream of our model and deforms
multiple squares to ﬁt the skeleton; "Line-and-Square ﬁt-
ting" method learns the deformation of multiple lines and
squares together using a single MLP to approximate the
skeleton; "Ours w/o laplacian" stands for our model with-
out laplacian smoothness term. Note that, laplacian smooth-
ness loss is also used for the training of "Line-only ﬁtting",
"Square-only ﬁtting" and "Line-and-Square ﬁtting".
Quantitative results All of these methods are evaluated on
CD metric and the results are shown in Tab 2. It can be seen
that our ﬁnal model outperforms all the others. Another dis-
covery is that laplacian regularizer is very helpful to reach
better accuracy.
Qualitative results We then report the visual comparisons
of these methods on a sampled example in Fig 8. As shown,

4547

Figure 8. (a)Input images; (b)Point-ﬁtting only; (c)Line-ﬁtting only; (d)Square-ﬁtting only; (e)Line-and-square ﬁtting; (f)Ours w/o lapla-
cian; (g)Ours ﬁnal; (h)Ground truth.

Methods
CD
Point-only ﬁtting
1.185
Line-only ﬁtting
1.649
Square-only ﬁtting
1.185
Line-and-Square ﬁtting 1.252
Ours w/o laplacian
1.621
Ours
1.103

Table 2. The quantitative comparisons on the variants of our skele-
ton inference method. The Chamfer Distance(× 103 ) are reported.

5. Conclusion

Recovering the 3D shape of an object from one of its per-
spectives is a very fundamental yet challenging task in com-
puter vision ﬁeld. The proposed framework splits this chal-
lenge task into three stages. It ﬁrstly recovers a 3D meso-
skeleton represented as points, these skeletal points are then
converted to its volumetric representation and passed to
a 3DCNN for a solid volume synthesis. From which, a
coarse mesh can be extracted. A GCNN is ﬁnally trained to
learn the mesh deformation for producing geometric details.
As demonstrated in our experiments both qualitatively and
quantitatively, the proposed pipeline outperforms all exist-
ing methods. There are two directions worth being explored
in the future: 1)how to change the whole pipeline to be an
end-to-end network; 2) trying to apply adversarial learning
on skeletal point inference, volume generation, and mesh
reﬁnement, for further improving the quality of ﬁnal output
mesh.

6. Acknowledge

This work is supported in part by the National Natu-
ral Science Foundation of China (Grant No.: 61771201),
the Program for Guangdong Introducing Innovative and
Enterpreneurial Teams (Grant No.:
2017ZT07X183),
the Pearl River Talent Recruitment Program Innova-
tive and Entrepreneurial Teams in 2017 (Grant No.:
2017ZT07X152), and the Shenzhen Fundamental Re-
search Fund (Grants No.: KQTD2015033114415450 and
ZDSYS201707251409055).

4548

Figure 9. (a)Input images; (b)Final meshes whose base meshes are
generated using OGN; (c)The generated meshes of our method;
(d)Ground truth.

Figure 10. (a)Input images; (b)Inferred skeleton points; (c)The
sythesized meshes whose base meshes are extracted from the
coarsened skeletal volume using the corrosion techinique; (d) The
generated meshes of our method; (e)Ground truth.

point-only ﬁtting results in scattered points no mention to
the structures. Line only ﬁtting fails to recover the surface-
shaped skeleton parts. Square-only ﬁtting can not capture
the long and thin rods and legs. The method of Line-and-
Square ﬁtting causes messy outputs since a single MLP is
difﬁcult to approximate diverse local structures. As ob-
served, the involvement of laplacian loss effectively im-
proves the visual appearance of the results.

References

[1] D. Boscaini, J. Masci, E. Rodolà, and M. M. Bronstein.
Learning shape correspondence with anisotropic convolu-
tional neural networks. neural information processing sys-
tems, pages 3189–3197, 2016.

[2] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. arXiv preprint arXiv:1608.04236, 2016.

[3] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015.

[4] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d ob-
ject reconstruction. european conference on computer vision,
pages 628–644, 2016.

[5] M. Defferrard, X. Bresson, and P. Vandergheynst. Convo-
lutional neural networks on graphs with fast localized spec-
tral ﬁltering. neural information processing systems, pages
3844–3852, 2016.

[6] H. Fan, H. Su, and L. J. Guibas. A point set generation
network for 3d object reconstruction from a single image.
In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 2463–2471, 2017.

[7] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta.
Learning a predictable and generative vector representation
for objects.
In European Conference on Computer Vision,
pages 484–499. Springer, 2016.

[8] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. Atlasnet: A papier-mâché approach to learning
3d surface generation. computer vision and pattern recogni-
tion, 2018.

[9] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-
resolution shape completion using deep neural networks for
global structure and local geometry inference. In 2017 IEEE
International Conference on Computer Vision (ICCV), pages
85–93, 2017.

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[11] H. Kato, Y. Ushiku, and T. Harada. Neural 3d mesh renderer.
computer vision and pattern recognition, pages 3907–3916,
2018.

[12] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation
with graph convolutional networks. international conference
on learning representations, 2017.

[13] L. Kobbelt, S. Campagna, and H.-P. Seidel. A general
framework for mesh decimation. In Graphics interface, vol-
ume 98, pages 43–50, 1998.

[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet
classiﬁcation with deep convolutional neural networks.
In
Advances in Neural Information Processing Systems 25,
pages 1097–1105, 2012.

[15] J. Li, K. Xu, S. Chaudhuri, E. Yumer, H. Zhang, and
L. Guibas. Grass: Generative recursive autoencoders for

shape structures. ACM Transactions on Graphics (Proc. of
SIGGRAPH 2017), 36(4):to appear, 2017.
[16] C.-H. Lin, C. Kong, and S. Lucey. Learning efﬁcient point
cloud generation for dense 3d object reconstruction. arXiv
preprint arXiv:1706.07036, 2017.
[17] W. Lorensen and H. E. Cline. Marching cubes: A high reso-
lution 3d surface construction algorithm. Computers Graph-
ics, 1987.
[18] C. Niu, J. Li, and K. Xu. Im2struct: Recovering 3d shape
structure from a single rgb image. computer vision and pat-
tern recognition, 2018.
[19] J. Pan, J. Li, X. Han, and K. Jia. Residual meshnet: Learning
to deform meshes for single-view 3d reconstruction. In 2018
International Conference on 3D Vision (3DV), pages 719–
727. IEEE, 2018.
[20] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learn-
ing deep 3d representations at high resolutions. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, volume 3, 2017.
[21] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. medical
image computing and computer assisted intervention, pages
234–241, 2015.
[22] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and
G. Monfardini. The graph neural network model.
IEEE
Transactions on Neural Networks, 20(1):61–80, 2009.
[23] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition.
international
conference on learning representations, 2015.
[24] A. Sinha, A. Unmesh, Q. Huang, and K. Ramani. Surfnet:
Generating 3d shape surfaces using deep residual networks.
In 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 791–800, 2017.
[25] X. Sun, J. Wu, X. Zhang, Z. Zhang, C. Zhang, T. Xue, J. B.
Tenenbaum, and W. T. Freeman. Pix3d: Dataset and methods
for single-image 3d shape modeling. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2974–2983, 2018.
[26] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. E. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In 2015 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
1–9, 2015.
[27] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3d outputs.
In 2017 IEEE International
Conference on Computer Vision (ICCV), pages 2107–2115,
2017.
[28] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang.
Pixel2mesh: Generating 3d mesh models from single rgb im-
ages. arXiv preprint arXiv:1804.01654, 2018.
[29] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong.
O-cnn: Octree-based convolutional neural networks for 3d
shape analysis. ACM Transactions on Graphics (TOG),
36(4):72, 2017.
[30] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.
Learning a probabilistic latent space of object shapes via 3d

4549

generative-adversarial modeling. In Advances in Neural In-
formation Processing Systems, pages 82–90, 2016.
[31] S. Wu, H. Huang, M. Gong, M. Zwicker, and D. Cohen-Or.
Deep points consolidation. ACM Transactions on Graphics
(TOG), 34(6):176, 2015.
[32] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1912–1920, 2015.
[33] C. Zou, E. Yumer, J. Yang, D. Ceylan, and D. Hoiem. 3d-
prnn: Generating shape primitives with recurrent neural net-
works. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2017.

4550

A Style-Based Generator Architecture for Generative Adversarial Networks

Tero Karras
NVIDIA
tkarras@nvidia.com

Samuli Laine
NVIDIA
slaine@nvidia.com

Timo Aila
NVIDIA
taila@nvidia.com

Abstract

We propose an alternative generator architecture for gen-
erative adversarial networks, borrowing from style transfer
literature. The new architecture leads to an automatically
learned, unsupervised separation of high-level attributes
(e.g., pose and identity when trained on human faces) and
stochastic variation in the generated images (e.g., freckles,
hair), and it enables intuitive, scale-specific control of the
synthesis. The new generator improves the state-of-the-art
in terms of traditional distribution quality metrics, leads to
demonstrably better interpolation properties, and also bet-
ter disentangles the latent factors of variation. To quantify
interpolation quality and disentanglement, we propose two
new, automated methods that are applicable to any genera-
tor architecture. Finally, we introduce a new, highly varied
and high-quality dataset of human faces.

1. Introduction

The resolution and quality of images produced by gener-
ative methods — especially generative adversarial networks
(GAN) [21] — have seen rapid improvement recently [28,
41, 4]. Yet the generators continue to operate as black boxes,
and despite recent efforts [2], the understanding of various
aspects of the image synthesis process, e.g., the origin of
stochastic features, is still lacking. The properties of the la-
tent space are also poorly understood, and the commonly
demonstrated latent space interpolations [12, 48, 34] provide
no quantitative way to compare different generators against
each other.
Motivated by style transfer literature [26], we re-design
the generator architecture in a way that exposes novel ways
to control the image synthesis process. Our generator starts
from a learned constant input and adjusts the “style” of
the image at each convolution layer based on the latent
code, therefore directly controlling the strength of image
features at different scales. Combined with noise injected
directly into the network, this architectural change leads to
automatic, unsupervised separation of high-level attributes

(e.g., pose, identity) from stochastic variation (e.g., freckles,
hair) in the generated images, and enables intuitive scale-
specific mixing and interpolation operations. We do not
modify the discriminator or the loss function in any way, and
our work is thus orthogonal to the ongoing discussion about
GAN loss functions, regularization, and hyper-parameters
[23, 41, 4, 37, 40, 33].
Our generator embeds the input latent code into an inter-
mediate latent space, which has a profound effect on how
the factors of variation are represented in the network. The
input latent space must follow the probability density of the
training data, and we argue that this leads to some degree
of unavoidable entanglement. Our intermediate latent space
is free from that restriction and is therefore allowed to be
disentangled. As previous methods for estimating the de-
gree of latent space disentanglement are not directly ap-
plicable in our case, we propose two new automated met-
rics — perceptual path length and linear separability — for
quantifying these aspects of the generator. Using these met-
rics, we show that compared to a traditional generator archi-
tecture, our generator admits a more linear, less entangled
representation of different factors of variation.
Finally, we present a new dataset of human faces (Flickr-
Faces-HQ, FFHQ) that offers much higher quality and
covers considerably wider variation than existing high-
resolution datasets (Appendix A). We have made this dataset
publicly available, along with our source code and pre-
trained networks.1 The accompanying video can be found
under the same link.

2. Style-based generator

Traditionally the latent code is provided to the gener-
ator through an input layer, i.e., the first layer of a feed-
forward network (Figure 1a). We depart from this design
by omitting the input layer altogether and starting from a
learned constant instead (Figure 1b, right). Given a la-
tent code z in the input latent space Z , a non-linear map-
ping network f : Z → W first produces w ∈ W (Fig-
ure 1b, left). For simplicity, we set the dimensionality of

1https://github.com/NVlabs/stylegan

4401

Latent

Latent

Normalize

Normalize

Fully-connected

Mapping
network

FC

FC

FC

FC

FC

FC

FC

FC

PixelNorm

Conv 3×3

PixelNorm

Upsample

Conv 3×3

PixelNorm

Conv 3×3

PixelNorm

4×4

8×8

Synthesis network

Const 4×4×512

A

A

A

A

style

AdaIN

Conv 3×3

style

AdaIN

4×4

Upsample

Conv 3×3

style

AdaIN

Conv 3×3

style

AdaIN

8×8

B

B

B

B

(a) Traditional

(b) Style-based generator

Figure 1. While a traditional generator [28] feeds the latent code
though the input layer only, we first map the input to an interme-
diate latent space W , which then controls the generator through
adaptive instance normalization (AdaIN) at each convolution layer.
Gaussian noise is added after each convolution, before evaluating
the nonlinearity. Here “A” stands for a learned affine transform, and
“B” applies learned per-channel scaling factors to the noise input.
The mapping network f consists of 8 layers and the synthesis net-
work g consists of 18 layers — two for each resolution (42−10242 ).
The output of the last layer is converted to RGB using a separate
1 × 1 convolution, similar to Karras et al. [28]. Our generator has
a total of 26.2M trainable parameters, compared to 23.1M in the
traditional generator.

both spaces to 512, and the mapping f is implemented using
an 8-layer MLP, a decision we will analyze in Section 4.1.
Learned affine transformations then specialize w to styles
y = (ys , yb ) that control adaptive instance normalization
(AdaIN) [26, 16, 20, 15] operations after each convolution
layer of the synthesis network g . The AdaIN operation is
defined as

AdaIN(xi , y) = ys,i

xi − µ(xi )
σ(xi )

+ yb,i ,

(1)

where each feature map xi is normalized separately, and
then scaled and biased using the corresponding scalar com-
ponents from style y. Thus the dimensionality of y is twice
the number of feature maps on that layer.
Comparing our approach to style transfer, we compute
the spatially invariant style y from vector w instead of an
example image. We choose to reuse the word “style” for
y because similar network architectures are already used
for feedforward style transfer [26], unsupervised image-to-
image translation [27], and domain mixtures [22]. Com-
pared to more general feature transforms [35, 53], AdaIN is
particularly well suited for our purposes due to its efficiency
and compact representation.

Noise

Method

CelebA-HQ

FFHQ

a Baseline Progressive GAN [28]
b + Tuning (incl. bilinear up/down)
c + Add mapping and styles
d + Remove traditional input
e + Add noise inputs
f + Mixing regularization

7.79
6.11
5.34
5.07

5.06

5.17

8.04
5.25
4.85
4.88
4.42

4.40

Table 1. Fréchet inception distance (FID) for various generator de-
signs (lower is better). In this paper we calculate the FIDs using
50,000 images drawn randomly from the training set, and report
the lowest distance encountered over the course of training.

Finally, we provide our generator with a direct means to
generate stochastic detail by introducing explicit noise in-
puts. These are single-channel images consisting of uncor-
related Gaussian noise, and we feed a dedicated noise im-
age to each layer of the synthesis network. The noise image
is broadcasted to all feature maps using learned per-feature
scaling factors and then added to the output of the corre-
sponding convolution, as illustrated in Figure 1b. The im-
plications of adding the noise inputs are discussed in Sec-
tions 3.2 and 3.3.

2.1. Quality of generated images

Before studying the properties of our generator, we
demonstrate experimentally that the redesign does not com-
promise image quality but, in fact, improves it considerably.
Table 1 gives Fréchet inception distances (FID) [24] for vari-
ous generator architectures in CelebA-HQ [28] and our new
FFHQ dataset (Appendix A). Results for other datasets are
given in the supplement. Our baseline configuration (a)
is the Progressive GAN setup of Karras et al. [28], from
which we inherit the networks and all hyperparameters ex-
cept where stated otherwise. We first switch to an improved
baseline (b) by using bilinear up/downsampling operations
[58], longer training, and tuned hyperparameters. A de-
tailed description of training setups and hyperparameters is
included in the supplement. We then improve this new base-
line further by adding the mapping network and AdaIN op-
erations (c), and make a surprising observation that the net-
work no longer benefits from feeding the latent code into the
first convolution layer. We therefore simplify the architec-
ture by removing the traditional input layer and starting the
image synthesis from a learned 4 × 4 × 512 constant tensor
(d). We find it quite remarkable that the synthesis network
is able to produce meaningful results even though it receives
input only through the styles that control the AdaIN opera-
tions.
Finally, we introduce the noise inputs (e) that improve
the results further, as well as novel mixing regularization (f)
that decorrelates neighboring styles and enables more fine-
grained control over the generated imagery (Section 3.1).
We evaluate our methods using two different loss func-
tions: for CelebA-HQ we rely on WGAN-GP [23], while

4402

2.2. Prior art

Much of the work on GAN architectures has focused on
improving the discriminator by, e.g., using multiple discrim-
inators [17, 43, 10], multiresolution discrimination [55, 51],
or self-attention [57]. The work on generator side has mostly
focused on the exact distribution in the input latent space [4]
or shaping the input latent space via Gaussian mixture mod-
els [3], clustering [44], or encouraging convexity [48].
Recent conditional generators feed the class identifier
through a separate embedding network to a large number
of layers in the generator [42], while the latent is still pro-
vided though the input layer. A few authors have considered
feeding parts of the latent code to multiple generator layers
[8, 4]. In parallel work, Chen et al. [5] “self modulate” the
generator using AdaINs, similarly to our work, but do not
consider an intermediate latent space or noise inputs.

3. Properties of the style-based generator

Our generator architecture makes it possible to control
the image synthesis via scale-specific modifications to the
styles. We can view the mapping network and affine trans-
formations as a way to draw samples for each style from a
learned distribution, and the synthesis network as a way to
generate a novel image based on a collection of styles. The
effects of each style are localized in the network, i.e., modi-
fying a specific subset of the styles can be expected to affect
only certain aspects of the image.
To see the reason for this localization, let us consider how
the AdaIN operation (Eq. 1) first normalizes each channel
to zero mean and unit variance, and only then applies scales
and biases based on the style. The new per-channel statistics,
as dictated by the style, modify the relative importance of
features for the subsequent convolution operation, but they
do not depend on the original statistics because of the nor-
malization. Thus each style controls only one convolution
before being overridden by the next AdaIN operation.

3.1. Style mixing

To further encourage the styles to localize, we employ
mixing regularization, where a given percentage of images
are generated using two random latent codes instead of one
during training. When generating such an image, we sim-
ply switch from one latent code to another — an operation
we refer to as style mixing — at a randomly selected point
in the synthesis network. To be specific, we run two latent
codes z1 , z2 through the mapping network, and have the cor-
responding w1 , w2 control the styles so that w1 applies be-
fore the crossover point and w2 after it. This regularization
technique prevents the network from assuming that adjacent
styles are correlated.
Table 2 shows how enabling mixing regularization during
training improves the localization considerably, indicated by

4403

Figure 2. Uncurated set of images produced by our style-based gen-
erator (config f) with the FFHQ dataset. Here we used a variation
of the truncation trick [38, 4, 31] with ψ = 0.7 for resolutions
42 − 322 . Please see the accompanying video for more results.

FFHQ uses WGAN-GP for configuration a and non-
saturating loss [21] with R1 regularization [40, 47, 13] for
configurations b–f. We found these choices to give the best
results. Our contributions do not modify the loss function.
We observe that the style-based generator (e) improves
FIDs quite significantly over the traditional generator (b), al-
most 20%, corroborating the large-scale ImageNet measure-
ments made in parallel work [5, 4]. Figure 2 shows an uncu-
rated set of novel images generated from the FFHQ dataset
using our generator. As confirmed by the FIDs, the aver-
age quality is high, and even accessories such as eyeglasses
and hats get successfully synthesized. For this figure, we
avoided sampling from the extreme regions of W using the
so-called truncation trick [38, 4, 31] — Appendix B details
how the trick can be performed in W instead of Z . Note
that our generator allows applying the truncation selectively
to low resolutions only, so that high-resolution details are
not affected.
All FIDs in this paper are computed without the trun-
cation trick, and we only use it for illustrative purposes in
Figure 2 and the video. All images are generated in 10242
resolution.

B

e
c
r

u

o

S

Source A

B

e
c

r

u
o

s

m

o

r
f

s

e

l

y

t

s

e

s

r

a

o

C

B

e
c

r

u
o

s

m

o

r
f

s

e

l

y

t

s

e

l

d
d

i

M

B

m

o

r
f

e

n

i

F

Figure 3. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated by
copying a specified subset of styles from source B and taking the rest from source A. Copying the styles corresponding to coarse spatial
resolutions (42 – 82 ) brings high-level aspects such as pose, general hair style, face shape, and eyeglasses from source B, while all colors
(eyes, hair, lighting) and finer facial features resemble A. If we instead copy the styles of middle resolutions (162 – 322 ) from B, we inherit
smaller scale facial features, hair style, eyes open/closed from B, while the pose, general face shape, and eyeglasses from A are preserved.
Finally, copying the fine styles (642 – 10242 ) from B brings mainly the color scheme and microstructure.

4404

Mixing
regularization

Number of latents during testing
2
3

1

4

e 0%
50%
f 90%
100%

4.42
4.41

4.40

4.83

8.22
6.10

5.11

5.17

12.88
8.71
6.88

6.63

17.41
11.61
9.03

8.40

Table 2. FIDs in FFHQ for networks trained by enabling the mixing
regularization for different percentage of training examples. Here
we stress test the trained networks by randomizing 1 . . . 4 latents
and the crossover points between them. Mixing regularization im-
proves the tolerance to these adverse operations significantly. La-
bels e and f refer to the configurations in Table 1.

(a)

(b)

(a) Generated image

(b) Stochastic variation (c) Standard deviation

Figure 4. Examples of stochastic variation.
(a) Two generated
images.
(b) Zoom-in with different realizations of input noise.
While the overall appearance is almost identical, individual hairs
are placed very differently. (c) Standard deviation of each pixel
over 100 different realizations, highlighting which parts of the im-
ages are affected by the noise. The main areas are the hair, silhou-
ettes, and parts of background, but there is also interesting stochas-
tic variation in the eye reflections. Global aspects such as identity
and pose are unaffected by stochastic variation.

improved FIDs in scenarios where multiple latents are mixed
at test time. Figure 3 presents examples of images synthe-
sized by mixing two latent codes at various scales. We can
see that each subset of styles controls meaningful high-level
attributes of the image.

3.2. Stochastic variation

There are many aspects in human portraits that can be
regarded as stochastic, such as the exact placement of hairs,
stubble, freckles, or skin pores. Any of these can be random-
ized without affecting our perception of the image as long as
they follow the correct distribution.
Let us consider how a traditional generator implements
stochastic variation. Given that the only input to the net-
work is through the input layer, the network needs to invent
a way to generate spatially-varying pseudorandom numbers
from earlier activations whenever they are needed. This con-

(c)

(d)

Figure 5. Effect of noise inputs at different layers of our generator.
(a) Noise is applied to all layers. (b) No noise. (c) Noise in fine lay-
ers only (642 – 10242 ). (d) Noise in coarse layers only (42 – 322 ).
We can see that the artificial omission of noise leads to featureless
“painterly” look. Coarse noise causes large-scale curling of hair
and appearance of larger background features, while the fine noise
brings out the finer curls of hair, finer background detail, and skin
pores.

sumes network capacity and hiding the periodicity of gen-
erated signal is difficult — and not always successful, as ev-
idenced by commonly seen repetitive patterns in generated
images. Our architecture sidesteps these issues altogether by
adding per-pixel noise after each convolution.
Figure 4 shows stochastic realizations of the same un-
derlying image, produced using our generator with different
noise realizations. We can see that the noise affects only
the stochastic aspects, leaving the overall composition and
high-level aspects such as identity intact. Figure 5 further
illustrates the effect of applying stochastic variation to dif-
ferent subsets of layers. Since these effects are best seen
in animation, please consult the accompanying video for a
demonstration of how changing the noise input of one layer
leads to stochastic variation at a matching scale.
We find it interesting that the effect of noise appears
tightly localized in the network. We hypothesize that at any
point in the generator, there is pressure to introduce new con-
tent as soon as possible, and the easiest way for our network
to create stochastic variation is to rely on the noise provided.
A fresh set of noise is available for every layer, and thus there
is no incentive to generate the stochastic effects from earlier
activations, leading to a localized effect.

4405

(a) Distribution of
features in training set

(b) Mapping from
Z to features

(c) Mapping from
W to features

Figure 6. Illustrative example with two factors of variation (im-
age features, e.g., masculinity and hair length). (a) An example
training set where some combination (e.g., long haired males) is
missing. (b) This forces the mapping from Z to image features to
become curved so that the forbidden combination disappears in Z
to prevent the sampling of invalid combinations. (c) The learned
mapping from Z to W is able to “undo” much of the warping.

3.3. Separation of global effects from stochasticity

The previous sections as well as the accompanying video
demonstrate that while changes to the style have global ef-
fects (changing pose, identity, etc.), the noise affects only in-
consequential stochastic variation (differently combed hair,
beard, etc.). This observation is in line with style transfer lit-
erature, where it has been established that spatially invariant
statistics (Gram matrix, channel-wise mean, variance, etc.)
reliably encode the style of an image [19, 36] while spatially
varying features encode a specific instance.
In our style-based generator, the style affects the entire
image because complete feature maps are scaled and biased
with the same values. Therefore, global effects such as pose,
lighting, or background style can be controlled coherently.
Meanwhile, the noise is added independently to each pixel
and is thus ideally suited for controlling stochastic variation.
If the network tried to control, e.g., pose using the noise,
that would lead to spatially inconsistent decisions that would
then be penalized by the discriminator. Thus the network
learns to use the global and local channels appropriately,
without explicit guidance.

4. Disentanglement studies

There are various definitions for disentanglement [50, 46,
1, 6, 18], but a common goal is a latent space that consists of
linear subspaces, each of which controls one factor of vari-
ation. However, the sampling probability of each combina-
tion of factors in Z needs to match the corresponding den-
sity in the training data. As illustrated in Figure 6, this pre-
cludes the factors from being fully disentangled with typical
datasets and input latent distributions.2
A major benefit of our generator architecture is that the
intermediate latent space W does not have to support sam-

2 The few artificial datasets designed for disentanglement studies (e.g.,
[39, 18]) tabulate all combinations of predetermined factors of variation
with uniform frequency, thus hiding the problem.

pling according to any fixed distribution; its sampling den-
sity is induced by the learned piecewise continuous map-
ping f (z). This mapping can be adapted to “unwarp” W so
that the factors of variation become more linear. We posit
that there is pressure for the generator to do so, as it should
be easier to generate realistic images based on a disentan-
gled representation than based on an entangled representa-
tion. As such, we expect the training to yield a less entan-
gled W in an unsupervised setting, i.e., when the factors of
variation are not known in advance [9, 32, 45, 7, 25, 30, 6].
Unfortunately the metrics recently proposed for quantify-
ing disentanglement [25, 30, 6, 18] require an encoder net-
work that maps input images to latent codes. These metrics
are ill-suited for our purposes since our baseline GAN lacks
such an encoder. While it is possible to add an extra network
for this purpose [7, 11, 14], we want to avoid investing effort
into a component that is not a part of the actual solution. To
this end, we describe two new ways of quantifying disen-
tanglement, neither of which requires an encoder or known
factors of variation, and are therefore computable for any
image dataset and generator.

4.1. Perceptual path length

As noted by Laine [34], interpolation of latent-space vec-
tors may yield surprisingly non-linear changes in the image.
For example, features that are absent in either endpoint may
appear in the middle of a linear interpolation path. This is a
sign that the latent space is entangled and the factors of vari-
ation are not properly separated. To quantify this effect, we
can measure how drastic changes the image undergoes as we
perform interpolation in the latent space. Intuitively, a less
curved latent space should result in perceptually smoother
transition than a highly curved latent space.
As a basis for our metric, we use a perceptually-based
pairwise image distance [59] that is calculated as a weighted
difference between two VGG16 [54] embeddings, where the
weights are fit so that the metric agrees with human percep-
tual similarity judgments. If we subdivide a latent space in-
terpolation path into linear segments, we can define the total
perceptual length of this segmented path as the sum of per-
ceptual differences over each segment, as reported by the im-
age distance metric. A natural definition for the perceptual
path length would be the limit of this sum under infinitely
fine subdivision, but in practice we approximate it using a
small subdivision epsilon ǫ = 10−4 . The average perceptual
path length in latent space Z , over all possible endpoints, is
therefore

lZ = E h 1

ǫ2

d(cid:0)G(slerp(z1 , z2 ; t)),
G(slerp(z1 , z2 ; t + ǫ))(cid:1)i,

(2)

where z1 , z2 ∼ P (z), t ∼ U (0, 1), G is the generator (i.e.,
g ◦ f for style-based networks), and d(·, ·) evaluates the per-

4406

Method

Path length
full
end

Separa-
bility

b Traditional generator Z
d Style-based generator W
e + Add noise inputs W
+ Mixing 50%
f + Mixing 90%

W

W

412.0
446.2

200.5

231.5
234.0

415.3
376.6

160.6

182.1
195.9

10.78
3.61
3.54

3.51

3.79

Table 3. Perceptual path lengths and separability scores for various
generator architectures in FFHQ (lower is better). We perform the
measurements in Z for the traditional network, and in W for style-
based ones. Making the network resistant to style mixing appears
to distort the intermediate latent space W somewhat. We hypothe-
size that mixing makes it more difficult for W to efficiently encode
factors of variation that span multiple scales.

ceptual distance between the resulting images. Here slerp
denotes spherical interpolation [52], which is the most ap-
propriate way of interpolating in our normalized input latent
space [56]. To concentrate on the facial features instead of
background, we crop the generated images to contain only
the face prior to evaluating the pairwise image metric. As
the metric d is quadratic [59], we divide by ǫ2 . We compute
the expectation by taking 100,000 samples.
Computing the average perceptual path length in W is
carried out in a similar fashion:

lW = E h 1

ǫ2

d(cid:0)g(lerp(f (z1 ), f (z2 ); t)),
g(lerp(f (z1 ), f (z2 ); t + ǫ))(cid:1)i,

(3)

where the only difference is that interpolation happens in
W space. Because vectors in W are not normalized in any
fashion, we use linear interpolation (lerp).
Table 3 shows that this full-path length is substantially
shorter for our style-based generator with noise inputs, in-
dicating that W is perceptually more linear than Z . Yet,
this measurement is in fact slightly biased in favor of the in-
put latent space Z . If W is indeed a disentangled and “flat-
tened” mapping of Z , it may contain regions that are not on
the input manifold — and are thus badly reconstructed by the
generator — even between points that are mapped from the
input manifold, whereas the input latent space Z has no such
regions by definition. It is therefore to be expected that if we
restrict our measure to path endpoints, i.e., t ∈ {0, 1}, we
should obtain a smaller lW while lZ is not affected. This is
indeed what we observe in Table 3.
Table 4 shows how path lengths are affected by the map-
ping network. We see that both traditional and style-based
generators benefit from having a mapping network, and ad-
ditional depth generally improves the perceptual path length
as well as FIDs. It is interesting that while lW improves in
the traditional generator, lZ becomes considerably worse, il-
lustrating our claim that the input latent space can indeed be
arbitrarily entangled in GANs.

Method

b Traditional 0 Z
Traditional 8 Z
Traditional 8 W
Style-based 0 Z
Style-based 1 W
Style-based 2 W
f Style-based 8 W

FID

5.25
4.87
4.87
5.06
4.60
4.43

4.40

Path length
full
end

412.0
896.2
324.5
283.5
219.9

217.8

234.0

415.3
902.0
212.2
285.5
209.4
199.9

195.9

Separa-
bility

10.78
170.29
6.52
9.88
6.81
6.25

3.79

Table 4. The effect of a mapping network in FFHQ. The number
in method name indicates the depth of the mapping network. We
see that FID, separability, and path length all benefit from having
a mapping network, and this holds for both style-based and tra-
ditional generator architectures. Furthermore, a deeper mapping
network generally performs better than a shallow one.

4.2. Linear separability

If a latent space is sufficiently disentangled, it should
be possible to find direction vectors that consistently corre-
spond to individual factors of variation. We propose another
metric that quantifies this effect by measuring how well the
latent-space points can be separated into two distinct sets via
a linear hyperplane, so that each set corresponds to a specific
binary attribute of the image.
In order to label the generated images, we train auxiliary
classification networks for a number of binary attributes,
e.g., to distinguish male and female faces. In our tests, the
classifiers had the same architecture as the discriminator
we use (i.e., same as in [28]), and were trained using the
CelebA-HQ dataset that retains the 40 attributes available
in the original CelebA dataset. To measure the separability
of one attribute, we generate 200,000 images with z ∼ P (z)
and classify them using the auxiliary classification network.
We then sort the samples according to classifier confidence
and remove the least confident half, yielding 100,000 labeled
latent-space vectors.
For each attribute, we fit a linear SVM to predict the la-
bel based on the latent-space point — z for traditional and w
for style-based — and classify the points by this plane. We
then compute the conditional entropy H(Y |X ) where X are
the classes predicted by the SVM and Y are the classes de-
termined by the pre-trained classifier. This tells how much
additional information is required to determine the true class
of a sample, given that we know on which side of the hyper-
plane it lies. A low value suggests consistent latent space
directions for the corresponding factor(s) of variation.
We
calculate
the
final
separability
score
as
exp(Pi H(Yi |Xi )), where i enumerates the 40 attributes.
Similar to the inception score [49],
the exponentiation
brings the values from logarithmic to linear domain so that
they are easier to compare.
Tables 3 and 4 show that W is consistently better sep-
arable than Z , suggesting a less entangled representation.

4407

Figure 7. The FFHQ dataset offers a lot of variety in terms of age, ethnicity, viewpoint, lighting, and image background.

Furthermore, increasing the depth of the mapping network
improves both image quality and separability in W , which
is in line with the hypothesis that the synthesis network in-
herently favors a disentangled input representation. Inter-
estingly, adding a mapping network in front of a traditional
generator results in severe loss of separability in Z but im-
proves the situation in the intermediate latent space W , and
the FID improves as well. This shows that even the tradi-
tional generator architecture performs better when we intro-
duce an intermediate latent space that does not have to follow
the distribution of the training data.

5. Conclusion

Based on both our results and parallel work by Chen et
al. [5], it is becoming clear that the traditional GAN gen-
erator architecture is in every way inferior to a style-based
design. This is true in terms of established quality metrics,
and we further believe that our investigations to the sepa-
ration of high-level attributes and stochastic effects, as well
as the linearity of the intermediate latent space will prove
fruitful in improving the understanding and controllability
of GAN synthesis.
We note that our average path length metric could easily
be used as a regularizer during training, and perhaps some
variant of the linear separability metric could act as one, too.
In general, we expect that methods for directly shaping the
intermediate latent space during training will provide inter-
esting avenues for future work.

6. Acknowledgements

We thank Jaakko Lehtinen, David Luebke, and Tuomas
Kynkäänniemi for in-depth discussions and helpful com-
ments; Janne Hellsten, Tero Kuosmanen, and Pekka Jänis
for compute infrastructure and help with the code release.

A. The FFHQ dataset

We have collected a new dataset of human faces, Flickr-
Faces-HQ (FFHQ), consisting of 70,000 high-quality im-
ages at 10242 resolution (Figure 7). The dataset includes
vastly more variation than CelebA-HQ [28] in terms of age,
ethnicity and image background, and also has much bet-
ter coverage of accessories such as eyeglasses, sunglasses,
hats, etc. The images were crawled from Flickr (thus in-

ψ = 1

ψ = 0.7

ψ = 0.5

ψ = 0

ψ = −0.5 ψ = −1

Figure 8. The effect of truncation trick as a function of style scale
ψ . When we fade ψ → 0, all faces converge to the “mean” face
of FFHQ. This face is similar for all trained networks, and the in-
terpolation towards it never seems to cause artifacts. By applying
negative scaling to styles, we get the corresponding opposite or
“anti-face”. It is interesting that various high-level attributes of-
ten flip between the opposites, including viewpoint, glasses, age,
coloring, hair length, and often gender.

heriting all the biases of that website) and automatically
aligned [29] and cropped. Only images under permissive
licenses were collected. Various automatic filters were used
to prune the set, and finally Mechanical Turk allowed us
to remove the occasional statues, paintings, or photos of
photos. We have made the dataset publicly available at

https://github.com/NVlabs/ffhq-dataset

B. Truncation trick in W

If we consider the distribution of training data, it is clear
that areas of low density are poorly represented and thus
likely to be difficult for the generator to learn. This is a sig-
nificant open problem in all generative modeling techniques.
However, it is known that drawing latent vectors from a trun-
cated [38, 4] or otherwise shrunk [31] sampling space tends
to improve average image quality, although some amount of
variation is lost.
We can follow a similar strategy. To begin, we compute
the center of mass of W as ¯w = E
FFHQ this point represents a sort of an average face (Fig-
ure 8, ψ = 0). We can then scale the deviation of a given
w from the center as w′ = ¯w + ψ(w − ¯w), where ψ < 1.
While Brock et al. [4] observe that only a subset of networks
is amenable to such truncation even when orthogonal reg-
ularization is used, truncation in W space seems to work
reliably even without changes to the loss function.

z∼P (z) [f (z)]. In case of

4408

References

[1] A. Achille and S. Soatto. On the emergence of invari-
ance and disentangling in deep representations. CoRR,
abs/1706.01350, 2017. 6
[2] D. Bau, J. Zhu, H. Strobelt, B. Zhou, J. B. Tenenbaum, W. T.
Freeman, and A. Torralba. GAN dissection: Visualizing
and understanding generative adversarial networks. In Proc.
ICLR, 2019. 1
[3] M. Ben-Yosef and D. Weinshall. Gaussian mixture genera-
tive adversarial networks for diverse datasets, and the unsu-
pervised clustering of images. CoRR, abs/1808.10356, 2018.
3
[4] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN
training for high fidelity natural image synthesis. CoRR,
abs/1809.11096, 2018. 1, 3, 8
[5] T. Chen, M. Lucic, N. Houlsby, and S. Gelly. On self
modulation for generative adversarial networks.
CoRR,
abs/1810.01365, 2018. 3, 8
[6] T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud. Isolat-
ing sources of disentanglement in variational autoencoders.
CoRR, abs/1802.04942, 2018. 6
[7] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. InfoGAN: interpretable representation learn-
ing by information maximizing generative adversarial nets.
CoRR, abs/1606.03657, 2016. 6
[8] E. L. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep
generative image models using a Laplacian pyramid of ad-
versarial networks. CoRR, abs/1506.05751, 2015. 3
[9] G. Desjardins, A. Courville, and Y. Bengio. Disentan-
gling factors of variation via generative entangling. CoRR,
abs/1210.5474, 2012. 6
[10] T. Doan, J. Monteiro, I. Albuquerque, B. Mazoure, A. Du-
rand, J. Pineau, and R. D. Hjelm. Online adaptative curricu-
lum learning for GANs. CoRR, abs/1808.00020, 2018. 3
[11] J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial fea-
ture learning. CoRR, abs/1605.09782, 2016. 6
[12] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to
generate chairs with convolutional neural networks. CoRR,
abs/1411.5928, 2014. 1
[13] H. Drucker and Y. L. Cun. Improving generalization perfor-
mance using double backpropagation. IEEE Transactions on
Neural Networks, 3(6):991–997, 1992. 3
[14] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky,
O. Mastropietro, and A. Courville. Adversarially learned in-
ference. In Proc. ICLR, 2017. 6
[15] V. Dumoulin, E. Perez, N. Schucher, F. Strub, H. d. Vries,
A. Courville, and Y. Bengio.
Feature-wise transforma-
tions. Distill, 2018. https://distill.pub/2018/feature-wise-
transformations. 2
[16] V. Dumoulin, J. Shlens, and M. Kudlur. A learned represen-
tation for artistic style. CoRR, abs/1610.07629, 2016. 2
[17] I. P. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-
adversarial networks. CoRR, abs/1611.01673, 2016. 3
[18] C. Eastwood and C. K. I. Williams. A framework for the
quantitative evaluation of disentangled representations.
In
Proc. ICLR, 2018. 6

[19] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In Proc. CVPR, 2016.
6
[20] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens.
Exploring the structure of a real-time, arbitrary neural artistic
stylization network. CoRR, abs/1705.06830, 2017. 2
[21] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio. Generative
Adversarial Networks. In NIPS, 2014. 1, 3
[22] W.-S. Z. Guang-Yuan Hao, Hong-Xing Yu. MIXGAN: learn-
ing concepts from different domains for mixture generation.
CoRR, abs/1807.01659, 2018. 2
[23] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C.
Courville. Improved training of Wasserstein GANs. CoRR,
abs/1704.00028, 2017. 1, 2
[24] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. GANs trained by a two time-scale update rule
converge to a local Nash equilibrium. In Proc. NIPS, pages
6626–6637, 2017. 2
[25] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,
M. Botvinick, S. Mohamed, and A. Lerchner.
beta-vae:
Learning basic visual concepts with a constrained variational
framework. In Proc. ICLR, 2017. 6
[26] X. Huang and S. J. Belongie. Arbitrary style transfer
in real-time with adaptive instance normalization. CoRR,
abs/1703.06868, 2017. 1, 2
[27] X. Huang, M. Liu, S. J. Belongie, and J. Kautz. Mul-
timodal unsupervised image-to-image translation. CoRR,
abs/1804.04732, 2018. 2
[28] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of GANs for improved quality, stability, and varia-
tion. CoRR, abs/1710.10196, 2017. 1, 2, 7, 8
[29] V. Kazemi and J. Sullivan. One millisecond face alignment
with an ensemble of regression trees. In Proc. CVPR, 2014.
8
[30] H. Kim and A. Mnih. Disentangling by factorising. In Proc.
ICML, 2018. 6
[31] D. P. Kingma and P. Dhariwal. Glow: Generative flow with
invertible 1x1 convolutions. CoRR, abs/1807.03039, 2018.
3, 8
[32] D. P. Kingma and M. Welling. Auto-encoding variational
bayes. In ICLR, 2014. 6
[33] K. Kurach, M. Lucic, X. Zhai, M. Michalski, and S. Gelly.
The gan landscape: Losses, architectures, regularization, and
normalization. CoRR, abs/1807.04720, 2018. 1
[34] S. Laine. Feature-based metrics for exploring the latent space
of generative models. ICLR workshop poster, 2018. 1, 6
[35] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang.
Universal style transfer via feature transforms. In Proc. NIPS,
2017. 2
[36] Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style
transfer. CoRR, abs/1701.01036, 2017. 6
[37] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bous-
quet. Are GANs created equal? a large-scale study. CoRR,
abs/1711.10337, 2017. 1
[38] M. Marchesi. Megapixel size image creation using generative
adversarial networks. CoRR, abs/1706.00082, 2017. 3, 8

4409

[59] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.
The unreasonable effectiveness of deep features as a percep-
tual metric. In Proc. CVPR, 2018. 6, 7

[39] L. Matthey,
I. Higgins, D. Hassabis, and A. Lerch-
ner.
dsprites: Disentanglement
testing sprites dataset.
https://github.com/deepmind/dsprites-dataset/, 2017. 6
[40] L. Mescheder, A. Geiger, and S. Nowozin. Which train-
ing methods for GANs do actually converge?
CoRR,
abs/1801.04406, 2018. 1, 3
[41] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spec-
tral normalization for generative adversarial networks. CoRR,
abs/1802.05957, 2018. 1
[42] T. Miyato and M. Koyama. cGANs with projection discrim-
inator. CoRR, abs/1802.05637, 2018. 3
[43] G. Mordido, H. Yang, and C. Meinel. Dropout-gan: Learn-
ing from a dynamic ensemble of discriminators. CoRR,
abs/1807.11346, 2018. 3
[44] S. Mukherjee, H. Asnani, E. Lin, and S. Kannan. Cluster-
GAN : Latent space clustering in generative adversarial net-
works. CoRR, abs/1809.03627, 2018. 3
[45] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
backpropagation and approximate inference in deep genera-
tive models. In Proc. ICML, 2014. 6
[46] K. Ridgeway. A survey of inductive biases for factorial
representation-learning. CoRR, abs/1612.05299, 2016. 6
[47] A. S. Ross and F. Doshi-Velez. Improving the adversarial ro-
bustness and interpretability of deep neural networks by reg-
ularizing their input gradients. CoRR, abs/1711.09404, 2017.
3
[48] T. Sainburg, M. Thielk, B. Theilman, B. Migliori, and T. Gen-
tner. Generative adversarial interpolative autoencoding: ad-
versarial training on latent space interpolations encourage
convex latent distributions. CoRR, abs/1807.06650, 2018. 1,
3
[49] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung,
A. Radford, and X. Chen. Improved techniques for training
GANs. In NIPS, 2016. 7
[50] J. Schmidhuber. Learning factorial codes by predictability
minimization. Neural Computation, 4(6):863–879, 1992. 6
[51] R. Sharma, S. Barratt, S. Ermon, and V. Pande.
Improved
training with curriculum gans. CoRR, abs/1807.09295, 2018.
3
[52] K. Shoemake. Animating rotation with quaternion curves. In
Proc. SIGGRAPH ’85, 1985. 7
[53] A. Siarohin, E. Sangineto, and N. Sebe. Whitening and col-
oring transform for GANs. CoRR, abs/1806.00420, 2018. 2
[54] K. Simonyan and A. Zisserman.
Very deep convolu-
tional networks for large-scale image recognition. CoRR,
abs/1409.1556, 2014. 6
[55] T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro.
High-resolution image synthesis and semantic manipulation
with conditional GANs. CoRR, abs/1711.11585, 2017. 3
[56] T. White. Sampling generative networks: Notes on a few
effective techniques. CoRR, abs/1609.04468, 2016. 7
[57] H. Zhang,
I. Goodfellow, D. Metaxas, and A. Odena.
Self-attention generative adversarial networks.
CoRR,
abs/1805.08318, 2018. 3
[58] R. Zhang. Making convolutional networks shift-invariant
again, 2019. 2

4410

A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction

Shumian Xin1 , Sotiris Nousias2
3 , Kiriakos N. Kutulakos2 , Aswin C. Sankaranarayanan1 ,
Srinivasa G. Narasimhan1 , and Ioannis Gkioulekas1

,

1Carnegie Mellon University 2University of Toronto 3University College London

Abstract

We present a novel theory of Fermat paths of light be-
tween a known visible scene and an unknown object not in
the line of sight of a transient camera. These light paths ei-
ther obey specular reﬂection or are reﬂected by the object’s
boundary, and hence encode the shape of the hidden object.
We prove that Fermat paths correspond to discontinuities
in the transient measurements. We then derive a novel con-
straint that relates the spatial derivatives of the path lengths
at these discontinuities to the surface normal. Based on this
theory, we present an algorithm, called Fermat Flow, to es-
timate the shape of the non-line-of-sight object. Our method
allows, for the ﬁrst time, accurate shape recovery of com-
plex objects, ranging from diffuse to specular, that are hid-
den around the corner as well as hidden behind a diffuser.
Finally, our approach is agnostic to the particular technol-
ogy used for transient imaging. As such, we demonstrate
mm-scale shape recovery from pico-second scale transients
using a SPAD and ultrafast laser, as well as micron-scale
reconstruction from femto-second scale transients using in-
terferometry. We believe our work is a signiﬁcant advance
over the state-of-the-art in non-line-of-sight imaging.

1. Introduction

Most computer vision research assumes that the scene of
interest is directly visible to the camera.
In other words,
the photons from a source that reach a camera are assumed
to have interacted with only the visible scene. However,
some of the source photons are reﬂected by the visible scene
toward parts—say, the back of an object facing a camera,
an object around a corner, or an object viewed through a
diffuser — that are hidden from the direct line of sight of
the camera. In turn, the hidden scene scatters the photons
back toward the visible scene, which then redirects photons
toward the camera. Imaging and understanding the scene
hidden from the camera’s view is of signiﬁcant importance
to many security and safety applications.
Capturing non-line-of-sight (NLOS) photons is chal-

lenging as they are vastly outnumbered by line-of-sight
(LOS) photons. Passive approaches analyze the subtle um-
bra and penumbra of the shadow cast by the hidden scene
to estimate rough motion and structure [6, 2, 38], or use co-
herence properties of light to localize hidden objects [5, 3].
These approaches do not have sufﬁcient information to
compute precise 3D shape of an unknown arbitrary hidden
scene. Extracting additional information about the hidden
scene is possible by using active illumination, including
coherent lighting [42, 21, 4, 22] and steady-state intensity
sources [25, 44, 51, 45]. The majority of approaches for
reconstructing hidden shape information employ fast modu-
lated light sources together with time-resolved sensors (e.g.,
continuous-wave ToF [15, 20], ultrafast photodiodes [24],
streak cameras [49, 48, 13], and single-photon avalanche
photodetectors (SPADs) [11, 32]). These sensors record
not only the number of incident photons (intensity) but also
their arrival times, at a range of temporal resolutions (milli-
to femto-seconds) [49, 12, 31, 32, 11, 18]. Such measure-
ments are called transients and the approach is called tran-
sient NLOS imaging.

By measuring transients at various locations of a known
visible scene, most active techniques perform a volumet-
ric 3D reconstruction by attempting to invert the time-
resolved radiometric image formation process. Examples
include elliptic backprojection [48, 7, 1, 26, 36], regular-
ized linear system approaches [13, 15, 14, 20], the light-
cone transform [33], and analysis-by-synthesis using ren-
dering [35, 47]. These methods have two fundamental dis-
advantages: (1) they rely on radiometric information and
existing SPADs produce poor intensity estimates due to ef-
fects such as pile-up and after-pulsing [16], as well as due
to extreme sensitivity to photon noise and ambient lighting;
and (2) to simplify the inverse problem, all existing recon-
struction techniques rely on an assumption of Lambertian
reﬂectance for the NLOS object.

In this paper, we overcome the above limitations by de-
veloping techniques that use only geometric, rather than in-
tensity, constraints derived from transient measurements of
an NLOS scene. For this, we present a new theory of NLOS
photons that follow speciﬁc geometric paths, called Fermat

16800

Figure 1: Non-line-of-sight imaging. We consider the problem of reconstructing surfaces that are: (a) outside the ﬁeld of
view of sensor, or (b) occluded from it by a diffuser. We develop an algorithm that can use transient imaging measurements to
accurately reconstruct the shape of the non-line-of-sight surface. The ﬁgure shows example reconstructions of a US quarter
from measurements captured by a femtosecond-scale transient imaging system.
In (c), we compare our reconstructions
against groundtruth, obtained using a direct depth scan of the object with the same transient imaging system.

paths between the LOS and NLOS scene. Based on Fer-
mat’s principle [43], we observe that these paths follow ei-
ther the law of specular reﬂection or reﬂect at speciﬁc points
at the object’s boundary. We then prove that Fermat paths
correspond to discontinuities in the transient measurements.
The temporal locations of the discontinuities are a function
of only the shape of the NLOS object and not its reﬂectance
(BRDF). We additionally show that the shape of the tran-
sient around the discontinuity is related to the curvature of
the hidden surface. This theory generalizes previous work
on the paths of ﬁrst-returning photons [46], which are a spe-
cial case of Fermat paths.

We use the above theory to derive an algorithm, called
Fermat ﬂow, for accurate NLOS shape reconstruction. We
show that the spatial derivative of the Fermat pathlength
provides a simple constraint that uniquely determines both
the depth and normal of a hidden scene point. This deriva-
tive is estimated numerically by ﬁtting a smooth pathlength
function to a sparser set of measurements. We then apply a
ﬁnal reﬁnement step that computes a smooth mesh by com-
bining both the depth and normal information [23, 9]. While
most previous approaches reconstruct an albedo volume of
the NLOS object, our approach is one of the few that recon-
struct its surface. Compared to alternative surface recon-
struction algorithms based on analysis-by-synthesis from
intensity measurements [47], our approach uses only geo-
metric constraints, which makes it BRDF-invariant and ro-
bust to imperfections in intensity measurements.

Our theory is agnostic to the speciﬁc transient imag-
ing technology used. We validate our theory and demon-
strate results at both pico-second and femto-second tempo-
ral scales, using a pulsed laser and SPAD for the former and
interferometry for the latter. Hence, for the ﬁrst time, we
are able to compute millimeter-scale and micrometer-scale
NLOS shapes of curved objects with BRDFs ranging from
purely diffuse to purely specular. In addition, our theory ap-

plies to both reﬂective NLOS (looking around the corner)
and transmissive NLOS (seeing through a diffuser) scenar-
ios. Figure 1 shows the estimated micrometer-scale relief
of a coin seen around the corner as well as through thick
paper (diffuser). The obtained height proﬁles compare well
with the reconstruction of the coin when imaged in the line
of sight. This result demonstrates the signiﬁcant theoreti-
cal and practical contribution of this work to active NLOS
imaging, pushing the boundary of what is possible.

2. Fermat Paths in NLOS Transients

Problem setup. We consider a transient imaging sys-
tem [19], comprising a light source and detector, located at
points s, d ∈ R3 , respectively. Our theory is agnostic to the
speciﬁc transient imaging technology used, and in Section 4
we describe implementations, one based on a pulsed laser
and a picosecond detector, and another based on interfer-
ometry. The visible scene V ⊂ R3 is the union of surfaces
contained within the common line of sight of the source and
detector. In addition to V , we assume that there exist sur-
faces outside their line of sight; this could be because either
these surfaces are outside the ﬁeld of view, or they are in-
side it but occluded by another surface. We are only inter-
ested in such surfaces that can indirectly receive light from
the light source by means of a single reﬂection or transmis-
sion through the visible scene, and can indirectly send light
to the detector in a likewise manner. We call the union of
such surfaces the non-line-of-sight (NLOS) scene X . Some
situations where these conditions apply, and which will be
relevant to our experiments, are shown in Figure 1.
We assume that the light source and detector are illumi-
nating and imaging the same visible point v ∈ V , which can
be any point in the visible scene. This corresponds to the
confocal scanning scheme, proposed by O’Toole et al. [33].
We emphasize that this assumption is only to simplify ex-
position: All of our theory generalizes to the non-confocal

6801

case, as we brieﬂy discuss in various places throughout the
paper, and detail in the supplement. In particular, in Sec-
tion 4, we show results from non-confocal experiments.
The detector records a transient I (t; v), which equals
the irradiance from photons with time of ﬂight t. We as-
sume that all recorded photons follow paths of the form
s → v → x → v → d, where x ∈ X . This three-
bounce assumption is commonplace in NLOS imaging ap-
plications, for two reasons: First, NLOS transient imaging
systems typically have time-gating mechanisms that can be
used to remove direct photons that only interact with the
visible scene. Second, photons with more than one interac-
tions with the NLOS scene X have greatly reduced signal-
to-noise ratio, and in practice are difﬁcult to detect [7, 35].
Finally, we assume that we have calibrated the distance
τV (v) , ks − vk + kd − vk from the source to the visible
point, and from there to the detector. Then, we can use
the pathlength traveled in X , τ , ct − τV (v) where c is
the speed of light, to uniquely reparameterize transients as
I (τ ; v). Under these assumptions, we can write [10, 37]:

I (τ ; v) = ZX
where τ (x; v) , 2 · kx − vk, (p, q) ∈ [0, 1]2 is a pa-

f (x; v) δ (τ − τ (x; v)) dA (p, q)

(1)

rameterization of the NLOS surface X , A (p, q) is the cor-
responding area measure, and the throughput f absorbs
inverse-square fall-off, shading, reﬂectance, and visibility.

2.1. Fermat paths

We assume that the NLOS scene X is formed as the
union of smooth surfaces, and ∂X ⊂ X is the set of points
on the NLOS surface where a surface normal is not deﬁned.
We will be referring to ∂X as the boundary of X for sim-
plicity, but note that, in addition to boundary points, it in-
cludes points at discontinuous intersections of the smooth
surfaces that make up X . Then, we will be focusing on spe-
ciﬁc distinguished points x ∈ X as follows.

Deﬁnition 1. For any visible point v :

• The specular set S (v) ⊂ X consists of all points x ∈
X \ ∂X such that the vector v − x is orthogonal to the
tangent plane TxX of X at x.

• The boundary set B (v) ⊂ ∂X consists of all points
x ∈ ∇X such that the vector v − x is orthogonal to
the tangent vector ˆt (x) of ∂X at x.

• The Fermat set F (v) ⊂ X is the union of these two

sets, F (v) , S (v) ∪ B (v).

Deﬁnition 1 implies that, at points x ∈ S (v), the vector
v − x is also parallel to the surface normal ˆn (x). Equiv-
alently, the path p (x; v) , v → x → v corresponds to a

specular reﬂection at x, explaining the name specular set.
The name Fermat set is due to the following classical propo-
sition of geometric optics [17, 8, 29, 43].

Proposition 2. Let (p, q) ∈ [0, 1]2 be a parameterization of
the NLOS surface X . Then, for any visible point v ,

S (v) = (cid:8)x ∈ X : ∇(p,q) τ (x (p, q) ; v) = 0(cid:9) .

Let r ∈ [0, 1] be a parameterization of the NLOS surface
boundary ∂X . Then, for any visible point v ,

(2)

B (v) = {x ∈ ∂X : ∂ τ (x (r) ; v) /∂ r = 0} .

(3)

For completeness, we provide a proof in the supplement.
Proposition 2 is known as Fermat’s principle, and charac-
terizes paths of stationary length with respect to their local
variations. We note that, even though Fermat’s principle is
often described as the “shortest path principle”, it allows for
longest or saddle-point light paths, as we demonstrate later
in this section. Depending on X , F (v) will have at least
one, and potentially multiple, points, as shown in Figure 2.
We will associate each point x ∈ F (v) with the sphere
Sph (τ (x; v) /2; v) of center v and radius τ (x; v) /2. We
call this the tangent sphere, because Proposition 2 implies
that, for x ∈ S (v) or x ∈ B (v), the Sph (τ (x; v) /2; v)
is tangent to X or ∂X , respectively, at x [29, 43].
Relationship to ﬁrst-returning photons. Fermat paths are
a superset of the paths of the ﬁrst-returning photons de-
scribed by Tsai et al. [46]. In particular, the pathlength of
the ﬁrst-returning photon is the global minimum of τ (x; v).
Then, Proposition 2 implies that x ∈ F (v). Observations
2 and 3 of Tsai et al. [46], which make an assumption of
local smoothness, correspond to the case where addition-
ally x ∈ S (v): Observation 3 describes the specular path
p (x; v), and Observation 2 describes the tangent sphere.

2.2. Fermat pathlengths as transient discontinuities

Except when the BRDF of the X surface is perfectly
specular, transients I (τ ; v) will include contributions from
photons that follow both Fermat and non-Fermat paths
p (x; v). Without prior knowledge of the scene, it would
seem impossible to identify parts of the transient due to Fer-
mat paths. However, we make the following observation.

Proposition 3. Assume that the BRDF of the X surface is
non-zero in the specular direction. Then, for all x ∈ F (v),
the transient I (τ ; v) will have a discontinuity at pathlength
τ (x; v). If x ∈ S (v), then I (τ ; v) will additionally have
a vertical asymptote at τ (x; v).

Proof sketch. We sketch a proof for the specular case, and
provide the full proof in the supplement. Let Sph (ρ; v) be
the sphere of center v and radius ρ. Let the curve C (ρ; v)
be the intersection of Sph (ρ; v) with X , parameterized by

6802

NLOS
surface

X

x

xF ,1

xF ,3

Sph (τF ,3 ; v)

xF ,2

Sph (τF ,2 ; v)

τF ,1

τF ,2

τF ,3

)

v

;

τ
(

I

t

n

e

i

s

n

a

r

t

v

visible surface V

)

v

;

x

(

τ

h

t

g
n

e

l

h

t

a

p

τF ,3

τF ,2

τF ,1

pathlength τ
NLOS surface point x
Figure 2: Theory of Fermat paths. We illuminate and
image an NLOS surface X from a point v on a visible
surface V . (We show the camera and light sources in Fig-
ure 1.) Among all points x ∈ X , some points on the sur-
face (xF ,2 ,xF ,3 ) and boundary (xF ,1 ) will create paths that
satisfy Fermat’s principle, corresponding to local minima
(xF ,1 ,xF ,2 ) or maxima (xF ,3 ) of the pathlength function
τ (x; v) (bottom right). The paths for the non-boundary
points (xF ,2 ,xF ,3 ) will additionally be specular. We can
identify the lengths of these Fermat paths from the fact that
the transient I (τ ; v) (bottom left) will be discontinuous at
the corresponding pathlengths (τF ,1 ,τF ,2 ,τF ,3 ).

r ∈ [0, 1]. Then, we can use (r, ρ) ∈ [0, 1] × [0, ∞) to

reparameterize X , and rewrite the integral of Equation (1):

J (r,ρ)

−1

dA (r, ρ) , (4)

I (τ ; v) =
f (x; v) δ (τ − τ (x; v)) (cid:12)(cid:12)(cid:12)

(p,q) (x)(cid:12)(cid:12)(cid:12)
where J (r,ρ)
(p,q) (x) is the Jacobian of the transformation
(p, q) 7→ (r, ρ). We now consider a point xS ∈ S (v). Rec-
ognizing that ρ (xS ) = τ (xS ; v) /2, we have from Equa-
tion (2) that ∇(p,q)ρ (xS ) = 0. Consequently,
(p,q) (xS )(cid:12)(cid:12)(cid:12)
(5)
Then, from Equation (4), at τ = τ (xS ; v), the transient
converges to inﬁnity, resulting in a discontinuity.

∂ ρ (xS )
∂ p

∂ r (xS )
∂ q

−

∂ ρ (xS )
∂ q

∂ r (xS )
∂ p

= 0.

J (r,ρ)

=

ZX

(cid:12)(cid:12)(cid:12)

Figure 2 visualizes
this proposition for a two-
dimensional Lambertian scene X , and a visible point v such
that S (v) = {xF ,2 , xF ,3 }, B (v) = {xF ,1 }. We note that,
in two dimensions, the boundary ∂X is not a curve but just

isolated points, and therefore the tangency property of Def-
inition 1 and the tangent sphere are not meaningful.
BRDF invariance. Proposition 3 implies that the path-
lengths where the transient I (τ ; v) is discontinuous are
determined completely by the function τ (x; v).
In turn,
τ (x; v) depends only on the geometry of v and X . There-
fore, the discontinuity pathlengths are independent of the
BRDF of the NLOS surface X . The BRDF is included in
the throughput term f in Equation (4), and thus only affects
the intensity of the transient at the discontinuity pathlength.
Figure 3 demonstrates this reﬂectance invariance property.
Identifying type of stationarity. Proposition 3 allows us
to identify the lengths of all Fermat paths that contribute to
a transient I (τ ; v), as the pathlengths where I (τ ; v) is dis-
continuous. From Proposition 2, each of these pathlengths
is a stationary point of the function τ (x; v). When the
BRDF of X is not perfectly specular, we can additionally
identify the type of stationarity from the shape of the tran-
sient at a neighborhood of the discontinuity. We use Fig-
ure 2 for intuition, and refer to the supplement for details.
Speciﬁcally, let τF be a pathlength where the transient
is discontinuous. If τF is a local maximum, the disconti-
nuity in the transient I (τ ; v) occurs at the limit from the
left, τ → τ −
F , and the transient decreases to the right of τF
(Figure 2, τF ,3 ). Conversely, when τF is a local minimum,
the discontinuity occurs at the limit from the right, τ → τ +
(Figure 2, τF ,1 ,τF ,2 ). Finally, when τF is a saddle point, the
discontinuity and intensity rise are two-sided. An example
of this is shown in Figure 3 (paraboloid case).
Identifying the stationarity type of specular discontinu-
ities provides us with curvature information about X .

F

Proposition 4. Let a transient I (τ ; v) have a specular dis-
continuity at τS , corresponding to a point xS ∈ S (v). If
κmin , κmax are the principal curvatures of X at xS , then:

• If τS is a local minimum of τ (x; v), 2/τS < κmin .

• If τS is a local maximum of τ (x; v), κmax < 2/τS .

• If τS is a saddle point of τ (x; v), κmin ≤ 2/τS ≤ κmax .

We provide the proof in the supplement, but we can use
Figure 2 to provide intuition: The pathlength τF ,2 of point
xF ,2 is a local minimum. All X points in the neighbor-
hood of xF ,2 are at a distance from v greater than τF ,2 ,
and therefore outside the tangent sphere Sph (τF ,2 /2; v).
This implies that the (minimal, in 3D) principal radius of
curvature is greater than τF ,2 /2. And conversely for the
pathlength τF ,3 of point xF ,3 , which is a local maximum.
We note that a sufﬁcient condition for X to produce only
locally-minimum specular pathlengths is that X is convex.
However, this is not a necessary condition: As explained in
Proposition 4, it is possible for X to contain concavities and
still produce only locally-minimum specular pathlengths.

6803

E (τF (v s , vd ) ; v s , vd )

Sph (τF (v) ; v)

xF

X

∇vs τF (vs , vd )

∇v τF (v)

2

V

vs
v
vd
Figure 4: The Fermat ﬂow equation. We visualize both
the confocal (black lines) and non-confocal (green lines)
cases. In the confocal case, we consider the Fermat path
connecting the visible point v with an NLOS point xF . The
spatial gradient ∇v τF (v) /2 of the length of this path is a
unit vector parallel to the vector xF − v . If the Fermat path
is also specular, then ∇v τF (v) /2 will additionally be the
opposite of the NLOS surface normal at xF . In the non-
confocal case, we can compute the gradient with respect to
either of the two visible points, vs and vd , and it will be
parallel to the vector xF − v s or xF − vd , respectively.

paraboloid a saddle point, and the plane a local minimum.

We then cover each object with aluminum foil, to create
a rough specular BRDF, and repeat our measurements. We
notice that the measured transients (Figure 3, purple) are
discontinuous at the same locations as the diffuse transients,
in agreement with our discussion of BRDF invariance.

These measurements additionally help evaluate the ro-
bustness of our theoretical predictions in the presence of
the Poisson noise and temporal jitter inherent in SPAD mea-
suremens [16]: Even though the discontinuity shapes devi-
ate from the ideal shapes in the simulated transient of Fig-
ure 2, the theoretically predicted features are still visible.

Figure 3: Experimental demonstration. We measure
transients for three objects in a looking-around-the-corner
conﬁguration: A plane, a paraboloid, and a concave sphere.
We measure each object twice, once with the object cov-
ered with diffuse paint, and a second time with the object
covered with aluminum foil. As predicted by our theory,
the measured transients have discontinuities corresponding
to specular paths of type local minimum, saddle point, and
local maximum, respectively. Additionally, the location of
the discontinuities is not affected by the change in BRDF.

Non-confocal case.
The results of this section have
straightforward generalizations to the case where the source
and detector are pointing at different points v s and vd
on the visible surface V . Propositions 2 and 3 apply ex-
actly, except that τ (x; v) is replaced with τ (x; vs , vd ) ,
kx − v s k + kx − vd k. The analogue of the tangent sphere
is the osculating ellipsoid E (τ ; v s , vd ), of pathlength τ and
foci v s , vd . Finally, Proposition 4 can be generalized to
analogous relations between the principal curvatures of X
and E (τ ; vs , vd ). We provide details in the supplement.

2.3. Experimental demonstration

3. Surface Reconstruction Using Fermat Paths

To demonstrate our theoretical ﬁndings in practice, we
use a picosecond-resolution transient imaging setup (see
Section 4) to capture measurements of a few real-world
objects, in a looking-around-the-corner conﬁguration (Fig-
ure 1(a)). Figure 3 shows the objects: A concave hemi-
sphere, an extruded paraboloid, and a plane. All objects
have size 20 cm × 20 cm and are painted with diffuse paint.
We place each object at a distance 40 cm from the visible
wall, then measure a transient from a visible point such that
there is a specular path corresponding roughly to the cen-
ter of the object. We observe from the measured transients
(Figure 3, orange) that, in agreement with Proposition 4, the
hemisphere produces a local maximum discontinuity, the

Using the results of Section 2, given a transient mea-
surement I (τ ; v), we can identify its discontinuities as
the lengths τF of Fermat paths contributing to the tran-
sient. Each length τF constrains the corresponding point
xF ∈ F (v) to lie on the tangent sphere Sph (τF /2; v) and,
if xF ∈ S (v), also constrains its normal and curvature.
We now develop a procedure for completely determining
the point xF and its normal. Then, given a collection of
Fermat pathlengths, our procedure will produce an oriented
point cloud (locations and normals) for the NLOS surface
X . Figure 5 visualizes our reconstruction procedure.
The Fermat ﬂow equation. We begin by introducing a key
technical result at the heart of our reconstruction procedure.

6804

X

I (τ ; v)

τF ,1 (v)

τF ,2 (v)

v
v

vy

V

τ

vx

vx

vx

vy

vy

(a) scanning

(b) discontinuity detection

(c) gradient estimation by interpolation

(d) point cloud reconstruction

Figure 5: Reconstruction pipeline. (a) We ﬁrst collect transient measurements I (τ , v) at multiple points v on the visible
surface V . (b) For each measured transient, we detect pathlengths where the transient is discontinuous. These correspond
to samples of the multi-valued Fermat pathlength function τF (v). In the example shown, τF (v) has two branches τF ,1 (v)
and τF ,2 (v), shown in blue and red respectively. (c) Within each branch of τF (v), we interpolate to compute the gradient
∇v τF (v). (d) Finally, by applying the Fermat ﬂow equation (7), we reconstruct from each branch a set of points, either on
the boundary (branch τF ,1 (v), blue) or at the interior (branch τF ,2 (v), red) of the NLOS shape.

We ﬁrst deﬁne the Fermat pathlength function τF (v):

τF (v) = {τ : I (τ ; v) is discontinuous} .

(6)

As each transient I (τ ; v) can be discontinuous at more than
one pathlengths, τF (v) is a multi-valued function. We now
prove the following property for this function.

Proposition 5. Consider a branch of the Fermat pathlength
function τF (v) evaluated at v ∈ V . Assume that there is a
unique point xF ∈ F (v) with τ (xF ; v) = τF (v). Then,

∇v τF (v) = −2

xF − v

kxF − vk

.

(7)

We provide the proof in the supplement. It is instructive
to consider the case of the branch of τF (v) corresponding
to the global minimum of τ (x; v). This branch is the path-
length of the ﬁrst-returning photons of Tsai et al. [46], and
is also equal to twice the distance function D (v , X ), where

D (v , X ) , minx∈X kx − vk .

(8)

Then, Equation (7) is equivalent to the well-known eikonal
equation for distance functions [30, 40, 39]. In this con-
text, the uniqueness requirement of Proposition 5 is equiva-
lent to a requirement that the minimizer of Equation (8) be
unique, which is also a requirement for the eikonal equation.
Proposition 5 generalizes the eikonal equation to apply to all
branches of τF (v), corresponding to all stationary points of
τ (x; v) and not only those for ﬁrst-returning photons.
Using τF (v) = 2 kxF − vk, we rewrite Equation (7) as

xF = v − (τF (v) /4)∇v τF (v) .

(9)

In this form, the Fermat ﬂow equation states that an NLOS
point xF ∈ F (v) generating a Fermat path can be uniquely
reconstructed from the corresponding visible point v , the
length τF (v), and the gradient ∇v τF (v). This recon-
struction can be done with a simple geometric operation,




6805

v − λ∇v τF (v) /2.

by intersecting the sphere Sph (τF (v) /2; v) with the line
If the Fermat path is also specular,
xF ∈ S (v), then we can also reconstruct the normal at xF
as ˆn (xF ) = −∇v τF (v) /2. This is shown in Figure 4.
Non-confocal case. We describe here a similar proposition
for the non-confocal case, which we prove in the supple-
ment. In this case, the Fermat pathlength function has two
arguments, τF (vs , vd ). Then, as shown in Figure 4,
xF − v i
kxF − v i k

∇vi τF (v s , vd ) = −

(10)

,

where v i can be either of the two visible points v s and vd .
Gradient estimation. Using Equation (9) requires know-
ing the gradient ∇v τF (v), computed in the local coordinate
frame of the visible surface V at v . We cannot measure this
gradient directly, but we can estimate it through interpola-
tion. For simplicity, we discuss here only the case when the
visible surface V is planar, deferring the general case for the
supplement. When the visible surface V is planar, the local
coordinate system at v can be assumed to be the x − y plane
of the global coordinate system. Given a Fermat pathlength
τF (v) at a point v , we can estimate its partial derivatives
∂ τF /∂x and ∂ τF /∂ y by locally interpolating Fermat path-
lengths from transients measured at nearby points on the
plane. We can then infer the derivative with respect to z by
noting that Equation (7) implies that k∇v τF (v)k = 2,

∇v τF (v) =

∂ τF
∂x

,

∂ τF
∂ y

, s4 − (cid:18) ∂ τF
∂x (cid:19)2

− (cid:18) ∂ τF
∂ y (cid:19)2

.

(11)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)v



We note that Equation (11) fails when the uniqueness con-
dition of Proposition 5 is not satisﬁed. In practice, this can
only happen at isolated points v on the visible surface, cor-
responding to non-generic symmetries of the NLOS surface
X . Additionally, we note that the above interpolation pro-
cedure needs to be performed separately for each branch of
the Fermat pathlength function τF (v).

Figure 6: Comparison with groundtruth. We perform
one-dimensional scans of 3D-printed objects, in a looking-
around-the-corner conﬁguration. For each object, we show
a photograph under ambient light (left), and reconstruction
results (red points) superimposed against the groundtruth
mesh used to fabricate the object (middle and right).

Surface ﬁtting. The above procedure produces an oriented
point cloud, of density comparable to the density of mea-
surements on V . We can then use algorithms that take ad-
vantage of normal information to ﬁt a surface representation
(e.g., triangular mesh) to the point cloud with increased ac-
curacy [23]. Given such an initial surface reconstruction,
in the supplement we describe an optimization procedure,
based on the theory of specular path perturbations [9, 17],
that reﬁnes the ﬁtted surface to account for possible errors
due to inaccurate estimation of the gradients ∇v τF (v).

4. Experiments

We discuss results from NLOS reconstruction experi-
ments we have performed to validate and evaluate the Fer-
mat ﬂow algorithm. All experiments are based on measure-
ments captured with two transient imaging setups, one op-
erating at picosecond and the other at femtosecond temporal
scales. We show additional experiments in the supplement.

4.1. Picosecond(cid:173)scale experiments

Imaging system. We use a SPAD-based transient imag-
ing system [11, 33, 31], consisting of a picosecond laser
(NKT SuperK EXW-12), a SPAD detector (MPD module),
and a time-correlated single photon counter (TCSPC, Pico-
Quant PicoHarp). The temporal binning resolution of the
TCSPC unit is 4 ps, for an absolute upper bound in depth
resolution of 1.2 mm. In practice, the resolution is lower,
because of laser and TCSPC jitter. We use galvo mirrors
to independently control viewpoint and illumination direc-
tion, and perform both confocal and non-confocal scanning
in the looking-around-the-corner setting of Figure 1(a).

Figure 7: Comparison of point cloud and surface recon-
structions. We scan a rough specular kettle, shown in the
left under ambient light. We reconstruct an oriented point
cloud, shown in the middle from two views, where points
are colored according to their normal. Finally, we ﬁt a sur-
face to the point cloud, shown to the right under two views.

Comparison with groundtruth. We fabricated small ob-
jects from CAD meshes, providing us with ground-truth
shape for comparison. The objects were painted with matte
white paint to create Lambertian reﬂectance. The objects
are approximately 15 cm in each dimension, and are placed
at a distance 25 cm from a planar visible surface. All ob-
jects are ruled surfaces, to allow reconstruction of their pro-
ﬁle from only one-dimensional scans. We capture measure-
ments under a non-confocal setting, by ﬁxing the point im-
aged by the SPAD and scanning the point illuminated by the
source along multiple horizontal lines on the visible wall.
Along each line, we scan 200 points, at a distance of ap-
proximately 1 mm from each other. Figure 6 shows point
clouds reconstructed from these measurements using the
Fermat ﬂow procedure, superimposed against the meshes
used for fabrication. The reconstructions closely reproduce
the shape of the objects, including their concave and convex
surfaces, and match the groundtruth within 2 mm.

Table-top objects. We scanned a variety of every day ob-
jects (Figures 7-8), with convex and concave geometry of
different BRDFs, including translucent (plastic jug), glossy
(bowl, vase), rough specular (kettle) and smooth specular
(sphere). Most of the objects have a major dimension of
approximately 20 − 30 cm, and are placed at a distance
of 80 cm from the visible wall. We use confocal scan-
ning with a grid of 64 × 64 points distributed in an area
of 80 cm × 80 cm on the visible wall.

Figure 7 visualizes point cloud, normal, and ﬁnal sur-
face reconstruction for one of the objects, an electrical ket-
tle with rough-specular reﬂectance. We observe that our re-
construction procedure produces a point cloud that closely
matches the shape of the object, including accurate normals
on its front surface. We note that we do not reconstruct nor-
mals at the handle of the object: This is expected, because
these parts of the object produce Fermat paths of bound-
ary, rather than specular, type, and such paths do not pro-
vide normal information. The ﬁnal ﬁtted surface further
improves the reconstruction quality. Figure 8 shows recon-

6806

Figure 8: Table-top objects. We scan objects that span a variety of shapes (convex, concave) and reﬂectances (translucent,
glossy, specular). For each object, we show a photograph under ambient light, and two views of its surface reconstruction.

structions for the remaining table-top objects. In all cases,
the reconstruction closely matches the object shape, demon-
strating the ability of our algorithm to handle a variety of
complex geometry and reﬂectance combinations.

4.2. Femtosecond(cid:173)scale experiments

Imaging system. We use a time-domain, full-frame optical
coherent tomography system [12]. We use this system to
perform confocal scans under both the looking-around-the-
corner and looking-through-diffuser settings (Figure 1). We
use spatially and temporally incoherent LED illumination,
which allows us to combine transient imaging with diago-
nal probing [34]. In the context of confocal scanning, this
means that we can simultaneously collect transients I (τ , v)
at all points on the visible surface without scanning, as tran-
sient measurements taken at one point will not be contami-
nated with light emanating from a different point. Our im-
plementation has depth resolution of 10 µm.
Coin reconstructions. We perform experiments in
both the looking-around-the-corner and looking-through-
diffuser settings (Figure 1), where for the diffuser we use
a thin sheet of paper. In both cases, the NLOS object is a
US quarter, with the obverse side facing the visible surface.
We place the coin at a distance of 10 mm from the visible
surface, and collect transient measurements on an area of
about 40 mm × 40 mm, at an 1 MPixel grid of points. For
validation, we additionally use the same setup to directly
scan the coin without occlusion. Figure 1 shows our results.
In both cases, we can reconstruct ﬁne detail on the coin,
sufﬁcient to infer its denomination. The reconstructed de-
tail is also in close agreement with the groundtruth shape
measured with the coin directly in the line-of-sight.

5. Discussion

Reconstruction quality can additionally suffer if we do not
have sufﬁciently dense measurements for estimating Fermat
pathlength gradients through interpolation. Finally, using
only pathlength information provides BRDF invariance, but
it also means that we do not take advantage of information
available in measured intensities about the NLOS scene.

The theory we developed offers new insights into the
NLOS imaging problem, linking it to classical areas such as
specular and differential geometry, and providing ample op-
portunity for transfer of ideas from these areas to the NLOS
imaging setting. By allowing us to treat NLOS reconstruc-
tion from a purely geometric perspective, our theory intro-
duces a new methodology for tackling this problem, dis-
tinct from but complementary to approaches such as (ellip-
tic) backprojection [48, 33] and analysis-by-synthesis [47],
which focus on the radiometric aspects of the problem.

Interestingly, concurrect work [28] has unconvered an in-
triguing link between our and backprojection approaches,
by showing that the latter cannot reconstruct NLOS points
not on Fermat paths, even if those points otherwise con-
tribute to measured transients. Therefore, both approaches
reproduce the same part of the NLOS scene. Further explo-
ration of connections between the geometric and backpro-
jection approaches can help shed light into their fundamen-
tal limits and strengths, potentially by allowing us to de-
rive results applicable to both classes of approaches using
whichever mathematical framework (geometric, radiomet-
ric) is more convenient for analysis.

More broadly, an exciting future direction of research
is combining the two classes of approaches, not only for
NLOS imaging, but also for other related applications,
including acoustic and ultrasound imaging [27], lensless
imaging [50], and seismic imaging [41].

We discuss some limitations of our approach. Our re-
construction procedure does not require radiometric cali-
bration, as it does not use intensity information, instead
relying on estimation of the pathlengths where measured
transients are discontinuous. Consequently, our reconstruc-
tions can be sensitive to inaccurate discontinuity detection.

Acknowledgments. We thank Chia-Yin Tsai for valuable
discussions. This work was supported by the DARPA RE-
VEAL program under contract HR0011-16-C-0025. SX,
ACS, SGN, and IG were additionally supported by NSF Ex-
peditions award CCF-1730147. KNK was supported by the
NSERC RGPIN and RTI programs.

6807

References

[1] Victor Arellano, Diego Gutierrez, and Adrian Jarabo.
Fast back-projection for non-line of sight reconstruc-
tion. Optics Express, 25(10):11574–11583, 2017. 1

[2] Manel Baradad, Vickie Ye, Adam B Yedidia, Fr ´edo
Durand, William T Freeman, Gregory W Wornell, and
Antonio Torralba.
Inferring light ﬁelds from shad-
ows. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 6267–
6275, 2018. 1

[3] Mufeed Batarseh, Sergey V Sukhov, Zhiqin Shen,
Heath Gemar, Reza Rezvani, and Aristide Dogariu.
Passive sensing around the corner using spatial coher-
ence. Nature communications, 9(1):3629, 2018. 1

[4] Jacopo Bertolotti, Elbert G van Putten, Christian
Blum, Ad Lagendijk, Willem L Vos, and Allard P
Mosk. Non-invasive imaging through opaque scatter-
ing layers. Nature, 491(7423):232, 2012. 1

[5] Jeremy Boger-Lombard and Ori Katz. Non line-
of-sight localization by passive optical time-of-ﬂight.
ArXiv e-prints, Aug. 2018. 1

[6] Katherine L. Bouman, Vickie Ye, Adam B. Yedidia,
Fr ´edo Durand, Gregory W Wornell, Antonio Torralba,
and William T Freeman. Turning corners into cam-
eras: Principles and methods. In ICCV, 2017. 1

[7] Mauro Buttafava, Jessica Zeman, Alberto Tosi, Kevin
Eliceiri, and Andreas Velten. Non-line-of-sight imag-
ing using a time-gated single photon avalanche diode.
Optics express, 23(16):20997–21011, 2015. 1, 3

[8] Min Chen and James Arvo. Perturbation methods for
interactive specular reﬂections. IEEE Transactions on
Visualization and Computer Graphics, 6(3):253–264,
2000. 3

[9] Min Chen and James Arvo. Theory and application
of specular path perturbation. ACM Transactions on
Graphics (TOG), 19(4):246–278, 2000. 2, 7

[10] Philip Dutr ´e, Kavita Bala, and Philippe Bekaert. Ad-
vanced global illumination. AK Peters, Ltd., 2006. 3

[11] Genevieve Gariepy, Nikola Krstaji ´c, Robert Hender-
son, Chunyong Li, Robert R Thomson, Gerald S
Buller, Barmak Heshmat, Ramesh Raskar, Jonathan
Leach, and Daniele Faccio.
Single-photon sensi-
tive light-in-ﬁght imaging. Nature communications,
6:6021, 2015. 1, 7

struction of hidden 3d shapes using diffuse reﬂections.
Optics express, 20(17):19096–19108, 2012. 1

[14] Felix Heide, Matthew O’Toole, Kai Zhang, David Lin-
dell, Steven Diamond, and Gordon Wetzstein. Robust
non-line-of-sight imaging with single photon detec-
tors. arXiv preprint arXiv:1711.07134, 2017. 1

[15] Felix Heide, Lei Xiao, Wolfgang Heidrich, and
Matthias B Hullin. Diffuse mirrors: 3d reconstruc-
tion from diffuse indirect illumination using inexpen-
sive time-of-ﬂight sensors. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 3222–3229, 2014. 1

[16] Quercus Hernandez, Diego Gutierrez, and Adrian
Jarabo. A computational model of a single-photon
avalanche diode sensor for transient imaging. arXiv
preprint arXiv:1703.02635, 2017. 1, 5

[17] Wenzel Jakob and Steve Marschner. Manifold explo-
ration: A markov chain monte carlo technique for ren-
dering scenes with difﬁcult specular transport. ACM
Transactions on Graphics, 2012. 3, 7

[18] Adrian Jarabo, Julio Marco, Adolfo Mu ˜noz, Raul
Buisan, Wojciech Jarosz, and Diego Gutierrez. A
Framework for Transient Rendering. ACM Trans.
Graph., 33(6):177:1–177:10, Nov. 2014. 1

[19] Adrian Jarabo, Belen Masia, Julio Marco, and Diego
Gutierrez. Recent advances in transient imaging: A
computer graphics and vision perspective. Visual In-
formatics, 1(1):65–79, 2017. 2

[20] Achuta Kadambi, Hang Zhao, Boxin Shi, and Ramesh
Raskar. Occluded imaging with time-of-ﬂight sen-
sors. ACM Transactions on Graphics (ToG), 35(2):15,
2016. 1

[21] Ori Katz, Pierre Heidmann, Mathias Fink, and Syl-
vain Gigan. Non-invasive single-shot imaging through
scattering layers and around corners via speckle corre-
lations. Nature photonics, 8(10):784, 2014. 1

[22] Ori Katz, Eran Small, and Yaron Silberberg. Looking
around corners and through thin turbid layers in real
time with scattered incoherent light. Nature photonics,
6(8):549–553, 2012. 1

[23] Michael Kazhdan, Matthew Bolitho, and Hugues
Hoppe. Poisson surface reconstruction. In Proc. Euro-
graphics Symposium on Geometry Processing, pages
61–70, 2006. 2, 7

[12] Ioannis Gkioulekas, Anat Levin, Fr ´edo Durand, and
Todd Zickler. Micron-scale light transport decom-
position using interferometry. ACM Transactions on
Graphics, 2015. 1, 8

[24] Ahmed Kirmani, Tyler Hutchison, James Davis, and
Ramesh Raskar. Looking around the corner using
ultrafast transient imaging.
International journal of
computer vision, 95(1):13–28, 2011. 1

[13] Otkrist Gupta, Thomas Willwacher, Andreas Velten,
Ashok Veeraraghavan, and Ramesh Raskar. Recon-

[25] Jonathan Klein, Christoph Peters, Jaime Mart´ın, Mar-
tin Laurenzis, and Matthias B Hullin. Tracking objects

6808

outside the line of sight using 2d intensity images. Sci-
entiﬁc reports, 6, 2016. 1

[26] Marco La Manna, Fiona Kine, Eric Breitbach,
Jonathan Jackson, and Andreas Velten. Error back-
projection algorithms for non-line-of-sight imaging. 1

[27] David Lindell, Gordon Wetzstein,
and Vladlen
Koltun. Acoustic non-line-of-sight imaging. In CVPR,
2019. 8

[28] Xiaochun Liu, Sebastian Bauer, and Andreas Velten.
Analysis of feature visibility in non-line-of-sight mea-
surements. In CVPR, 2019. 8

[29] Don Mitchell and Pat Hanrahan.
Illumination from
curved reﬂectors.
In ACM SIGGRAPH Computer
Graphics, volume 26, pages 283–291. ACM, 1992. 3

[30] Stanley Osher and Ronald Fedkiw. Level set methods
and dynamic implicit surfaces, volume 153. Springer
Science & Business Media, 2006. 6

[31] Matthew O’Toole, Felix Heide, David B Lindell, Kai
Zang, Steven Diamond, and Gordon Wetzstein. Re-
constructing transient images from single-photon sen-
sors. CVPR, 2017. 1, 7

[32] Matthew O’Toole, Felix Heide, Kai Lindell, David
B.and Zang, Steven Diamond, and Gordon Wetzstein.
Reconstructing transient images from single-photon
sensors. In CVPR, 2017. 1

[33] Matthew O’Toole, David B Lindell, and Gordon Wet-
zstein. Confocal non-line-of-sight imaging based on
the light-cone transform. Nature, 555(7696):338,
2018. 1, 2, 7, 8

[34] M. O’Toole, R. Raskar, and K.N. Kutulakos. Primal-
dual coding to probe light transport. ACM Transac-
tions on Graphics, 2012. 8

[35] Adithya Pediredla, Mauro Buttafava, Alberto Tosi,
Oliver Cossairt, and Ashok Veeraraghavan.
Re-
constructing rooms using photon echoes: A plane
based model and reconstruction algorithm for looking
around the corner. ICCP, 2017. 1, 3

[36] Adithya Pediredla, Akshat Dave, and Ashok Veer-
araghavan. Snlos: Non-line-of-sight scanning through
temporal focusing. ICCP, 2019. 1

[37] Adithya Pediredla, Ashok Veeraraghavan, and Ioannis
Gkioulekas. Elliptic path sampling for time-gated ren-
dering. ACM Transactions on Graphics (TOG), 2019.
3

[38] Charles Saunders, John Murray-Bruce, and Vivek K
Goyal. Computational periscopy with an ordinary dig-
ital camera. Nature, 565(7740):472, 2019. 1

[39] James A Sethian. Fast marching methods. SIAM re-
view, 41(2):199–235, 1999. 6

[40] James A Sethian. Level set methods and fast marching
methods: evolving interfaces in computational geom-
etry, ﬂuid mechanics, computer vision, and materials
science, volume 3. Cambridge university press, 1999.
6

[41] James A Sethian and A Mihai Popovici. 3-d traveltime
computation using the fast marching method. Geo-
physics, 64(2):516–523, 1999. 8

[42] Brandon M. Smith, Matthew O’Toole, and Mohit
Gupta. Tracking multiple objects outside the line of
sight using speckle imaging. In CVPR, 2018. 1

[43] Orestes Stavroudis. The optics of rays, wavefronts,
and caustics. Elsevier, 1972. 2, 3

[44] Matthew Tancik, Guy Satat, and Ramesh Raskar.
Flash photography for data-driven hidden scene recov-
ery. arXiv preprint arXiv:1810.11710, 2018. 1

[45] Christos Thrampoulidis, Gal Shulkind, Feihu Xu,
William T Freeman, Jeffrey Shapiro, Antonio Tor-
ralba, Franco Wong, and Gregory Wornell. Exploiting
occlusion in non-line-of-sight active imaging.
IEEE
Transactions on Computational Imaging, 2018. 1

[46] Chia-Yin Tsai, Kiriakos N Kutulakos, Srinivasa G
Narasimhan, and Aswin C Sankaranarayanan. The ge-
ometry of ﬁrst-returning photons for non-line-of-sight
imaging. In CVPR, 2017. 2, 3, 6

[47] Chia-Yin Tsai, Aswin C. Sankaranarayanan, and Ioan-
nis Gkioulekas. Beyond volumetric albedo—a surface
optimization framework for non-line-of-sight imag-
ing. In CVPR, 2019. 1, 2, 8

[48] Andreas Velten, Thomas Willwacher, Otkrist Gupta,
Ashok Veeraraghavan, Moungi G Bawendi, and
Ramesh Raskar. Recovering three-dimensional shape
around a corner using ultrafast time-of-ﬂight imaging.
2012. 1, 8

[49] Andreas Velten, Di Wu, Adrian Jarabo, Belen Ma-
sia, Christopher Barsi, Chinmaya Joshi, Everett Law-
son, Moungi Bawendi, Diego Gutierrez, and Ramesh
Raskar. Femto-photography: Capturing and Visual-
izing the Propagation of Light. ACM Trans. Graph.,
32(4):44:1–44:8, July 2013. 1

[50] Di Wu, Gordon Wetzstein, Christopher Barsi, Thomas
Willwacher, Qionghai Dai, and Ramesh Raskar. Ultra-
fast lensless computational imaging through 5d fre-
quency analysis of time-resolved light transport. Inter-
national journal of computer vision, 110(2):128–140,
2014. 8

[51] Feihu Xu, Gal Shulkind, Christos Thrampoulidis, Jef-
frey H Shapiro, Antonio Torralba, Franco NC Wong,
and Gregory W Wornell. Revealing hidden scenes by
photon-efﬁcient occlusion-based opportunistic active
imaging. Optics express, 26(8):9945–9962, 2018. 1

6809

BubbleNets: Learning to Select the Guidance Frame in
Video Object Segmentation by Deep Sorting Frames

Brent A. Grifﬁn
Jason J. Corso
University of Michigan

{griffb,jjcorso}@umich.edu

Abstract

Semi-supervised video object segmentation has made
signiﬁcant progress on real and challenging videos in re-
cent years. The current paradigm for segmentation meth-
ods and benchmark datasets is to segment objects in video
provided a single annotation in the ﬁrst frame. However, we
ﬁnd that segmentation performance across the entire video
varies dramatically when selecting an alternative frame for
annotation. This paper addresses the problem of learn-
ing to suggest the single best frame across the video for
user annotation—this is, in fact, never the ﬁrst frame of
video. We achieve this by introducing BubbleNets, a novel
deep sorting network that learns to select frames using a
performance-based loss function that enables the conver-
sion of expansive amounts of training examples from al-
ready existing datasets. Using BubbleNets, we are able to
achieve an 11% relative improvement in segmentation per-
formance on the DAVIS benchmark without any changes to
the underlying method of segmentation.

1. Introduction

Video object segmentation (VOS), the dense separation
of objects in video from background, remains a hotly stud-
ied area of video understanding. Motivated by the high cost
of densely-annotated user segmentations in video [5, 38],
our community is developing many new VOS methods that
are regularly evaluated on the benchmark datasets support-
ing VOS research [22, 31, 33, 37, 45]. Compared to unsu-
pervised VOS [12, 21, 29, 44], semi-supervised VOS, the
problem of segmenting objects in video given a single user-
annotated frame, has seen rampant advances, even within
just the past year [2, 4, 7, 8, 9, 16, 17, 25, 28, 30, 35, 46].
The location and appearance of objects in video can
change signiﬁcantly from frame-to-frame, and, from our
own analysis, we ﬁnd that using different frames for anno-
tation changes performance dramatically, as shown in Fig-
ure 1. Annotating video data is an arduous process, so it

Frame 1: Baseline

Frame 12: +69%

Frame 20: -53%

Figure 1. The current paradigm for video object segmentation is
to segment an object annotated in the ﬁrst frame of video (yellow,
left). However, selecting a different frame for annotation changes
performance across the entire video [for better (green) or worse
(red)]. To best use an annotator’s time, our deep sorting framework
suggests a frame that will improve segmentation performance.

is critical that we improve performance of semi-supervised
VOS methods by providing the best single annotation frame
possible. However, we are not aware of any work that seeks
to learn which frame to annotate for VOS.
To that end, this paper addresses the problem of select-
ing a single video frame for annotation that will lead to
greater performance. Starting from an untouched video, we
select an annotation frame using our deep bubble sorting
framework, which makes relative performance predictions
between pairs of frames using our custom network, Bub-
bleNets. BubbleNets iteratively compares and swaps adja-
cent video frames until the frame with the greatest predicted
performance is ranked highest, at which point, it is selected
for the user to annotate and use for VOS. To train Bub-
bleNets, we use an innovative relative-performance-based
loss that increases the number of training examples by or-
ders of magnitude without increasing frame labeling re-
quirements. Finally, we evaluate BubbleNets annotation
frame selection on multiple VOS datasets and achieve as
much as an 11% relative improvement in combined Jaccard

18914

measure and region contour accuracy (J +F ) over the same
segmentation method given ﬁrst-frame annotations.
The ﬁrst contribution of our paper is demonstrating the
utility of alternative annotation frame selection strategies
for VOS. The current paradigm is to annotate an object in
the ﬁrst frame of video and then automatically segment that
object in the remaining frames. We provide thorough analy-
sis across four datasets and identify simple frame-selection
strategies that are immediately implementable for all VOS
methods and lead to better performance than ﬁrst-frame se-
lection. To the best of our knowledge, this represents the
ﬁrst critical investigation of segmentation performance for
different annotation frame selection strategies.
The second contribution of our paper is the deep bubble
sorting framework and corresponding implementation that
improves VOS performance. We are not aware of a sin-
gle paper that investigates selection of the annotated frame
in VOS. The necessary innovation for our network-based
approach is our loss formulation, which allows extensive
training on relatively few initial examples. We provide de-
tails on generating application-speciﬁc performance labels
from pre-existing datasets, and our deep sorting formula-
tion is general to all video processes that train on individ-
ual frames and have a measurable performance metric. Us-
ing our custom network architecture and a modiﬁed loss
function inspired by our VOS frame-selection analysis, we
achieve the best frame-selection-based segmentation perfor-
mance across all four evaluation datasets.
We provide source code for the current work at

https://github.com/griffbr/BubbleNets
and a video at https://youtu.be/0kNmm8SBnnU.

2. Related Work

2.1. Video Object Segmentation

Multiple benchmarks are available to evaluate VOS
methods, including: SegTrackv2 [22, 37]; DAVIS 2016,
2017, and 2018 [5, 31, 33]; and YouTube-VOS [45]. Mov-
ing away from the single-object hypothesis of DAVIS 2016,
these datasets are increasingly focused on the segmentation
of multiple objects, which increases the need for a user-
provided annotation to specify each object of interest and
has led to the development of more semi-supervised VOS
methods using an annotated frame. With some exceptions
[1, 13, 27, 32], the majority of semi-supervised VOS meth-
ods use an artiﬁcial neural network.
The amount of training data available for learning-based
VOS methods has increased dramatically with the introduc-
tion of YouTube-VOS, which contains the most annotated
frames of all current VOS benchmarks. However, due to
the high cost of user annotation [5, 38], YouTube-VOS only
provides annotations for every ﬁfth frame. Operating on the
assumption that every frame should be available to the user

for annotation, we obtain training data from, and base the
majority of our analysis from, DAVIS 2017, which contains
the most training and validation examples of all fully an-
notated datasets and has many challenging video categories
(e.g., occlusions, objects leaving view, appearance change,
and multiple interacting objects).
For our BubbleNets implementation that selects anno-
tated frames for VOS, we segment objects using One-Shot
Video Object Segmentation (OSVOS) [4], which is state-
of-the-art in VOS and has inﬂuenced other leading methods
[25, 42]. OSVOS uses a base network trained on ImageNet
[10] to recognize image features, re-trains on DAVIS 2016
to segment objects in video, and then ﬁne-tunes the net-
work for each video using a user-provided annotation. One
unique property of OSVOS is that it does not require tempo-
ral consistency, i.e., the order that OSVOS segments frames
is inconsequential. Conversely, even when segmentation
methods operate sequentially [2, 16, 18, 23, 28, 30, 46], seg-
mentation can propagate forward and backward from anno-
tated frames selected later in a video.

2.2. Active Learning

Active learning (AL) is an area of research enabling
learning algorithms to perform better with less training by
letting them choose their own training data. AL is espe-
cially useful in cases where large portions of data are un-
labeled and manual labeling is expensive [3]. Selecting the
best single annotated frame to train OSVOS represents a
particularly hard problem in AL, starting to learn with no
initial labeled instances, i.e., the cold start problem [26].
Within AL, we are particularly interested in error reduc-
tion. Error reduction is an intuitive sub-ﬁeld that directly
optimizes the objective of interest and produces more accu-
rate learners with fewer labeled instances than uncertainty
or hypothesis-based AL approaches [34]. However, rather
than going through all video frames and then formally pre-
dicting the expected error reduction associated with any one
annotation frame, BubbleNets simpliﬁes the problem by
only comparing the relative performance of two frames at a
time. By combining our decision framework with a bubble
sort, we iterate this selection process across the entire video
and promote the frame with the best relative performance to
be our selected annotation frame.
Within computer vision, previous AL work includes
measures to reduce costs associated with annotating im-
ages and selecting extra training frames after using an initial
set of user annotations. Cost models predicting annotation
times can be learned using a decision-theoretic approach
[38, 40]. Other work has focused on increasing the effec-
tiveness of crowd-sourced annotations [39]. To improve
tracking performance, active structured prediction has been
used to suggest extra training frames after using an initial
set of user annotations [43]. Within VOS, other work in-

8915

creases segmentation accuracy by having a user review seg-
mentations and then add annotations on frames with poor
performance [4]. The DAVIS 2018 challenge includes em-
phasis on maximizing segmentation performance with de-
creased user annotation time [5].
In contrast, we are not
estimating annotation costs or selecting extra annotation
frames. To support all semi-supervised VOS methods with-
out increasing user effort, we are selecting a single frame
for annotation that increases performance.

3. BubbleNets

We
design
an
artiﬁcial
neural
network, Bub-
bleNets (BN),
that
learns to suggest video frames for
annotation that improve video object segmentation (VOS)
performance. To learn performance-based frame selection
on our custom network, we generate our own labeled train-
ing data. Labeled video data are expensive, so we design
our network loss to learn from fewer initial frame labels, as
discussed in Section 3.1. In Section 3.2, we introduce our
deep bubble sorting framework that uses BN performance
predictions to select a single frame for annotation. We
provide details for our BN architecture in Section 3.3. In
Section 3.4, we present our BN implementation for VOS
with complete training and conﬁguration details.

3.1. Predicting Relative Performance

Assume we are given a set of m training videos wherein
each video has n frames with labels corresponding to some
performance metric, y ∈ R, which we leave unspeciﬁed
here but deﬁne in Section 3.4.1. Our goal is to learn to select
the frame with the greatest performance from each video.
One way to accomplish this task is to use the entire
video as input to a network (e.g., using an LSTM or 3D-
ConvNet [6]) and output the frame index with the greatest
predicted performance; however, this approach only has m
labeled training examples. A second way to formulate this
problem is to use individual frames as input to a network
and output the predicted performance of each frame. Us-
ing this formulation, the frame with the maximum predicted
performance can be selected from each video and there are
m × n labeled training examples. While this is a signiﬁ-
cant improvement over m examples, the second formulation
only provides one training example per frame, which, for
complicated and high annotation-cost processes like video
object segmentation, makes the task of generating enough
data to train a performance-prediction network impractical.
To that end, instead of directly estimating the predicted
performance y of each training frame, BN predicts the rel-
ative difference in performance of two frames being com-
pared (i.e., yi − yj for frames i and j from the same video).
This difference may seem trivial, but it effectively increases
the number of labels and training examples from m × n to
2 .

2 (cid:1) ≈ mn2

m × (cid:0)n

To further increase the number of unique training exam-
ples and increase BN’s accuracy, we use k random video
reference frames as an additional network input. When pre-
dicting the relative performance between two frames, ad-
ditional consideration can be given to the frame that bet-
ter represents the reference frames. Thus, similar to archi-
tectures that process entire videos, reference frames pro-
vide some context for the video as a whole. We ﬁnd that
reference frames not only increase BN’s accuracy in prac-
tice but also increase the number of training examples from
2 (cid:1) to m × (cid:0) n
.
Finally, we deﬁne our performance loss function as:

k+2(cid:1) ≈ mn(k+2)
k+2

m × (cid:0)n

L(W) := |(yi − yj ) − f (xi , xj , Xref. , W)| ,

(1)

where W are the trainable parameters of BN, yi is the per-
formance label associated with the ith video frame, xi is
the image and normalized frame index associated with the
ith video frame, Xref. is the set of k reference images and
frame indices, and f is the predicted relative performance.
For later use, denote the normalized frame index for the ith
frame of an n-frame video as

Ii =

i
n

.

(2)

Including I as an input enables BN to also consider tempo-
ral proximity of frames for predicting performance.

3.2. Deep Bubble Sorting

Assume we train BubbleNets to predict the relative per-
formance difference of two frames using the loss function
(1) from Section 3.1. To select the frame with the great-
est performance from a video, we use BN’s relative perfor-
mance predictions within a deep bubble sorting framework,
iteratively comparing and swapping adjacent frames until
we identify the frame with the greatest predicted relative
(and overall) performance.
Our deep bubble sorting framework begins by comparing
the ﬁrst two video frames. If BN predicts that the preceding
frame has greater relative performance, the order of the two
frames is swapped. Next, the leading frame is compared
(and potentially swapped) with the next adjacent frame, and
this process passes forward until reaching the end of the
video (see Figure 2). The frame ranked highest at the end of
the sort is selected as the predicted best-performing frame.
Normally, bubble sort is deterministic and only needs
one pass through a list to promote the greatest element to
the top; conversely, our deep bubble sorting framework is
stochastic. BN uses k random video reference frames as in-
put for each prediction, and using a different set of reference
frames can change that prediction; thus, a BN comparison
for the same two frames can change. While bubble sort’s re-
dundancy is sub-optimal relative to other comparison sorts
in many applications [20], revisiting previous comparisons

8916

Network 
Input

ResNet
Preprocessing

Performance Prediction 
Layers

Video 
Frame i

ResNet
50

2048

Network 
Output

Best Possible Sort

Video 
Frame j

ResNet
50

2048

Fully 
Connected 
256

FC 
128

FC 
64

FC 
32

f

Video
Frame i
Frame i
Reference
Frames

ResNet
ResNet
ResNet
50
50
50

2048
2048
2048

Input Frame Indices

Compare Frames using Network

Swap if          > 0

f

Video
Frame 2

Video
Frame 1

…

Video
Frame i

Video
Frame j

…

Video
Frame n

BubbleNets Sort

F

+

J

1.0

0.5

0.0

1

10

20

30

40

Initial Video Frame Indices

Figure 2. BubbleNets Framework: Deep sorting compares and
swaps adjacent frames using their predicted relative performance.

Figure 3. BubbleNets Prediction Sort of Motorbike Video. The
green bar is the annotated training frame selected by BubbleNets.

is particularly effective given BN’s stochastic nature. Ac-
cordingly, our deep bubble sorting framework makes n for-
ward passes for an n-frame video, which is sufﬁcient for
a complete frame sort and increases the likelihood that the
best-performing frame is promoted to the top.
One way to increase BN’s consistency is to batch each
network prediction over multiple sets of video reference
frames. By summing the predicted relative performance
over the entire batch, we reduce the variability of each
frame comparison. However, two consequences of increas-
ing batch size are: 1) increasing the chance of hitting a local
minimum (i.e., some frame pairs are ordered incorrectly but
never change) and 2) increasing execution time.
In Sec-
tion 4, we perform an ablation study to determine the best
batch size for our speciﬁc application.
Although BN is not explicitly trained to ﬁnd the best-
performing frame in a video, our complete deep bubble sort-
ing framework is able to accomplish this task, as shown in
Figure 3. Even in cases where the best performing frames
are not promoted to the top, an important secondary effect
of our deep sorting framework is demoting frames that lead
to poorer performance (e.g., Frame 20 in Figure 1); avoid-
ing such frames is critical for annotation frame selection in
video object segmentation.

3.3. BubbleNets Architecture

Our BubbleNets architecture is shown in Figure 2. The
input has two comparison images, three reference images,
and normalized indices (2) for all ﬁve frames.
Increas-
ing the number of reference frames, k , increases video-
wide awareness for predicting relative frame performance
but also increases network complexity;
in practice, we
ﬁnd that k = 3 is a good compromise. The input im-
ages are processed using a base Residual Neural Network

(ResNet 50, [15]) that is pre-trained on ImageNet, which
has been shown to be a good initialization for segmenta-
tion [9] and other video tasks [47]. Frame indices and
ResNet features are fed into BN’s performance prediction
layers, which consist of four fully-connected layers with
decreasing numbers of neurons per layer. All performance
prediction layers include the normalized frame indices as
input and use a Leaky ReLU activation function [24]; the
later three prediction layers have 20% dropout for all in-
puts during training [36]. After the performance prediction
layers, our BN architecture ends with one last fully con-
nected neuron that is the output relative performance pre-
diction f (xi , xj , Xref. , W) ∈ R in (1).

3.4. BubbleNets Implementation for
Video Object Segmentation

Assume a user wants to segment an object in video and
provides an annotation of that object in a single frame. Be-
cause annotating video data is time consuming, we use Bub-
bleNets and deep sorting to automatically select the annota-
tion frame for the user that results in the best segmentation
performance possible. We segment objects from the anno-
tated frame in the remainder of the video using One-Shot
Video Object Segmentation (OSVOS) [4].

3.4.1 Generating Performance Labels for Training

Generating performance-based labels to train BN requires
a quantitative measure of performance that is measurable
on any given video frame. For our VOS performance mea-
sure, we choose a combination of region similarity J and
contour accuracy F . Region similarity (also known as in-
tersection over union or Jaccard index [11]) provides an in-
tuitive, scale-invariant evaluation for the number of misla-

8917

beled foreground pixels with respect to a ground truth an-
notation. Given a foreground mask M and ground truth
annotation G, J = M

Table 2. BubbleNets Conﬁgurations.

Input
Conﬁg. Frame
Ref.
Loss
Total Training
ID
Indices Frames Function Iterations Time
BN0
Yes
Yes
L (1) 3,125
5m 11s 59.7
No
Yes
L (1) 2,500
3m 52s 58.7
Yes
Yes
LF (5) 8,125 15m 30s 57.8
Yes
No
L (1) 3,125
2m 20s 55.4
Yes
Yes
LSP (4) 1,875
2m 32s 55.1

BNNRF
BNLSP

BNNIFI

BNLF

DAVIS 2017
Val. Mean

J

F

65.5
65.0
63.8
62.3
62.3

Table 3. Ablation Study on DAVIS 2017 Val. Set: Study of BN
input batch size for bubble sort comparisons and end performance.

Batch
Size
1
3
5
10
20

BNNIFI

Performance (J + F )
BNLF
120.5
121.6
121.7
120.3
120.7

122.9
122.0
123.8
122.0
123.4

BN0
124.1
125.2
125.2
125.2
123.6

Mean Video
Sort Time
3.88 s
4.83 s
5.32 s
6.52 s
9.34 s

decrease training time, all DAVIS 2017 training frames are
preprocessed through the ResNet portion of the architec-
ture, which does not change during BN training. We use
a batch size of 1,024 randomly selected videos; each video
uses up to ﬁve frames that are randomly selected without re-
placement (e.g., two comparison and three reference). We
add an L1 weight regularization loss with a coefﬁcient of
2 × 10−6 , and use the Adam Optimizer [19] with a 1 × 10−3
learning rate. The number of training iterations and training
time for each conﬁguration is summarized in Table 2.
We evaluate all models using the original bubble sort-
ing framework, although BNLSP requires two forward net-
work passes per sort comparison and BNNRF is deterministic
without the random reference frames. Tasked with learning
frame-based loss and frame performance differences, BNLF
requires the most training iterations of all BN networks.
BNLSP trains in fewer iterations due to simpliﬁed loss, and
both BNLSP and BNNRF train faster due to fewer input im-
ages. As shown in Table 2, the BN0 model outperforms
BNLSP and BNNRF , justifying our claims in Section 3.1 for
using relative frame performance and reference frames.

4. Experimental Results

4.1. Setup

Our primary experiments and analysis use the DAVIS
2017 Validation set. As with the training set in Sec-
tion 3.4.1, we ﬁnd the segmentation performance for every
possible annotated frame, which enables us to do a complete
analysis that includes the best and worst possible frame se-
lections and simple frame selection strategies. We deter-
mine the effectiveness of each frame selection strategy by
calculating the mean J + F for the resulting segmentations
on the entire dataset; the mean is calculated on a per video-
object basis (e.g., a video with two annotated objects will
contribute to the mean twice). Best and worst frame se-
lections are determined using the combined J + F score
for each video object. The simple frame selection strategies
are selecting the ﬁrst frame (current VOS standard), mid-
dle frame (found using ﬂoor division of video length), last
frame, and a random frame from each video for each object.
Finally, because BN results can vary from using random ref-
erence frames as input, we only use results from the ﬁrst run
of each conﬁguration (same with random frame selection).

4.2. Ablation Study

We perform an ablation study to determine the best
batch size for BN predictions. Recall from Section 3.2 that
batches reduce variability by using multiple sets of random
reference frames. As shown in Table 3, a batch size of 5
leads to the best performance for all BN conﬁgurations and
is chosen as the standard setting for all remaining results.
The mean video sort times in Table 3 are for BNLF , which
consistently has the highest sort times. As a practical con-
sideration, we emphasize that the frame selection times in
Table 3 are negligible compared to the time it takes a user
to annotate a frame [5].

4.3. DAVIS Validation

Complete annotated frame selection results for the
DAVIS 2016 and 2017 validation sets are provided in Ta-
ble 4. To put these results in perspective, the current dif-
ference in J + F for the two leading VOS methods on the
DAVIS 2016 Val. benchmark is 2.1 [25, 42].
For ﬁrst frame selection, it is worth acknowledging that
both datasets intend for annotation to take place on the ﬁrst
frame, which guarantees that objects are visible for anno-
tation (in some videos, objects become occluded or leave
the view). Despite this advantage, middle frame selection
outperforms ﬁrst frame selection on both datasets overall
and on 3/5 of the videos on DAVIS 2017 Val. In fact, on
both datasets ﬁrst frame selection is, on average, closer to
the worst possible frame selection than the best. Last frame
selection has the worst performance and, using the coefﬁ-
cient of variation, the most variable relative performance.
Finally, the best performing annotation frame is never the
ﬁrst or last frame for any DAVIS validation video.
Middle frame selection has the best performance of all
simple strategies. We believe that the intuition for this is
simple. Because the middle frame has the least cumulative
temporal distance from all other frames, it is on average
more representative of the other frames with respect to an-
notated object positions and poses. Thus, the middle frame
is, on average, the best performing frame for segmentation.
All BN conﬁgurations outperform the simple selection
strategies, and BN0 performs best of all BN conﬁgurations.
When selecting different frames, BN0 beats middle frame
selection on 3/5 videos and ﬁrst frame selection on 4/5
videos for DAVIS 2017 Val. By comparing the performance

8919

Table 4. Dataset Annotated Frame Selection Results.

Table 5. Results on Datasets with Limited Frames Per Video.

Segmentation Performance (J + F )
Coef. of
Variation

Range

Segmentation Performance (J + F )
Coef. of
Variation

Range

Mean

141.2
125.2
123.8
121.7
119.2
116.5
113.3
104.7
86.3

171.2
159.8
157.3
155.6
155.2
152.8
147.5
147.5
127.7

Annotation
Frame
Selection

Best
BN0

BNNIFI

BNLF
Middle
Random
First
Last
Worst

Best
BN0

BNNIFI

BNLF
Middle
First
Random
Last
Worst

y

c

n

e

u
q

e

r

F

20

15

10

5

0

Median
DAVIS 2017 Val.
143.2
14.9–194.9
128.9
7.6–194.2
129.9
8.7–194.2
128.0
7.6–194.3
124.0
7.6–193.6
119.7
1.6–193.2
117.2
3.5–192.5
110.3
4.4–190.1
88.2
1.6–188.9
DAVIS 2016 Val.
176.3
130.6–194.9
168.5
72.6–194.5
165.7
72.6–194.5
170.5
72.6–193.8
169.5
77.1–193.8
153.4
115.2–191.7
157.3
83.1–194.5
153.0
72.0–189.6
141.3
68.3–188.9

0.26
0.34
0.35
0.38
0.41
0.38
0.39
0.42
0.56

0.11
0.18
0.18
0.21
0.21
0.15
0.25
0.23
0.31

BNLF

BNNIFI

BN0

Annotation
Frame
Selection

BNLF
Middle

BNNIFI

BN0
Last
First

BNLF
Middle

BNNIFI

BN0
First
Last

Mean

134.7
134.5
134.3
130.6
123.6
122.3

115.5
115.0
111.8
110.4
107.3
101.2

Median
SegTrackv2
145.9
14.3–184.6
143.5
14.3–182.8
144.2
33.9–178.5
127.3
50.0–183.2
130.4
14.3–178.4
122.5
45.8–181.7
YT-VOS (1st 1,000)
126.6
0.0–197.3
124.2
0.0–196.2
121.0
0.0–196.3
121.5
0.0–194.1
114.0
0.0–196.3
108.1
0.0–195.4

0.32
0.32
0.30
0.30
0.36
0.31

0.46
0.46
0.47
0.49
0.49
0.56

Table 6. Cross Evaluation of Benchmark Methods: OSVOS and
OnAVOS DAVIS ‘17 Val. results using identical frame selections.

Segmentation
Method
OSVOS
OnAVOS

Frame Selection and DAVIS J & F Mean
First
Middle
BNLF
BN0
56.6
59.6
60.8
62.6
63.9
68.4
68.5
69.2

61.9
68.4

BNNIFI

Table 7. Frames Per Video and Relative Performance: BN per-
formances relative to ﬁrst frame on DAVIS 2017 Validation.

Videos from
DAVIS 2017 Val.
10 Longest
All
10 Shortest

Number of
Frames
81–104
34–104
34–43

BNNIFI

Relative Mean (J + F )
BN0
BNLF
+ 11.8% + 10.9% + 4.0%
+ 10.5% + 9.3% + 7.4%
+ 4.9%
+ 5.0%
+ 3.3%

Figure 6. BNLF –Middle Frame Comparison: Two best (left) and
worst (right) BNLF selections relative to the middle frame.

index-based selections made this conﬁguration more robust
to reductions in candidate annotation frames.

4.5. Results on Different Segmentation Methods

Cross evaluation results for different segmentation meth-
ods are provided in Table 6. All BN conﬁgurations se-
lect annotation frames that improve the performance of
OnAVOS, despite BN training exclusively on OSVOS-
generated labels. Nonetheless, the label-generating formu-
lation in Section 3.4.1 is general to other semi-supervised
VOS methods; thus, new BN training labels can always be
generated for other methods. Note that ﬁrst frame results
in Table 6 differ from the online benchmark due to dataset-
speciﬁc conﬁgurations (e.g., [41]), non-deterministic com-
ponents, and our segmenting and evaluating objects from
multi-object videos separately, which is more challenging.

8920

0

0.25

0.5

0.75

Normalized Frame Index (I )

1

Figure 5. Frame-Selection Locations in Video: Normalized in-
dices (2) of all BN annotation frame selections on DAVIS ‘17 Val.

of BN0 and BNNIFI , we ﬁnd that BN0 ’s use of normalized
frame indices (2) is beneﬁcial for performance.
Finally, it is clear from the frame-selection locations in
Figure 5 that BNLF ’s modiﬁed loss function (5) successfully
biases selections toward the middle of each video.

4.4. Results on Datasets with Limited Frames

Annotated frame selection results for SegTrackv2 and
YouTube-VOS are provided in Table 5. As emphasized in
Section 3.4.1, the videos in these datasets have a limited
number of frames available for annotation, which limits the
effectiveness of BN frame selection. Because the YouTube-
VOS validation set only provides annotations on the ﬁrst
frame, we instead evaluate on the ﬁrst 1,000 objects of the
YouTube-VOS training set, which provides annotations on
every ﬁfth frame. This reduces the number of candidate
annotation frames that BN can compare, sort, and select
to one ﬁfth of that available in a standard application for
the same videos. While all BN conﬁgurations outperform
ﬁrst and last frame selection, BNLF is the only conﬁgura-
tion that consistently outperforms all other selection strate-
gies. We postulate that the additional bias of BNLF toward

Figure 7. Qualitative Comparison on DAVIS 2017 Validation Set: Segmentations from different annotated frame selection strategies.

4.6. Final Considerations for Implementation

Selecting the middle frame for annotation is the best per-
forming simple selection strategy on all datasets and is easy
to implement in practice. However, BNLF is more reliable
than middle frame selection and results in better segmen-
tation performance on all datasets. As shown in Figure 5,
BNLF selects frames close to the middle of each video, but
deviates toward frames that, on average, result in better per-
formance than the middle frame (see Tables 4 & 5). On
DAVIS 2017 Val., BNLF deviations from the middle frame
results in better performance 70% of the time. We believe
the underlying mechanism for this improvement is recog-
nizing when the middle frame exhibits less distinguishable
ResNet features or is less representative of the video refer-
ence frames. To demonstrate beneﬁcial and counterproduc-
tive examples of this behavior, the two best and worst BNLF
selections relative to the middle frame on DAVIS 2017 Val.
are shown with relative performance % in Figure 6.

BN0 has the greatest relative segmentation improve-
ments over simple selection strategies on the DAVIS valida-
tion datasets (see example comparison in Figure 7). How-
ever, this performance did not translate to datasets with a
limited number of annotation frames available. To deter-
mine if this is due to domain shift of fewer frames, we an-
alyze the 10 longest and shortest videos from DAVIS 2017
Val. in Table 7 as an additional experiment. The key result
is that BN0 and BNNIFI ’s relative performance gains double
once approximately forty annotation frames are available.
This is encouraging as most real-world videos have many

more frames available for annotation, which is conducive
for BN0 ’s best annotated frame selection results.

5. Conclusions

We emphasize that automatic selection of the best-
performing annotation frames for video object segmenta-
tion is a hard problem. Still, as video object segmentation
methods become more learning-based and data-driven, it is
critical that we make the most of training data and users’
time for annotation. The most recent DAVIS challenge
has shifted focus toward improving performance given lim-
ited annotation feedback [5]. However, we demonstrate
in this work that there are already simple strategies avail-
able that offer a signiﬁcant performance improvement over
ﬁrst frame annotations without increasing user effort; like-
wise, our BubbleNets framework further improves perfor-
mance using learned annotated frame selection. To continue
progress in this direction and improve video object segmen-
tation algorithms in practice, dataset annotators should give
full consideration to alternative frame selection strategies
when preparing future challenges.
Finally, while the current BubbleNets implementation is
speciﬁc to video object segmentation, it is more widely ap-
plicable. In future work, we plan to apply BubbleNets to
improve performance in other video-based applications.

Acknowledgements

This work was partially supported by the DARPA MediFor
program under contract FA8750-16-C-0168.

8921

References

[1] S. Avinash Ramakanth and R. Venkatesh Babu. Seamseg:
Video object segmentation using patch seams. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2014. 2

[2] L. Bao, B. Wu, and W. Liu. CNN in MRF: video object seg-
mentation via inference in A cnn-based higher-order spatio-
temporal MRF. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 1, 2

[3] J. Bernard, M. Hutter, M. Zeppelzauer, D. Fellner, and
M. Sedlmair. Comparing visual-interactive labeling with ac-
tive learning: An experimental study. IEEE Transactions on
Visualization and Computer Graphics, 24(1):298–308, Jan
2018. 2

[4] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix ´e,
D. Cremers, and L. Van Gool. One-shot video object seg-
mentation.
In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017. 1, 2, 3, 4

[5] S. Caelles, A. Montes, K. Maninis, Y. Chen, L. V. Gool,
F. Perazzi, and J. Pont-Tuset. The 2018 DAVIS challenge on
video object segmentation. CoRR, abs/1803.00557, 2018. 1,
2, 3, 6, 8

[6] J. Carreira and A. Zisserman. Quo vadis, action recognition?
a new model and the kinetics dataset. In 2017 IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017. 3

[7] Y. Chen, J. Pont-Tuset, A. Montes, and L. Van Gool. Blaz-
ingly fast video object segmentation with pixel-wise met-
ric learning.
In Computer Vision and Pattern Recognition
(CVPR), 2018. 1

[8] J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang.
Fast and accurate online video object segmentation via track-
ing parts. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2018. 1

[9] J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang. Segﬂow:
Joint learning for video object segmentation and optical
ﬂow. In IEEE International Conference on Computer Vision
(ICCV), 2017. 1, 4

[10] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2009. 2

[11] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (VOC) chal-
lenge. International journal of computer vision, 88(2):303–
338, 2010. 4

[12] A. Faktor and M. Irani. Video segmentation by non-local
consensus voting.
In British Machine Vision Conference
(BMVC), 2014. 1

[13] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen.
Jumpcut: non-successive mask transfer and interpolation for
video cutout. ACM Trans. Graph., 34(6):195, 2015. 2

[14] B. A. Grifﬁn and J. J. Corso. Tukey-inspired video object
segmentation. In IEEE Winter Conference on Applications
of Computer Vision (WACV), 2019. 5

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016. 4
[16] V. Jampani, R. Gadde, and P. V. Gehler. Video propagation
networks. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2017. 1, 2
[17] W. D. Jang and C. S. Kim. Online video object segmentation
via convolutional trident network. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
7474–7483, July 2017. 1
[18] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele.
Lucid data dreaming for object tracking. The 2017 DAVIS
Challenge on Video Object Segmentation - CVPR Work-
shops, 2017. 2
[19] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In International Conference on Learning Rep-
resentations (ICLR), 2014. 6
[20] D. Knuth. The Art of Computer Programming, volume 1-
3. Addison-Wesley Longman Publishing Co., Inc., Boston,
MA, USA, 1998. 3
[21] Y. J. Lee, J. Kim, and K. Grauman. Key-segments for video
object segmentation. In IEEE International Conference on
Computer Vision (ICCV), 2011. 1
[22] F. Li, T. Kim, A. Humayun, D. Tsai, and J. M. Rehg.
Video segmentation by tracking many ﬁgure-ground seg-
ments. In The IEEE International Conference on Computer
Vision (ICCV). 1, 2, 5
[23] X. Li, Y. Qi, Z. Wang, K. Chen, Z. Liu, J. Shi, P. Luo,
C. C. Loy, and X. Tang. Video object segmentation with re-
identiﬁcation. The 2017 DAVIS Challenge on Video Object
Segmentation - CVPR Workshops, 2017. 2
[24] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer nonlin-
earities improve neural network acoustic models. In ICML
Workshop on Deep Learning for Audio, Speech and Lan-
guage Processing, 2013. 4
[25] K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal-Taix,
D. Cremers, and L. V. Gool. Video object segmentation
without temporal information.
IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, pages 1–1, 2018. 1,
2, 6
[26] A. McCallum and K. Nigam. Employing EM and pool-based
active learning for text classiﬁcation.
In In International
Conference on Machine Learning (ICML), 1998. 2
[27] N. Mrki, F. Perazzi, O. Wang, and A. Sorkine-Hornung. Bi-
lateral space video segmentation. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
743–751, June 2016. 2
[28] S. W. Oh, J.-Y. Lee, K. Sunkavalli, and S. J. Kim. Fast video
object segmentation by reference-guided mask propagation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2018. 1, 2
[29] A. Papazoglou and V. Ferrari. Fast object segmentation in
unconstrained video.
In Proceedings of the IEEE Interna-
tional Conference on Computer Vision (ICCV), 2013. 1
[30] F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and
A.Sorkine-Hornung. Learning video object segmentation
from static images. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 1, 2

8922

[31] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmentation.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016. 1, 2, 5
[32] F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung.
Fully connected object proposals for video segmentation. In
IEEE International Conference on Computer Vision (ICCV),
2015. 2
[33] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel ´aez, A. Sorkine-
Hornung, and L. Van Gool. The 2017 davis challenge on
video object segmentation. arXiv:1704.00675, 2017. 1, 2, 5
[34] B. Settles. Active learning. Synthesis Lectures on Artiﬁcial
Intelligence and Machine Learning, 6(1):1–114, 2012. 2
[35] J. Shin Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and
I. So Kweon. Pixel-level matching for video object segmen-
tation using convolutional neural networks. In IEEE Inter-
national Conference on Computer Vision (ICCV), 2017. 1
[36] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research, 15:1929–1958, 2014. 4
[37] D. Tsai, M. Flagg, A. Nakazawa, and J. M. Rehg. Motion
coherent tracking using multi-label mrf optimization. Inter-
national journal of computer vision, 100(2):190–202, 2012.
1, 2, 5
[38] S. Vijayanarasimhan and K. Grauman. What’s it going to
cost you?: Predicting effort vs. informativeness for multi-
label image annotations. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2009. 1, 2
[39] S. Vijayanarasimhan and K. Grauman. Large-scale live ac-
tive learning: Training object detectors with crawled data

and crowds.
International Journal of Computer Vision,
108(1):97–114, May 2014. 2

[40] S. Vijayanarasimhan, P. Jain, and K. Grauman. Far-sighted
active learning on a budget for image and video recognition.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2010. 2

[41] P. Voigtlaender and B. Leibe. Online adaptation of convolu-
tional neural networks for the 2017 davis challenge on video
object segmentation. The 2017 DAVIS Challenge on Video
Object Segmentation - CVPR Workshops, 2017. 7

[42] P. Voigtlaender and B. Leibe. Online adaptation of convo-
lutional neural networks for video object segmentation. In
British Machine Vision Conference (BMVC), 2017. 2, 6

[43] C. Vondrick and D. Ramanan. Video annotation and tracking
with active learning. In J. Shawe-Taylor, R. S. Zemel, P. L.
Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 24, pages 28–36.
Curran Associates, Inc., 2011. 2

[44] S. Wehrwein and R. Szeliski. Video segmentation with back-
ground motion models. In British Machine Vision Confer-
ence (BMVC), 2017. 1

[45] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and
T. Huang. Youtube-vos: A large-scale video object segmen-
tation benchmark. arXiv preprint arXiv:1809.03327, 2018.
1, 2, 5
[46] L. Yang, Y. Wang, X. Xiong, J. Yang, and A. K. Katsaggelos.
Efﬁcient video object segmentation via network modulation.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2

[47] L. Zhou, C. Xu, and J. Corso. Towards automatic learning of
procedures from web instructional videos. In AAAI Confer-
ence on Artiﬁcial Intelligence, 2018. 4

8923

Cascaded Projection: End-to-End Network Compression and Acceleration

Breton Minnehan
Rochester Institute of Technology

Andreas Savakis
Rochester Institute of Technology

blm2144@rit.edu

andreas.savakis@rit.edu

Abstract

We propose a data-driven approach for deep convolu-
tional neural network compression that achieves high accu-
racy with high throughput and low memory requirements.
Current network compression methods either ﬁnd a low-
rank factorization of the features that requires more mem-
ory, or select only a subset of features by pruning entire
ﬁlter channels. We propose the Cascaded Projection (CaP)
compression method that projects the output and input ﬁl-
ter channels of successive layers to a uniﬁed low dimen-
sional space based on a low-rank projection. We optimize
the projection to minimize classiﬁcation loss and the dif-
ference between the next layer’s features in the compressed
and uncompressed networks. To solve this non-convex opti-
mization problem we propose a new optimization method
of a proxy matrix using backpropagation and Stochastic
Gradient Descent (SGD) with geometric constraints. Our
cascaded projection approach leads to improvements in all
critical areas of network compression: high accuracy, low
memory consumption, low parameter count and high pro-
cessing speed. The proposed CaP method demonstrates
state-of-the-art results compressing VGG16 and ResNet
networks with over 4× reduction in the number of compu-
tations and excellent performance in top-5 accuracy on the
ImageNet dataset before and after ﬁne-tuning.

1. Introduction

The compression of deep neural networks is gaining
attention due to the effectiveness of deep networks and
their potential applications on mobile and embedded de-
vices. The powerful deep networks developed today are
often overparameterized [9] and require large amounts of
memory and computational resources [3]. Thus, efﬁcient
network compression, that reduces the number of computa-
tions and memory required to process images, enables the
broader application of deep neural networks.
Methods for network compression can be categorized
into four types, based on quantization, sparsiﬁcation, fac-
torization and pruning. In this work we leverage the advan-

Figure 1. Visual representation of network compression methods
on a single CNN layer. Top row: Factorization compression with
a reprojection step that increases memory. Middle row: Pruning
compression where individual ﬁlters are removed. Bottom row:
Proposed CaP method which forms linear combinations of the ﬁl-
ters without requiring reprojection.

tages of factorization and pruning methods, as they are the
most popular. Quantization methods accelerate deep net-
works and reduce storage by using mixed precision arith-
metic and hashing codes [4, 6, 13]. However most of them
require mixed precision arithmetic, which is not always
available on standard hardware. Sparsiﬁcation methods
eliminate individual connections between nodes that have
minimal impact on the network, however, they are not well
suited for current applications because most neural network
libraries are not optimized for sparse convolution operations
and fail to achieve signiﬁcant speedup.

Factorization methods [10, 29, 33, 55] reduce computa-
tions by factorizing the network kernels, often by splitting
large kernels into a series of convolutions with smaller ﬁl-
ters. These methods have the drawback of increasing mem-
ory consumption due to the intermediate convolution op-
erations. Such memory requirements pose a problem for
mobile applications, where network acceleration is needed
most. Pruning methods [13, 19, 35, 37, 39, 44, 54, 56] com-
press layers of a network by removing entire convolutional

10715

ﬁlters and the corresponding channels in the ﬁlters of the
next layer. They do not require feature map reprojection,
however they discard a large amount of information when
eliminating entire ﬁlter channels.
In this paper, we propose the Cascaded Projection (CaP)
compression method which combines the superior recon-
struction ability of factorization methods with the multi-
layer cascaded compression of pruning methods.
Instead
of selecting a subset of features, as is done in pruning meth-
ods, CaP forms linear combinations of the original features
that retain more information. However, unlike factorization
methods, CaP brings the kernels in the next layer to low di-
mensional feature space and therefore does not require ad-
ditional memory for reprojection.
Figure 1 provides a visual representation of the differ-
ences between the three methods: factorization (top row)
reprojects to higher dimensional space and increases mem-
ory, pruning (middle row) masks ﬁlters and eliminates their
channels, and our proposed CaP methods (bottom row)
combines ﬁlters to a smaller number without reprojecting.
Our results demonstrate that by forming ﬁlters based on lin-
ear combinations instead of pruning with a mask, more in-
formation is kept in the ﬁltering operations and better net-
work classiﬁcation accuracy is achieved. The primary con-
tributions of this paper are the following:
1. We propose the CaP compression method that ﬁnds a
low dimensional projection of the feature kernels and
cascades the projection to compress the input channels
of the kernels in the next layers.
2. We introduce proxy matrix projection backpropaga-
tion, the ﬁrst method to optimize the compression pro-
jection for each layer using end-to-end training with
standard backpropagation and stochastic gradient de-
scent.
3. Our optimization method allows us to use a new loss
function that combines the reconstruction loss with
classiﬁcation loss to ﬁnd a better solution.
4. The CaP method is the ﬁrst to simultaneously optimize
the compression projection for all layers of residual
networks.
5. Our results illustrate that CaP compressed networks
achieve state-of-the-art accuracy while reducing the
network’s number of parameters, computational load
and memory consumption.

2. Related Work

The goal of network compression and acceleration is
to reduce the number of parameters and computations
performed in deep networks without sacriﬁcing accuracy.
Early work in network pruning dates back to the 1990’s
[14]. However, the area did not gain much interest until
deep convolutional networks became common [31, 32, 43]
and the redundancy of network parameters became apparent

[9]. Recent works aim to develop smaller network architec-
tures that require fewer resources [20, 25, 42].
Quantization techniques [4, 6, 13, 28] use integer or
mixed precision arithmetic only available on state-of-the-
art GPUs [38]. These methods reduce the computation time
and the amount of storage required for the network param-
eters. They can be applied in addition to other methods
to further accelerate compressed networks, as was done in
[30].
Network sparsiﬁcation [36], sometimes referred to as un-
structured pruning, reduces the number of connections in
deep networks by imposing sparsity constraints. The work
in [21] proposed recasting the sparsiﬁed network into sepa-
rate groups of operations where the ﬁlters in each layer are
only connected to a subset of the input channels. In [52]
k-means clustering is used to encourage similarity between
features to aid in compression. However, these methods re-
quire training the network from scratch which is not practi-
cal or efﬁcient.
Filter factorization methods reduce computations at the
cost of increased memory load for storing intermediate fea-
ture maps. Initial works focused on factorizing the three-
dimensional convolutional kernels into three separable one-
dimensional ﬁlters [10, 29]. In [33] CP-decomposition is
used to decompose the convolutional layers into ﬁve lay-
ers with lower complexity. More recently [55] performed a
channel decomposition that found a projection of the con-
volutional ﬁlters in each layer such that the asymmetric re-
projection error was minimized.
Channel pruning methods [35, 37, 39, 44, 56] remove
entire feature kernels for network compression. In [13] ker-
nels are pruned based on their magnitudes, under the as-
sumption that kernels with low magnitudes provide little in-
formation to the network. Li et al. [35] suggested a similar
pruning technique based on kernel statistics. He et al. [19]
proposed pruning ﬁlters based on minimizing the recon-
struction error of each layer. Luo et al. [37] further extended
the concepts in [19] to prune ﬁlters that have minimal im-
pact on the reconstruction of the next layer. Yu et al. [54]
proposed Neuron Importance Score Propagation (NISP) to
calculate the importance of each neuron based on its contri-
bution to the ﬁnal feature representation and prune feature
channels that provide minimal information to the ﬁnal fea-
ture representation.
Other recent works have focused less on ﬁnding the op-
timal set of features to prune and more on ﬁnding the op-
timal amount of features to remove from each layer of the
network. This is important to study because the amount of
pruning performed in each layer is often set arbitrarily or
through extensive experimentation. In [53, 54] the authors
propose automatic pruning architecture methods based on
statistical measures.
In [18, 24] methods are proposed
which use reinforcement learning to learn an optimal net-

10716

work compression architecture. Additional work has been
done to reduce the number of parameters in the ﬁnal lay-
ers of deep networks [5], however the fully connected layer
only contributes a small fraction of the overall computa-
tions.

3. Cascaded Projection Methodology

In this section we provide an in depth discussion of the
CaP compression and acceleration method. We ﬁrst intro-
duce projection compression when applied to a single layer,
and explain the relationship between CaP and previous ﬁlter
factorization methods [55]. One of the main goals of CaP
compression is eliminating the feature reprojection step per-
formed in factorization methods. To accomplish this, CaP
extends the compression in the present layer to the inputs
of the kernels in the next layer by projecting them to the
same low dimensional space, as shown in Figure 2. Next
we demonstrate that, with a few alterations, the CaP com-
pression method can perform simultaneous optimization of
the projections for all of the layers in residual networks [15].
Lastly we present the core component of the CaP method,
which is our new end-to-end optimization method that op-
timizes the layer compression projections using standard
back-propagation and stochastic gradient descent.

3.1. Problem Deﬁnition

In a convolutional network, as illustrated in the top row
of Fig. 2, the ith layer takes as input a 4-Tensor Ii of dimen-
sion (n×ci×hi×wi ), where n is the number of images (mini-
batch size) input into the network, ci is the number channels
in the input and wi and hi are the height and width of the
input. The input is convolved with a set of ﬁlters Wi repre-
sented as a 4-Tensor with dimensions (ci+1×ci×k×k), where
ci+1 is the number of kernels and k is the spatial dimensions
of the kernels, generally 3 pixels. In many networks, there
is an additional bias, bi , of dimension (ci+1 × 1 × 1 × 1),
that is added to each channel of the output. More formally,
the convolution operation for layer i of a CNN is given as:

Oi = Ii ∗ Wi + bi

(1)

where (∗) is the convolution operator. The input to the next
layer is calculated by applying a nonlinearity to the output
as Ii+1 = G(Oi ), where G(·) is often a ReLU [40].
Network compression aims to reduce the number of ﬁl-
ters so that the classiﬁcation accuracy of the network is min-
imally impacted. In this work we ﬁnd a projection Pi that
maps the features to a lower dimensional space by minimiz-
ing both the reconstruction error and the classiﬁcation loss,
as described in the rest of this section.

3.2. Single Layer Projection Compression

We ﬁrst present how projection based compression is
used to compress a single layer of a network. To com-

Figure 2. Visual representation of the compression of a CNN layer
using the CaP method to compress the ﬁlters Wi and Wi+1 in the
current and next layers using projections Pi and PT
respectively.
The reconstruction error in the next layer is computed after the
nonlinearity G(·).

i

press layer i, the output features are projected to low di-
mensional representation of rank r using an orthonormal
projection matrix Pi represented as a 4-Tensor of dimen-
sion (ci+1 × r × 1× 1). The optimal projection, P∗
i for layer
i, based on minimizing the reconstruction loss is given as:

P∗

i = argmin

Pi

(cid:13)(cid:13)

Oi − (Ii ∗Wi ∗Pi + bi ∗Pi ) ∗PT

2

F

(2)

i (cid:13)(cid:13)

P∗

i = argmin

where k·k2
F is the Frobenious norm.
Inspired by [55], we alter our optimization criteria to
minimize the reconstruction loss of the input to the next
layer. This results in the optimization:
(cid:13)(cid:13)G(Oi )−G((Ii ∗Wi ∗Pi +bi ∗Pi ) ∗PT
(3)
The inclusion of the nonlinearity makes this a more difﬁcult
optimization problem. In [55] the problem is relaxed and
solved using Generalized SVD [12, 49, 50]. Our Cascaded
Projection method is based on the end-to-end approach de-
scribed next.

i )(cid:13)(cid:13)

Pi

2

F

3.3. Cascaded Projection Compression

Factorization methods, including the single layer projec-
tion compression discussed above, are inefﬁcient due to the
additional convolution operations required to reproject the
features to high dimensional space. Pruning methods avoid
reprojection by removing all channels associated with the
pruned ﬁlters. CaP takes a more powerful approach that
forms linear combination of the kernels by projecting with-
out the extra memory requirements of factorization meth-
ods. Following the diagram in Figure 2, we consider two
successive convolutional layers, labeled i and i + 1, with
kernels Wi , Wi+1 and biases bi , bi+1 respectively. The

10717

input to layer i is Ii , while the output of layer i + 1 is the
input to layer i + 2, denoted by Ii+2 and given below.

Ii+2 = G(G(Ii ∗ Wi + bi ) ∗ Wi+1 + bi+1 )

(4)

After substituting our compressed representation with re-
projection for layer i in the above we get:

Ii+2 = G(G((Ii∗Wi∗Pi+bi∗Pi )∗PT
i )∗Wi+1+bi+1 ) (5)

To avoid reprojecting the low dimensional features back
to higher dimensional space with PT
i , we seek two projec-
tions. The ﬁrst PO
i which captures the optimal lower di-
mensional representation of the features in the current layer,
and the second PI
i which pulls the kernels of the next layer
down to lower dimensional space. This formulation leads
to an optimization problem over the projection operators:

∗

{PI

i

, PO

i

∗

} = argmin

PI
i ,PO

i

kIi+2 −G(G((Ii ∗Wi ∗PO

i

(6)

+bi ∗PO

i )) ∗PI

i ∗Wi+1 +bi+1 )k2
F

To make the problem tractable, we enforce two strong
constraints on the projections. We require that they are
orthonormal and transposes of each other: PI
i )T .
For the remainder of this work we replace PO
i and PI
i with
i , respectively. These constraints make the opti-
mization problem more feasible by reducing the parameter
search space to a single projection operator for each layer.

Pi and PT

i = (PO

P∗

i = argmin

Pi ,Pi∈On×m

kIi+2 −G(G((Ii ∗Wi ∗Pi

(7)

+bi ∗Pi )) ∗PT

i ∗Wi+1 +bi+1 )k2
F

We solve the optimization of a single projection oper-
ator for each layer using a novel data-driven optimization
method for projection operators discussed in Section 3.6.

3.3.1 Kernel Compression and Relaxation

Once the projection optimization is complete, we re-
place the kernels and biases in the current layer with their
projected versions WO
i = bi ∗Pi respec-
tively. We also replace the kernels in the next layer with
their input compressed versions WI
i ∗Wi+1 . Thus,

i =Wi ∗Pi and bO

i+1 = PT

Ii+2 = G(G((Ii ∗ WO
i + bO

i )) ∗ WI

i+1 + bi+1 )

(8)

Figure 2 depicts how the ﬁlters WI
i+1 in the next layer
are compressed using the projection PT
i and are therefore
smaller than the kernels in the original network. Utilizing
the compressed kernels WO
i and WI
i results in twice the
speedup over traditional factorization methods for all com-
pressed intermediate layers (other than ﬁrst and last layers).
Following kernel projection, we perform an additional
round of training in which only the compressed kernels are
optimized. We refer to this step as kernel relaxation because
we are allowing the kernels to ﬁnd a better optimal solution
after our projection optimization step.

3.4. Mixture Loss

A beneﬁt of gradient based optimization is that a loss
function can be altered to minimize both reconstruction and
classiﬁcation error. Previous methods have focused on ei-
ther reconstruction error minimization [19, 37] or classi-
ﬁcation [54] based metrics when pruning each layer. We
propose using a combination of the standard cross entropy
classiﬁcation loss, LC lass , and the reconstruction loss LR ,
shown in Figure 2. The reconstruction loss for layer i is
given as:

LR (i) = kIi+2 − G(G((Ii ∗ Wi ∗ Pi

+bi ∗ Pi )) ∗ PT

i ∗ Wi+1 + bi+1 )k2
F

(9)

The mixture loss used to optimize the projections in layer i
is given as

L(i) = LR (i) + γLC lass

(10)

where γ is a mixture parameter that allows adjusting the im-
pact of each loss during training. By using a combination of
the two losses we obtain a compressed network that main-
tains classiﬁcation accuracy while having feature represen-
tations for each layer which contain the maximal amount of
information from the original network.

3.5. Compressing Multi(cid:173)Branch Networks

Multi-branch networks are popular due to their excellent
performance and come in a variety of forms such as the In-
ception networks [46, 47, 45], Residual networks (ResNets)
[15] and Dense Networks (DenseNets) [22] among others.
We primarily focus on applying CaP network compression
to ResNets, but our method can be integrated with other
multi-branch networks. We select the ResNet architecture
for two reasons. First, ResNets have a proven record of pro-
ducing state-of-the art results [15, 16]. And second, the skip
connections work well with network compression, as they
allow propagating information through the network regard-
less of the compression process within the individual layers.
Our CaP modiﬁcation for ResNet compression is illus-
trated in Figure 3.
In our approach, we do not alter the
structure of the residual block outputs, therefore we do not
compress the outputs of the last convolution layers in each
residual block, as was done by [37]. In [35, 54] pruning
is performed on the residual connections, but we do not af-
fect them, because pruning these layers has a large negative
impact on the network’s accuracy.
We calculate the reconstruction error in ResNets at the
outputs of each residual block, as shown in Fig. 3, in con-
trast to single branch networks where we calculate the re-
construction error at the next layer as shown in Fig. 2. By
calculating the reconstruction error after the skip connec-
tions, we leverage the information in the skip connections
in our projection optimization.

10718

Figure 3. Illustration of simultaneous optimization of the projections for each layer of the ResNet18 network using a mixture loss that
includes the classiﬁcation loss and the reconstruction losses in each layer for intermediate supervision. We do not alter the structure of the
residual block outputs, therefore we do not affect residual connections and we do not compress the outputs of the last convolution layers in
each residual block.

3.5.1 Simultaneous Layer Compression

3.6. Back(cid:173)Propagated Projection Optimization

Most network compression methods apply a greedy
layer-wise compression scheme, where one layer is com-
pressed or pruned at a time. However, this layer-by-layer
approach to network compression can lead to sub-optimal
results [54]. We now present a version of CaP where all
layers are simultaneously optimized. This approach allows
the latter layers to help guide the projections of the earlier
layers and minimize the total reconstruction error through-
out the network.
In our experiments, we found that simultaneous opti-
mization of the projection matrices has the risk of becom-
ing unstable when we compress more than one layer in each
residual block. To overcome this problem we split the train-
ing of the projections in residual blocks with more than one
compressible layer into two rounds. In the ﬁrst round, the
projections for the odd layers are optimized, and in the sec-
ond round the even layer projections are optimized.
Additionally, we found that using the reconstruction loss
at the ﬁnal layers did not provide enough supervision to the
network. We therefore introduced deep supervision for each
layer by minimizing the sum of normalized reconstruction
losses for each layer, given by:

argmin

Pi∈P

N
Xi=0

LR (i) + γLC lass

(11)

where Pi is the projection for the ith layer, and N is the
total number of layers. We outline our approach to ﬁnding
a solution for the above optimization using iterative back-
propagation next.

In this section we present an end-to-end Proxy Matrix
Projection (PMaP) optimization method, which is an it-
erative optimization of the projection using backpropaga-
tion with Stochastic Gradient Descent (SGD). The proposed
method efﬁciently optimizes the network compression by
combining backpropagation with geometric constraints.
In our framework, we restrict the projection operators
T Pi = I. The set of
to be orthogonal and thus satisfy Pi
(n × m) real-valued orthogonal matrices On×m , forms a
smooth manifold known as a Grassmann manifold. There
are several optimization methods on Grassmann manifolds,
most of which include iterative optimization and retraction
methods [7, 1, 48, 2, 51].
With CaP compression, the projection for each layer is
dependent on the projections in all previous layers adding
dependencies in the optimization across layers. Little work
had been done in the ﬁeld of optimization over multiple de-
pendent Grassmann manifolds. Huang et al. [23] impose
orthogonality constraints on the weights of a neural network
during training using a method for backpropagation of gra-
dients through structured linear algebra layers developed in
[26, 27]. Inspired by these works, we utilize a similar ap-
proach where instead of optimizing each projection matrix
directly, we use a proxy matrix Xi for each layer i and a
transformation Φ(·) such that Φ(Xi ) = Pi .
We obtain the transformation Φ(·) that projects each
proxy matrix Xi
to the closest location on the Grass-
mann manifold by performing Singular Value Decomposi-
tion (SVD) on Xi , such that Xi = UiΣiVT
i , where Ui and
i are orthogonal matrices and Σi is the matrix of singular

VT

10719

values. The projection to the closest location on the Grass-
mann manifold is performed as Φ(Xi ) = UiVT
During training, the projection matrix Pi is not updated
directly; instead the proxy parameter Xi is updated based
on the partial derivatives of the loss with respect to Ui and
and ∂L
respectively. The partial derivative of the
loss L with respect to the proxy parameter Xi was derived
in [26, 27] using the chain rule and is given by:

Vi , ∂L
∂Ui

i = Pi .

∂Vi

∂L

∂Xi

= Ui(2Σi (cid:18)KT
i ◦(cid:18)VT

i

∂L

∂Vi (cid:19)(cid:19)sym

+

∂L

∂Σi )VT

i

(12)
where ◦ is the Hadamard product, Asym is the symmet-
ric part of matrix A given as Asym = 1
2 (AT + A). Since
i , the loss does not depend on the matrix Σi .
= 0, and equation (12) becomes:

Φ(Xi ) = UiVT

Thus, ∂L

∂Σi

Figure 4. Plot of the reconstruction error (vertical axis) for the
range of compression (left axis) for each layer of the network
(right axis). The reconstruction error is lower when early layers
are compressed.

∂L

∂Xi

= Ui(2Σi (cid:18)KT
i ◦(cid:18)VT

i

∂L

∂Vi (cid:19)(cid:19)sym)VT

i

(13)

The above allows us to optimize our compression pro-
jection operators for each layer of the network using back-
propagation and SGD. Our method allows for end-to-end
network compression using standard deep learning frame-
works for the ﬁrst time.

4. Experiments

We ﬁrst perform experiments on independent layer com-
pression of the VGG16 network to investigate how each
layer responds to various levels of compression. We then
perform a set of ablation studies on the proposed CaP algo-
rithm to determine the impact for each step of the algorithm
on the ﬁnal accuracy of the compressed network. We com-
pare CaP to other state-of-the-art methods by compressing
the VGG16 network to have over 4× fewer ﬂoating point
operations. Finally we present our experiments with vary-
ing levels of compression of ResNet architectures, with 18
or 50 layers, trained on the CIFAR10 dataset.
All experiments were performed using PyTorch 0.4 [41]
on a work station running Ubuntu 16.04. The workstation
had an Intel i5-6500 3.20GHz CPU with 15 GB of RAM
and a NVIDIA Titan V GPU.

4.1. Layer(cid:173)wise Experiments

In these experiment we investigate how each layer of
the network is affected by increasing amounts of compres-
sion. We perform ﬁlter compression using CaP for each
layer independently, while leaving all other layers uncom-
pressed. We considered a range of compression for each
layer, from 5% to 99%, and display the results in Figure
4. This plot shows two trends. Firstly the reconstruction
error does not increase much until 70% compression, indi-
cating that a large portion of the parameters in each layer

Figure 5. Plot of the classiﬁcation accuracy (vertical axis) for the
range of compression (left axis) for each layer of the network
(right axis). The classiﬁcation accuracy remains unaffected for
large amounts of compression in a single layer anywhere in the
network.

are redundant and could be reduced without much loss in
accuracy. The second trend is the increase in reconstruction
error for each level of compression for the deeper layers of
the network (right axis).
In Figure 5 we plot the network accuracy resulting from
each level of compression for each layer. The network is rel-
atively unaffected for a large range of compression, despite
the fact that there is a signiﬁcant amount of reconstruction
error introduced by the compression shown in Figure 4.

4.2. CaP Ablation Experiments

We ran additional experiments to determine the contri-
bution of the projection optimization and kernel relaxation
steps of our algorithm. We ﬁrst trained the ResNet18 net-
work on the CIFAR100 dataset and achieved a baseline ac-
curacy of 78.23%. We then compressed the network to 50%
of the original size using only parts of the CaP method to as-
sess the effects of different components. We present these
results in Table 1.
We also trained a compressed version ResNet18 from
scratch for 350 epochs, to provide a baseline for the com-

10720

pressed ResNet18 network. When only projection compres-
sion is performed on the original ResNet18 network, there
was a drop in accuracy of 1.58%. This loss in classiﬁca-
tion accuracy decreased to 0.76% after kernel relaxation. In
contrast, when the optimized projections are replaced with
random projections and only kernel relaxation training is
performed, there is a 1.96% drop in accuracy, a 2.5 times
increase in classiﬁcation error. These results demonstrate
that the projection optimization is an important aspect of
our network compression algorithm, and the combination
of both steps outperforms training the compressed network
from scratch.

ResNet18 Network Variation
ResNet18 Uncompressed (upper bound)
Compressed ResNet18 from Scratch
CaP Compression with Projection Only
CaP with Random Proj. & Kernel Relax
CaP with Projection & Kernel Relax

Accuracy
78.23
77.22
76.65
76.27
77.47

Table 1. Network compression ablation study of the CaP method
compressing the ResNet18 Network trained on the CIFAR100
dataset. (Bold numbers are best).

Figure 6. Classiﬁcation accuracy drop on CIFAR10, relative to
baseline, of compression methods (CaP, PCAS [53], PFEC [35]
and LPF [24]) for a range of compression levels on ResNet18
(Top) and ResNet50 (Bottom).

4.3. ResNet Compression on CIFAR 10

We perform two sets of experiments using ResNet18 and
ResNet50 trained on the CIFAR10 dataset [31]. We com-

ResNet Method
PFEC [35]
CP [19]
SFP [17]
AMC [18]
CaP
PFEC [35]
NISP [54]
CP [19]
SFP [17]
AMC [18]
DCP [56]
CaP

56

FT FLOPs% Acc. / Base
N
72.4
91.31 / 93.04
N
50.0
90.90 / 92.80
N
47.4
92.26 / 93.59
N
50.0
90.1 / 92.8
N
50.2
92.92 / 93.51
Y
72.4
93.06 / 93.04
Y
57.4
(-0.03) *
Y
50.0
91.80 / 92.80
Y
47.4
93.35 / 93.59
Y
50.0
91.9 / 92.8
Y
35.0
93.7 / 93.6
Y
50.2
93.22 / 93.51

110

PFEC [35]
MIL [11]
SFP [17]
CaP
PFEC [35]
NISP [54]
SFP [17]
CaP

N
N
N
N
Y
Y
Y
Y

61.4
65.8
59.2
50.1
61.4
56.3
59.2
50.1

92.94 / 93.53
93.44 / 93.63
93.38 / 93.68
93.95/ 94.29
93.30 / 93.53
(-0.18) *
93.86 / 93.68
94.14/ 94.29

Table 2. Comparison of CaP with pruning and factorization based
methods using ResNet56 and ResNet110 trained on CIFAR10. FT
denotes ﬁne-tuning. (Bold numbers are best). * Only the relative
drop in accuracy was reported in [54] without baseline accuracy.

press 18 and 50 layer ResNets with varying levels of com-
pression and compare the relative drop in accuracy of CaP
with other state-of-the-art methods [53, 35, 24]. We plot the
drop in classiﬁcation accuracy for ResNet18 and ResNet50
in Figure 6. For both networks, the CaP method outper-
forms the other methods for the full range of compression.

In Table 2, we present classiﬁcation accuracy of
ResNet56 and ResNet110 with each residual block com-
pressed to have 50% fewer FLOPs using CaPs. We compare
the results obtained by CaP with those of [17, 18, 35, 54, 19]
where the networks have been subjected to similar compres-
sion ratios. We report accuracy results with and without
ﬁne-tuning and include the baseline performance for com-
parison.

Results with ﬁne-tuning are generally better, except in
cases when there is over-ﬁtting. However, ﬁne-tuning for
a long period of time can hide the poor performance of
a compression algorithm by retraining the network ﬁlters
away from the compression results. The results of the CaP
method without ﬁne-tuning are based on projection opti-
mization and kernel relaxation on the compressed ﬁlters
with reconstruction loss, while the ﬁne-tuning results are
produced with an additional round of training based on mix-
ture loss for all of the layers in the network.

10721

Method
VGG16 [43] (Baseline)
Low-Rank [29]
Asym. [55]
Channel Pruning [19]
CaP (based on [19] arch)
CaP Optimal

Parameters Memory (Mb) FLOPs GPU Speedup Top-5 Accuracy / Baseline
14.71M
3.39
30.9B
1
89.9
-
-
-
1.01*
80.02 / 89.9
5.11M
3.90
3.7B
1.55*
86.06 / 89.9
7.48M
1.35
6.8B
2.5*
82.0 / 89.9
7.48M
1.35
6.8B
3.05
86.57 / 90.38
7.93M
1.11
6.8B
3.44
88.23 / 90.38

Table 3. Network compression results of pruning and factorization based methods without ﬁne-tuning. The top-5 accuracy of the baseline
VGG16 network varies slightly for each of the methods due to different models and frameworks. (Bold numbers are best). Results marked
with * were obtained from [19].

Method

VGG16 [43]
Scratch [19]
COBLA [34]
Tucker [30]
CP [19]
ThiNet-2 [37]
CaP

Mem.
(Mb)
3.39
1.35
4.21
4.96
1.35
1.44
1.11

FLOPs

30.9B
6.8B
7.7B
6.3B
6.8B
6.7B
6.8B

Top-5 Acc.
/ Baseline
89.9
88.1
88.9 / 89.9
89.4 / 89.9
88.9 / 89.9
88.86 / 90.01
89.39 / 90.38

Table 4. Network compression results of pruning and factorization
based methods with ﬁne-tuning. (Bold numbers are best).

4.4. VGG16 Compression with ImageNet

We compress the VGG16 network trained on Ima-
geNet2012 [8] and compare the results of CaP with other
state-of-the-art methods. We present two sets of results,
without ﬁne-tuning and with ﬁne-tuning, in Tables 3 and 4
respectively. Fine-tuning on ImageNet is time intensive and
requires signiﬁcant computation power. This is a hindrance
for many applications where users do not have enough re-
sources to retrain a compressed network.
In Table 3 we compare CaP with factorization and prun-
ing methods, all without ﬁne-tuning. As expected, factor-
ization methods suffer from increased memory load due
to their additional intermediate feature maps. The chan-
nel pruning method in [19] has a signiﬁcant reduction in
memory consumption but under-performs the factorization
method in [55] without ﬁne-tuning. We present two sets of
results for the CaP algorithm, each with different levels of
compression for each layer. To match the architecture used
in [19] we compressed layers 1-7 to 33% of their original
size, and ﬁlters in layers 8-10 to 50% of their original size,
while the remaining layers are left uncompressed . We also
used the CaP method with a compression architecture that
was selected based on our layer-wise training experiments.
The results in Table 3 demonstrate that the proposed CaP
compression achieves higher speedup and higher classiﬁca-
tion accuracy than the factorization or pruning methods.
In Table 4 we compare CaP with state-of-the-art net-

work compression methods, all with ﬁne-tuning. The un-
compressed VGG16 results are from [43]. We include re-
sults from training a compressed version of VGG16 from
scratch on the ImageNet dataset as reported in [19]. We
compare CaP with the results of two factorization methods
[34, 30] and two pruning methods [19], [37]. Both factor-
ization methods achieve impressive classiﬁcation accuracy,
but this comes at the cost of increased memory consump-
tion. The pruning methods reduce both the FLOPs and
the memory consumption of the network, while maintain-
ing high classiﬁcation accuracy. However, they rely heavily
on ﬁne-tuning to achieve high accuracy. We lastly provide
the results of the CaP compression optimized at each layer.
Our results demonstrate that the CaP algorithm gives state-
of-the-art results, has the largest reduction in memory con-
sumption, and outperforms the pruning methods in terms of
top-5 accuracy.

5. Conclusion

In this paper, we propose cascaded projection, an end-
to-end trainable framework for network compression that
optimizes compression in each layer. Our CaP approach
forms linear combinations of kernels in each layer of the
network in a manner that both minimizes reconstruction er-
ror and maximizes classiﬁcation accuracy. The CaP method
is the ﬁrst in the ﬁeld of network compression to optimize
the low dimensional projections of the layers of the network
using backpropagation and SGD, using our proposed Proxy
Matrix Projection optimization method.
We demonstrate state-of-the-art performance compared
to pruning and factorization methods, when the CaP method
is used to compress standard network architectures trained
on standard datasets. A side beneﬁt of the CaP formula-
tion is that it can be performed using standard deep learning
frameworks and hardware, and it does not require any spe-
cialized libraries for hardware for acceleration.
In future
work, the CaP method can be combined with other meth-
ods, such as quantization and hashing, to further accelerate
deep networks.

10722

References

[1] P. A. Absil, R. Mahony, and R. Sepulchre. Optimization al-
gorithms on matrix manifolds. Princeton University Press,
2009.

[2] P. A. Absil and J. Malick. Projection-like retractions on ma-
trix manifolds. SIAM Journal on Optimization, 22(1):135–
158, 2012.

[3] A. Canziani, A. Paszke, and E. Culurciello. An analysis of
deep neural network models for practical applications. arXiv
preprint arXiv:1605.07678, 2016.

[4] W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen.
Compressing neural networks with the hashing trick. In Pro-
ceedings of the International Conference on Machine Learn-
ing (ICML) (ICML), pages 2285–2294, 2015.

[5] Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. Choudhary,
and S.-F. Chang. An exploration of parameter redundancy
in deep networks with circulant projections. In Proceedings
of the IEEE International Conference on Computer Vision
(ICCV), pages 2857–2865, 2015.

[6] M. Courbariaux, Y. Bengio, and J.-P. David. Training deep
neural networks with low precision multiplications. In Pro-
ceedings of the International Conference on Machine Learn-
ing (ICML) Workshop, 2014.

[7] J. P. Cunningham and Z. Ghahramani. Linear dimensionality
reduction: survey, insights, and generalizations. Journal of
Machine Learning Research, 16(1):2859–2900, 2015.

[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. Imagenet: A large-scale hierarchical image database. In
Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR), pages 248–255, 2009.

[9] M. Denil, B. Shakibi, L. Dinh, N. De Freitas, et al. Pre-
dicting parameters in deep learning. In Advances in Neural
Information Processing Systems (NIPS), pages 2148–2156,
2013.

[10] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fer-
gus. Exploiting linear structure within convolutional net-
works for efﬁcient evaluation. In Advances in Neural Infor-
mation Processing Systems (NIPS), pages 1269–1277, 2014.

[11] X. Dong, J. Huang, Y. Yang, and S. Yan. More is less: A
more complicated network with less inference complexity. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), 2017.

[12] J. C. Gower, G. B. Dijksterhuis, et al. Procrustes problems,
volume 30. Oxford University Press on Demand, 2004.

[13] S. Han, H. Mao, and W. J. Dally. Deep compression: Com-
pressing deep neural networks with pruning, trained quanti-
zation and huffman coding. In International Conference on
Learning Representations (ICLR), 2015.

[14] B. Hassibi and D. G. Stork. Second order derivatives for
network pruning: Optimal brain surgeon.
In Advances in
Neural Information Processing Systems (NIPS, pages 164–
171, 1993.

[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In Proceedings of the IEEE confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 770–778, 2016.

[16] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in
deep residual networks. In Proceedings of the IEEE Euro-
pean Conference on Computer Vision (ECCV), pages 630–
645. Springer, 2016.
[17] Y. He, G. Kang, X. Dong, Y. Fu, and Y. Yang. Soft ﬁlter
pruning for accelerating deep convolutional neural networks.
In International Joint Conference on Artiﬁcial Intelligence
(IJCAI), 2018.
[18] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han. Amc:
Automl for model compression and acceleration on mobile
devices.
In Proceedings of the European Conference on
Computer Vision (ECCV), pages 784–800, 2018.
[19] Y. He, X. Zhang, and J. Sun. Channel pruning for accelerat-
ing very deep neural networks. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), Oct
2017.
[20] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. arXiv preprint arXiv:1704.04861, 2017.
[21] G. Huang, S. Liu, L. van der Maaten, and K. Q. Weinberger.
Condensenet: An efﬁcient densenet using learned group con-
volutions. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2018.
[22] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.
Densely connected convolutional networks. In Proceedings
of the IEEE conference on Computer Vision and Pattern
Recognition (CVPR), volume 1, page 3, 2017.
[23] L. Huang, X. Liu, B. Lang, A. W. Yu, Y. Wang, and B. Li.
Orthogonal weight normalization: Solution to optimization
over multiple dependent stiefel manifolds in deep neural net-
works. arXiv preprint arXiv:1709.06079, 2017.
[24] Q. Huang, K. Zhou, S. You, and U. Neumann. Learning
to prune ﬁlters in convolutional neural networks.
In Pro-
ceedings of the IEEE Winter Conference on Applications of
Computer Vision (WACV), pages 709–718, 2018.
[25] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J.
Dally, and K. Keutzer. Squeezenet: Alexnet-level accuracy
with 50x fewer parameters and¡ 0.5 mb model size. arXiv
preprint arXiv:1602.07360, 2016.
[26] C. Ionescu, O. Vantzos, and C. Sminchisescu. Matrix Back-
propagation for Deep Networks with Structured Layers. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), 2015.
[27] C. Ionescu, O. Vantzos, and C. Sminchisescu. Training deep
networks with structured layers by matrix backpropagation.
arXiv preprint arXiv:1509.07838, 2015.
[28] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard,
H. Adam, and D. Kalenichenko. Quantization and training
of neural networks for efﬁcient integer-arithmetic-only infer-
ence. arXiv preprint arXiv:1712.05877, 2017.
[29] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up
convolutional neural networks with low rank expansions.
In Proceedings of the British Machine Vision Conference
(BMVC), 2014.
[30] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin.
Compression of deep convolutional neural networks for fast

10723

IEEE onference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 1–9, 2015.
[47] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition (CVPR), June 2016.
[48] H. D. Tagare. Notes on optimization on stiefel manifolds.
Technical report, Technical report, Yale University, 2011.
[49] Y. Takane and H. Hwang. Regularized linear and kernel re-
dundancy analysis. Computational Statistics & Data Analy-
sis, 52(1):394–405, 2007.
[50] Y. Takane and S. Jung. Generalized constrained redundancy
analysis. Behaviormetrika, 33(2):179–192, 2006.
[51] Z. Wen and W. Yin. A feasible method for optimization
with orthogonality constraints. Mathematical Programming,
142(1-2):397–434, 2013.
[52] J. Wu, Y. Wang, Z. Wu, Z. Wang, A. Veeraraghavan, and
Y. Lin. Deep k-means: Re-training and parameter sharing
with harder cluster assignments for compressing deep con-
volutions.
In Proceedings of the 35th International Con-
ference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 5363–5372, Stock-
holmsmssan, Stockholm Sweden, 10–15 Jul 2018.
[53] K. Yamamoto and K. Maeno. Pcas: Pruning channels with
attention statistics. arXiv preprint arXiv:1806.05382, 2018.
[54] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han,
M. Gao, C.-Y. Lin, and L. S. Davis. Nisp: Pruning net-
works using neuron importance score propagation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.
[55] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efﬁcient
and accurate approximations of nonlinear convolutional net-
works. In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition (CVPR), pages 1984–1992,
2015.
[56] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y. Guo, Q. Wu,
J. Huang, and J. Zhu. Discrimination-aware channel pruning
for deep neural networks. In Advances in Neural Information
Processing Systems 31, pages 875–886. 2018.

and low power mobile applications. In International Confer-
ence on Learning Representations (ICLR), 2016.
[31] A. Krizhevsky. Learning multiple layers of features from tiny
images. Technical report, Department of Computer Science,
University of Toronto, 2009.
[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet
classiﬁcation with deep convolutional neural networks. Ad-
vances in Neural Information Processing Systems (NIPS),
25:1097–1105, 2012.
[33] V. Lebedev, Y. Ganin, M. Rakhuba,
I. Oseledets, and
V. Lempitsky.
Speeding-up convolutional neural net-
works using ﬁne-tuned cp-decomposition. arXiv preprint
arXiv:1412.6553, 2014.
[34] C. Li and C. Richard Shi. Constrained optimization based
low-rank approximation of deep neural networks.
In Pro-
ceedings of the IEEE European Conference on Computer Vi-
sion (ECCV), pages 732–747, 2018.
[35] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf.
Pruning ﬁlters for efﬁcient convnets. In International Con-
ference on Learning Representations (ICLR), 2016.
[36] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky.
Sparse convolutional neural networks. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 806–814, 2015.
[37] J.-H. Luo, J. Wu, and W. Lin. Thinet: A ﬁlter level prun-
ing method for deep neural network compression.
In Pro-
ceedings of the IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
[38] S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, and J. S.
Vetter. Nvidia tensor core programmability, performance &
precision. arXiv preprint arXiv:1803.04014, 2018.
[39] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz.
Pruning convolutional neural networks for resource efﬁcient
inference. In Proceedings of the International Conference on
Learning Representations (ICLR), 2017.
[40] V. Nair and G. E. Hinton. Rectiﬁed linear units improve re-
stricted boltzmann machines. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML), pages 807–
814, 2010.
[41] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-
Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto-
matic differentiation in pytorch. In NIPS-W, 2017.
[42] J. Redmon and A. Farhadi. Yolov3: An incremental improve-
ment. arXiv, 2018.
[43] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In Proceedings
of the International Conference on Learning Representations
(ICLR), 2015.
[44] S. Srinivas and R. V. Babu. Data-free parameter pruning for
deep neural networks. In Proceedings of the British Machine
Vision Conference (BMVC), 2015.
[45] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. In AAAI, volume 4, page 12, 2017.
[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In Proceedings of the

10724

CollaGAN: Collaborative GAN for Missing Image Data Imputation

Dongwook Lee1 , Junyoung Kim1 , Won-Jin Moon2 , Jong Chul Ye1
1 : Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Korea

{dongwook.lee, junyoung.kim, jong.ye}@kaist.ac.kr

2 : Konkuk University Medical Center, Seoul, Korea

mdmoonwj@kuh.ac.kr

Figure 1: Image translation tasks using (a) cross-domain models, (b) StarGAN, and (c) the proposed collaborative GAN
(CollaGAN). Cross-domain model needs large number of generators to handle multi-class data. StarGAN and CollaGAN
use a single generator with one input and multiple inputs, respectively, to synthesize the target domain image.

Abstract

In many applications requiring multiple inputs to obtain
a desired output, if any of the input data is missing, it of-
ten introduces large amounts of bias. Although many tech-
niques have been developed for imputing missing data, the
image imputation is still difﬁcult due to complicated na-
ture of natural images. To address this problem, here we
proposed a novel framework for missing image data im-
putation, called Collaborative Generative Adversarial Net-
work (CollaGAN). CollaGAN convert the image imputa-
tion problem to a multi-domain images-to-image transla-
tion task so that a single generator and discriminator net-
work can successfully estimate the missing data using the
remaining clean data set. We demonstrate that CollaGAN

produces the images with a higher visual quality compared
to the existing competing approaches in various image im-
putation tasks.

1. Introduction

In many image processing and computer vision applica-
tions, multiple set of input images are required to generate
the desired output. For example, in brain magnetic reso-
nance imaging (MRI), MR images with T1, T2, or FLAIR
(FLuid-Attenuated Inversion Recovery) contrast are all re-
quired for accurate diagnosis and segmentation of cancer
margin [6]. In generating a 3-D volume from multiple view
camera images [5], most algorithms require the pre-deﬁned
set of view angles. Unfortunately, the complete set of input

12487

data are often difﬁcult to obtain due to the acquisition cost
and time, (systematic) errors in the data set, etc. For exam-
ple, in synthetic MR contrast generation using the Magnetic
Resonance Image Compilation (MAGiC, GE Healthcare)
sequence, it is often reported that there exists a systematic
error in synthetic T2-FLAIR contrast images, which leads
to erroneous diagnosis [30]. Missing data can also cause
substantial biases, making errorrs in data processing and
anlysis and reducing the statistical efﬁciency [22].

Rather than acquiring all the datasets again in this unex-
pected situation, which is often not feasible in clinical en-
vironment, it is often necessary to replace the missing data
with substituted values. This process is often referred to
as imputation. Once all missing values have been imputed,
the data set can be used as an input for standard techniques
designed for the complete data set.

There are several standard methods to impute missing
data based on the modeling assumption for the whole set
such as mean imputation, regression imputation, stochastic
imputation, etc [2, 9]. Unfortunately, these standard algo-
rithms have limitations for high-dimensional data such as
images, since the image imputation requires knowledge of
high-dimensional image data manifold.

Similar technical issues exist in image-to-image trans-
lation problems, whose goal is to change a particular as-
pect of a given image to another. The tasks such as super-
resolution, denoising, deblurring, style transfer, semantic
segmentation, depth prediction, etc can be treated as map-
ping an image from one domain to a corresponding im-
age in another domain [10, 3, 7, 8]. Here, each domain
has a different aspect such as resolution, facial expression,
angle of light, etc, and one needs to know the intrinsic
manifold structure of the image data set to translate be-
tween the domains. Recently, these tasks have been sig-
niﬁcantly improved thanks to the generative adversarial net-
works (GANs) [11]. Speciﬁcally, CycleGAN [35] or Disco-
GAN [18] have been the main workhorse to transfer image
between two domains [17, 21]. These approaches are, how-
ever, ineffective in generalizing to multiple domain image
transfer, since N (N -1) number of generators are required
for N -domain image transfer (Fig. 1 (a)). To generalize the
idea for multi-domain translation, Choi et al [4] proposed
a so-called StarGAN which can learn translation mappings
among multiple domains by single generator (Fig. 1 (b)).
Similar multi-domain transfer network have been proposed
recently [33].

These GAN-based image transfer techniques are closely
related to image data imputation, since the image transla-
tion can be considered as a process of estimating the miss-
ing image database by modeling the image manifold struc-
ture. However, there are fundamental differences between
image imputation and image translation. For example, Cy-
cleGAN and StarGAN are interested in transferring one im-

age to another as shown in Fig. 1 (a)(b) without considering
the remaining domain data set. However, in image imputa-
tion problems, the missing data occurs infrequently, and the
goal is to estimate the missing data by utilizing the other
clean data set. Therefore, an image imputation problem can
be correctly described as in Fig. 1(c), where one genera-
tor can estimate the missing data using the remaining clean
data set. Since the missing data domain is not difﬁcult to es-
timate a priori, the imputation algorithm should be designed
such that one algorithm can estimate the missing data in any
domain by exploiting the data for the rest of the domains.
The proposed image imputation technique called Collab-
orative Generative Adversarial Network (CollaGAN) offers
many advantages over existing methods:

• The underlying image manifold can be learned more
synergistically from the multiple input data set sharing
the same manifold structure, rather than from a single
input. Therefore, the estimation of missing data using
CollaGAN is more accurate.

• CollaGAN still retains the one-generator architecture
similar to StarGAN, which is more memory-efﬁcient
compared to CycleGAN.

We demonstrate the proposed algorithm shows the best per-
formance among the state-of-the art algorithms for various
image imputation tasks.

2. Related Work

2.1. Generative Adversarial Network

Typical GAN framework [11] consists of two neural net-
works:
the generator G and the discriminator D . While
the discriminator tries to ﬁnd the features to distinguish be-
tween fake/real samples during the train process, the gener-
ator learns to eliminate/synthesize the features which the
discriminator use to judge fake/real. Thus, GANs could
generate more realistic samples which cannot be distin-
guished by the discriminator between real and fake. GANs
have shown remarkable results in various computer vi-
sion tasks such as image generation,
image translation,
etc [16, 21, 18].

2.2. Image(cid:173)to(cid:173)image translation

Unlike the original GAN, Conditional GAN (Co-
GAN) [26] controls the output by adding some information
labels as an additional parameter to the generator. Here,
instead of generating a generic sample from an unknown
noise distribution, the generator learns to produce a fake
sample with a speciﬁc condition or characteristics (such as
a label associated with an image or a more detailed tag). A
successful application of conditional GAN is for the image-
to-image translation, such as pix2pix [17] for paired data,
and CycleGAN for unpaired data [23, 35].

22488

Figure 2: Flow of the proposed method. D has two branches: domain classiﬁcation Dclsf and source classﬁciation Dgan
(real/fake). First, Dclsf is only trained by (1) the loss calculated from real samples (left). Then G reconstructs the target
domain image using the set of input images (middle). For the cycle consistency, the generated fake image re-entered to the
G with inputs images and G produces the multiple reconstructed outputs in original domains. Here, Dclsf and Dgan are
simultaneously trained by the loss from only (1) real images and both (1) real & (2) fake images, respectively (right).

CycleGAN [35] and DiscoGAN [18] attempt to preserve
key attributes between the input and the output images by
utilizing a cycle consistency loss. However, these frame-
works are only able to learn the relationships between two
different domains at a time. These approaches have scal-
ability limitations when dealing with multi-domains, since
each domain pair needs a separate generator-pair and total
N (N -1) number of generators are required to handle the
N -distinct domains.
StarGAN [4] and Radial GAN [33] are recent frame-
works that deal with multiple domains using a single gen-
erator. For example, in StarGAN [4], the depth-wise con-
catenation from input image and the mask vector repre-
senting the target domain helps to map the input to recon-
structed image in the target domain. Here, the discriminator
should be designed to play another role for domain classiﬁ-
cation. Speciﬁcally, the discriminator decides that not only
the sample is real or fake, but also the class of the sample.

3. Theory

Here, we explain our Collaborative GAN framework to
handle multiple inputs to generate more realistic and more
feasible output for image imputation. Compared to Star-
GAN, which handles single-input and single-output, the
multiple-inputs from multiple domains are processed using
the proposed method.

3.1. Image imputation using multiple inputs

For ease of explanation, we assume that there are four
types (N = 4) of domains: a, b, c, and d. To handle the
multiple-inputs using a single generator, we train the gener-
ator to synthesize the output image in the target domain, ˆxa ,
via a collaborative mapping from the set of the other types

of multiple images, {xa }C = {xb , xc , xd }, where the su-
perscript C denotes the complementary set. This mapping
is formally described by

ˆxκ = G (cid:0){xκ }C ; κ(cid:1)

(1)

where κ ∈ {a, b, c, d} denotes the target domain index that
guides to generate the output for the proper target domain,
κ. As there are N number of combinations for multiple-
input and single-output combination, we randomly choose
these combination during the training so that the generator
learns the various mappings to the multiple target domains.

3.2. Network losses

Multiple cycle consistency loss One of the key concepts
for the proposed method is the cycle consistency for mul-
tiple inputs. Since the inputs are multiple images, the cy-
cle loss should be redeﬁned. Suppose that the output from
the forward generator G is ˆxa . Then, we could generate
N − 1 number of new combinations as the other inputs for
the backward ﬂow of the generator (Fig. 2 middle). For ex-
ample, when N = 4, there are three combinations of multi-
input and single-output so that we can reconstruct the three
images of original domains using backward ﬂow of the gen-
erator as:

˜xb|a = G({ ˆxa , xc , xd }; b)
˜xc|a = G({ ˆxa , xb , xd }; c)
˜xd|a = G({ ˆxa , xb , xc }; d)

Then, the associated multiple cycle consistency loss can be
deﬁned as following:

Lmcc,a = ||xb − ˜xb|a ||1 + ||xc − ˜xc|a ||1 + ||xd − ˜xd|a ||1

32489

where || · ||1 is the l1 -norm. In general, the cycle consistency
loss for the forward generator ˆxκ can be written by

Thus, the following loss should be minimized with respect
to G:

||xκ′ − ˜xκ′ |κ ||1

(2)

Lf ake

clsf (G) = E ˆxκ|κ [− log(Dclsf (κ; ˆxκ|κ ))]

(5)

Structural Similarity Index Loss Structural Similarity In-
dex (SSIM) is one of the state-of-the-art metrics to measure
the image quality [32]. The l2 loss, which is widely used for
the image restoration tasks, has been reported to cause the
blurring artifacts on the results [21, 25, 34]. SSIM is one of
the perceptual metrics and it is also differentiable, so it can
be backpropagated [34]. The SSIM for pixel p is deﬁned as

SSIM(p) =

2µX µY + C1
µ2
X + µ2
Y + C1

·

2σX Y + C2
σ2
X + σ2
Y + C2

(6)

where µX is an average of X , σ2
X is a variance of X and
σXX ∗ is a covariance of X and X ∗ . There are two variables
to stabilize the division such as C1 = (k1L)2 and C2 =
(k2L)2 . L is a dynamic range of the pixel intensities. k1
and k2 are constants by default k1 = 0.01 and k2 = 0.03.
Since the SSIM is deﬁned between 0 and 1, the loss function
for SSIM can be written by:

LSSIM (X, Y ) = − log 

1
2|P | X

(1 + SSIM(p))



 (7)
where P denotes the pixel location set and |P | is its cardi-
nality. The SSIM loss was applied as an additional multiple
cycle consistency loss as follows:

p∈P (X,Y )

Lmcc,κ = X
κ′ 6=κ

where

(3)

˜xκ′ |κ = G (cid:0){ ˆxκ }C ; κ′ (cid:1) .
Discriminator Loss As mentioned before, the discrimina-
tor has two roles: one is to classify the source which is real
or fake, and the other is to classify the type of domain which
is class a, b, c or d. Therefore, the discriminator loss con-
sists of two parts: adversarial loss and domain classiﬁcation
loss. As shown in Fig. 2, this can be realized using a dis-
criminator with two paths Dgan and Dclsf that share the
same neural network weights except the last layers.
Speciﬁcally, the adversarial loss is necessary to make the
generated images as real as possible. The regular GAN
loss might lead to the vanishing gradients problem during
the learning process [24, 1]. To overcome such problem
and improve the robustness of the training, the adversarial
loss of Least Square GAN [24] was utilized instead of the
original GAN loss. In particular for the optimization of the
discriminatorDgan , the following loss is minimized:

Ldsc

gan (Dgan ) = Exκ [(Dgan (xκ )−1)2 ]+E ˜xκ|κ [(Dgan ( ˜xκ|κ ))2 ],

whereas the generator is optimized by minimizing the fol-
lowing loss:

Lgen

gan (G) = E ˜xκ|κ [(Dgan ( ˜xκ|κ ) − 1)2 ]

Lmcc−SSIM,κ = X
κ′ 6=κ

LSSIM (cid:0)xκ′ , ˜xκ′ |κ (cid:1) .

(8)

Lreal
clsf and Lf ake

where ˜xκ|κ is deﬁned in (3).
Next, the domain classiﬁcation loss consists of two parts:
clsf . They are the cross entropy loss for domain
classiﬁcation from the real images and the fake image, re-
spectively. Recall that the goal of training G is to generate
the image properly classiﬁed to the target domain. Thus, we
ﬁrst need a best classiﬁer Dclsf that should only be trained
with the real data to guide the generator properly. Accord-
ingly, we ﬁrst minimize the loss Lreal
clsf to train the classiﬁer
clsf is minimized by training G with ﬁxing
Dclsf so that the generator can be trained to generate sam-
ples that can be classiﬁed correctly.
Speciﬁcally, to optimize the Dclsf , the following Lreal
should be minimizied with respect to Dclsf :

Dclsf , then Lf ake

clsf

3.3. Mask vector

To use the single generator, we need to add the target la-
bel as a form of mask vector to guide the generator. The
mask vector is a binary matrix which has same dimension
with the input images to be easily concatenated. The mask
vector has N class number of channel dimensions to repre-
sent the target domain as one-hot vector along the channel
dimension. This is the simpliﬁed version of mask vector
which was originally introduced in StarGAN [4].

4. Method

4.1. Datasets

Lreal

clsf (Dclsf ) = Exκ [− log(Dclsf (κ; xκ ))]

(4)

where Dclsf (κ; xκ ) can be interpreted as the probability to
correctly classify the real input xκ as the class κ. On the
other hand, the generator G should be trained to generate
fake samples which are properly classiﬁed by the Dclsf .

MR contrast synthesis Total 280 axis brain images were
scanned by multi-dynamic multi-echo sequence and the ad-
ditional T2 FLAIR (FLuid-Attenuated Inversion Recovery)
sequence from 10 subjects. There are four types of MR con-
trast images in the dataset: T1-FLAIR (T1F), T2-weighted
(T2w), T2-FLAIR (T2F), and T2-FLAIR* (T2F*). The

42490

Figure 3: MR contrast imputation results. The generated images (right) were reconstructed from the other contrast inputs
(left). The yellow and green arrows point out the remarkable parts of the results. For CycleGAN and StarGAN, the T2-
FLAIR* contrast was used as an input for the T1-FLAIR/ T2-weighted/ T2-FLAIR contrast imputation, and the T1-FLAIR
contrast was used as an input for the T2-FLAIR* contrast imputation. The image to impute is marked as the question mark.
The average values of NMSE / SSIM for the testset are displayed on each result.

ﬁrst three contrasts were acquired from MAGnetic reso-
nance image Compilation (MAGiC, GE Healthcare) and
T2-FLAIR* was acquired by the addtional scan with dif-
ferent MR scan parameter of the third contrast (T2F). The
details of MR acquisition parameters are available in Sup-
plementary material.
CMU Multi-PIE For the illumination translation task, the
subset of Carnegie Mellon Univesity Multi-Pose Illumina-
tion and Expression face database [12] was used. There
were 250 participants in the ﬁrst session and the frontal
face of neutral expression were selected with the following
ﬁve illumination conditions: -90◦ (right), -45◦ , 0◦ (front),
45◦ and 90◦ (left). The images were cropped by 240×240
where the faces are centered as shown in Fig. 4.
RaFD The Radboud Faces Database (RaFD) [20] contains
eight different facial expressions collected from the 67 par-
ticipants; neutral, angry, contemptuous, disgusted, fearful,
happy, sad, and surprised. Also, there are three different
gaze directions and therefore total 1,608 images were di-
vided by subjects for train, validation and test set. We crop
the images to 640×640 and resize them to 128×128.

4.2. Network Implementation

The proposed method consists of two networks, the gen-
erator and the discriminator (Fig. 2). To achieve the best

performance for each task, we redesigned the generators
and discriminator to ﬁt for the property of each task, while
the general network architecture are similar.

Generators

The generators are based on the U-net [27] structure. U-
net consists of the encoder/decoder parts and the each
parts between encoder/decoder are connected by contract-
ing paths [27]. The instance normalization [31] and Leaky-
ReLU [13] was used instead of batch normalization and
ReLU, respectively. We also redesigned the architecture of
the networks to ﬁt for each task as described in the follow-
ings.
MR contrast translation There are various MR contrasts
such as T1 weight contrast, T2 weight contrast, etc. The
speciﬁc MR contrast scan is determined by the MRI scan
parameters such as repetition time (TR), echo time (TE)
and so on. The pixel intensities of the MR contrast im-
age are decided based on the physical property of the tis-
sues called MR parameters of the tissues, such as T1, T2,
proton density, etc. The MR parameter is the voxel-wise
property. This means that for the convolutional neural net-
work, the pixel-by-pixel processing is just as important as
processing with the information from neighborhood and/or
a large FOV. Thus, instead of using single convolution, the
generator uses two convolution branches with 1x1 and 3x3

52491

Figure 4: Illumination imputation results at (-90◦ , -45◦ , 45◦ and 90◦ ). The imputed images (right) were reconstructed from
the inputs with multiple illuminations (left). The yellow arrows shows remarkable parts. The frontal illumination (0◦ ) image
was given as the input of CycleGAN and StarGAN. The image to impute is marked as the question mark. The average values
of NMSE / SSIM for the testset are displayed on each result.

ﬁlters to handle the multi-scale feature information. The
two branches of the convolutions are concatenated similar
to the inception network [29].
Illumination translation For the illumination translation
task, the original U-net structure with instance normaliza-
tion [31] was used instead of batch normalization.
Facial expression translation For the facial expression
translation task, the inputs are multiple facial images with
various facial expressions. Since there exists the head
movements of the subjects between the facial expressions,
the images are not strictly aligned pixel-wise manner. If we
use the original U-net for the facial expresion images-to-
image task, the generator show poor performance because
the informations from the multiple facial expressions are
mixed up in the very early stage of the network. From the
intuition, the features from the facial expressions should
be mixed up in the middle stage of the generator where
the features are calculated from the large FOV or already
downsampled by pooling layers. Thus, the generators are
redesigned with eight branches of encoders for each eight
facial expressions and they are concatenated after the en-
coding process at the middle stage of the generator. The
structure of the decoder is similar to decoder parts of U-net
except for the use of the residual blocks [14] to add more
convolutional layers. The more details about the generator
are available in Supplementary material.

Discriminator
The discriminators commonly composed of a series of con-
volution layer and Leaky-ReLU [13]. As shown in Fig. 2,

the discriminator has two output headers: one is the classi-
ﬁcation header for real or fake and the other is classiﬁcation
header for the domain. PatchGAN [17, 35] was utilized to
classify whether local image patches are real or fake. The
dropout [15, 28] was very effective to prevent the overﬁt-
ting of the discriminator. Exceptionally, the discriminator
of MR contrast translation has branches for multi-scale pro-
cessing. The details of the speciﬁc discriminator architec-
ture is available in Supplementary Material.

4.3. Network Training

All the models were optimized using Adam [19] with a
learning rate of 0.00001, β1 = 0.9 and β2 = 0.999. As
mentioned before, the performance of the classﬁer should
be associated only to real labels which means it should be
trained only using the real data. Thus, we ﬁrst trained the
classiﬁer on real images with its corresponding labels for
the ﬁrst 10 epochs, and then we trained the generator and
the discriminator simultaneously. Training takes about six
hours, half a day, and one day for the MR contrast trans-
lation task, illumination translation, and facial expression
translation task, respectively, using a single NVIDIA GTX
1080 GPU.
For the illumination translation task, YCbCr color cod-
ing was used instead of RGB color coding. YCbCr coding
consists of the Y-luminance and CbCr-color space. There
are ﬁve different illumination images. They almost share
the CbCr codings and the only difference is Y-luminance
channel. Thus, the only Y-luminance channels were pro-

62492

Figure 5: Four facial expression imputation results. The generated images (right) were reconstructed from the inputs with the
multiple facial expressions (left). The image of neutral facial expression was used as an input in CycleGAN and StarGAN.
The average values of NMSE / SSIM are displayed on each result. The results for the rest of domains are included in
Supplementary material.

cessed for the illumination translation tasks and then the
reconstructed images coverted to RGB coded images. We
used RGB channels for facial expression translation task,
and the MR contrast dataset consists of single-channel im-
ages.

5. Experimental Results

For all three image imputation tasks, each datasets were
divided into the train, validation and test sets by the sub-
jects. Thus, all our experiments were performed using the
unseen images during the training phase. We compared the
performance of the proposed method with CycleGAN [35]
and StarGAN [4] which are the representative models for
image translation tasks.

5.1. Results of MR contrast imputation

First, we trained the models on MR contrast dataset to
learn the task of synthesizing the other contrasts. In fact,
this was the original motivation of this study that was in-
spired by the clinical needs. There are four different MR
contrasts in the dataset and the generator learns the map-
ping from one contrast to the other contrast.
As shown in Fig. 3, the proposed method reconstructed
the four different MR contrasts, which are very similar to
the targets, while StarGAN shows poor results. For the
quantitative evaluation, a normalized mean squared error
(NMSE) and SSIM were calculated between the reconstruc-
tion and the target. Compared to the results of CycleGAN
and StarGAN, the four contrast MR images were recon-
structed with minimum errors using the proposed method.
Since there are so many variables that affect the pixel inten-
sity of MR images, it is necessary to use the pixels from at
least three different contrast to accurately estimate the in-
tensity of the other contrast. Thus, there exists a limitation

on CycleGAN or StarGAN, since they uses a single input
contrast.
For example, consider the reconstruction of T2 weighted
image from the T2 FLAIR* input in Fig. 3. The cere-
brospinal ﬂuid (CSF) in the T2-weighted image should be
bright, while in the T2-FLAIR* it should be dark (yellow
and green arrows in Fig. 3). When StarGAN tries to gener-
ate the T2 weighted image from the T2 FLAIR*, this should
be difﬁcult because the input pixels are close to zero. Star-
GAN somehow reconstructed the CSF pixels near the gray
matter (yellow arrow in Fig. 3) with the help of the neigh-
borhood, but the larger CSF area (green arrow in Fig. 3)
cannot be reconstructed because the help of neighborhood
pixels is limited. The proposed method, however, utilized
the combination of the inputs to accurately reconstruct ev-
ery pixel.

5.2. Results of illumination imputation

We trained CycleGAN, StarGAN and the proposed
method using CMU Multi-PIE dataset for the illumination
imputation task. Given ﬁve different illumination direc-
tions, the input domain for CycleGAN and StarGAN was
ﬁxed as the frontal illumination (0◦ ).
As shown in Fig. 4, the proposed method clearly gen-
erates the natural illuminations while properly maintaining
the color, brightness balance, and textures of the facial im-
ages. Compared to the results of CycleGAN and StarGAN,
CollaGAN produces the natural illuminations with mini-
mum errors (NMSE/SSIM in Fig. 4). The CycleGAN and
StarGAN also generate the four different illumination im-
ages from the frontal illumination input. In the result of Cy-
cleGAN, however, we can see the emphasis of the red chan-
nel and the image looks reddish overall. Also the resulting

72493

Figure 6: Comparison of incomplete and complete input data set for image imputation results by CollaGAN. For the incom-
plete input cases, the generated images (right) were reconstructed from the inputs with multiple facial expressions (left) with
one substituted facial expression from another person (red box). The image to impute is marked as the question mark.

image looks like a graphic model or a drawing, rather than a
photo. The resulting image of StarGAN was only adjusted
to the left and right of the illumination smoothly, but did
not reﬂect detailed illumination such as the structure of the
face. And unnatural lighting changes were observed on the
result of StarGAN.
The proposed method shows the most natural lighting
images among the three algorithms. While CycleGAN and
StarGAN had simply adjusted the brightness of the left and
right sides of the images, the shadow caused by the shape
of the nose, the cheek and the jaw is expressed naturally in
the proposed method (Fig. 4 yellow arrows).

5.3. Results of facial expression imputation

The eight facial expressions in RaFD were used to train
the proposed model for facial expression imputation. The
input domain for CycleGAN and StarGAN was deﬁned as
a neutral expression among the eight different facial ex-
pressions. Different facial expressions were reconstructed
naturally using the proposed method as shown in Fig. 5.
The CollaGAN produces the most natural images with min-
imum NMSE and best SSIM scores compared to the Cycle-
GAN and StarGAN as you can see in Fig. 5. Compared with
the results of StarGAN, which uses only the single input, the
proposed method utilizes as much information as possible
from the combinations of facial expressions. As shown in
the generated results of CycleGAN and StarGAN (Fig. 5),
the generated results of ‘sad’ were very similar to the gener-
ated image of ‘neutral’ which was the input of them, while
the proposed method expressed the ‘sad’ very well. With
a help of multiple cycle consistency, the proposed method
clearly generates the natural facial expressions while pre-
serving the identity correctly.

5.4. Effect of incomplete input set

In order to investigate the robustness of the proposed
method, we demonstrated CollaGAN results from incom-
plete input set. If there are two missing facial expressions
(eg. ‘happy’ and ‘neutral’) and one is interested in recon-
struct the missing image (eg. ‘happy’), one can substitute
one image (eg.‘neutral’) from the other subject as one of
the input for the CollaGAN. As shown in Fig. 6, the gen-
erated image from incomplete input set with the substitute
data from others shows similar results compared to the com-
plete input set. CollaGAN utilized the other subject’s facial
information (eg. ‘neutral’) to impute the missing facial ex-
pression (eg. ‘happy’).

6. Conclusion

In this paper, we presented a novel CollaGAN architec-
ture for missing image data imputation by synergistically
combining the information from the available data with the
help of a single generator and discriminator. We showed
that the proposed method produces images of higher visual
quality compared to the existing methods. Therefore, we
believe that CollaGAN is a promising algorithm for miss-
ing image data imputation in many real world applications.

Acknowledgement. This work was supported by Na-
tional Research Foundation of Korea under Grant NRF-
2016R1A2B3008104 and Institute for Information & Com-
munications Technology Promotion (IITP) grant funded by
the Korea government (MSIT) [2016-0-00562(R0124-16-
0002), Emotional Intelligence Technology to Infer Human
Emotion and Carry on Dialogue Accordingly].

82494

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN.
arXiv preprint arXiv:1701.07875, 2017.
[2] A. N. Baraldi and C. K. Enders. An introduction to mod-
ern missing data analyses. Journal of school psychology,
48(1):5–37, 2010.
[3] T. Chen, M.-M. Cheng, P. Tan, A. Shamir, and S.-M. Hu.
Sketch2photo: Internet image montage.
In ACM Transac-
tions on Graphics (TOG), volume 28(5), page 124. ACM,
2009.
[4] Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo.
StarGAN: Uniﬁed generative adversarial networks for multi-
domain image-to-image translation. arXiv preprint, 1711,
2017.
[5] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3D-
R2N2: A uniﬁed approach for single and multi-view 3D ob-
ject reconstruction. In European conference on computer vi-
sion, pages 628–644. Springer, 2016.
[6] A. Drevelegas and N. Papanikolaou. Imaging modalities in
brain tumors. In Imaging of Brain Tumors with Histological
Correlations, pages 13–33. Springer, 2011.
[7] A. A. Efros and W. T. Freeman.
Image quilting for tex-
ture synthesis and transfer. In Proceedings of the 28th an-
nual conference on Computer graphics and interactive tech-
niques, pages 341–346. ACM, 2001.
[8] D. Eigen and R. Fergus. Predicting depth, surface normals
and semantic labels with a common multi-scale convolu-
tional architecture. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2650–2658, 2015.
[9] C. K. Enders. Applied missing data analysis. Guilford press,
2010.
[10] R. Fergus, B. Singh, A. Hertzmann, S. T. Roweis, and W. T.
Freeman. Removing camera shake from a single photo-
graph.
In ACM transactions on graphics (TOG), volume
25(3), pages 787–794. ACM, 2006.
[11] I. Goodfellow,
J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.
[12] R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker.
Multi-PIE.
Image and Vision Computing, 28(5):807–813,
2010.
[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into
rectiﬁers: Surpassing human-level performance on imagenet
classiﬁcation. In Proceedings of the IEEE international con-
ference on computer vision, pages 1026–1034, 2015.
[14] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.
[15] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and
R. R. Salakhutdinov.
Improving neural networks by pre-
venting co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.
[16] X. Huang, Y. Li, O. Poursaeed, J. E. Hopcroft, and S. J. Be-
longie. Stacked generative adversarial networks. In CVPR,
volume 2, page 3, 2017.

[17] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
Image-
to-image translation with conditional adversarial networks.
arXiv preprint, 2017.
[18] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. arXiv preprint arXiv:1703.05192, 2017.
[19] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
[20] O. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T.
Hawk, and A. Van Knippenberg. Presentation and valida-
tion of the radboud faces database. Cognition and emotion,
24(8):1377–1388, 2010.
[21] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a gener-
ative adversarial network.
In CVPR, volume 2(3), page 4,
2017.
[22] R. J. Little and D. B. Rubin. Statistical analysis with missing
data, volume 333. John Wiley & Sons, 2014.
[23] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In Advances in Neural Informa-
tion Processing Systems, pages 700–708, 2017.
[24] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In Com-
puter Vision (ICCV), 2017 IEEE International Conference
on, pages 2813–2821. IEEE, 2017.
[25] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015.
[26] M. Mirza and S. Osindero. Conditional generative adversar-
ial nets. arXiv preprint arXiv:1411.1784, 2014.
[27] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015.
[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: a simple way to prevent neural
networks from overﬁtting. The Journal of Machine Learning
Research, 15(1):1929–1958, 2014.
[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1–9, 2015.
[30] L. N. Tanenbaum, A. J. Tsiouris, A. N. Johnson, T. P.
Naidich, M. C. DeLano, E. R. Melhem, P. Quarterman,
S. Parameswaran, A. Shankaranarayanan, M. Goyen, et al.
Synthetic MRI for clinical neuroimaging: Results of the
Magnetic Resonance Image Compilation (MAGiC) prospec-
tive, multicenter, multireader trial. American Journal of Neu-
roradiology, 2017.
[31] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normal-
ization: The missing ingredient for fast stylization. arXiv
preprint arXiv:1607.08022, 2016.
[32] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-
celli.
Image quality assessment:
from error visibility to

92495

structural similarity.
IEEE transactions on image process-
ing, 13(4):600–612, 2004.
[33] J. Yoon, J. Jordon, and M. van der Schaar. RadialGAN:
Leveraging multiple datasets to improve target-speciﬁc pre-
dictive models using generative adversarial networks. arXiv
preprint arXiv:1802.06403, 2018.
[34] H. Zhao, O. Gallo, I. Frosio, and J. Kautz. Loss functions for
image restoration with neural networks. IEEE Transactions
on Computational Imaging, 3(1):47–57, 2017.
[35] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. arXiv preprint, 2017.

102496

ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging

Samarth Brahmbhatt1 , Cusuh Ham1 , Charles C. Kemp1 and James Hays1
1 Institute for Robotics and Intelligent Machines, Georgia Tech 2Argo AI

,

2

{samarth.robo,cusuh}@gatech.edu, charlie.kemp@bme.gatech.edu, hays@gatech.edu

Figure 1: Example contact maps from ContactDB, constructed from multiple 2D thermal images of hand-object contact
resulting from human grasps.

Abstract

Grasping and manipulating objects is an important hu-
man skill. Since hand-object contact is fundamental to
grasping, capturing it can lead to important insights. How-
ever, observing contact through external sensors is chal-
lenging because of occlusion and the complexity of the hu-
man hand. We present ContactDB, a novel dataset of con-
tact maps for household objects that captures the rich hand-
object contact that occurs during grasping, enabled by use
of a thermal camera. Participants in our study grasped 3D
printed objects with a post-grasp functional intent. Con-
tactDB includes 3750 3D meshes of 50 household objects
textured with contact maps and 375K frames of synchro-
nized RGB-D+thermal images. To the best of our knowl-
edge, this is the ﬁrst large-scale dataset that records de-
tailed contact maps for human grasps. Analysis of this data
shows the inﬂuence of functional intent and object size on
grasping, the tendency to touch/avoid ‘active areas’, and
the high frequency of palm and proximal ﬁnger contact. Fi-
nally, we train state-of-the-art image translation and 3D
convolution algorithms to predict diverse contact patterns
from object shape. Data, code and models are available at
https://contactdb.cc.gatech.edu.

1. Introduction

Humans excel at grasping and then performing tasks
with household objects. Human grasps exhibit contact lo-

cations, forces and stability that allows post-grasp actions
with objects, and are also signiﬁcantly inﬂuenced by the
post-grasp intent [8, 2, 45]. For example, people typically
grasp a knife by the handle to use it, but grasp it by the blunt
side of the blade to hand it off.
A large body of previous work [20, 29, 36, 46, 49, 3,
50, 52, 21, 36, 21, 6, 46] has recorded human grasps, with
methods ranging from data gloves that measure joint con-
ﬁguration to manually arranged robotic hands. ContactDB
differs signiﬁcantly from these previous datasets by focus-
ing primarily on the contact resulting from the rich inter-
action between hand and object. Speciﬁcally, we represent
contact through the texture of 3D object meshes, which we
call ‘contact maps’ (see Figure 1).
There are multiple motivations for recording grasping
activity through contact maps. Since it is object-centric,
it enables detailed analysis of grasping preferences inﬂu-
enced by functional intent, object shape, size and seman-
tic category, and learning object shape features for grasp
prediction, and grasp re-targeting to kinematically diverse
hand models. Previously employed methods of recording
grasping activity do not easily support such analysis, as we
discuss in Section 2.
We created ContactDB by recording human participants
grasping a set of 3D printed household objects in our labora-
tory, with two different post-grasp functional intents–using
the object and handing it off. See Section 3 for more details
on the data collection procedure, size of the dataset and the
kinds of data included.
Except for contact edges viewed from select angles, and

18709

contact with transparent objects, contact regions are typi-
cally occluded from visual light imaging. Hence, existing
studies on the capture and analysis of hand-object contact
are extremely limited. Fundamental questions such as the
role of the palm in grasping everyday objects are unan-
swered. We propose a novel procedure to capture contact
maps on the object surface at unprecedented detail using an
RGB-D + thermal camera calibrated rig.
We make the following contributions in this paper:
• Dataset: Present a dataset recording functional human
grasping consisting of 3750 meshes textured with contact
maps and 375K frames of paired RGBD-thermal data.
• Analysis: Demonstrate the inﬂuence of object shape, size
and functional intent on grasps, and show the importance
of non-ﬁngertip contact.
• Prediction: Explore data representations and diverse
prediction algorithms to predict contact maps from ob-
ject shape.

2. Related Work

2.1. Datasets of Human Grasps

Since contact between the human hand and an object is
fundamental to grasping and manipulation, capturing this
contact can potentially lead to important insights about hu-
man grasping and manipulation. In practice, however, this
has been a challenging goal. The human hand is highly
complex with extensive soft tissue and a skeletal structure
that is often modeled with 26 degrees of freedom. Hence,
previous work has focused on recording grasping activ-
ity in other forms like hand joint conﬁguration by man-
ual annotation [49, 3], data gloves [20, 29] or wired mag-
netic trackers [54, 16] (which can interfere with natural
grasping), or model-based hand pose estimation [50]. At
a higher level, grasping has been observed through third-
person [52, 21, 36] or ﬁrst-person [21, 6, 46] videos, in
which frames are annotated with the category of grasp ac-
cording to a grasp taxonomy [12, 23]. Tactile sensors are
embedded on a glove [4] or in the object [38] to record
grasp contact points. Such methods are limited by the reso-
lution of tactile sensors. Puhlmann et al [39] capture hand-
table contact during grasping with a touchscreen. Rogez
et al [42] manually conﬁgure a hand model to match grasps
from a taxonomy, and use connected component analysis on
hand vertices intersecting with an object model to estimate
contact regions on the hand.
Due to hand complexity and lack of understanding of
how humans control their hands, approaches like those
mentioned above have so far been limited to providing
coarse or speculative contact estimates. In contrast, our ap-
proach allows us to directly observe where contact between
the object and the human hand has taken place with an un-
precedented level of ﬁdelity.

2.2. Predicting Grasp Contact

Our work is related to that of Lau et al [26], which
crowdsources grasp tactile saliency. Online annotators are
instructed to choose a point they would prefer to touch, from
a pair sampled from the object surface. This pairwise infor-
mation is integrated to construct the tactile saliency map.
In contrast, ContactDB contact maps are full observations
of real human grasps with functional intent (see supple-
mentary material for a qualitative comparison). Akizuki
et al [1] use hand pose estimation and model-based ob-
ject tracking in RGB-D videos to record a set of contact
points on the object surface. This is vulnerable to inaccu-
racies in the hand model and hand pose tracking. Hamer at
al [19] record human demonstrations of grasping by regis-
tering depth images to get object geometry and object- and
hand-pose. Contact is approximated as a single point per
ﬁngertip. A large body of work in robotics aims to pre-
dict a conﬁguration of the end-effector [32, 9, 28] suitable
for grasping. In contrast to ContactDB, these works model
contact as a single point per hand digit, ignoring other con-
tact.
Diverse Predictions: Grasping is a task where multi-
ple predictions can be equally correct. Lee et al [27] and
Firman et al [14] have developed theoretical frameworks
allowing neural networks to make diverse and meaningful
predictions. Recently, Ghazaei et al [17] have used simi-
lar techniques to predict diverse grasp conﬁgurations for a
parallel jaw gripper.

3. The ContactDB Dataset

Here we present the design choices and process in creat-
ing the ContactDB, which consists of 50 3D printed house-
hold objects being grasped with two functional intents by
50 participants (see Table 1).
Observing Contact Through a Thermal Camera. At
the core of our data collection process is the use of a thermal
camera to observe the precise locations of contact between
human hand and object. Thermal cameras have recently
been used to capture humans and their interaction with the
environment. For example, Luo et al [31] observe humans
interacting with objects for egocentric SLAM, while Lar-
son et al [25] observe human ﬁnger interaction with arbi-
trary surfaces to make them interactive. Both note the phe-
nomenon of thermally observable contact, but do not inves-
tigate it rigorously or collect a large-scale dataset.
When a participant grasps an object, heat from the hand
transfers onto the object surface. If the object material does
not dissipate the heat rapidly, the precise contact areas can
be clearly observed in the thermal image after the object is
released (see Figure 2b).
Intensity at a pixel in the ther-
mal image is a function of the infrared energy emitted by
the corresponding world point [51]. Hence, object pixel in-

8710

(a) Data collection area setup

(b) Data processing pipeline, explained in detail in Section 3.3

Figure 2: Data collection and processing for ContactDB. Participants grasp 3D printed objects and put them on the rotating
turntable. Thermal images from multiple views are texture-mapped to the object mesh.

Participants
Objects
Textured meshes
RGBD-Thermal frames

Functional Intent
Use
Hand-off
50
50 (same)
27
48 (overlapping)
1350
2400
135K
240K

Total

50
3750
375K

Table 1: Size of the ContactDB Dataset

tensity in our thermal images is related to heat of the skin,
duration of contact, heat conduction (including diffusion to
nearby object locations), and contact pressure. By keep-
ing these factors roughly constant during data collection, we
veriﬁed empirically that heat conduction from hand-object
contact is the dominant factor in the observed thermal mea-
surements. See the supplementary material for more discus-
sion on heat dissipation and accuracy.

3.1. Object Selection and Fabrication

We decided to focus on household objects since an un-
derstanding of contact preferences and the ability to predict
them are most likely to improve human-robot interaction
in household settings. Other standard grasping datasets [7]
and competitions [10] have a similar focus. We started
with the YCB dataset [7] to choose the 50 objects in our
dataset. We excluded similarly-shaped objects (e.g. cereal
and cracker boxes) that are unlikely to produce different
kinds of grasps, deformable objects (e.g. sponge, plastic
chain, nylon rope), very small (e.g. dominoes, washers),
and very large objects (e.g. cooking skillet, Windex bot-

tle). We added common ones such as ﬂashlight, eyeglasses,
computer mouse, and objects popular in computer graphics
(e.g. Stanford bunny and Utah teapot). Since object size
has been shown to inﬂuence the grasp [11, 8] and we are
interested in contact during grasping of abstract shapes, we
included 5 primitive objects–cube, cylinder, pyramid, torus
and sphere–at 3 different scales (principal axes 12, 8 and 4
cm). See the supplementary material for a full object list.
We chose to 3D print all the objects to ensure uniform
heat dissipation properties. Additionally, we empirically
found that the PLA material used for 3D printing is ex-
cellent for retaining thermal handprints. We used open-
source resources to select suitable models for each object,
and printed them at 15% inﬁll density using white PLA ﬁl-
ament on a Dremel 3D20 printer. 3D printing the objects
has additional advantages. Having an accurate 3D model
of the object makes 6D pose estimation of the object from
recorded pointcloud data easier (see Section 3.3), which we
use for texture mapping contact maps to the object mesh.
3D printing the objects also allows participants to focus on
the object geometry during grasping.

3.2. Data Collection Protocol

Figure 2a shows our setup. We rigidly mounted a FLIR
Boson 640 thermal camera on a Kinect v2 RGB-D sensor.
The instrinsics of both the cameras and extrinsics between
them are calibrated using ROS [41], so that both RGB and
depth images from the Kinect can be accurately registered
to the thermal image. We invited 50 participants (mostly
20-25 years of age, able-bodied males and females), and

8711

used the following protocol approved by the Georgia Tech
Institutional Review Board.
50 3D printed objects were placed at random locations
on a table in orientations commonly encountered in prac-
tice. Participants were asked to grasp each object with a
post-grasp functional intent. They held the object for 5 sec-
onds to allow heat transfer from the hand to the object, and
then hand it to an experimenter. The experimenter wore an
insulating glove to prevent heat transfer from their hand,
and places the object on a turntable about 1 m away from
the cameras. Participants were provided with chemical hand
warmers to increase the intensity of thermal handprints. The
cameras recorded a continuous stream of RGB, depth and
thermal images as the turntable rotated in a 360 degree
arc. The turntable paused at 9 equally spaced locations on
this arc, where the rotation angle of the turntable was also
recorded. In some cases, objects were ﬂipped and scanned a
second time to capture any thermal prints that were unseen
in the previous rotation.
We used two post-grasp functional intents:
‘use’ and
‘hand-off ’. Participants were instructed to grasp 48 objects
with the intent of handing them off to the experimenter, and
to grasp a subset of 27 objects (after the previous thermal
handprints had dissipated) with the intent of using them.
We used only a subset of 27 objects for ‘use’, since other
objects (e.g. pyramid, Stanford bunny) lack clear use cases.
See the supplementary material for speciﬁc use instructions.
Participants were asked to avoid in-hand manipulation after
grasping to avoid smudging the thermal handprints.

3.3. Data Processing

As the turntable rotates with the object on it, the stream
of RGB-D and thermal images capture the object from mul-
tiple viewpoints. The aim of data processing is to texture-
map the thermal images to the object 3D mesh and generate
a coherent contact map (examples are shown in Figure 1).
The entire process is shown in Figure 2b. We ﬁrst ex-
tracted the corresponding turntable angle and RGB, depth
and thermal images at the 9 locations where the turntable
pauses. Next, we converted the depth maps to pointclouds
and useed a least-squares estimate of the turntable plane and
white color segmentation to segment the object. We used
the Iterative Closest Point (ICP) [5] algorithm implemented
in PCL [44] to estimate the full 6D pose of the object in
the 9 segmented pointclouds. Object origins in the 9 views
were used to get a least squares estimate of the 3D circle
described by the moving object. This circle was used to in-
terpolate the object poses for views which are unsuitable for
the ICP step because of noise in the depth map or important
shape elements of the object being hidden in that view, or
for rotating symmetric objects around the axis of symmetry.
Finally, the 3D mesh along with the 9 pose estimates and
thermal images were input to the colormap optimization al-

Active Area
Banana tip (either tip)
Binoculars (both barrels)
Camera shutter button
Eyeglasses (both temples)
Flashlight button
Hammer (head)
Mouse (both click buttons)
PS controller (both front buttons)
PS controller (both analog sticks)
Scissors (handle)
Scissors (blade)
Water-bottle cap
Wine glass stem

handoff
22.45
12.50
34.00
4.00
28.00
38.00
16.00
2.00
2.00
38.00
60.00
16.00
56.00

use
63.27
93.88
69.39
64.58
62.00
0.00
84.00
40.81
22.44
100.00
0.00
67.35
30.61

Table 2: Fraction of participants that touched active areas
for different functional intents. See Fig. 3 for examples.

gorithm of [55], which is implemented in Open3D [56]. It
locally optimizes object poses to minimize the photomet-
ric texture projection error and generates a mesh coherently
textured with contact maps.

4. Analysis of Contact Maps

In this section we present analysis of some aspects of hu-
man grasping, using the data in ContactDB. We processed
each contact map separately to increase contrast by apply-
ing a sigmoid function to the texture-mapped intensity val-
ues that maps the minimum to 0.05 and maximum to 0.95.
Effect of Functional Intent. We observed that
the
functional
intent (‘use’ or ‘hand off ’) signiﬁcantly in-
ﬂuences the contact patterns for many objects.
To
show qualitative examples, we clustered the contact maps
within each object and functional intent category using
k-medoids clustering [24] (k = 3) on the XYZ values
of points which have contact value above 0.4. The dis-
tance function between two sets of points was deﬁned

as d(p1 , p2 ) = (cid:0) ¯d(p1 , p2 ) + ¯d(p2 , p1 )(cid:1) / (|p1 | + |p2 |),

(i)
1 − p

(j )

i=1 min|p2 |
j=1 ||p

where ¯d(p1 , p2 ) = P|p1 |
2 ||2 . For sym-
metric objects, we chose the angle of rotation around the
axis of symmetry that minimized d(p1 , p2 ). Figure 3 shows
dominant contact maps (center of the largest cluster) for the
two different functional intents.
To quantify the inﬂuence of functional intent, we deﬁne
‘active areas’ (highlighted in green in Figure 3) on the sur-
face of some objects and show the fraction of participants
that touched that area (evidenced by the map value being
greater than 0.4) in Table 2.
Effect of object size. Figure 4 shows the dominant con-
tact maps for objects of the same shape at three different
sizes. Small objects exhibit grasps with two or three ﬁn-
gertips, while larger objects are often grasped with more
ﬁngers and more than the ﬁngertips in contact with the ob-

8712

Figure 3: Inﬂuence of functional intent on contact: Two views of the dominant grasp (center of the largest cluster after
k-medoids clustering across participants). Green circles indicate ‘active areas’. This inﬂuence is quantiﬁed in Table 2.

single point. However, the contact maps in Figures 1, 3 and
4 show that human grasps have much more than ﬁngertip
contact. Single-point contact modeling is inspired by the
prevalence of rigid manipulators on robots, but with the re-
cent research interest in soft robots [13, 15], we now have
access to manipulators that contact the object at other areas
on the ﬁnger. Data in ContactDB shows the use of non-
ﬁngertip contact for highly capable soft manipulators: hu-
man hands. For each contact map, we calculated the contact
area by integrating the area of all the contacted faces in the
mesh. A face is contacted if any of its three vertices have a
contact value greater than 0.4. Figures 5(b) and 5(c) show
the contact areas for all objects under both functional in-
tents, averaged across participants. Next, we calculated an
upper bound on the contact area if only all 5 ﬁngertips were
touching the object. This was done by capturing the partici-
pants’ palm print on a ﬂat plate, where it is easy to manually
annotate the ﬁngertip regions (shown in Figure 5(a)). The
total surface area of ﬁngertips in the palm print is the de-
sired upper bound. It was doubled for objects for which we
observe bimanual grasps. This upper bound was averaged
across four participants, and is shown as the red line in Fig-
ures 5(b) and 5(c). Note that this is a loose upper bound,
since many real-world ﬁngertip-only grasps don’t involve
all ﬁve ﬁngertips, and we mark the entire object category

8713

Figure 4: Inﬂuence of object size on contact: Two dominant
grasps for objects of same shape and varying size.

ject. Grasps for large objects are bi-modal: bimanual using
the full hands, or single-handed using ﬁngertips. To quan-
tify this, we manually labelled grasps as bimanual/single-
handed, and show their relation to hand size in Fig. 6.
The ﬁgure shows that people with smaller hands prefer to
grasp large objects (for ‘handoff ’) with bimanual grasps.
No bimanual grasps were observed for the medium and
small object sizes.

How much of the contact is ﬁngertips? Contact is tradi-
tionally modelled in robotics [47] and simulation [53] as a

Figure 5: (a): Palm contact on plate, annotated ﬁngertips. (b, c): Contact areas for objects in ContactDB, averaged across
participants. The red line indicates a loose upper bound on contact area for a ﬁngertip-only grasp, which is doubled for
objects which have bimanual grasps.

Figure 6: Relationship between hand length (wrist to mid
ﬁngertip) and single-handed/bimanual grasps. The intervals
show mean and 1 standard deviation. Cube, cylinder, pyra-
mid and sphere are of the large size.

as bimanual if even one participant performs a bimanual
grasp. Total contact area for many objects is signiﬁcantly
higher than the upper bound on ﬁngertip-only contact area,
indicating the large role that the soft tissue of the human
hand plays in grasping and manipulation. This motivates
the inclusion of non-ﬁngertip areas in grasp prediction and
modeling algorithms, and presents an opportunity to inform
the design of soft robotic manipulators. Interestingly, the
average contact area for some objects (e.g. bowl, mug, PS
controller, toothbrush) differs across functional intent, due
to different kinds of grasps used.

5. Predicting Contact Maps

In this section, we describe experiments to predict con-
tact maps for objects based on their shape. ContactDB is the
ﬁrst large scale dataset that enables training data-intensive
deep learning models for this task. Since ContactDB in-
cludes diverse contact maps for each object, the mapping
from object shape to contact map is one-to-many and makes
the task challenging. We explore two representations for ob-
ject shape: single-view RGB-D, and full 3D. Since the con-
tact patterns are signiﬁcantly inﬂuenced by the functional
intent, we train separate models for ‘hand-off ’ and ‘use’.

Figure 7: Training procedure for single-view contact map
prediction. The discriminator has 5 conv layers followed by
batch norm and leaky ReLU.

5.1. Single-view Prediction

Object shape is represented by an RGB-D image, and a
2D contact map is predicted for the visible part of the ob-
ject. A single view might exclude information about im-
portant aspects of the object shape, and ‘interesting’ parts
of the contact map might lie in the unseen half of the ob-
ject. However, this representation has the advantage of be-
ing easily applicable to real-world robotics scenarios where
mobile manipulators are often required to grasp objects af-
ter observing them from a single view. We used generative
adversarial network (GAN)-based image-to-image transla-
tion [22, 57, 30] for this task, since the optimization proce-
dure of conditional GANs is able to model a one-to-many
input-output mapping [35, 18].

Figure 7 shows our training procedure and network ar-
chitecture, which has roughly 54M and 3M parameters in
the generator and discriminator respectively. We modiﬁed
pix2pix [22] to accept a 4-channel RGB-D input and pre-
dict a single-channel contact map. The RGB-D stream from
object scanning was registered to the thermal images, and
used as input. Thermal images were used as a proxy for
the single-view contact map. To focus the generator and
discriminator on the object, we cropped a 256×320 patch
around the object and masked all images by the object sil-

8714

Figure 8: Single-view predictions from the pix2pix model
for three unseen object classes: mug, pan and wine glass.
Top: handoff intent, bottom: use intent. Rightmost column:
uninterpretable predictions.

houette. All images from mug, pan, and wineglass were
held out and used for testing. Figure 8 shows some pre-
dicted contact maps for these unseen objects, selected for
looking realistic. Mug predictions for use have ﬁnger con-
tact on the handle, whereas contact is observed over the
top for handoff. Pan use predictions show grasps at the
handle, while handoff predictions additionally show a bi-
manual grasp of the handle and side. Similarly, the wine
glass indicates contact with a side grasp for use and over
the opening for handoff.

5.2. 3D Prediction

Full 3D representation gives access to the entire shape
of the object, and alleviates the view-consistency problems
observed during single-view prediction.
Learning a one-to-many-mapping. Stochastic Multiple
Choice Learning [27] (sMCL) trains an ensemble of k pre-
dictors to generate k contact maps for each input (see Fig-
ure 9a). Each input has multiple equally correct ground
truth maps. During training, the loss is backpropagated
from each ground truth contact map to the network that
makes the prediction closest to it. To encourage all mem-
bers of the ensemble to be trained equally, as mentioned
in [43], we made this association soft by routing the gradi-
ent to the closest network with a 0.95 weight and distributed
the rest equally among other members of the ensemble, and
randomly dropped entire predictions with a 0.1 probability.
We trained models with k = 1 and k = 10.
In contrast, DiverseNet [14] generates diverse predic-
tions from a single predictor network by changing the value
of a one-hot encoded control variable c that is concatenated
to internal feature maps of the network (See Figure 9b).
Each ground truth contact map is associated with the closest
prediction and gradients are routed through the appropriate
c value. Diverse predictions can be generated at test time
by varying c. Compared to sMCL, DiverseNet requires sig-
niﬁcantly fewer trainable parameters. We used 10 one-hot
encoded c values in our experiments.
3D representation. We represented the 3D object shape in

two forms: pointcloud and voxel occupancy grid. Point-
Net [40] operates on a pointcloud representation of the ob-
ject shape, with points randomly sampled from the object
surface. We normalized the XYZ position of each point to
ﬁt the object in a unit cube. The XYZ position and the nor-
malization scale factor were used as 4-element features for
each point. The network was trained by cross entropy loss
to predict whether each voxel is in contact. We used a Point-
Net architecture with a single T-Net and 1.2M parameters.

VoxNet [33] operates on a solid occupancy grid of the
object in a 643 voxelized space, and predicts whether each
voxel is contacted. It uses 3D convolutions to learn shape
features. The four features used for PointNet were used in
addition to the binary occupancy value to form a 5-element
feature vector for each voxel. Cross entropy loss was en-
forced only on the voxels on the object surface. The net-
work architecture is shown in Figure 9b, and has approxi-
mately 1.2M parameters.

Experiments We conducted experiments with both VoxNet
and PointNet, using the sMCL and DiverseNet strategies for
learning a one-to-many-mapping. For DiverseNet, we con-
catenated c to the output of the ﬁrst and ﬁfth conv layers
in VoxNet, and to the input transformed by T-Net and the
output of the second-last MLP in PointNet. Voxelization
of the meshes was done using the algorithm of [37] imple-
mented in binvox [34]. The PointNet input was generated
by randomly sampling 3000 points from the object surface.
We thresholded the contact maps at 0.4 after applying the
sigmoid described in Section 4, to generate ground truth for
classiﬁcation. We augmented the dataset by randomly ro-
tating the object around the yaw axis. PointNet input was
also augmented by randomly choosing an axis and scaling
the points along that axis by a random factor in [0.6, 1.4].
Dropout with p = 0.2 was applied to VoxNet-DiverseNet
input. We found that similar dropout did not improve results
for other models. Random sampling of surface points auto-
matically acts like dropout for PointNet models, and sMCL
models already incorporate a different dropout strategy as
mentioned in Section 5.2. The cross entropy loss for con-
tacted voxels was weighted by a factor of 10, to account for
class imbalance. All models were trained with SGD with a
learning rate of 0.1, momentum of 0.9 and weight decay of
5e-4. Batch size was 5 for models with k = 10, and 25 for
models with k = 1.

Table 3 shows results on held-out test objects (mug, pan
and wine glass). We conclude that the voxel occupancy grid
representation is better for this task, and that a model lim-
ited to making a single prediction does not capture the com-
plexity in ContactDB. Figures 10a and 10b show some of
the ‘use’ intent predictions for unseen object classes and
unseen shapes of training object classes respectively, se-
lected for looking realistic. Mug predictions show hori-
zontal grasps around the body. Predictions for the pan are

8715

(a) sMCL with a PointNet predictor

(b) DiverseNet with a VoxNet predictor. CP: 33 conv with batch norm, ReLU and
max pooling, CU: 33 conv with batch norm, ReLU and nearest neighbor upsam-
pling. Black numbers: size of voxel grid, red numbers: number of channels.

Figure 9: 3D data representations and training strategies for predicting diverse contact maps. sMCL [27] requires multiple
instances of a network, while DiverseNet [14] uses a single instance with an integer valued control variable. PointNet [40]
operates on unordered point-clouds, whereas VoxNet [33] uses voxel occupancy grids.

Test object

pan
wine glass
mug
average

Handoff
sMCL (k = 1)
sMCL (k = 10)
DiverseNet (k = 10)
VoxNet
PointNet VoxNet
PointNet VoxNet
PointNet
76.80
-
7.13
20.43
8.48
19.68
59.37
-
11.11
14.59
28.69
17.28
29.93
-
16.68
27.10
15.77
21.60
55.37
-
11.64
20.71
17.65
19.52

Use
sMCL (k = 1)
sMCL (k = 10)
DiverseNet (k = 10)
VoxNet
PointNet VoxNet
PointNet VoxNet
PointNet
17.22
-
8.25
43.57
5.12
22.58
50.18
-
11.06
14.79
13.98
10.47
66.03
-
32.51
31.30
7.06
32.41
44.48
-
17.27
29.89
8.72
21.82

Table 3: Diverse 3D contact map prediction errors (%) for the models presented in Section 5.2. Errors were calculated by
matching each ground truth contact map with the closest from k diverse predictions, discarding predictions with no contact.
‘-’ indicates that no contact was predicted.

(a) Contact map predictions for unseen object classes

(b) Contact map predictions for an unseen shape of training object classes

Figure 10: Two views of diverse 3D contact map predictions. (a) Unseen object classes: mug, pan, and wine glass, (b)
Unseen shape of training object classes: camera and hammer. Intent: use, Model: VoxNet-DiverseNet, Red: contact.

concentrated at the handle, with one grasp being biman-
ual. Wine glass predictions show grasps at the body-stem
intersection. Camera predictions show contact at the shut-
ter button and sides, while predictions for the hammer show
contact at the handle (and once at the head).

6. Conclusion and Future Work

We presented ContactDB, the ﬁrst large-scale dataset of
contact maps from functional grasping, analyzed the data
to reveal interesting aspects of grasping behavior, and ex-
plored data representations and training strategies for pre-

dicting contact maps from object shape. We hope to spur
future work in multiple areas. Contact patterns could in-
form the design of soft robotic manipulators by aiming to
be able to cover object regions touched by humans. Re-
search indicates that in some situations hand pose can be
guided by contact points [53, 48]. Using contact maps to
recover and/or assist in predicting the hand pose in func-
tional grasping is an exciting problem for future research.

Acknowledgements: We would like to thank Varun
Agrawal for lending the 3D printer, Ari Kapusta for initial
discussions about thermal cameras, and NVIDIA for a Titan
Xp GPU grant.

8716

References

[1] Shuichi Akizuki and Yoshimitsu Aoki. Tactile logging for
understanding plausible tool use based on human demon-
stration. In British Machine Vision Conference 2018, BMVC
2018, Northumbria University, Newcastle, UK, September 3-
6, 2018, page 334, 2018. 2

[2] Caterina Ansuini, Livia Giosa, Luca Turella, Gianmarco
Alto `e, and Umberto Castiello. An object for an action, the
same object for other actions: effects on hand shaping. Ex-
perimental Brain Research, 185(1):111–119, 2008. 1

[3] Ravi Balasubramanian, Ling Xu, Peter D Brook, Joshua R
Smith, and Yoky Matsuoka.
Physical human interac-
tive guidance: Identifying grasping principles from human-
planned grasps. IEEE Transactions on Robotics, 4(28):899–
910, 2012. 1, 2

[4] Keni Bernardin, Koichi Ogawara, Katsushi Ikeuchi, and
Ruediger Dillmann. A sensor fusion approach for recog-
nizing continuous human grasping sequences using hidden
markov models. IEEE Transactions on Robotics, 21(1):47–
57, 2005. 2

[5] PJ Besl and Neil D McKay. A method for registration of 3-
d shapes. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 14(2):239–256, 1992. 4

[6] Ian M Bullock, Thomas Feix, and Aaron M Dollar. The
yale human grasping dataset: Grasp, object, and task data
in household and machine shop environments. The Interna-
tional Journal of Robotics Research, 34(3):251–255, 2015.
1, 2

[7] Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srini-
vasa, Pieter Abbeel, and Aaron M Dollar. Benchmarking
in manipulation research: The ycb object and model set and
benchmarking protocols. arXiv preprint arXiv:1502.03143,
2015. 3

[8] Umberto Castiello. The neuroscience of grasping. Nature
Reviews Neuroscience, 6(9):726, 2005. 1, 3

[9] Changhyun Choi, Wilko Schwarting, Joseph DelPreto, and
Daniela Rus. Learning object grasping for soft robot hands.
IEEE Robotics and Automation Letters, 2018. 2

[10] Nikolaus Correll, Kostas E Bekris, Dmitry Berenson, Oliver
Brock, Albert Causo, Kris Hauser, Kei Okada, Alberto Ro-
driguez, Joseph M Romano, and Peter R Wurman. Analysis
and observations from the ﬁrst amazon picking challenge.
IEEE Transactions on Automation Science and Engineering,
15(1):172–188, 2018. 3

[11] Raymond H Cuijpers, Jeroen BJ Smeets, and Eli Brenner. On
the relation between object shape and grasping kinematics.
Journal of Neurophysiology, 91(6):2598–2606, 2004. 3

[12] Mark R Cutkosky. On grasp choice, grasp models, and the
design of hands for manufacturing tasks. IEEE Transactions
on robotics and automation, 5(3):269–279, 1989. 2

[13] Raphael Deimel and Oliver Brock. A novel type of com-
pliant and underactuated robotic hand for dexterous grasp-
ing. The International Journal of Robotics Research, 35(1-
3):161–185, 2016. 5

[14] Michael Firman, Neill DF Campbell, Lourdes Agapito, and
Gabriel J Brostow. Diversenet: When one right answer is not

enough.
In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 5598–5607,
2018. 2, 7, 8
[15] Kevin C Galloway, Kaitlyn P Becker, Brennan Phillips, Jor-
dan Kirby, Stephen Licht, Dan Tchernov, Robert J Wood,
and David F Gruber. Soft robotic grippers for biological sam-
pling on deep reefs. Soft robotics, 3(1):23–33, 2016. 5
[16] Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul
Baek, and Tae-Kyun Kim. First-person hand action bench-
mark with rgb-d videos and 3d hand pose annotations.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 409–419, 2018. 2
[17] Ghazal Ghazaei, Iro Laina, Christian Rupprecht, Federico
Tombari, Nassir Navab, and Kianoush Nazarpour. Dealing
with ambiguity in robotic grasping via multiple predictions.
arXiv preprint arXiv:1811.00793, 2018. 2
[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets.
In Advances
in neural information processing systems, pages 2672–2680,
2014. 6
[19] Henning Hamer, Juergen Gall, Thibaut Weise, and Luc
Van Gool. An object-dependent hand pose prior from sparse
training data. In 2010 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pages 671–678.
IEEE, 2010. 2
[20] Guido Heumer, Heni Ben Amor, Matthias Weber, and Bern-
hard Jung. Grasp recognition with uncalibrated data gloves-
a comparison of classiﬁcation methods.
In Virtual Reality
Conference, 2007. VR’07. IEEE, pages 19–26. IEEE, 2007.
1, 2
[21] De-An Huang, Minghuang Ma, Wei-Chiu Ma, and Kris M.
Kitani. How do we use our hands? discovering a diverse set
of common grasps. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2015. 1, 2
[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adversar-
ial networks. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5967–5976. IEEE,
2017. 6
[23] Noriko Kamakura, Michiko Matsuo, Harumi Ishii, Fumiko
Mitsuboshi, and Yoriko Miura. Patterns of static prehension
in normal hands. American Journal of Occupational Ther-
apy, 34(7):437–445, 1980. 2
[24] Leonard Kaufman and Peter Rousseeuw. Clustering by
means of medoids. North-Holland, 1987. 4
[25] Eric Larson, Gabe Cohn, Sidhant Gupta, Xiaofeng Ren, Bev-
erly Harrison, Dieter Fox, and Shwetak Patel. Heatwave:
Thermal imaging for surface user interaction. In Proceedings
of the SIGCHI Conference on Human Factors in Computing
Systems, CHI ’11, pages 2565–2574, New York, NY, USA,
2011. ACM. 2
[26] Manfred Lau, Kapil Dev, Weiqi Shi, Julie Dorsey, and Holly
Rushmeier. Tactile mesh saliency. ACM Transactions on
Graphics (TOG), 35(4):52, 2016. 2
[27] Stefan Lee, Senthil Purushwalkam Shiva Prakash, Michael
Cogswell, Viresh Ranjan, David Crandall, and Dhruv Batra.

8717

Stochastic multiple choice learning for training diverse deep
ensembles. In Advances in Neural Information Processing
Systems, pages 2119–2127, 2016. 2, 7, 8
[28] Ian Lenz, Honglak Lee, and Ashutosh Saxena. Deep learning
for detecting robotic grasps. The International Journal of
Robotics Research, 34(4-5):705–724, 2015. 2
[29] Yun Lin and Yu Sun. Grasp planning based on strategy ex-
tracted from demonstration. In Intelligent Robots and Sys-
tems (IROS 2014), 2014 IEEE/RSJ International Conference
on, pages 4458–4463. IEEE, 2014. 1, 2
[30] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In Advances in Neural
Information Processing Systems, pages 700–708, 2017. 6
[31] Rachel Luo, Ozan Sener, and Silvio Savarese. Scene seman-
tic reconstruction from egocentric rgb-d-thermal videos. In
2017 International Conference on 3D Vision (3DV), pages
593–602. IEEE, 2017. 2
[32] Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey,
Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken
Goldberg. Dex-net 2.0: Deep learning to plan robust grasps
with synthetic point clouds and analytic grasp metrics. arXiv
preprint arXiv:1703.09312, 2017. 2
[33] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-
volutional neural network for real-time object recognition. In
Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ In-
ternational Conference on, pages 922–928. IEEE, 2015. 7,
8
[34] Patrick Min. binvox. http://www.patrickmin.com/
binvox, 2004 - 2017. Accessed: 2018-11-16. 7
[35] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. CoRR, abs/1411.1784, 2014. 6
[36] Yuzuko C Nakamura, Daniel M Troniak, Alberto Rodriguez,
Matthew T Mason, and Nancy S Pollard. The complexities
of grasping in the wild. In Humanoid Robotics (Humanoids),
2017 IEEE-RAS 17th International Conference on, pages
233–240. IEEE, 2017. 1, 2
[37] Fakir S. Nooruddin and Greg Turk.
Simpliﬁcation and
repair of polygonal models using volumetric techniques.
IEEE Transactions on Visualization and Computer Graph-
ics, 9(2):191–205, 2003. 7
[38] Tu-Hoa Pham, Nikolaos Kyriazis, Antonis A Argyros, and
Abderrahmane Kheddar. Hand-object contact force estima-
tion from markerless visual tracking.
IEEE transactions
on pattern analysis and machine intelligence, 40(12):2883–
2896, 2018. 2
[39] Steffen Puhlmann, Fabian Heinemann, Oliver Brock, and
Marianne Maertens. A compact representation of human
single-object grasping.
In 2016 IEEE International Con-
ference on Intelligent Robots and Systems (IROS), page
1954–1959. IEEE, 2016. 2
[40] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 652–660,
2017. 7, 8
[41] Morgan Quigley, Josh Faust, Tully Foote, and Jeremy Leibs.
Ros: an open-source robot operating system. 3

[42] Gr ´egory Rogez, James S Supancic, and Deva Ramanan. Un-
derstanding everyday hands in action from rgb-d images. In
2015 IEEE International Conference on Computer Vision
(ICCV), pages 3889–3897. IEEE, 2015. 2

[43] Christian Rupprecht, Iro Laina, Robert DiPietro, Maximil-
ian Baust, Federico Tombari, Nassir Navab, and Gregory D
Hager. Learning in an uncertain world: Representing am-
biguity through multiple hypotheses. In Proceedings of the
IEEE International Conference on Computer Vision, pages
3591–3600, 2017. 7

[44] Radu Bogdan Rusu and Steve Cousins. 3D is here: Point
Cloud Library (PCL). In IEEE International Conference on
Robotics and Automation (ICRA), Shanghai, China, May 9-
13 2011. 4

[45] Luisa Sartori, Elisa Straulino, and Umberto Castiello. How
objects are grasped: the interplay between affordances and
end-goals. PloS one, 6(9):e25203, 2011. 1

[46] Artur Saudabayev, Zhanibek Rysbek, Raykhan Khassenova,
and Huseyin Atakan Varol. Human grasping database for
activities of daily living with depth, color and kinematic data
streams. Scientiﬁc data, 5, 2018. 1, 2

[47] Tanner Schmidt, Katharina Hertkorn, Richard Newcombe,
Zoltan Marton, Michael Suppa, and Dieter Fox. Depth-based
tracking with physical constraints for robot manipulation. In
2015 IEEE International Conference on Robotics and Au-
tomation (ICRA), pages 119–126. IEEE, 2015. 5

[48] Srinath Sridhar, Franziska Mueller, Michael Zollh ¨ofer, Dan
Casas, Antti Oulasvirta, and Christian Theobalt. Real-time
joint tracking of a hand manipulating an object from rgb-d
input. In European Conference on Computer Vision, pages
294–310. Springer, 2016. 8

[49] Danhang Tang, Hyung Jin Chang, Alykhan Tejani, and Tae-
Kyun Kim. Latent regression forest: Structured estimation of
3d articulated hand posture. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
3786–3793, 2014. 1, 2

[50] Jonathan Tompson, Murphy Stein, Yann Lecun, and Ken
Perlin. Real-time continuous pose recovery of human hands
using convolutional networks. ACM Transactions on Graph-
ics (ToG), 33(5):169, 2014. 1, 2

[51] Michael Vollmer and Klaus-Peter M ¨ollmann. Infrared ther-
mal imaging: fundamentals, research and applications. John
Wiley & Sons, 2017. 2

[52] Yezhou Yang, Cornelia Fermuller, Yi Li, and Yiannis Aloi-
monos. Grasp type revisited: A modern perspective on a
classical feature for vision. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2015. 1,
2

[53] Yuting Ye and C Karen Liu. Synthesis of detailed hand ma-
nipulations using contact sampling. ACM Transactions on
Graphics (TOG), 31(4):41, 2012. 5, 8

[54] Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, and Tae-
Kyun Kim. Bighand2. 2m benchmark: Hand pose dataset
and state of the art analysis. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4866–4874, 2017. 2

8718

[55] Qian-Yi Zhou and Vladlen Koltun. Color map optimization
for 3d reconstruction with consumer depth cameras. ACM
Transactions on Graphics (TOG), 33(4):155, 2014. 4
[56] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A
modern library for 3D data processing. arXiv:1801.09847,
2018. 4
[57] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In IEEE International Con-
ference on Computer Vision, 2017. 6

8719

Content Authentication for Neural Imaging Pipelines: End-to-end
Optimization of Photo Provenance in Complex Distribution Channels

Pawe l Korus1,2 and Nasir Memon1
New York University1 , AGH University of Science and Technology2

http://kt.agh.edu.pl/~korus

Abstract

Forensic analysis of digital photo provenance relies
on intrinsic traces left in the photograph at the time
of its acquisition. Such analysis becomes unreliable af-
ter heavy post-processing, such as down-sampling and
re-compression applied upon distribution in the Web.
This paper explores end-to-end optimization of the en-
tire image acquisition and distribution workﬂow to fa-
cilitate reliable forensic analysis at the end of the dis-
tribution channel. We demonstrate that neural imag-
ing pipelines can be trained to replace the internals of
digital cameras, and jointly optimized for high-ﬁdelity
photo development and reliable provenance analysis. In
our experiments, the proposed approach increased im-
age manipulation detection accuracy from 45% to over
90%. The ﬁndings encourage further research towards
building more reliable imaging pipelines with explicit
provenance-guaranteeing properties.

1. Introduction

Ensuring integrity of digital images is one of the
most challenging and important problems in multime-
dia communications. Photographs and videos are com-
monly used for documentation of important events, and
as such, require eﬃcient and reliable authentication
protocols. Our current media acquisition and distribu-
tion workﬂows are built with entertainment in mind,
and not only fail to provide explicit security features,
but actually work against them.
Image compression
standards exploit heavy redundancy of visual signals
to reduce communication payload, but optimize for hu-
man perception alone. Security extensions of popular
standards lack in adoption [20].
Two general approaches to assurance and veriﬁca-
tion of digital image integrity include [24, 30, 32]: (1)
pro-active protection methods based on digital signa-
tures or watermarking; (2) passive forensic analysis

which exploits inherent statistical properties result-
ing from the photo acquisition pipeline. While the
former provides superior performance and allows for
advanced protection features (like precise tampering
localization [38], or reconstruction of tampered con-
tent [25]), it failed to gain widespread adoption due to
the necessity to generate protected versions of the pho-
tographs, and the lack of incentives for camera vendors
to modify camera design to integrate such features [4].
Passive forensics, on the other hand, relies on our
knowledge of the photo acquisition pipeline, and statis-
tical artifacts introduced by its successive steps. While
this approach is well suited for potentially analyzing
any digital photograph, it often falls short due to the
complex nature of image post-processing and distribu-
tion channels. Digital images are not only heavily com-
pressed, but also enhanced or even manipulated before,
during or after dissemination. Popular images have
many online incarnations, and tracing their distribu-
tion and evolution has spawned a new ﬁeld of image
phylogeny [13, 14] which relies on visual diﬀerences be-
tween multiple images to infer their relationships and
editing history. However, phylogeny does not provide
any tools to reason about the authenticity or history
of individual images. Hence, reliable authentication of
real-world online images remains untractable [39].
At the moment, forensic analysis often yields useful
results in near-acquisition scenarios. Analysis of native
images straight from the camera is more reliable, and
even seemingly benign implementation details - like the
rounding operators used in the camera’s image signal
processor [1] - can provide useful clues. Most forensic
traces quickly become unreliable as the image under-
goes further post-processing. One of the most reliable
tools at our disposal involves analysis of the imaging
sensor’s artifacts (the photo response non-uniformity
pattern) which can be used both for source attribution
and content authentication problems [10].
In the near future, rapid progress in computational
imaging will challenge digital image forensics even in

8621

Cross entropy loss

Classification labels: pristine, post­proc. A ­ Z

MUX

Downscale

JPEG 
Compression 

FAN

=

RAW 
measurements 

NIP

Developed
image

L2 loss

=

Desired
output image 

Post­proc. 
A 

Post­proc. 
B

Post­proc. 
Y 

Post­proc. 
Z

Acquisition

Post-processing / editing

Distribution

Analysis

Figure 1. Optimization of the image acquisition and distribution channel to facilitate photo provenance analysis. The neural
imaging pipeline (NIP) is trained to develop images that both resemble the desired target images, but also retain meaningful
forensic clues at the end of complex distribution channels.

near-acquisition authentication. In the pursuit of bet-
ter image quality and convenience, digital cameras and
smartphones employ sophisticated post-processing di-
rectly in the camera, and soon few photographs will
resemble the original images captured by the sensor(s).
Adoption of machine learning has recently challenged
many long-standing limitations of digital photogra-
phy,
including:
(1) high-quality low-light photogra-
phy [9]; (2) single-shot HDR with overexposed con-
tent recovery [15]; (3) practical high-quality digital
zoom from multiple shots [37]; (4) quality enhancement
of smartphone-captured images with weak supervision
from DSLR photos [18].
These remarkable results demonstrate tangible ben-
eﬁts of replacing the entire acquisition pipeline with
neural networks. As a result, it will be necessary to
investigate the impact of the emerging neural imaging
pipelines on existing forensics protocols. While impor-
tant, such evaluation can be seen rather as damage
assessment and control and not as a solution for the
future. We believe it is imperative to consider novel
possibilities for security-oriented design of our cameras
and multimedia dissemination channels.
In this paper, we propose to optimize neural imaging
pipelines to improve photo provenance in complex dis-
tribution channels. We exploit end-to-end optimization
of the entire photo acquisition and distribution chan-
nel to ensure that reliable forensics decisions can be
made even after complex post-processing, where clas-
sical forensics fails (Fig. 1). We believe that immi-
nent revolution in camera design creates a unique op-
portunity to address some of the long-standing limi-
tations of the current technology. While adoption of
digital watermarking in image authentication was lim-

ited by the necessity to modify camera hardware, our
approach exploits the ﬂexibility of neural networks to
learn relevant integrity-preserving features within the
expressive power of the model. We believe that with
solid security-oriented understanding of neural imaging
pipelines, and with the rare opportunity of replacing
the well-established and security-oblivious pipeline, we
can signiﬁcantly improve digital image authentication
capabilities.
We aim to inspire discussion about novel camera de-
signs that could improve photo provenance analysis ca-
pabilities. We demonstrate that it is possible to opti-
mize an imaging pipeline to signiﬁcantly improve de-
tection of photo manipulation at the end of a complex
real-world distribution channel, where state-of-the-art
deep-learning techniques fail. The main contributions
of our work include:

1. The ﬁrst end-to-end optimization of the imaging
pipeline with explicit photo provenance ob jectives;

2. The ﬁrst security-oriented discussion of neural
imaging pipelines and the inherent trade-oﬀs;

3. Signiﬁcant improvement of forensic analysis per-
formance in challenging, heavily post-processed
conditions;

4. A neural model of the entire photo acquisition and
distribution channel with a fully diﬀerentiable ap-
proximation of the JPEG codec.

To facilitate further research in this direction, and
enable reproduction of our results, our neural imaging

toolbox is available at https://github.com/pkorus/
neural-imaging.

8622

2. Related Work

Trends in Pipeline Design Learning individual
steps of the imaging pipeline (e.g., demosaicing) has
a long history [21] but regained momentum in the re-
cent years thanks to adoption of deep learning. Nat-
urally, the research focused on the most diﬃcult op-
erations, i.e., demosaicing [17, 23, 34, 35] and denois-
ing [6, 26, 40]. Newly developed techniques delivered
not only improved performance, but also additional fea-
tures. Gharbi et al. proposed a convolutional neural
network (CNN) trained for joint demosaicing and de-
noising [17]. A recent work by Syu et al. proposes to
exploit CNNs for joint optimization of the color ﬁlter
array and a corresponding demosaicing ﬁlter [34].
Optimization of digital camera design can go even
further. The recently proposed L3 model by Jiang et al.
replaces the entire imaging pipeline with a large collec-
tion of local linear ﬁlters [19]. The L3 model reproduces
the entire photo development process, and aims to facil-
itate research and development eﬀorts for non-standard
camera designs. In the original paper, the model was
used for learning imaging pipelines for RGBW (red-
green-blue-white) and RGB-NIR (red-green-blue-near-
infra-red) color ﬁlter arrays.
Replacing the entire imaging pipeline with a mod-
ern CNN can also overcome long-standing limitations
of digital photography. Chen et al.
trained a UNet
model
[31] to develop high-quality photographs in
low-light conditions [9] by exposing it to paired ex-
amples of images taken with short and long expo-
sure. The network learned to develop high-quality well-
exposed color photographs from underexposed raw in-
put, and yielded better performance than traditional
image post-processing based on brightness adjustment
and denoising. Eilertsen et al. also trained a UNet
model to develop high-dynamic range images from a
single shot [15]. The network not only learned to cor-
rectly perform tone mapping, but was also able to re-
cover overexposed highlights. This signiﬁcantly sim-
pliﬁes HDR photography by eliminating the need for
bracketing and dealing with ghosting artifacts.

Trends in Forensics The current research in foren-
sic image analysis focuses on two main directions: (1)
learning deep features relevant to low-level forensic
analysis for problems like manipulation detection [3,
41], identiﬁcation of the social network of origin [2],
camera model identiﬁcation [11], or detection of arti-
ﬁcially generated content [27]; (2) adoption of high-
level vision to automate manual analysis that exposes
physical inconsistencies, such as reﬂections [33, 36], or
shadows [22]. To the best of our knowledge, there are
currently no eﬀorts to either assess the consequences

of the emerging neural imaging pipelines, or to exploit
this opportunity to improve photo reliability.

3. End-to-end Optimization of Photo
Provenance Analysis

Digital image forensics relies on intrinsic statisti-
cal artifacts introduced to photographs at the time of
their acquisition. Such traces are later used for reason-
ing about the source, authenticity and processing his-
tory of individual photographs. The main problem is
that contemporary media distribution channels employ
heavy compression and post-processing which destroy
the traces and inhibit forensic analysis.
The core of the proposed approach is to model the
entire acquisition and distribution channel, and opti-
mize the neural imaging pipeline (NIP) to facilitate
photo provenance analysis after content distribution
(Fig. 1). The analysis is performed by a forensic anal-
ysis network (FAN) which makes a decision about the
authenticity/processing history of the analyzed photo-
graph. In the presented example, the model is trained
to perform manipulation detection, i.e., aims to clas-
sify input images as either coming straight from the
camera, or as being aﬀected by a certain class of post-
processing. The distribution channel mimics the be-
havior of modern photo sharing services and social net-
works which habitually down-sample and re-compress
the photographs. As will be demonstrated later, foren-
sic analysis in such conditions is severely inhibited.
The parameters of the NIP are updated to guaran-
tee both faithful representation of a desired color pho-
tograph (L2 loss), and accurate decisions of forensics
analysis at the end of the distribution channel (cross-
entropy loss). Hence, the parameters of the NIP and
FAN models are chosen as:

θ∗

θfan Xn Xc

θ∗

θnip Xn (cid:16)kyn − nip(xn | θnip )k2

nip = argmin
(1a)
+ Xc
log(cid:0)fanc (dc (nip(xn | θnip )) | θfan )(cid:1)(cid:17) (1b)
fan = argmin
log(cid:0)fanc (dc (nip(xn | θnip )) | θfan )(cid:1)
where: θnip/fan are the parameters of the NIP and FAN
networks, respectively; xn are the raw sensor measure-
ments for the n-th example patch; yn is the correspond-
ing target color image; nip(xn ) is the color RGB image
developed by NIP from xn ; dc () denotes a color image
patch processed by manipulation c; fanc () is the prob-
ability that an image belongs to the c-th manipulation
class, as estimated by the FAN model.

8623

RAW 
(h, w, 1) 

LUT 
mapping 

Normalization
& calibration 

White
balancing 

Demosaicing 

Denoising 

Color space
conversion 

Gamma 
correction 

RGB 
(h, w, 3)

½h, ½w, 4

NIP 

RGB 
(h, w, 3) 

Figure 2. Adoption of a neural imaging pipeline to develop raw sensor measurements into color RGB images: (top) the
standard imaging pipeline; (bottom) adoption of the NIP model.

3.1. The Neural Imaging Pipeline

We replace the entire imaging pipeline with a CNN
which develops raw sensor measurements into color
RGB images (Fig. 2). Before feeding the images to
the network, we pre-process them by reversing the non-
linear value mapping according to the camera’s LUT,
subtracting black levels from the edge of the sensor,
normalizing by sensor saturation values, and apply-
ing white-balancing according to shot settings. We
also standardized the inputs by reshaping the tensors
to have feature maps with successive measured color
channels. This ensures a well-formed input of shape
( h
2 , 4) with values normalized to [0, 1]. All of the
remaining steps of the pipeline are replaced by a NIP.
See Section 4.1 for details on the considered pipelines.

2 , w

3.2. Approximation of JPEG Compression

To enable end-to-end optimization of the entire ac-
quisition and distribution channel, we need to ensure
that every processing step remains diﬀerentiable.
In
the considered scenario, the main problem is JPEG
compression. We designed a dJPEG model which ap-
proximates the standard codec, and expresses its suc-
cessive steps as matrix multiplications or convolution
layers that can be implemented in TensorFlow (see sup-
plementary materials for a detailed network deﬁnition):

• RGB to/from YCbCr color-space conversions are
implemented as 1 × 1 convolutions.

• Isolation of 8× 8 blocks for independent processing
is implemented by a combination of space-to-depth
and reshaping operations.

• Forward/backward 2D discrete cosine transforms
are implemented by matrix multiplication accord-
ing to DxDT where x denotes a 8 × 8 input, and
D denotes the transformation matrix.

• Division/multiplication of DCT coeﬃcients by the
corresponding quantization steps are implemented
as element-wise operations with properly tiled and
concatenated quantization matrices (for both the
luminance and chrominance channels).

• The actual quantization is approximated by a con-
tinuous function ρ(x)(see details below).

The key problem in making JPEG diﬀerentiable lies
in the rounding of DCT coeﬃcients. We considered
two approximations (Fig. 3a). Initially, we used a Tay-
lor series expansion, which can be made arbitrarily
accurate by including more terms. Finally, we used
a smoother, and simpler sinusoidal approximation ob-
tained by matching its phase with the sawtooth func-
tion:

(2)

ρ(x) = x −

sin(2πx)
2π
We validated our dJPEG model by comparing pro-
duced images with a reference codec from libJPEG. The
results are shown in Fig. 3bcd for a standard rounding
operation, and the two approximations, respectively.
We used 5 terms for the harmonic rounding. The devel-
oped module produces equivalent compression results
with standard rounding, and a good approximation for
its diﬀerentiable variants. Fig. 3e-h show a visual com-
parison of an example image patch, and its libJPEG
and dJPEG -compressed counterparts.
In our distribution channel, we used quality level 50.

3.3. The Forensic Analysis Network

The forensic analysis network (FAN) is implemented
as a CNN following the most recent recommendations
on construction of neural networks for forensics analy-
sis [3]. Bayar and Stamm proposed a new layer type,
which constrains the learned ﬁlters to be valid residual
ﬁlters [3]. Adoption of the layer helps ignore visual con-
tent and facilitates extraction of forensically-relevant
low-level features. In summary, our network operates
on 128 × 128 × 3 patches in the RGB color space and
includes (see supplement for full network deﬁnition):

• A constrained convolutions layer learning 5 × 5
residual ﬁlters and with no activation function.

• Four 5× 5 convolutional layers with doubling num-
ber of output feature maps (starting from 32). The
layers use leaky ReLU activation and are followed
by 2 × 2 max pooling.

8624

(a) rounding approximations

(b) libJPEG vs dJPEG (standard)

(c) libJPEG vs dJPEG (harmonic)

(d) libJPEG vs dJPEG (sin)

round
harmonic
sinus

−1.5 −1 −0.5

0

0.5

1

1.5

Q10

30

Q

Q

Q90

Q80

Q70
Q60
Q50
Q40
Q30

Q20

Q90

Q80

Q70
Q60
Q50
Q40
Q30

Q20

Q10

Q90

Q80

Q70
Q60
Q50
Q40
Q30

Q20

Q10

40

35

30

35
40
45
PSNR for dJPEG [dB]

50

55

30

35
40
45
PSNR for dJPEG [dB]

50

55

30

35
40
45
PSNR for dJPEG [dB]

50

55

(e) uncompressed (96x96px)

(f ) libJPEG(50), PSNR=34.4 dB

(g) dJPEG(50), PSNR=35.5 dB

(h) dJPEG(50), PSNR=37.9 dB

]

B

d

[

G

E
P

J
b

i
l

r

o

f

R
N

S

P

Figure 3. Implementation of JPEG compression as a fully diﬀerentiable dJPEG module: (a) continuous approximations
of the rounding function; (b)-(d) validation of the dJPEG module against the standard libJPEG library with standard
rounding, and the harmonic and sinusoidal approximations; (e) an example image patch; (f ) standard JPEG compression
with quality 50; (g)-(h) dJPEG-compressed patches with the harmonic and sinusoidal approximations.

• A 1 × 1 convolutional layer mapping 256 features
into 256 features.

• A global average pooling layer reducing the num-
ber of features to 256.

• Two fully connected layers with 512 and 128 nodes
activated by leaky ReLU.

• A fully connected layer with N = 5 output nodes
and softmax activation.

The network has 1,341,990 parameters in total, and
outputs probabilities of 5 possible processing histories
(4 manipulation classes & straight from camera).

4. Experimental Evaluation

We started our evaluation by using several NIP mod-
els to reproduce the output of a standard imaging
pipeline (Sec. 4.1). Then, we used the FAN model to
detect popular image manipulations (Sec. 4.2).
Ini-
tially, we validated that the models work correctly by
using it without a distribution channel (Sec. 4.3). Fi-
nally, we performed extensive evaluation of the entire
acquisition and distribution network (Sec. 4.4).
We collected a data-set with RAW images from 8
cameras (Table 1). The photographs come from two
public (Raise [12] and MIT-5k [7]) and from one private
data-set. For each camera, we randomly selected 150
images with landscape orientation. These images will
later be divided into separate training/validation sets.

Table 1. Digital cameras used in our experiments
Camera
SR1 #Images2 Source Bayer

Canon EOS 5D
Canon EOS 40D
Nikon D5100
Nikon D700
Nikon D7000
Nikon D750
Nikon D810
Nikon D90

12
10
16
12
16
24
36
12

864 dng MIT-5k RGGB
313 dng MIT-5k RGGB
288 nef
Private RGGB
590 dng MIT-5k RGGB
>1k nef
Raise RGGB
312 nef
Private RGGB
205 nef
Private RGGB
>1k nef
Raise GBRG

1 Sensor Resolution in Megapixels [Mpx]
2 RAW ﬁle formats: nef (Nikon); dng (generic, Adobe)

4.1. Neural Imaging Pipelines

We considered three NIP models with various com-
plexity and design principles (Table 2): INet - a sim-
ple convolutional network with layers corresponding to
successive steps of the standard pipeline; UNet - the
well-known UNet architecture [31] adapted from [9];
DNet - adaptation of the model originally used for
joint demosaicing and denoising [17]. Details of the
networks’ architectures are included in the supplement.
We trained a separate model for each camera. For
training, we used 120 full-resolution images.
In each
iteration, we extracted random 128 × 128 patches and
formed a batch with 20 examples (one patch per im-
age). For validation, we used a ﬁxed set of 512 × 512 px
patches extracted from the remaining 30 images. The
models were trained to reproduce the output of a stan-

8625

optimization target bilinear demosaicing

converged pipeline

n
o

i

t

Native patch

Sharpen

Gaussian ﬁltering

JPEGNet

Resampling

l

a
u
p
n
a

i

m

l

y
n

O

)

a

(

n
o

i

(a) Improved demosaicing example (Canon EOS 40D)

optimization target

converged pipeline

(b) Improved denoising example (Nikon D5100)

Figure 4. Examples of serendipitous image quality improve-
ments obtained by neural imaging pipelines: (a) better de-
mosaicing performance; (b) better denoising.

Table 2. Considered neural imaging pipelines

INet

UNet

DNet

# Parameters
PSNR
SSIM
Train. speed [it/s]
Train. time

321
42.8
0.989
8.80
17 - 26 min

7,760,268
44.3
0.990
1.75
2-4 h

493,976
46.2
0.995
0.66
12 - 22 h

dard imaging pipeline. We used our own implemen-
tation based on Python and rawkit [8] wrappers over
libRAW [28]. Demosaicing was performed using an
adaptive algorithm by Menon et al. [29].
All NIPs successfully reproduced target images with
high ﬁdelity. The resulting color photographs are vi-
sually indistinguishable from the targets. Ob jective ﬁ-
delity measurements for the validation set are collected
in Table 2 (average for all 8 cameras). Interestingly, the
trained models often revealed better denoising and de-
mosaicing performance, despite the lack of a denoising
step in the simulated pipeline, and the lack of explicit
optimization ob jectives (see Fig. 4).
Of all of the considered models, INet was the easi-
est to train - not only due to its simplicity, but also
because it could be initialized with meaningful pa-
rameters that already produced valid results and only
needed ﬁne-tuning. We initialized the demosaicing ﬁl-
ters with bilinear interpolation, color space conversion
with a known multiplication matrix, and gamma cor-
rection with a toy model separately trained to repro-
duce this non-linearity. The UNet model was initial-
ized randomly, but improved rapidly thanks to skip
connections. The DNet model took the longest and
for a long time had problems with faithful color ren-
dering. The typical training times are reported in Ta-

t

u
b

i

r
t
s

i

d

r
e
t
f

A

)

b

(

Figure 5. An example image patch with all of the considered
manipulations: (a) only manipulation; (b) after the distri-
bution channel (down-sampled and JPEG compressed).

ble 2. The measurements were collected on a Nvidia
Tesla K80 GPU. The models were trained until the rel-
ative change of the average validation loss for the last 5
dropped below 10−4 . The maximum number of epochs
was 50,000. For the DNet model we adjusted the stop-
ping criterion to 10−5 since the training progress was
slow, and often terminated prematurely with incorrect
color rendering.

4.2. Image Manipulation

Our experiment mirrors the standard setup for im-
age manipulation detection [3, 5, 16]. The FAN simply
decides which manipulation class the input patch be-
longs to. This approximates identiﬁcation of the last
post-processing step, which is often used to mask traces
of more invasive modiﬁcations.
We consider four mild post-processing operations:
sharpening - implemented as an unsharp mask operator
with the following kernel:

1

6 "−1 −4 −1
−1 −4 −1#
−4
26 −4

(3)

applied to the luminance channel in the HSV color
space; resampling - implemented as successive 1:2
down-sampling and 2:1 up-sampling using bilinear in-
terpolation; Gaussian ﬁltering - implemented using a
convolutional layer with a 5 × 5 ﬁlter and standard
deviation 0.83; JPG compression - implemented using
the dJPEG module with sinusoidal rounding approx-
imation and quality level 80. Fig. 5 shows the post-
processed variants of an example image patch: (a) just
after manipulation; and (b) after the distribution chan-
nel (as seen by the FAN module).

4.3. FAN Model Validation

To validate our FAN model, we initially imple-
mented a simple experiment, where analysis is per-
formed just after image manipulation (no distribution
channel distortion, as in [3]). We used the UNet model

8626

33

35

37

39

41

43

P

S

R
N

[

d

B

]

0

2

4

6

8

10

12

14

16

18

20

.2

.4

.6

.8

1.0

Training progress [sampling steps]

F

N
A

c
c
a

u

r

c
a

y

[

%

]

FAN only
Joint FAN+NIP

Figure 6. Typical progression of validation metrics (Nikon
D90) for standalone FAN training (F) and joint optimiza-
tion of FAN and NIP models (F+N).

to develop smaller image patches to guarantee the same
size of the inputs for the FAN model (128×128×3 RGB
images). In such conditions, the model works just as
expected, and yields classiﬁcation accuracy of 99% [3].

4.4. Imaging Pipeline Optimization

In this experiment, we perform forensic analysis at
the end of the distribution channel. We consider two
optimization modes: (F) only the FAN network is op-
timized given a ﬁxed NIP model; (F+N) both the FAN
and NIP models are optimized jointly. In both cases,
the NIPs are pre-initialized with previously trained
models (Section 4.1). The training was implemented
with two separate Adam optimizers, where the ﬁrst
one updates the FAN (and in the F+N mode also the
NIP) and the second one updates the NIP based on the
image ﬁdelity ob jective.
Similarly to previous experiments, we used 120 full-
resolution images for training, and the remaining 30
images for validation. From training images, in each
iteration we randomly extract new patches. The val-
idation set is ﬁxed at the beginning and includes 100
random patches per each image (3,000 patches in to-
tal) for classiﬁcation accuracy assessment. To speed-up
image ﬁdelity evaluation, we used 2 patches per im-
age (60 patches in total).
In order to prevent over-
representation of empty patches, we bias the selection
by outward rejection of patches with pixel variance <
0.01, and by 50% chance of keeping patches with vari-
ance < 0.02. More diverse patches are always accepted.
Due to computational constraints, we performed the
experiment for 4 cameras (Canon EOS 40D and EOS
5D, and Nikon D7000, and D90, see Table 1) and for
the INet and UNet models only. (Based on prelimi-
nary experiments, we excluded the DNet model which
rapidly lost and could not regain image representation

Table 3. Typical confusion matrices (Nikon D90). Entries
≈ 0 are not shown; entries / 3% are marked with (*).

(a) standalone FAN optimization (UNet) → 44.2%

True

Predicted

n

a

t

.

s

h

a

.

a
g

u

.

j

g
p

e
r

s

.

native
sharpen
gaussian
jpg
resample

27
*
7
26
14

24
93
4
23
7

18
3
59
18
38

20
*
12
21
20

11
*
18
12
21

(b) joint FAN+NIP optimization (INet) → 55.2%

True

Predicted

n

a

t

.

s

h

a

.

a
g

u

.

j

g
p

e
r

s

.

native
sharpen
gaussian
jpg
resample

41
7
18
41
5

16
85
8
16
*

22
3
54
21
14

18
4
10
19
*

4
*
10
4
77

(c) joint FAN+NIP optimization (UNet) → 94.0%

True

Predicted

n

a

t

.

s

h

a

.

a
g

u

.

j

g
p

e
r

s

.

native
sharpen
gaussian
jpg
resample

90

*

9

100

97
3
*

*
84

*

13

99

ﬁdelity.) We ran the optimization for 1,000 epochs,
starting with a learning rate of 10−4 and systematically
decreasing by 15% every 100 epochs. Each run was re-
peated 10 times. The typical progression of validation
metrics for the UNet model (classiﬁcation accuracy
and distortion PSNR) is shown in Fig. 6 (sampled ev-
ery 50 epochs). The distribution channel signiﬁcantly
disrupts forensic analysis, and the classiﬁcation accu-
racy drops to ≈ 45% for standalone FAN optimization
(F). In particular, the FAN struggles to identify three
low-pass ﬁltering operations (Gaussain ﬁltering, JPEG
compression, and re-sampling; see confusion matrix in
Tab. 3a), which bear strong visual resemblance at the
end of the distribution channel (Fig. 5b). Optimization
of the imaging pipeline signiﬁcantly increases classiﬁca-
tion accuracy (over 90%) and makes the manipulation
paths easily distinguishable (Tab. 3c). Most mistakes
involve the native and jpeg compressed classes. Fig. 7
collects the obtained results for both the UNet and INet
models. While UNet delivers consistent and signiﬁcant
beneﬁts, INet is much simpler and yields only modest
improvements in accuracy - up to ≈ 55%. It also lacked
consistency and for one of the tested cameras yielded
virtually no beneﬁts.
The observed improvement in forensic accuracy
comes at the cost of image distortion, and leads to
artifacts in the developed photographs. In our experi-
ments, photo development ﬁdelity dropped to ≈36 dB

8627

(a) the INet model

(b) the UNet model

Nikon D90 (F+N)

Nikon D90 (F)

Nikon D7000 (F+N)

Nikon D7000 (F)

Canon EOS 40D (F+N)

Canon EOS 40D (F)

Canon EOS 5D (F+N)

Canon EOS 5D (F)

.4

.5

.6
.7
.8
Classiﬁcation accuracy [%]

.9

1

.4

.5

.6
.7
.8
Classiﬁcation accuracy [%]

.9

1

Figure 7. Validation accuracy for image manipulation detection after the distribution channel: (F) denotes standalone FAN
training given a ﬁxed NIP; (F+N) denotes joint optimization of the FAN and NIP models.

e
c

n

e
r
e

f

e
r

t

e

N
U

)

F

(

t
s
r

o

w

-

t

e

N
U

)

N
+

F

(

t
s
e

b

-

t

e

N
U

)

N
+

F

(

e
c

n

e
r
e

f

e
r

t

e

N

I

)

F

(

l

a
c

i

p
y

t

-

t

e

N

I

)

N
+

F

(

Figure 8. Example image patches developed with joint NIP
and FAN optimization (F+N) for Nikon D90. The UNet
examples show the best/worst artifacts. The INet examples
show typical outputs.

(PSNR) / 0.96 (SSIM). Detailed results are collected
in Tab. 4. The severity of the distortions varied with
training repetitions. Qualitative illustration of the ob-
served artifacts is shown in Fig. 8. The ﬁgure shows
diverse image patches developed by several NIP vari-
ants (diﬀerent training runs). The artifacts vary in
severity from disturbing to imperceptible, and tend to
be well masked by image content. INet ’s artifacts tend
to be less perceptible, which compensates for modest
improvements in FAN’s classiﬁcation accuracy.

Table 4. Fidelity-accuracy trade-oﬀ for joint optimization.

Camera

PSNR [dB]

SSIM

Acc. [%]

D90
42.7 → 35.9
D7000
43.0 → 35.6
EOS 40D 42.8 → 36.1
EOS 5D
43.0 → 36.2

D90
43.3 → 37.2
D7000
40.6 → 35.4
EOS 40D 41.6 → 33.1
EOS 5D
41.5 → 40.7

UNet model

0.990 → 0.960
0.990 → 0.955
0.990 → 0.962
0.989 → 0.961

INet model

0.992 → 0.969
0.985 → 0.959
0.985 → 0.934
0.986 → 0.984

0.45 → 0.91
0.42 → 0.91
0.39 → 0.92
0.45 → 0.89

0.44 → 0.55
0.43 → 0.53
0.38 → 0.50
0.42 → 0.42

5. Discussion and Future Work

We replaced the photo acquisition pipeline with a
CNN, and developed a fully-diﬀerentiable model of the
entire acquisition and distribution workﬂow. This al-
lows for joint optimization of photo analysis and acqui-
sition. Our results show that it is possible to optimize
the imaging pipeline to facilitate provenance analysis at
the end of complex distribution channels. We observed
signiﬁcant improvements in manipulation detection ac-
curacy w.r.t state-of-the-art classical forensics [3] (in-
crease from ≈ 45% to over 90%).
Competing optimization ob jectives lead to imaging
artifacts which tend to be well masked in textured ar-
eas, but are currently too visible in ﬂat regions. Sever-
ity of the artifacts varies between training runs and NIP
architectures, and suggests that better conﬁgurations
should be achievable by learning to control the ﬁdelity-
accuracy trade-oﬀ. Since ﬁnal decisions are often taken
by pooling predictions for many patches, we believe a
good balance should be achievable even with relatively
simple models. Further improvements may also possi-
ble by exploring diﬀerent NIP architectures, or explic-
itly modeling HVS characteristics. Future work should
also assess generalization capabilities to other manip-
ulations, and more complex forensic problems. It may
also be interesting to optimize other components of the
workﬂow, e.g., the lossy compression in the channel.

8628

References

[1] S. Agarwal and H. Farid. Photo forensics from jpeg dimples.
In Information Forensics and Security (WIFS), 2017 IEEE
Workshop on, pages 1–6. IEEE, 2017.
[2] I. Amerini, T. Uricchio, and R. Caldelli. Tracing images
back to their social network of origin: A cnn-based ap-
proach. In IEEE Int. Workshop Information Forensics and
Security, 2017.
[3] B. Bayar and M. Stamm. Constrained convolutional neural
networks: A new approach towards general purpose image
manipulation detection. IEEE Tran. Information Forensics
and Security, 13(11):2691–2706, 2018.
[4] P. Blythe and J. Fridrich. Secure digital camera. In Digital
Forensic Research Workshop, pages 11–13, 2004.
[5] M. Boroumand and J. Fridrich. Deep learning for detecting
processing history of images. Electronic Imaging, 2018(7):1–
9, 2018.
[6] H. Burger, C. Schuler, and S. Harmeling. Image denoising:
Can plain neural networks compete with bm3d?
In Int.
Conf. Computer Vision and Pattern Recognition (CVPR),
pages 2392–2399. IEEE, 2012.
[7] V. Bychkovsky, S. Paris, E. Chan, and E. Durand. Learning
photographic global tonal adjustment with a database of
input/output image pairs. In IEEE Conf. Computer Vision
and Pattern Recognition, 2011.
[8] P. Cameron and S. Whited. Rawkit. https://rawkit.

readthedocs.io/en/latest/. Visited Nov 2018.

Noiseprint:
a
arXiv preprint

[9] C. Chen, Q. Chen, J. Xu, and V. Koltun. Learning to see
in the dark. arXiv, 2018. arXiv:1805.01934.
[10] M. Chen, J. Fridrich, M. Goljan, and J. Lukas. Determining
image origin and integrity using sensor noise. IEEE Trans.
Inf. Forensics Security, 3(1):74–90, 2008.
[11] D. Cozzolino
and L. Verdoliva.
cnn-based camera model ﬁngerprint.
arXiv:1808.08396, 2018.
[12] D. Dang-Nguyen, C. Pasquini, V. Conotter, and G. Boato.
RAISE - a raw images dataset for digital image forensics.
In Proc. of ACM Multimedia Systems, 2015.
[13] Z. Dias, S. Goldenstein, and A. Rocha. Large-scale image
phylogeny: Tracing image ancestral relationships.
IEEE
Multimedia, 20(3):58–70, 2013.
[14] Z. Dias, S. Goldenstein, and A. Rocha. Toward image phy-
logeny forests: Automatically recovering semantically sim-
ilar image relationships. Forensic Science International,
231(1):178–189, 2013.
[15] G. Eilertsen, J. Kronander, G. Denes, R. Mantiuk, and J.
Unger. HDR image reconstruction from a single exposure
using deep CNNs. ACM Tran. Graphics, 36(6):1–15, nov
2017.
[16] W. Fan, K. Wang, and F. Cayre. General-purpose image
forensics using patch likelihood under image statistical mod-
els. In Proc. of IEEE Int. Workshop on Inf. Forensics and
Security, 2015.
[17] M. Gharbi, G. Chaurasia, S. Paris, and F. Durand. Deep
joint demosaicking and denoising. ACM Tran. Graphics,
35(6):1–12, nov 2016.
[18] A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and L.
Van Gool. Wespe: weakly supervised photo enhancer for
digital cameras. arXiv preprint arXiv:1709.01118, 2017.
[19] H. Jiang, Q. Tian, J. Farrell, and B. A. Wandell. Learning
the image processing pipeline. IEEE Tran. Image Process-
ing, 26(10):5032–5042, oct 2017.
[20] Joint Photographic Experts Group.

JPEG Privacy

& Security. https://jpeg.org/items/20150910_privacy_
security_summary.html. Visited Oct 2018.

[21] O. Kapah and H. Z. Hel-Or. Demosaicking using artiﬁcial

neural networks. In Applications of Artiﬁcial Neural Net-
works in Image Processing V, volume 3962, pages 112–121,
2000.
[22] E. Kee, J. O’brien, and H. Farid. Exposing photo manip-
ulation from shading and shadows. ACM Trans. Graph.,
33(5), 2014.
[23] F. Kokkinos and S. Lefkimmiatis. Deep image demosaick-
ing using a cascade of convolutional residual denoising net-
works. arXiv, 2018. arXiv:1803.05215v4.
[24] P. Korus. Digital image integrity–a survey of protection and
veriﬁcation techniques. Digital Signal Processing, 71:1–26,
2017.
[25] P. Korus, J. Bialas, and A. Dziech. Towards practical
self-embedding for JPEG-compressed digital images. IEEE
Trans. Multimedia, 17(2):157–170, Feb 2015.
[26] J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T.
Karras, M. Aittala, and T. Aila. Noise2noise: Learn-
ing image restoration without clean data. arXiv preprint
arXiv:1803.04189, 2018.
[27] H. Li, B. Li, S. Tan, and J. Huang. Detection of deep
network generated images using disparities in color compo-
nents. arXiv preprint arXiv:1808.07276, 2018.
[28] LibRaw LLC. libRAW. https://www.libraw.org/. Visited
Nov 2018.
[29] D. Menon, S. Andriani, and G. Calvagno. Demosaicing with
directional ﬁltering and a posteriori decision. IEEE Tran.
Image Processing, 16(1):132–141, 2007.
[30] A. Piva. An overview on image forensics.
Processing, 2013, 2013. Article ID 496701.
[31] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In Int.
Conf. Medical Image Computing and Computer-assisted In-
tervention. Springer, 2015.
[32] M. Stamm, M. Wu, and KJ Liu. Information forensics: An
overview of the ﬁrst decade. IEEE Access, 1:167–200, 2013.
[33] Z. H Sun and A. Hoogs. Ob ject insertion and removal in
images with mirror reﬂection. In IEEE Workshop on Infor-
mation Forensics and Security, pages 1–6, 2017.
[34] N.-S. Syu, Y.-S. Chen, and Y.-Y. Chuang.
Learning
deep convolutional networks for demosaicing. arXiv, 2018.
arXiv:1802.03769.
[35] R. Tan, K. Zhang, W. Zuo, and L. Zhang. Color image
demosaicking via deep residual learning. In IEEE Int. Conf.
Multimedia and Expo, 2017.
[36] E. Wengrowski, Z. Sun, and A. Hoogs. Reﬂection corre-
spondence for exposing photograph manipulation. In IEEE
Int. Conf. Image Processing, pages 4317–4321, 2017.
[37] B. Wronski and P. Milanfar. See better and further with su-

ISRN Signal

per res zoom on the pixel 3. https://ai.googleblog.com/
2018/10/see- better- and- further- with- super- res.html.

Visited in Oct 2018.
[38] C. P. Yan and C. M. Pun. Multi-scale diﬀerence map
fusion for tamper localization using binary ranking hash-
ing.
IEEE Tran. Information Forensics and Security,
12(9):2144–2158, 2017.
[39] M. Zampoglou, S. Papadopoulos, and Y. Kompatsiaris.
Large-scale evaluation of splicing localization algorithms
for web images. Multimedia Tools and Applications,
76(4):4801–4834, 2017.
[40] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang.
Beyond a gaussian denoiser: Residual
learning of deep
cnn for image denoising.
IEEE Tran. Image Processing,
26(7):3142–3155, 2017.
[41] P. Zhou, X. Han, V. Morariu, and L. S. Davis. Learning
rich features for image manipulation detection.
In IEEE
Int. Conf. Computer Vision and Pattern Recognition, pages
1053–1061, 2018.

8629

Convolutional Mesh Regression for Single-Image Human Shape Reconstruction

Nikos Kolotouros, Georgios Pavlakos, Kostas Daniilidis
University of Pennsylvania

Abstract

This paper addresses the problem of 3D human pose and
shape estimation from a single image. Previous approaches
consider a parametric model of the human body, SMPL,
and attempt to regress the model parameters that give rise
to a mesh consistent with image evidence. This parameter
regression has been a very challenging task, with model-
based approaches underperforming compared to nonpara-
metric solutions in terms of pose estimation. In our work,
we propose to relax this heavy reliance on the model’s pa-
rameter space. We still retain the topology of the SMPL tem-
plate mesh, but instead of predicting model parameters, we
directly regress the 3D location of the mesh vertices. This is
a heavy task for a typical network, but our key insight is that
the regression becomes signiﬁcantly easier using a Graph-
CNN. This architecture allows us to explicitly encode the
template mesh structure within the network and leverage the
spatial locality the mesh has to offer. Image-based features
are attached to the mesh vertices and the Graph-CNN is re-
sponsible to process them on the mesh structure, while the
regression target for each vertex is its 3D location. Having
recovered the complete 3D geometry of the mesh, if we still
require a speciﬁc model parametrization, this can be reli-
ably regressed from the vertices locations. We demonstrate
the ﬂexibility and the effectiveness of our proposed graph-
based mesh regression by attaching different types of fea-
tures on the mesh vertices. In all cases, we outperform the
comparable baselines relying on model parameter regres-
sion, while we also achieve state-of-the-art results among
model-based pose estimation approaches. 1

1. Introduction

Analyzing humans from images goes beyond estimat-
ing the 2D pose for one person [27, 47] or multiple peo-
ple [2, 32], or even estimating a simplistic 3D skele-
ton [24, 25]. Our understanding relies heavily on being able
to properly reconstruct the complete 3D pose and shape of
people from monocular images. And while this problem
is well addressed in settings with multiple cameras [8, 14],

1 Project Page: seas.upenn.edu/˜nkolot/projects/cmr

Figure 1: Summary of our approach. Given an input im-
age we directly regress a 3D shape with graph convolutions.
Optionally, from the 3D shape output we can regress the
parametric representation of a body model.

the excessive ambiguity, the limited training data, and the
wide range of imaging conditions make this task particu-
larly challenging in the monocular case.

Traditionally, optimization-based approaches [1, 18, 49]
have offered the most reliable solution for monocular pose
and shape recovery. However, the slow running time, the
reliance on a good initialization and the typical failures
due to bad local minima have recently shifted the focus
to learning-based approaches [15, 18, 28, 31, 39, 43], that
regress pose and shape directly from images. The majority
of these works investigate what is the most reliable modal-
ity to regress pose and shape from. Surface landmarks [18],
pose keypoints and silhouettes [31], semantic part segmen-
tation [28], or raw pixels [15] have all been considered as
the network input. And while the input representation topic
has received much debate, all the above approaches nicely
conform to the SMPL model [21] and use its parametric rep-
resentation as the regression target of choice. However, tak-
ing the decision to commit to a particular parametric space
can be quite constraining itself. For example, SMPL is not
modeling hand pose or facial expressions [14, 36]. What
is even more alarming is that the model parameter space
might not be appropriate as a regression target. In the case
of SMPL, the pose space is expressed in the form of 3D

14501

Attach image features

Per-vertex feature vectors

in the graph

Regressed 3D vertex

coordinates

Encoder

(xt
i , y t
i , z t

i )

(cid:0)xt
j , y t
j , z t

j (cid:1)

( ˆxi , ˆyi , ˆzi )

( ˆxj , ˆyj , ˆzj )

. . .

Template Mesh

Input: Image features ⊕

template 3D coordinates

Graph CNN

Output: 3D vertex coordinates

Output Mesh

Figure 2: Overview of proposed framework. Given an input image, an image-based CNN encodes it in a low dimensional
feature vector. This feature vector is embedded in the graph deﬁned by the template human mesh by attaching it to the 3D
coordinates (xt
i , y t
i , z t
i ) of every vertex i. We then process it through a series of Graph Convolutional layers and regress the
3D vertex coordinates ( ˆxi , ˆyi , ˆzi ) of the deformed mesh.

rotations, a pretty challenging prediction target [23, 26].
Depending on the selected 3D rotation representation (e.g.,
axis angle, rotation matrices, quaternions), we might face
problems of periodicity, non-minimal representation, or dis-
continuities, which complicate the prediction task. And in
fact, all the above model-based approaches underperfom in
pose estimation metrics compared to approaches regressing
a less informative, yet more accurate, 3D skeleton through
3D joint regression [3, 24, 29, 38].

In this work, we propose to take a more hybrid route to-
wards pose and shape regression. Even though we preserve
the template mesh introduced by SMPL, we do not directly
regress the SMPL model parameters. Instead, our regres-
sion target is the 3D mesh vertices. Considering the exces-
sive number of vertices of the mesh, if addressed naively,
this would be a particular heavy burden for the network.
Our key insight though, is that this task can be effectively
and efﬁciently addressed by the introduction of a Graph-
CNN. This architecture enables the explicit encoding of the
mesh structure in the network, and leverages the spatial lo-
cality of the graph. Given a single image (Figure 2), any
typical CNN can be used for feature extraction. The ex-
tracted features are attached on the vertex coordinates of
the template mesh, and the processing continues on the
graph structure deﬁned for the Graph-CNN. In the end,
each vertex has as target its 3D location in the deformed
mesh. This allows us to recover the complete 3D geome-
try of the human body without explicitly committing to a
pre-speciﬁed parametric space, leaving the mesh topology
as the only hand-designed choice. Conveniently, after es-

timating the 3D position for each vertex, if we need our
prediction to conform to a speciﬁc model, we can regress
its parameters quite reliably from the mesh geometry (Fig-
ure 1). This enables a more hybrid usage for our approach,
making it directly comparable to model-based approaches.
Furthermore, our graph-based processing is largely agnos-
tic to the input type, allowing us to attach features extracted
from RGB pixels [15], semantic part segmentation [28], or
even from dense correspondences [6].
In all these cases
we demonstrate that our approach outperforms the baselines
that regress model parameters directly from the same type
of features, while overall we achieve state-of-the-art pose
estimation results among model-based baselines.
Our contributions can be summarized as follows:

• We reformulate the problem of human pose and shape
estimation in the form of regressing the 3D locations
of the mesh vertices, to avoid the difﬁculties of direct
model parameter regression.

• We propose a Graph CNN for this task which encodes
the mesh structure and enables the convolutional mesh
regression of the 3D vertex locations.

• We demonstrate the ﬂexibility of our framework by
considering different
input representations, always
outperforming the baselines regressing the model pa-
rameters directly.

• We achieve state-of-the-art results among model-based
pose estimation approaches.

4502

2. Related work

There is rich recent literature on 3D pose estimation in
the form of a simplistic body skeleton, e.g., [3, 19, 22, 24,
25, 29, 30, 34, 35, 38, 40, 41, 42, 50, 51]. However, in this
Section, we focus on the more relevant works recovering
the full shape and pose of the human body.
Optimization-based shape recovery: Going beyond a
simplistic skeleton, and recovering the full pose and
shape, initially, the most successful approaches followed
optimization-based solutions. The work of Guan et al. [5]
relied on annotated 2D landmarks and optimized for the pa-
rameters of the SCAPE parametric model that generated
a mesh optimally matching this evidence. This procedure
was made automatic with the SMPLify approach of Bogo et
al. [1], where the 2D keypoints where localized through the
help of a CNN [32]. Lassner et al. [18] included auxiliary
landmarks on the surface of the human body, and addition-
ally considered the estimated silhouette during the ﬁtting
process. Zanﬁr et al. [49] similarly optimized for consis-
tency of the reprojected mesh with semantic parts of the hu-
man body, while extending the approach to work for multi-
ple people as well. Despite the reliable results obtained, the
main concern for approaches of this type is that they pose a
complicated non-convex optimization problem. This means
that the ﬁnal solution is very sensitive to the initialization,
the optimization can get stuck in local minima, and simul-
taneously the whole procedure can take several minutes to
complete. These drawbacks have motivated the increased
interest in learning-based approaches, like ours, where the
pose and shape are regressed directly from images.
Direct parametric regression: When it comes to pose
and shape regression, the vast majority of works adopt the
SMPL parametric model and consider regression of pose
and shape parameters. Lassner et al. [18] detect 91 land-
marks on the body surface and use a random forest to
regress the SMPL model parameters for pose and shape.
Pavlakos et al. [31] rely on a smaller number of keypoints
and body silhouettes to regress the SMPL parameters. Om-
ran et al. [28] follow a similar strategy but use a part seg-
mentation map as the intermediate representation. On the
other hand, Kanazawa et al. [15] attempt to regress the
SMPL parameters directly from images, using a weakly
supervised approach relying on 2D keypoint reprojection
and a pose prior learnt in an adversarial manner. Tung et
al. [43] present a self-supervised approach for the same
problem, while Tan et al. [39] rely on weaker supervision
in the form of body silhouettes. The common theme of all
these works is that they have focused on using the SMPL pa-
rameter space as a regression target. However, the 3D rota-
tions involved as the pose parameters have created issues in
the regression (e.g., discontinuities or periodicity) and typ-
ically underperform in terms of pose estimation compared
to skeleton-only baselines. In this work, we propose to take

an orthogonal approach to them, by regressing the 3D loca-
tion of the mesh vertices by means of a Graph-CNN. Our
approach is transparent to the type of the input represen-
tation we use, since the ﬂexibility of the Graph network
allows us to consider different types of input representa-
tions employed in prior work, like semantic part-based fea-
tures [28], features extracted directly from raw pixels [15],
or even dense correspondences [6].

Nonparametric shape estimation: Recently, nonparamet-
ric approaches have also been proposed for pose and shape
estimation. Varol et al. [44] use a volumetric reconstruction
approach with a voxel output. Different tasks are simulta-
neously considered for intermediate supervision. Jackson et
al. [12] also propose a form of volumetric reconstruction by
extending their recent face reconstruction network [11] to
work for full body images. The main drawback of these ap-
proaches adopting a completely nonparametric route, is that
even if they recover an accurate voxelized sculpture of the
human body, there is none or very little semantic informa-
tion captured. In fact, to recover the body pose, we need to
explicitly perform an expensive body model ﬁtting step us-
ing the recovered voxel map, as done in [44]. In contrast to
them, we retain the SMPL mesh topology, which allows us
to get dense semantic correspondences of our 3D prediction
with the image, and in the end we can also easily regress the
model’s parameters given the vertices 3D location.

Graph CNNs: Wang et al. [46] use a Graph CNN to re-
construct meshes of objects from images by deforming an
initial ellipsoid. However, mesh reconstruction of arbitrary
objects is still an open problem, because shapes of objects
even in the same class, e.g., chairs, do not have the same
genus. Contrary to generic objects, arbitrary human shapes
can be reconstructed as continuous deformations of a tem-
plate model. In fact, recently there has been a lot of research
in applying Graph Convolutions for human shape applica-
tions. Verma et al. [45] propose a new data-driven Graph
Convolution operator with applications on shape analysis.
Litany et al. [20] use a Graph VAE to learn a latent space
of human shapes, that is useful for shape completion. Ran-
jan et al. [33] use a mesh autoencoder network to recover
a latent representation of 3D human faces from a series of
meshes. The main difference of our approach is that we do
not aim to learn a generative shape model from 3D shapes,
but instead perform single-image shape reconstruction; the
input to our network is an image, not a 3D shape. The use
of a Graph CNN alone is not new, but we consider as a
contribution the insight that Graph CNNs provide a very
natural structure to enable our hybrid approach. They as-
sist us in avoiding the SMPL parameter space, which has
been reported to have issues with regression [24, 31], while
simultaneously allowing the explicit encoding of the graph
structure in the network, so that we can leverage spatial lo-
cality and preserve the semantic correspondences.

4503

3. Technical approach

In this Section we present our proposed approach for pre-
dicting 3D human shape from a single image. First, in Sub-
section 3.1 we brieﬂy describe the image-based architecture
that we use as a generic feature extractor. In Subsection 3.2
we focus on the core of our approach, the Graph CNN ar-
chitecture that is responsible to regress the 3D vertex coor-
dinates of the mesh that deforms to reconstruct the human
body. Then, Subsection 3.3 describes a way to combine
our non-parametric regression with the prediction of SMPL
model parameters. Finally, Subsection 3.4 focuses on im-
portant implementation details.

3.1. Image(cid:173)based CNN

The ﬁrst part of our pipeline consists of a typical image-
based CNN following the ResNet-50 architecture [7]. From
the original design we ignore the ﬁnal fully connected layer,
keeping only the 2048-D feature vector after the average
pooling layer. This CNN is used as a generic feature ex-
tractor from the input representation. To demonstrate the
ﬂexibility of our approach, we experiment with a variety
of inputs, i.e., RGB images, part segmentation and Dense-
Pose input [6]. For RGB images we simply use raw pixels
as input, while for the other representations, we assume that
another network [6], provides us with the predicted part seg-
mentation or DensePose. Although we present experiments
with a variety of inputs, our goal is not to investigate the
effect of the input representation, but rather we focus our
attention on the graph-based processing that follows.

3.2. Graph CNN

At the heart of our approach, we propose to employ a
Graph CNN to regress the 3D coordinates of the mesh ver-
tices. For our network architecture we draw inspiration
from the work of Litany et al. [20]. We start from a tem-
plate human mesh with N vertices as depicted in Figure 2.
Given the 2048-D feature vector extracted by the generic
image-based network, we attach these features to the 3D
coordinates of each vertex in the template mesh. From a
high-level perspective, the Graph CNN uses as input the 3D
coordinates of each vertex along with the input features and
has the goal of estimating the 3D coordinates for each vertex
in the output, deformed mesh. This processing is performed
by a series of Graph Convolution layers.
For the graph convolutions we use the formulation from
Kipf et al. [17] which is deﬁned as:
Y = ˜AXW

(1)

where X ∈ RN ×k is the input feature vector, W ∈
Rk×ℓ the weight matrix and and ˜A ∈ RN ×N is the row-
normalized adjacency matrix of the graph. Essentially, this
is equivalent to performing per-vertex fully connected op-
erations followed by a neighborhood averaging operation.

The neighborhood averaging is essential for producing a
high quality shape because it enforces neighboring ver-
tices to have similar features, and thus the output shape is
smooth. With this design choice we observed that there is
no need of a smoothness loss on the shape, as for exam-
ple in [16]. We also experimented with the more powerful
graph convolutions proposed in [45] but we did not observe
quantitative improvement in the results, so we decided to
keep our original and simpler design choice.
For the graph convolution layers, we make use of resid-
ual connections as they help in speeding up signiﬁcantly the
training and also lead in higher quality output shapes. Our
basic building block is similar to the Bottleneck residual
block [7] where 1 × 1 convolutions are replaced by per-
vertex fully connected layers and Batch Normalization [9]
is replaced by Group Normalization [48]. We noticed that
Batch Normalization leads to unstable training and poor test
performance, whereas with no normalization the training is
very slow and the network can get stuck at local minima and
collapse early during training.
Besides the 3D coordinates for each vertex, our Graph
CNN also regresses the camera parameters for a weak-
perspective camera model. Following Kanazawa et al. [15],
we predict a scaling factor s and a 2D translation vector t.
Since the prediction of the network is already on the cam-
era frame, we do not need to regress an additional global
camera rotation. The camera parameters are regressed from
the graph embedding and not from the image features di-
rectly. This way we get a much more reliable estimate that
is consistent with the output shape.
Regarding training, let ˆY ∈ RN ×3 be the predicted 3D
shape, Y the ground truth shape and X the ground truth
2D keypoint locations of the joints. From our 3D shape
we can also regress the location for the predicted 3D joints
ˆJ3D employing the same regressor that the SMPL model
is using to recover joints from vertices. Given these 3D
joints, we can simply project them on the image plane, ˆX =
sΠ( ˆJ3D ) + t. Now, we train the network using two forms
of supervision. First, we apply a per-vertex L1 loss between
the predicted and ground truth shape, i.e.,

Lshape =

N

X

i=1

|| ˆYi − Yi ||1 .

(2)

Empirically we found that using L1 loss leads to more stable
training and better performance than L2 loss. Additionally,
to enforce image-model alignment, we also apply an L1 loss
between the projected joint locations and the ground truth
keypoints, i.e.,

LJ =

M

X

i=1

|| ˆXi − Xi ||1 .

(3)

4504

MLP

θ

β

SMPL

Regressed shape

Parametric shape

Figure 3: Predicting SMPL parameters from regressed
shape. Given a regressed 3D shape from the network of
Figure 2, we can use a Multi-Layer Perceptron (MLP) to
regress the SMPL parameters and produce a shape that is
consistent with the original non-parametric shape

Finally, our complete training objective is:

L = Lshape + LJ .

(4)

This form of supervised training requires us to have ac-
cess to images with full 3D ground truth shape. However,
based on our empirical observation, it is not necessary for
all the training examples to come with ground truth shape.
In fact, following the observation of Omran et al. [28], we
can leverage additional images that provide only 2D key-
point ground truth.
In these cases, we simply ignore the
ﬁrst term of the previous equation and train only with the
keypoint loss. We have included evaluation under this set-
ting of weaker supervision in the Sup. Mat.

3.3. SMPL from regressed shape

Although we demonstrate that non-parametric regression
is an easier task for the network, there are still many ap-
plications where a parametric representation of the human
body can be very useful (e.g., motion prediction). In this
Subsection, we present a straightforward way to combine
our non-parametric prediction with a particular parametric
model, i.e., SMPL. To achieve this goal, we train another
network that regresses pose (θ) and shape (β ) parameters of
the SMPL parametric model given the regressed 3D shape
as input. The architecture of this network can be very sim-
ple, i.e., a Multi-Layer Perceptron (MLP) [37] for our im-
plementation. This network is presented in Figure 3 and the
loss function for training is:

L = Lshape + LJ + Lθ + λLβ .

(5)

Here, Lshape and LJ are the losses on the 3D shape and 2D
joint reprojection as before, while Lθ and Lβ are L2 losses
on the SMPL pose and shape parameters respectively.
As observed by previous works, e.g., [31, 24], it is chal-
lenging to regress the pose parameters θ , which represent

3D rotations in the axis-angle representation. To avoid this,
we followed the strategy employed by Omran et al. [28].
More speciﬁcally, we convert the parameters from axis-
angle representation to a rotation matrix representation us-
ing the Rodrigues formula, and we set the output of our
network to regress the elements of the rotation matrices. To
ensure that the output is a valid rotation matrix we project it
to the manifold of rotation matrices using the differentiable
SVD operation. Although this representation does not ex-
plicitly improve our quantitative results, we observed faster
convergence during training, so we selected it as a more
practical option.

3.4. Implementation details

An important detail regarding our Graph CNN is that we
do not operate directly on the original SMPL mesh, but we
ﬁrst subsample it by a factor of 4 and then upsample it again
to the original scale using the technique described in [33].
This is essentially performed by precomputing downsam-
pling and upsampling matrices D and U and left-multiply
them with the graph every time we need to do resampling.
This downsampling step helps to avoid the high redundancy
in the original mesh due to the spatial locality of the ver-
tices, and decrease memory requirements during training.
Regarding the training of the MLP, we employ a 2-step
training procedure. First we train the network that regresses
the non-parametric shape and then with this network ﬁxed
we train the MLP that predicts the SMPL parameters. We
also experimented with training them end-to-end but we ob-
served a decrease in the performance of the network for both
the parametric and non-parametric shape.

4. Empirical evaluation

In this Section, we present the empirical evaluation of
our approach. First, we discuss the datasets we use in our
evaluation (Subsection 4.1), then we provide training details
for our pipeline (Subsection 4.2), and ﬁnally, the quantita-
tive and qualitative evaluation (Subsection 4.3) follows.

4.1. Datasets

We employ two datasets that provide 3D ground truth for
training, Human3.6M [10] and UP-3D [18], while we eval-
uate our approach on Human3.6M and the LSP dataset [13].
Human3.6M: It is an indoor 3D pose dataset including sub-
jects performing activities like Walking, Eating and Smok-
ing. We use the subjects S1, S5, S6, S7 and S8 for training,
and keep the subjects S9 and S11 for testing. We present
results for two popular protocols (P1 and P2, as deﬁned
in [15]) and two error metrics (MPJPE and Reconstruction
error, as deﬁned in [51]).
UP-3D: It is a dataset created by applying SMPLify [1] on
natural images of humans and selecting the successful ﬁts.
We use the training set of this dataset for training.

4505

Method

MPJPE

Reconst. Error

SMPL Parameter Regression [15]
Mesh Regression (FC)
Mesh Regression (Graph)
Mesh Regression (Graph + SMPL)

-
200.8
102.1
113.2

77.6
105.8
69.0
61.3

Table 1: Evaluation of 3D pose estimation in Human3.6M
(Protocol 2). The numbers are MPJPE and Reconstruction
errors in mm. Our graph-based mesh regression (with or
without SMPL parameter regression) is compared with a
method that regresses SMPL parameters directly, as well
as with a naive mesh regression using fully connected (FC)
layers instead of a Graph-CNN.

LSP: It is a 2D pose dataset, including also segmentation
annotations provided by Lassner et al. [18]. We use the test
set of this dataset for evaluation.

4.2. Training details

For the image-based encoder, we use a ResNet50
model [7] pretrained on ImageNet [4]. All other network
components (Graph CNN and MLP for SMPL parameters)
are trained from scratch. For our training, we use the Adam
optimizer, and a batch size of 16, with the learning rate
set to 3e – 4. We did not use learning rate decay. Train-
ing with data only from Human3.6M lasts for 10 epochs,
while mixed training with data from Human3.6M and UP-
3D requires training for 25 epochs, because of the greater
image diversity. To train the MLP that regresses SMPL pa-
rameters from our predicted shape, we use 3D shapes from
Human3.6M and UP-3D. Finally, for the models using Part
Segmentation or DensePose [6] predictions as input, we use
the pretrained network of [6] to provide the corresponding
predictions.

4.3. Experimental analysis

Regression target: For the initial ablative study, we
aim to investigate the importance of our mesh regression
for 3D human shape estimation. To this end, we focus
on the Human3.6M dataset and we evaluate the regressed
shape through 3D pose accuracy. First, we evaluate the di-
rect regression of the 3D vertex coordinates, in compari-
son to generating the 3D shape implicitly through regres-
sion of the SMPL model parameters directly from images.
The most relevant baseline in this category is the HMR
method of [15]. In Table 1, we present the comparison of
this approach (SMPL parameter regression) with our non-
parametric shape regression (Mesh Regression - (Graph)).
For a more fair comparison, we also include our results
for the MLP that regresses SMPL parameters using our
non-parametric mesh as input (Mesh Regression - (Graph
+ SMPL)). In both cases, we outperform the strong base-
line of [15], which demonstrates the beneﬁt of estimating

Image

FC

Graph CNN

Figure 4: Using a series of fully connected (FC) layers to
regress the vertex 3D coordinates severely complicates the
regression task and gives non-smooth meshes, since the net-
work cannot leverage directly the topology of the graph.

Input

Regression Type

MPJPE
P1
P2

Reconst. Error
P1
P2

RGB

Parts

Parameter [15]
Mesh (Graph + SMPL)

Parameter [28]
Mesh (Graph + SMPL)

DP[6]

Parameter [15]
Mesh (Graph + SMPL)

88.0
74.7

-
80.4

82.7
78.9

-
71.9

-
77.4

79.5
74.2

58.1
51.9

-
56.1

57.8
55.3

56.8
50.1

59.9
53.3

54.9
51.0

Table 2: Comparison of direct SMPL parameter regression
versus our proposed mesh regression on Human3.6M (Pro-
tocol 1 and 2) for different input representations. The num-
bers are mean 3D joint errors in mm, with and without Pro-
crustes alignment (Rec. Error and MPJPE respectively).
Our results are computed after regressing SMPL parameters
from our non-parametric shape. Number are taken from the
respective works, except for the baseline of [15] on Dense-
Pose images, which is evaluated by us.

a more ﬂexible non-parametric regression target, instead of
regressing the model parameters in one shot.
Beyond the regression target, one of our contributions
is also the insight that the task of regressing 3D vertex co-
ordinates can be greatly simpliﬁed when a Graph CNN is
used for the prediction. To investigate this design choice,
we compare it with a naive alternative that regresses ver-
tex coordinates with a series of fully connected layers on
top of our image-based encoder (Mesh Regression - (FC)).
This design clearly underperforms compared to our Graph-
based architecture, demonstrating the importance of lever-
aging the mesh structure through the Graph CNN during
the regression. The beneﬁt of graph-based processing is
demonstrated also qualitatively in Figure 4.
Input representation: For the next ablative, we demon-
strate the effectiveness of our mesh regression for different

4506

Input

Output shape

MPJPE
P1
P2

Reconst. Error
P1
P2

RGB

Parts

DP[6]

Non parametric
Parametric

Non parametric
Parametric

Non parametric
Parametric

75.0
74.7

78.0
80.4

78.0
78.9

72.7
71.9

73.4
77.4

72.3
74.2

51.2
51.9

54.6
56.1

55.3
55.3

49.3
50.1

50.6
53.3

50.3
51.0

Table 3: Comparison on Human3.6M (Protocol 1 and 2) of
our non-parametric mesh with the SMPL parametric mesh
regressed from our shape. Numbers are 3D joint errors in
mm. The performance of the two baselines is similar.

Method

Reconst. Error

Lassner et al. [18]
SMPLify [1]
Pavlakos et al. [31]
NBF [28]
HMR [15]
Ours

93.9
82.3
75.9
59.9
56.8
50.1

Table 4: Comparison with the state-of-the-art on Hu-
man3.6M (Protocol 2). Numbers are Reconstruction errors
in mm. Our approach outperforms the previous baselines.

FB Seg.

Part Seg.

acc.

92.17
91.89
92.17
92.75

91.67
91.46

f1

0.88
0.88
0.88
0.84

0.87
0.87

acc.

88.82
87.71
88.24
-

87.12
88.69

f1

0.67
0.64
0.64
-

0.60
0.66

SMPLify oracle [1]
SMPLify [1]
SMPLify on [31]
Bodynet [44]

HMR [15]
Ours

Table 5: Segmentation evaluation on the LSP test set. The
numbers are accuracies and f1 scores. We include ap-
proaches that are purely regression-based (bottom) and ap-
proaches that perform some optimization (post)-processing
(top). Our approach is competitive with the state-of-the-art.

types of input representations, i.e., RGB images, Part Seg-
mentation as well as DensePose images [6]. The complete
results are presented in Table 2. The RGB model is trained
on Human3.6M + UP-3D whereas the two other models
only on Human3.6M. For every input type, we compare
with state-of-the-art methods [15, 28] and show that our
method outperforms them in all setting and metrics. Inter-
estingly, when training only with Human3.6M data, RGB
input performs worse than the other representations (Ta-
ble 1), because of over-ﬁtting. However, we observed that
RGB features capture richer information for in-the-wild im-
ages, thus we select it for the majority of our experiments.

Image

Non-parametric

Parametric

Figure 5: Examples of erroneous reconstructions. Typical
failures can be attributed to challenging poses, severe self-
occlusions, or interactions among multiple people.

SMPL from regressed shape: Additionally we examine
the effect of estimating the SMPL model parameters from
our predicted 3D shape. As it can be seen in Table 3, adding
the SMPL prediction, using a simple MLP on top of our
non-parametric shape estimate, only has a small effect in
the performance (positive in some cases, negative in others).
This means that our regressed 3D shape encapsulates all the
important information needed for the model reconstruction,
making it very simple to recover a parametric representation
(if needed), from our non-parametric shape prediction.

Comparison with the state-of-the-art: Next, we
present comparison of our approach with other state-of-
the-art methods for 3D human pose and shape estima-
tion. For Human3.6M, detailed results are presented in Ta-
ble 4, where we outperform the other baselines. We clarify
here that different methods use different training data (e.g.,
Pavlakos et al. [31] do not use any Human3.6M data for
training, NBF et al. [28] uses only data from Human3.6M,
while HMR [15] makes use of additional images with 2D
ground truth only). However, here we collected the best re-
sults reported by each approach on this dataset.

Besides 3D pose, we also evaluate 3D shape through
silhouette reprojection on the LSP test set. Our approach
outperforms the regression-based approach of Kanazawa et
al. [15], and is competitive to optimization-based baselines,
e.g., [1], which tend to perform better than regression ap-
proaches (like ours) in this task, because they explicitly op-
timize for the image-model alignment.

Qualitative evaluation: Figures 5 and 6 present qual-

4507

Image

Non-parametric

Parametric

Image

Non-parametric

Parametric

Figure 6: Successful reconstructions of our approach. Rows 1-3: LSP [13]. Rows 4-5: Human3.6M [10]. With light pink
color we indicate the regressed non parametric shape and with light blue the SMPL model regressed from the previous shape.

itative examples of our approach, including both the non-
parametric mesh and the corresponding SMPL mesh re-
gressed using our shape as input. Typical failures can be
attributed to challenging poses, severe self-occlusions, as
well as interactions among multiple people.
Runtime: On a 2080 Ti GPU, network inference for a
single image lasts 33ms, which is effectively real-time.

5. Summary

The goal of this paper was to address the problem of pose
and shape estimation by attempting to relax the heavy re-
liance of previous works on a parametric model, typically
SMPL [21]. While we retain the SMPL mesh topology, in-
stead of directly predicting the model parameters for a given
image, our target is to ﬁrst estimate the locations of the 3D
mesh vertices. For this to be achieved effectively, we pro-

pose a Graph-CNN architecture, which explicitly encodes
the mesh structure and processes image features attached to
its vertices. Our convolutional mesh regression outperforms
the relevant baselines that regress model parameters directly
for a variety of input representations, while ultimately, it
achieves state-of-the-art results among model-based pose
estimation approaches. Future work can focus on current
limitations (e.g., low resolution of output mesh, missing de-
tails in the recovered shape), as well as opportunities that
this non-parametric representation provides (e.g., capture
aspects missing in many human body models, like hand ar-
ticulation, facial expressions, clothing and hair).

Acknowledgements: We gratefully appreciate support through the fol-
lowing grants: NSF-IIP-1439681 (I/UCRC), NSF-IIS-1703319, NSF MRI
1626008, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093,
ARL DCIST CRA W911NF-17-2-0181, the DARPA-SRC C-BRIC, and
by Honda Research Institute.

4508

References

[1] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J Black. Keep it SMPL:
Automatic estimation of 3D human pose and shape from a
single image. In ECCV, 2016. 1, 3, 5, 7
[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Re-
altime multi-person 2D pose estimation using part afﬁnity
ﬁelds. In CVPR, 2017. 1
[3] Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer
Afaque, Abhishek Sharma, and Arjun Jain. Learning 3d hu-
man pose from structure and motion. In Proceedings of the
European Conference on Computer Vision (ECCV), pages
668–683, 2018. 2, 3
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 6
[5] Peng Guan, Alexander Weiss, Alexandru O Balan, and
Michael J Black. Estimating human shape and pose from
a single image. In ICCV, 2009. 3
[6] Rıza Alp G ¨uler, Natalia Neverova, and Iasonas Kokkinos.
Densepose: Dense human pose estimation in the wild.
In
CVPR, 2018. 2, 3, 4, 6, 7
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
2016. 4, 6
[8] Yinghao Huang, Federica Bogo, Christoph Classner, Angjoo
Kanazawa, Peter V Gehler, Ijaz Akhter, and Michael J Black.
Towards accurate marker-less human shape and pose estima-
tion over time. In 3DV, 2017. 1
[9] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML, 2015. 4
[10] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3.6M: Large scale datasets and predic-
tive methods for 3D human sensing in natural environments.
PAMI, 36(7):1325–1339, 2014. 5, 8
[11] Aaron S Jackson, Adrian Bulat, Vasileios Argyriou, and
Georgios Tzimiropoulos. Large pose 3D face reconstruction
from a single image via direct volumetric CNN regression.
In ICCV, 2017. 3
[12] Aaron S Jackson, Chris Manafas, and Georgios Tzimiropou-
los. 3D human body reconstruction from a single image via
volumetric regression. In ECCVW, 2018. 3
[13] Sam Johnson and Mark Everingham. Clustered pose and
nonlinear appearance models for human pose estimation. In
BMVC, 2010. 5, 8
[14] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-
ture: A 3D deformation model for tracking faces, hands, and
bodies. In CVPR, 2018. 1
[15] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In CVPR, 2018. 1, 2, 3, 4, 5, 6, 7
[16] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-
ral 3D mesh renderer. In CVPR, 2018. 4
[17] Thomas N Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. In ICLR, 2017.
4

[18] Christoph Lassner, Javier Romero, Martin Kiefel, Federica
Bogo, Michael J Black, and Peter V Gehler. Unite the peo-
ple: Closing the loop between 3D and 2D human representa-
tions. In CVPR, 2017. 1, 3, 5, 6, 7
[19] Sijin Li and Antoni B Chan. 3D human pose estimation from
monocular images with deep convolutional neural network.
In ACCV, 2014. 3
[20] Or Litany, Alex Bronstein, Michael Bronstein, and Ameesh
Makadia. Deformable shape completion with graph convo-
lutional autoencoders. In CVPR, 2018. 3, 4
[21] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. SMPL: A skinned multi-
person linear model. ACM Transactions on Graphics (TOG),
34(6):248, 2015. 1, 8
[22] Diogo C Luvizon, David Picard, and Hedi Tabia. 2D/3D
pose estimation and action recognition using multitask deep
learning. In CVPR, 2018. 3
[23] Siddharth Mahendran, Haider Ali, and Rene Vidal. A mixed
classiﬁcation-regression framework for 3D pose estimation
from 2D images. In BMVC, 2018. 2
[24] Julieta Martinez, Rayat Hossain, Javier Romero, and James J
Little. A simple yet effective baseline for 3D human pose
estimation. In ICCV, 2017. 1, 2, 3, 5
[25] Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko,
Helge Rhodin, Mohammad Shaﬁei, Hans-Peter Seidel,
Weipeng Xu, Dan Casas, and Christian Theobalt. Vnect:
Real-time 3D human pose estimation with a single rgb cam-
era. ACM Transactions on Graphics (TOG), 36(4):44, 2017.
1, 3
[26] Arsalan Mousavian, Dragomir Anguelov, John Flynn, and
Jana Koˇseck ´a. 3D bounding box estimation using deep learn-
ing and geometry. In CVPR, 2017. 2
[27] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.
1
[28] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-
ter Gehler, and Bernt Schiele. Neural body ﬁtting: Unifying
deep learning and model based human pose and shape esti-
mation. In 3DV, 2018. 1, 2, 3, 5, 6, 7
[29] Georgios Pavlakos, Xiaowei Zhou, and Kostas Daniilidis.
Ordinal depth supervision for 3D human pose estimation. In
CVPR, 2018. 2, 3
[30] Georgios Pavlakos, Xiaowei Zhou, Konstantinos G Derpa-
nis, and Kostas Daniilidis. Coarse-to-ﬁne volumetric predic-
tion for single-image 3D human pose. In CVPR, 2017. 3
[31] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas
Daniilidis. Learning to estimate 3D human pose and shape
from a single color image. In CVPR, 2018. 1, 3, 5, 7
[32] Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern
Andres, Mykhaylo Andriluka, Peter V Gehler, and Bernt
Schiele. DeepCut: Joint subset partition and labeling for
multi person pose estimation. In CVPR, 2016. 1, 3
[33] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and
Michael J Black. Generating 3D faces using convolutional
mesh autoencoders. In ECCV, 2018. 3, 5

4509

[34] Gr ´egory Rogez and Cordelia Schmid. Mocap-guided data
augmentation for 3D pose estimation in the wild. In NIPS,
2016. 3
[35] Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid.
LCR-Net: Localization-classiﬁcation-regression for human
pose. In CVPR, 2017. 3
[36] Javier Romero, Dimitrios Tzionas, and Michael J Black. Em-
bodied hands: Modeling and capturing hands and bodies to-
gether. ACM Transactions on Graphics (TOG), 36(6):245,
2017. 1
[37] David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. Learning internal representations by error propa-
gation. Technical report, California Univ San Diego La Jolla
Inst for Cognitive Science, 1985. 5
[38] Xiao Sun, Bin Xiao, Shuang Liang, and Yichen Wei. Integral
human pose regression. In ECCV, 2018. 2, 3
[39] J Tan, Ignas Budvytis, and Roberto Cipolla. Indirect deep
structured learning for 3D human body shape and pose pre-
diction. In BMVC, 2017. 1, 3
[40] Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent
Lepetit, and Pascal Fua. Structured prediction of 3D human
pose with deep neural networks. In BMVC, 2016. 3
[41] Bugra Tekin, Pablo Marquez Neila, Mathieu Salzmann, and
Pascal Fua. Learning to fuse 2D and 3D image cues for
monocular body pose estimation. In ICCV, 2017. 3
[42] Denis Tome, Christopher Russell, and Lourdes Agapito.
Lifting from the deep: Convolutional 3D pose estimation
from a single image. In CVPR, 2017. 3

[43] Hsiao-Yu Tung, Hsiao-Wei Tung, Ersin Yumer, and Katerina
Fragkiadaki. Self-supervised learning of motion capture. In
NIPS, 2017. 1, 3
[44] G ¨ul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin
Yumer, Ivan Laptev, and Cordelia Schmid. Bodynet: Volu-
metric inference of 3D human body shapes. In ECCV, 2018.
3, 7
[45] Nitika Verma, Edmond Boyer, and Jakob Verbeek. FeaStNet:
Feature-steered graph convolutions for 3D shape analysis. In
CVPR, 2018. 3, 4
[46] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei
Liu, and Yu-Gang Jiang. Pixel2Mesh: Generating 3D mesh
models from single rgb images. In ECCV, 2018. 3
[47] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser
Sheikh. Convolutional pose machines. In CVPR, 2016. 1
[48] Yuxin Wu and Kaiming He. Group normalization. In ECCV,
2018. 4
[49] Andrei Zanﬁr, Elisabeta Marinoiu, and Cristian Sminchis-
escu. Monocular 3D pose and shape estimation of multiple
people in natural scenes–the importance of multiple scene
constraints. In CVPR, 2018. 1, 3
[50] Xingyi Zhou, Qixing Huang, Xiao Sun, Xiangyang Xue, and
Yichen Wei. Towards 3D human pose estimation in the wild:
a weakly-supervised approach. In ICCV, 2017. 3
[51] Xiaowei Zhou, Menglong Zhu, Georgios Pavlakos, Spyridon
Leonardos, Konstantinos G Derpanis, and Kostas Daniilidis.
Monocap: Monocular human motion capture using a CNN
coupled with a geometric prior. PAMI, 41(4):901–914, 2019.
3, 5

4510

Deep Tree Learning for Zero-shot Face Anti-Spooﬁng

Yaojie Liu, Joel Stehouwer, Amin Jourabloo, Xiaoming Liu
Department of Computer Science and Engineering
Michigan State University, East Lansing MI 48824

{liuyaoj1, stay.jb, jourablo, liuxm}@msu.edu

Abstract

Face anti-spooﬁng is designed to prevent face recog-
nition systems from recognizing fake faces as the genuine
users. While advanced face anti-spooﬁng methods are de-
veloped, new types of spoof attacks are also being created
and becoming a threat to all existing systems. We deﬁne
the detection of unknown spoof attacks as Zero-Shot Face
Anti-spooﬁng (ZSFA). Previous ZSFA works only study 1-
2 types of spoof attacks, such as print/replay, which limits
the insight of this problem. In this work, we investigate the
ZSFA problem in a wide range of 13 types of spoof attacks,
including print, replay, 3D mask, and so on. A novel Deep
Tree Network (DTN) is proposed to partition the spoof sam-
ples into semantic sub-groups in an unsupervised fashion.
When a data sample arrives, being know or unknown at-
tacks, DTN routes it to the most similar spoof cluster, and
makes the binary decision. In addition, to enable the study
of ZSFA, we introduce the ﬁrst face anti-spooﬁng database
that contains diverse types of spoof attacks. Experiments
show that our proposed method achieves the state of the art
on multiple testing protocols of ZSFA.

1. Introduction

Face is one of the most popular biometric modalities due
to its convenience of usage, e.g., access control, phone un-
lock. Despite the high recognition accuracy, face recogni-
tion systems are not able to distinguish between real human
faces and fake ones, e.g., photograph, screen. Thus, they are
vulnerable to face spoof attacks, which deceives the systems
to recognize as another person. To safely use face recog-
nition, face anti-spooﬁng techniques are required to detect
spoof attacks before performing recognition.
Attackers can utilize a wide variety of mediums to
launch spoof attacks. The most common ones are replay-
ing videos/images on digital screens, i.e., replay attack, and
printed photograph, i.e., print attack. Different methods
are proposed to handle replay and print attacks, based on
either handcrafted features [7, 35, 38] or CNN-based fea-

Mask 1

s

k
c

Print

a

t
t

A

f

o
o
p

S

n

w

o
n
K

Replay

…

Makeup

Partial Paper

Partial Paper

Transparent 
Mask

s

k
c

a

t
t

A

f

o
o
p

S

n

w

o
n

k

n

U

Live

Known Spoof

Unknown Spoof

…

Figure 1: To detect unknown spoof attacks, we propose a Deep
Tree Network (DTN) to unsupervisely learn a hierarchic embed-
ding for known spoof attacks. Samples of unknown attacks will be
routed through DTN and classiﬁed at the destined leaf node.

tures [4, 18, 20, 32]. Recently, high-quality 3D custom mask
is also used for attacking, i.e., 3D mask attack. In [29–31],
methods for detecting print/replay attacks are found to be
less effective for this new spoof, and hence the authors
leverage the remote photoplethysmography (r-PPG) to de-
tect the heart rate pulse as the spooﬁng cue. Further, fa-
cial makeup may also inﬂuence the outcome of recognition,
i.e., makeup attack [12]. Many works [11–13] study facial
makeup, despite not as an anti-spooﬁng problem.
All aforementioned methods present algorithmic solu-
tions to the known spoof attack(s), where models are trained
and tested on the same type(s) of spoof attacks. However,
in real-world applications, attackers can also initiate spoof
attacks that we, the algorithm designers, are not aware of,
termed unknown spoof attacks1 . Researchers increasingly
pay attention to the generalization of anti-spooﬁng models,
i.e., how well they are able to detect spoof attacks that have
never been seen during the training? We deﬁne the prob-

1 There is subtle distinction between 1) unseen attacks, attack types that
are known to algorithm designers so that algorithms could be tailored to
them, but their data are unseen during training; 2) unknown attacks, attack
types that are neither known to designers nor seen during training. We do
not differentiate these two cases and term both unknown attacks.

14680

lem of detecting unknown face spoof attacks as Zero-Shot
Face Anti-spooﬁng (ZSFA). Despite the success of face
anti-spooﬁng on known attacks, ZSFA, on the other hand,
is a new and unsolved challenge to the community.
The ﬁrst attempts on ZSFA are [3, 45]. They address
ZSFA between print and replay attacks, and regard it as
an outlier detection problem for live faces (a.k.a. real hu-
man faces). With handcrafted features, the live faces are
modeled via standard generative models, e.g., GMM, auto-
encoder. During testing, an unknown attack is detected if
it lies outside the estimated live distribution. These ZSFA
works have three drawbacks:
Lacking spoof type variety: Prior models are developed
w.r.t. print and replay attacks only. The respective feature
design may not be applicable to different unknown attacks.
No spoof knowledge: Prior models only use live faces,
without leveraging the available known spoof data. While
the unknown attacks are different, the known spoof attacks
may still provide valuable information to learn the model.
Limitation of feature selection: They use handcrafted
features such as LBP to represent live faces, which were
shown to be less effective for known spoof detection [27,
32, 37, 48]. Recent deep learning models [20, 32] show the
advantage of CNN models for face anti-spooﬁng.
This work aims to address all three drawbacks. Since one
ZSFA model may perform differently when the unknown
spoof attack is different, it should be evaluated on a wide
range of unknown attacks types. In this work, we substan-
tially expand the study of ZSFA from 2 types of spoof at-
tacks to 13 types. Besides print and replay attacks, we in-
clude 5 types of 3D mask attacks, 3 types of makeup at-
tacks, and 3 partial attacks. These attacks cover both imper-
sonation spooﬁng, i.e., attempt to be authenticated as some-
one else, and obfuscation spooﬁng, i.e., attempt to cover at-
tacker’s own identity. We collect the ﬁrst face anti-spooﬁng
database that includes these diverse spoof attacks, termed
Spoof in the Wild database with Multiple Attack Types
(SiW-M).
To tackle the broader ZSFA, we propose a Deep Tree
Network (DTN). Assuming there are both homogeneous
features among different spoof types and distinct features
within each spoof type, a tree-like model is well-suited to
handle this case: learning the homogeneous features in the
early tree nodes and distinct features in later tree nodes.
Without any auxiliary labels of spoof types, DTN learns to
partition data in an unsupervised manner. At each tree node,
the partition is performed along the direction of the largest
data variation. In the end, it clusters the data into several
sub-groups at the leaf level, and learns to detect spoof at-
tacks for each sub-group independently, shown in Fig. 1.
During the testing, a data sample is routed to the most sim-
ilar leaf node to produce a binary decision of live vs. spoof.
In summary, our contributions in this work include :

• Conduct an extensive study of zero-shot face anti-
spooﬁng on 13 different types of spoof attacks;
• Propose a Deep Tree Network (DTN) to learn features
hierarchically and detect unknown spoof attacks;
• Collect a new database for ZSFA and achieve the state-
of-the-art performance on multiple testing protocols.

2. Prior Work

Face Anti-spooﬁng Image-based face anti-spooﬁng refers
to face anti-spooﬁng techniques that only take RGB im-
ages as input without extra information such as depth or
heat.
In early years, researchers utilize liveness cues,
such as eye blinking and head motion, to detect print at-
tacks [24, 36, 37, 39]. However, when encountering un-
known attacks, such as photograh with eye portion cut,
and video replay, those methods suffer from a total failure.
Later, research move to a more general texture analysis and
address print and replay attacks. Researchers mainly utilize
handcrafted features, e.g., LBP [7, 16, 17, 35], HoG [25, 47],
SIFT [38] and SURF [8], with traditional classiﬁers, e.g.,
SVM and LDA, to make a binary decision. Those methods
perform well on the testing data from the same database.
However, while changing the testing conditions such as
lighting and background, they often have a large perfor-
mance drop, which can be viewed as an overﬁtting issue.
Moreover, they also show limitations in handling 3D mask
attacks, mentioned in [30].
To overcome the overﬁtting issue, researchers make var-
ious attempts. Boulkenafet et al. extract the spooﬁng fea-
tures in HSV+YCbCR space [7]. Works in [2, 5, 6, 18, 46]
consider features in the temporal domain. Recent works [2,
4] augment the data by using image patches, and fuse the
scores from patches to a single decision. For 3D mask at-
tacks, the heart pulse rate is estimated to differentiate 3D
mask from real faces [28, 30]. In the deep learning era, re-
searchers propose several CNN works [4, 18, 20, 27, 32, 37,
48] that outperform the traditional methods.
Zero-shot learning and unknown spoof attacks Zero-
shot object recognition, or more generally, zero-shot learn-
ing, aims to recognize objects from unknown classes [40],
i.e., object classes unseen in training. The overall idea is
to associate the known and unknown classes via a semantic
embedding, whose embedding spaces can be attributes [26],
word vector [19], text description [49] and human gaze [22].
Zero-shot learning for unknown spoof attack, i.e., ZSFA,
is a relatively new topic with unique properties. Firstly,
unlike zero-shot object recognition, ZSFA emphasizes the
detection of spoof attacks, instead of recognizing speciﬁc
spoof types. Secondly, unlike generic objects with rich se-
mantic embedding, there is no explicit well-deﬁned seman-
tic embedding for spoof patterns [20]. As elaborated in
Sec. 1, prior ZSFA works [3,45] only model the live data via
handcrafted features and standard generative models, with

4681

Table 1: Comparing our SiW-M with existing face anti-spooﬁng datasets.

Dataset

CASIA-FASD [50]
Replay-Attack [15]
HKBU-MARs [30]
Oulu-NPU [9]
SiW [32]
SiW-M

Year

2012
2012
2016
2017
2018
2019

Num. of
subj./vid.

50/600
50/1, 200
35/1, 008
55/5, 940
165/4, 620
493/1, 630

Face variations
expression
No
No
No
No
Yes
Yes

pose
Frontal
Frontal
Frontal
Frontal

[−90◦ , 90◦ ]
[−90◦ , 90◦ ]

several drawbacks.
In this work, we propose a deep tree
network to unsupervisely learn the semantic embedding for
known spoof attacks. The partition of the data naturally
associates certain semantic attributes with the sub-groups.
During the testing, the unknown attacks are projected to the
embedding to ﬁnd the closest attributes for spoof detection.
Deep tree networks Tree structure is often found help-
ful in tackling language-related tasks such as parsing and
translation [14], due to the intrinsic relation of words and
sentences. E.g., tree models are applied to joint vision and
language problems such as visual question reasoning [10].
Tree structure also has the property for learning features
hierarchically. Face alignment works [23, 41] utilize the
regression trees to estimate facial landmarks from coarse
to ﬁne. Xiong et al. propose a tree CNN to handle the
large-pose face recognition [44]. In [21], Kaneko et al. pro-
pose a GAN with decision trees to learn hierarchically in-
terpretable representations. In our work, we utilize tree net-
works to learn the latent semantic embedding for ZSFA.
Face anti-spooﬁng databases Given the signiﬁcance
of a good-quality database,
researchers have released
several face anti-spooﬁng databases, such as CASIA-
FASD [50], Replay-Attack [15], OULU-NPU [9], and
SiW [32] for print/replay attacks, and HKBU-MARs [30]
for 3D mask attacks. Early databases such as CASIA-
FASD and Replay-Attack [50] have limited subject variety,
pose/expression/lighting variations, and video resolutions.
Recent databases [9, 30, 32] improve those aspects, and also
set up diverse evaluation protocols. However, up to now, all
databases focus on either print/replay attacks, or 3D mask
attacks. To provide a comprehensive study of face anti-
spooﬁng, especially the challenging ZSFA, we for the ﬁrst
time collect the database with diverse types of spoof attacks,
as in Tab. 1. The details of our database are in Sec. 4.

3. Deep Tree Network for ZSFA

The main purposes of DTN are twofold: 1) discover the
semantic sub-groups for known spoofs; 2) learn the features
in a hierarchical way. The architecture of DTN is shown in
Fig. 2. Each tree node consists of a Convolutional Residual
Unit (CRU) and a Tree Routing Unit (TRU), while the leaf
node consists of a CRU and a Supervised Feature Learning
(SFL) module. CRU is a block with convolutional layers
and the short-cut connection. TRU deﬁnes a node routing
function to route a data sample to one of the child nodes.

lighting
No
Yes
Yes
No
Yes
Yes

replay

print

Spoof attack types
3D mask makeup

partial

Total num. of
spoof types

1
1
0
1
1
1

2
1
0
1
1
1

0
0
2
0
0
5

0
0
0
0
0
3

0
0
0
0
0
3

3
2
2
2
2
13

The routing function partitions all visiting data along the
direction with the largest data variation. SFL module con-
catenates the classiﬁcation supervision and the pixel-wise
supervision to learn the spooﬁng features.

3.1. Unsupervised Tree Learning

3.1.1 Node Routing Function

For a TRU node, let’s assume the input x = f (I | θ) ∈ Rm
is the vectorized feature response, I is data input, θ is the pa-
rameters of the previous CRUs, and S is the set of data sam-
ples Ik , k = 1, 2, ..., K that visit this TRU node. In [44],
Xiong et al. deﬁne a routing function as:

ϕ(x) = xT · v + τ ,

(1)

where v denotes the projection vector and τ is the bias. Data
S can then be split into Slef t : {Ik |ϕ(xk ) < 0, Ik ∈ S } and
Sright : {Ik |ϕ(xk ) ≥ 0, Ik ∈ S }, and directed to the left
and right child node, respectively. To learn this function,
they propose to maximize the distance between the mean of
Slef t and Sright , while keeping the mean of S centered at
0. This unsupervised loss is formulated as:

L =

( 1

N PIk ∈S

ϕ(xk ))2

( 1

Nl PIk ∈Slef t

ϕ(xk ) − 1

Nr PIk ∈Sright

ϕ(xk ))2 ,

(2)

where N , Nl , Nr denote the number of samples in each set.
However, in practice, minizing Equ. 2 might not lead to
a satisfactory solution. Firstly, the loss can be minimized by
increasing the norm of either v or x, which is a trivial solu-
tion. Secondly, even when the norms of v, x are constrained,
Equ. 2 is affected by the density of data S and can be sensi-
tive to the outliers. In other words, the zero expectation of
ϕ(x) does not necessarily result in a balanced partition of
data S . Local minima could be achieved when all data are
split to one side. In some cases, the tree may suffer from
collapsing to a few (even one) leaf nodes.
To better partition the data, we propose a novel routing
function and an unsupervised loss. Regardless of τ , the dot
product between xT and v can be regarded as projecting x to
the direction of v. We design v such that we can observe the
largest variation after projection. Inspired by the concept
of PCA, the optimal solution naturally becomes the largest
PCA basis of data S . To achieve this, we ﬁrst constrain v to

4682

TRU

CRU

TRU

CRU

TRU

CRU

TRU

CRU

CRU

CRU

TRU

CRU

CRU

CRU

TRU

CRU

CRU

CRU

TRU

CRU

CRU

CRU

SFL

SFL

SFL

SFL

SFL

SFL

SFL

SFL

256×256×6
(RGB+HSV)

CRU

c

n
o

v

c

n
o

v

c

n
o

v

m

x
a

o
o
p

l

40

40

40

/2

%

&

'

(

)

Feature Space

SFL

c

n
o

v

c

n
o

v

f

c

t
c
e
v

o

r

i

z

e

f

c

c

n
o

v

0/1

M

1

40

40

500

2

ℱ

Leaf Node
32×32×40

Mask Map
32×32×1

Tree
Nodes

Leaf
Nodes

(a)

(b)

(c)

(d)

Figure 2: The proposed Deep Tree Network (DTN) architecture. (a) the overall structure of DTN. A tree node consists of a Convolutional
Residual Unit (CRU) and a Tree Routing Unit (TRU), and a leaf node consists of a CRU and a Supervised Feature Learning (SFL) module.
(b) the concept of Tree Routing Unit (TRU): ﬁnding the base with largest variations; (c) the structure of each Convolutional Residual Unit
(CRU); (d) the structure of the Supervised Feature Learning (SFL) in the leaf nodes.

be norm 1 and reformulate Equ. 1 as:

ϕ(x) = (x − µ)T · v,

kvk = 1,

(3)

where µ is the mean of data S . Then, ﬁnding v is identical
to ﬁnding the largest eigenvector of the covariance matrix
¯XS , where ¯XS = XS − µ, and XS ∈ RN ×K is the data
matrix. Based on the deﬁnition of eigen-analysis ¯X
λv, our optimization aims to maximize:

¯X

T

S

T

S

¯XS v =

arg max

v,θ

λ = arg max

v,θ

vT ¯X

T

S

¯XS v.

(4)

The loss for learning the routing function is formulated as:

Lroute = exp(−αvT ¯X

T

S

¯XS v) + βTr( ¯X

T

S

¯XS ),

(5)

where α, β are scalars, and set as 1e-3, 1e-2 in our experi-
ments. We apply the exponential function on the ﬁrst term
to make the maximization problem bounded. The second
term is introduced as a regularizer to prevent trivial solu-
tions by constraining the trace of covariance matrix of ¯XS .

3.1.2 Tree of Known Spoofs

With the routing function, we can build the entire binary
tree. Fig. 2 shows a binary tree of depth of 4, with 8 leaf
nodes. As mentioned early in Sec. 3, the tree is designed
to ﬁnd the semantic sub-groups from all known spoofs,
and is termed as spoof tree. Similarly, we may also train
live tree with live faces only, as well as general data tree
with both live and spoof data. Compared to spoof tree,
live and general data tree have some drawbacks. Live tree
does not convey semantic meaning for the spoof, and the
attributes learned at each node cannot help to route and bet-
ter detect spoof; General data tree may result in imbalanced
sub-groups, where samples of one class outnumber another.

Such imbalance would cause bias for supervised learning in
the next stage.
Hence, when we compute Equ. 5 to learn the routing
functions, we only consider the spoof samples to construct
XS . To have a balanced sub-group for each leaf, we sup-
press the responses of live data to zero, so that all live data
can be evenly partitioned to the child nodes. Meanwhile,
we also suppress the responses of the spoof data that do not
visit this node, so that every node models the distribution of
a unique spoof subset.
Formally, for each node, we maximize the routing func-
tion responses of spoof data that visit this node (denoted as
S ), while minimizing the responses of other data (denoted
as S − ), including all live data and spoof data that don’t visit
this node, i.e., that visit neighboring nodes. To achieve this
objective, we deﬁne the following loss:

Luniq = −

1

N X

Ik ∈S

(cid:13)(cid:13)¯xT

k

v(cid:13)(cid:13)

2

+

1

N − X

Ik ∈S−

(cid:13)(cid:13)¯xT

k

v(cid:13)(cid:13)

2

.

(6)

3.2. Supervised Feature Learning

Given the routing functions, a data sample Ik will be as-
signed to one of the leaf nodes. Let’s ﬁrst deﬁne the feature
output of leaf node as F (Ik | θ), shortened as Fk for sim-
plicity. At each leaf node, we deﬁne two node-wise super-
vised tasks to learn discriminative features: 1) binary clas-
siﬁcation drives the learning of a high-level understanding
of live vs. spoof faces, 2) pixel-wise mask regression draws
CNN’s attention to low-level local feature learning.
Classiﬁcation supervision To learn a binary classiﬁer, as
shown in Fig. 2(d), we apply two additional convolution
layers and two fully connected layers on Fk to generate a
feature vector ck ∈ R500 . We supervise the learning via the

4683

&

(), + , , , -)

1×1 conv

(), 16,16,20)

(), + , , , 20)

resize

(), 16×16×20)

reshape

batch norm
w/o scale

(), 16×16×20)

TRU

(

#

$(&)

(), 1)

Figure 3: The structure of the Tree Routing Unit (TRU).

softmax cross entropy loss:

Lclass =

1

N X

Ik ∈S

n(1 − yk )log(1 − pk ) − yk logpk o (7)

pk =

exp(w0

T ck )

exp(w1
T ck ) + exp(w1

,

T ck )

(8)

where S represents all the data samples that arrive this leaf
node, N denotes the number of samples in S , {w0 , w1 } are
the parameters in the last fully connected layer, and yk is
the label of data sample k (1 denotes spoof, and 0 live).
Pixel-wise supervision We also concatenate another con-
volution layer to Fk to generate a map response Mk ∈
R32×32 . Inspired by the prior work [32], we leverage the
semantic prior knowledge of face shapes and spoof attack
position to provide a pixel-wise supervision. Using the
dense face alignment model [33], we provide a binary mask
Dk ∈ R32×32 , shown in Fig. 4, to indicate the pixels of
spoof mediums. Thus, for a leaf node, the loss function for
the pixel-wise supervision is:

Lmask =

1

N X

Ik ∈S

kMk − Dk k1 .

(9)

Overall loss Finally, we apply the supervised losses on p
leaf nodes, the unsupervised losses on q TRU nodes, and
formulate our training loss as:

L =

p

X

i=1

(α1Li

class+α2Li
mask )+

q

X

j=1

(α3Lj

route+α4Lj
uniq ),

(10)
where α1 ,α2 ,α3 ,α4 are the regularization coefﬁcients for
each term, and are set as 0.001, 1.0, 2.0, 0.001 respectively.
For a 4-layer DTN, p = 8 and q = 7.

3.3. Network Architecture

Deep Tree Network (DTN) DTN is the main framework
of the proposed model.
It takes I ∈ R256×256×6 as in-
put, where the 6 channels are RGB+HSV color spaces. We

concatenate three 3 × 3 convolution layers with 40 chan-
nels and 1 max-pooling layer, and group them as one Con-
volutional Residual Unit (CRU). Each convolution layer is
equipped with ReLU and group normalization layer [43],
due to the dynamic batch size in the network. We also ap-
ply a shortcut connection for each convolution layer. For
each tree node, we deploy one CRU before the TRU. At the
leaf node, DTN produces the feature representation of input
I as F (I | θ) ∈ R32×32×40 , then uses one 1 × 1 convolution
layer to generate the binary mask map M.
Tree Routing Unit (TRU) TRU is the module routing the
data sample to one of the child CRUs. As shown in Fig. 3,
it ﬁrst compresses the feature by using an 1 × 1 convolu-
tion layer, and resizing the response spatially. For the root
node, we compress the CRU feature to x ∈ R32×32×10 ,
and for later tree node, we compress the CRU feature to
x ∈ R16×16×20 . Compressing the input feature to a smaller
size helps to reduce the burden of computating and saving
the covariance matrix in Equ. 5. E.g., the vectorized fea-
ture for the ﬁrst CRU is x ∈ R655,360 , and the covariance
matrix of x can take ∼ 400GB in memory. However, after
compression the vectorized feature is x ∈ R10,240 , and the
covariance matrix of x only needs ∼ 0.1GB of memory.
After that, we vectorize the output and apply the routing
function ϕ(x). To compute µ in Equ. 3, instead of opti-
mizing it as a variable of the network, we simply apply a
batch normalization layer without scaling to save the mov-
ing average of each mini-batch. In the end, we project the
compressed CRU response to the largest basis v and obtain
the projection coefﬁcient. Then we assign the samples with
negative coefﬁcient to the left child CRU and the samples
with positive coefﬁcient to the right child CRU.
Implementation details With the overall loss in Equ. 10,
our proposed network is trained in an end-to-end fashion.
All losses are computed based on each mini-batch. DTN
modules and TRU modules are optimized alternately. While
optimizing DTN, we keep the parameters of TRUs ﬁxed and
vice versa.

4. Spoof in the Wild Database with Multiple
Attack Types

To benchmark face anti-spooﬁng methods speciﬁcally
for unknown attacks, we collect the Spoof in the Wild
database with Multiple Attack Types (SiW-M). Compared
with the previous databases in Tab. 1, SiW-M shows a great
diversity in spoof attacks, subject identities, environments
and other factors.
For spoof data collection, we consider two spooﬁng sce-
narios: impersonation, which entails the use of spoof to be
recognized as someone else, and obfuscation, which entails
the use to remove the attacker’s own identity. In total, we
collect 968 videos of 13 types of spoof attacks listed hierat-
ically in Fig 4. For all 5 mask attacks, 3 partial attacks, ob-

4684

s

Live
(493 / 660)

Replay
(21 / 99)

Print
(60 / 118)

Half Mask
(12 / 72)

Silicone
(12 / 27)

Transparent
(88 / 88)

Papercraft
(6 / 17)

Mannequin
(12 / 40)

Obfuscation
(23 / 23)

Imperson.
(61 / 61)

Cosmetic
(37 / 50)

Funny Eye
(160 / 160)

Paperglasses
(122 / 127)

Partial Paper
(86 / 86)

3D Mask Attacks

Makeup Attacks

Partial Attacks

Figure 4: The examples of the live faces and 13 types of spoof attacks. The second row shows the ground truth masks for the pixel-wise
supervision Dk . For (m, n) in the third row, m/n denotes the number of subjects/videos for each type of data.

fuscation makeup and cosmetic makeup, we record 1080P
HD videos. For impersonation makeup, we collect 720P
videos from Youtube due to the lack of special makeup
artists. For print and replay attacks, we intend to collect
videos from harder cases where the existing system fails.
Hence, we deploy an off-the-shelf face anti-spooﬁng algo-
rithm [32] and record spoof videos when the algorithm pre-
dicts live.
For live data, we include 660 videos from 493 subjects.
In comparison, the number of subjects in SiW-M is 9 times
larger than Oulu-NPU [9] and CASIA-FASD [50], and 3
times larger than SiW [32].
In addition, subjects are di-
verse in ethnicity and age. The live videos are collected in
3 sessions: 1) a room environment where the subjects are
recorded with few variations such as pose, lighting and ex-
pression (PIE). 2) a different and much larger room where
the subjects are also recorded with PIE variations. 3) a
mobile phone mode, where the subjects are moving while
the phone camera is recording. Extreme pose angles and
lighting conditions are introduced. Similar to print and re-
play videos, we deploy the face anti-spooﬁng algorithm [32]
to ﬁnd out the videos where the algorithm predicts spoof.
Hence, this third session is a harder scenario.
In total, we collect 1, 630 videos and each lasts 5-7 sec-
onds. The 1080P videos are recorded by Logitech C920 we-
bcam and Canon EOS T6. To use SiW-M for the study of
ZSFA, we deﬁne the leave-one-out testing protocols. Each
time we train a model with 12 types of spoof attacks plus
the 80% of the live videos, and test on the left 1 attack type
plus the 20% of live videos. There is no overlapping sub-
jects between the training and testing sets of live videos.

5. Experimental Results

5.1. Experimental Setup

Databases We evaluate our proposed method on multiple
databases. We deploy the leave-one-out testing protocols
on SiW-M and report the results of 13 experiments. Also,
we test on previous face anti-spooﬁng databases, including
CASIA [50], Replay-Attack [15], and MSU-MFSD [42]),
compare with the state of the art.

Evaluation metrics We evaluate with the following
metrics: Attack Presentation Classiﬁcation Error Rate

(APCER) [1], Bona Fide Presentation Classiﬁcation Error
Rate (BPCER) [1], the average of APCER and BPCER,
Average Classiﬁcation Error Rate (ACER) [1], Equal Er-
ror Rate (EER), and Area Under Curve (AUC). Note that,
in the evaluation of unknown attacks, we assume there is no
validation set to tune the model and thresholds while calcu-
lating the metrics. Hence, we determine the threshold based
on the training set and ﬁx it for all testing protocols. A sin-
gle test sample is one video frame, instead of one video.

Parameter setting The proposed method is implemented
in Tensorﬂow, and trained with a constant learning rate of
0.001 with a batch size of 32. It takes 15 epochs to con-
verge. We randomly initialize all the weights using a normal
distribution of 0 mean and 0.02 standard deviation.

5.2. Experimental Comparison

5.2.1 Ablation Study

All ablation studies use the Funny Eye protocol.
Different fusion methods In the proposed model, both the
norm of the mask maps and binary spoof scores could be
utilized for the ﬁnal classiﬁcation. To ﬁnd the best fusion
method, we compute ACER from using map norm, softmax
score, the maximum of map norm and softmax score, and
the average of two values, and obtain 31.7%, 20.5%, 21.0%,
and 19.3% respectively. Since the average score of the mask
norm and binary spoof score performs the best, we use it
for the remaining experiments. Moreover, we set 0.2 as the
ﬁnal threshold to compute APCER, BPCER and ACER for
all the experiments.
Different routing methods Routing is a crucial step to ﬁnd
the best subgroup to detect spoofness of a testing sample.
To show the effect of proper routing, we evaluate 2 alter-
native routing strategies: random routing and pick-one-leaf.
Random routing denotes randomly selecting one leaf node
for a testing sample to produce prediction; Pick-one-leaf de-
notes constantly selecting one particular leaf node to pro-
duce results, for which we report the mean score and stan-
dard deviation of 8 selections. Shown in Tab. 3, both strate-
gies perform worse than the proposed routing function. In
addition, the large standard deviation of pick-one-leaf strat-
egy shows the large performance difference of 8 subgroups
on the same type of unknown attacks, and demonstrates the
necessity of a proper routing.

4685

Table 2: AUC (%) of the model testing on CASIA, Replay, and MSU-MFSD.

Methods

CASIA [50]
Replay-Attack [15]
Video Cut Photo Warped Photo Video Digital Photo
Printed Photo

MSU [42]
Printed Photo HR Video Mobile Video

Overall

78.7 ± 11.7
88.6 ± 16.3
86.7 ± 15.6

95.9 ± 6.2

73.7
87.3
78.9

99.6

s

64.8
47.7
50.6

81.6

87.4
99.5
99.9

99.9

74.7

97.6

93.5
97.5

N1

N2

N3

N4

N5

N6

N7

OC-SVMRBF +BSIF [3]
SVMRBF +LBP [9]
NN+LBP [45]

Ours

70.7
91.5

94.2

90.0

60.7
91.7
88.4

97.3

95.9
84.5
79.9

97.5

84.3
99.1
99.8

99.9

88.1
98.2
95.2

99.9

Table 3: Compare models with different routing strategies.

Strategies

APCER

BPCER

ACER

EER

Random routing
Pick-one-leaf
Proposed routing function

37.1
51.2 ± 20.0

17.0

16.1

18.1 ± 4.9
21.5

26.6
34.7 ± 8.8

19.3

24.7
24.1 ± 3.1

19.8

Table 4: Compare models with different tree losses and
strategies. The ﬁrst two terms of row 2-5 refer to using live
or spoof data in tree learning. The last row is our method.

Methods
Live data √, Spoof data √, Unique Loss ×
MPT [44]
Live data ×, Spoof data √, Unique Loss ×
Live data √, Spoof data √, Unique Loss √
Live data ×, Spoof data √, Unique Loss √

APCER BPCER ACER

31.4

1.4

70.0
54.2
17.0

24.2
73.3
12.7

12.5

21.5

27.8
37.3
41.3
33.4

19.3

EER

27.3
31.2
44.8
36.2

19.8

Advantage of each loss function We have three important
designs in our unsupervised tree learning: route loss Lroute ,
data used to compute the route loss, and the unique loss
Luniq . To show the effect of each loss and the training strat-
egy, we train and compare networks with each loss excluded
and alternative strategies. First, we train a network with the
routing function proposed in [44], and then 4 models with
different modules on and off, shown in Tab. 4. The model
with MPT [44] routes data only to 2 leaf nodes out of 8 (i.e.
tree collapse issue), which limits the performance. Models
without the unique loss exhibit the imbalance routing issue
where sub-groups cannot be trained properly . Models using
all data to learn the tree show worse performances than us-
ing spoof data only. Finally, the proposed method performs
the best among all options.

5.2.2 Testing on existing databases

Following the protocol proposed in [3], we use CASIA [50],
Replay-Attack [15] and MSU-MFSD [42] to perform ZSFA
testing between replay and print attacks. Tab. 2 compares
the proposed method with top three methods selected from
over 20 methods in [3, 9, 45]. Our proposed method outper-
forms the prior state of the art by a convincing margin of
7.3%, and our smaller standard deviation further indicates a
consistently good performance among unknown attacks.

5.2.3 Testing on SiW-M

We execute 13 leave-one-out testing protocols on SiW-
M. We compare with two of the most recent face anti-
spooﬁng methods [9, 32], and set [32] as the baseline, which
has demonstrated its SOTA performance on various bench-
marks. For a fair comparison with the baseline, we provide
the same pixel-wise labeling (as in Fig. 4), and set the same

−

0

+

Figure 5: Visulization of the Tree Routing.

threshold of 0.2 to compute APCER, BPCER, and ACER.
As shown in Tab. 5, our method achieves an overall bet-
ter APCER, ACER and EER, with the improvement of base-
line by 55%, 29%, and 5%. Speciﬁcally, we reduce the
ACERs of transparent mask, funny eye, and paper glasses
by 31%, 61%, and 51%, where the baseline models can be
considered as total failures since they recognize most of the
attacks as live. Note that, ACER is more valuable in the
context of ZSFA: no evaluation data for setting threshold
and considerably varied thresholds for obtaining the EER
performance. For instance, EERs of paper glasses model
are similar between the baseline and our method, but with a
preset threshold, our method offers a much better ACER.
Moreover,
the proposed method is a more compact
model than [32]. Given the input size of 256 × 256 × 6,
the baseline requires 87 GFlops to compute the result while
our method only needs 6 GFlops (×15 smaller). More anal-
ysis are shown with visualization in Sec. 5.2.4.

5.2.4 Visualization and Analysis

To provide a better understanding of the tree learning and
ZSFA, we visualize the results in several ways. First, we
illustrate the tree routing results.
In Fig. 5, we rank the
spoof data based on the routing function values ϕ(x), and
provide 8 examples with responses from the smallest to the
largest. This offers us an intuitive understanding of what
are learned at each tree node. We observe an obvious spoof
style transfer: for the ﬁrst two-layer nodes N1 , N2 and N3 ,

4686

Table 5: The evaluation and comparison of the testing on SiW-M.

Methods

Metrics (%) Replay

Print

Mask Attacks
Silicone
Trans.
Paper Manne. Obfusc.

Makeup Attacks
Imperson. Cosmetic

Partial Attacks
Paper Glasses

Average

Half

Funny Eye

Partial Paper

SVMRBF +LBP [9]

APCER
BPCER
ACER
EER

19.1
22.1
20.6
20.8
23.7

15.4
21.5
18.4
18.6
7.3

40.8
21.9
31.3
36.3
27.7

20.3
21.4
21.4
21.4

70.3
20.7
45.5
37.2
97.8

0.0

4.6
22.9
13.8
14.1
16.2

96.9
21.7
59.3
51.2
100.0
11.6
55.8
72.3

35.3
12.5
23.9
19.8
18.0

11.3

53.3
18.4
35.9
34.4
91.8

58.5
20.0
39.2
33.0
72.2

0.6
22.9
11.7
7.9
0.4

32.8 ± 29.8
21.0 ± 2.9
26.9 ± 14.5
24.5 ± 12.9
38.3 ± 37.4
23.6 ± 18.5
17.0 ± 17.7

23.1
11.6
7.5
8.3

22.2
16.7
16.1
16.3

Auxiliary [32]

APCER
BPCER
ACER
EER

18.2
11.6
14.9
12.4

10.1

6.5

10.9

6.2

7.8

9.3

9.3

7.1
11.7
9.4

6.2

8.8

10.3
5.3
4.0
0.2

8.9 ± 2.0

16.8
14.0

6.9
4.3

19.3

52.1

8.0
7.8
0.5
8.5

12.8
10.0

13.7
10.1

49.0
21.4

40.5

11.6
0.7

24.6
58.6

18.6
17.0

Ours

APCER
BPCER
ACER
EER

1.0

0.0

24.5
12.8
18.7
18.6

3.8

73.2
11.5
48.1
50.2

13.2

12.4
16.0
14.2
13.2

17.0

17.1 ± 23.3
16.8 ± 11.1
16.1 ± 12.2

18.6

11.9

29.3

13.4

23.0

9.6

21.5

22.6

16.8
8.5
8.8

16.6 ± 6.2

9.8
10.0

6.0
2.1

15.0

36.0

4.5
5.7

7.7
9.6

11.4
10.1

19.3
19.8

19.8

14.4

26.5

20.5

a f 4
a f 5
a f 6
a f 7
a f 8
a f 1
a f 2
a f 3
a f 4
a f 5
e
e
e
e
e
e
e
e
e
L
L
L
L
L
L
L
L
L
(a)                                                                         (b)

L

e

a f 6

L

e

a f 7

L

e

a f 8

Live

Replay

Print

Half Mask
0

Silicone Mask
5
0

57
Trans. (Test)
0

0
Paper Mask
0

Manne

3
Ob. Makeup
0

6
Im. Makeup
0

10
Co. Makeup
1

Funny Eye
36

Paper Glasses
9
41

Partial Paper
0
0

Live (Test)
7

35

0

0

46

44

56

1

1

1

17

23

11

1

44

32

2

3

0

0

40

0

1

0

0

1

40

0

29

9

15

6

0

0

0

0

0

0

0

0

31

7

7

3

4

0

0

19

3

2

46

56

0

52

56

1

0

0

1

7

35

61

0

0

0

0

0

0

2

0

1

2

1

8

7

47

28

20

24

0

38

15

4

5

12

1

1

2

8

2

0

2

12

29

2

15

27

0

41

15

1

2

25

2

4

1

0

2

1

1

0

1

0

0

0

3

3

63

6

96

74

0

10

20

30

40

50

60

70

80

90

L

e

a f 1

L

e

a f 2

L

e

a f 3

L

e

Live

Replay

Print

Half Mask

Silicone Mask

Trans. Mask

Paper Mask

Manne

Ob. Makeup

Im. Makeup

Co. Makeup

Funny Eye

Paper Glasses

Partial Paper

Live (Test)

9

0

0

3

0

0

0

11

0

0

8

2

3

8

9

0

3

6

3

0

0

16

4

0

0

1

0

7

1

45

11

24

20

0

51

18

14

6

8

0

0

0

3

11

33

4

17

0

0

5

0

0

10

1

1

0

13

16

0

0

0

0

0

0

0

0

0

4

34

45

8

9

28

20

6

21

54

43

49

47

14

59

0

0

0

34

13

1

2

42

4

27

19

13

0

0

0

91

73

78

79

Figure 6: Tree routing distribution of live/spoof data. X-axis de-
notes 8 leaf nodes, and y-axis denotes 15 types of data. The num-
ber in each cell represents the percentage (%) of data that fall in
that leaf node. Each row is sum to 1. (a) Print Protocol. (b) Trans-
parent Mask Protocol. Yellow box denotes the unknown attacks.

the transfer captures the change of general spoof attributes
such as image quality and color temperature; for the third-
layer tree nodes N4 , N5 , N6 , and N7 , the transfer involves
more spoof type speciﬁc changes. E.g., N7 transfers from
eye portion spoofs to full face 3D mask spoofs.
Further, Fig. 6 quantitatively analyzes the tree routing
distributions of all types of data. We utilize two models,
Print and Trans. Mask, to generate the distributions. It can
be observed that live samples are relatively more spread out
to 8 leaf nodes while the spoof attacks are routed to fewer
speciﬁc leaf nodes. Two distributions in Fig. 6 (a)&(b) share
similar semantic sub-groups, which demonstrates the suc-
cess of the proposed method on learning a tree. E.g., in
both models, about half of trans. mask samples share the
same leaf node as ob. makeup. By comparing two distri-
butions, most testing unknown spoofs in both models are
successfully routed to the most similar sub-groups.
In addition, we use t-SNE [34] to visualize the feature
space of Print model. The t-SNE is able to project the out-
put of the leaf node F (I | θ) ∈ R32×32×40 to 2D by preserv-
ing the KL divergence distance. Fig. 7 shows the features
of different types of spoof attacks are well-clustered into 8
semantic sub-groups even though we don’t provide any aux-
iliary labels. Based on these sub-groups, the features of un-
known print attacks are well lied in the sub-group of replay
and silicone mask, and thus are recognized as spoof. More-
over, with the visualization, we can explain the performance

co. makeup
im. makeup
ob. makeup
half mask
manne
paper mask
silicone mask
trans. mask

replay
funny eye
paperglasses
partial paper

live
co. makeup
im. makeup
ob. makeup
half mask
manne
paper mask
silicone mask
trans. mask
print
replay
funny eye
paperglasses
partial paper

Figure 7: t-SNE Visualization of the DTN leaf features.

variation among different spoof attacks, shown in Tab. 5.
Among all, the performance of trans. mask, funny eye, pa-
per glasses and ob. makeup are worse than other protocols.
The feature space shows that the live samples lies much
closer to those attacks than others (“→” places), and hence
it’s harder to distinguish them with the live samples. This
demonstrates the diverse property of different unknown at-
tacks and the necessity of such a wide range evaluation.

6. Conclusions

This paper tackles the zero-shot face antispooﬁng prob-
lem among 13 types of spoof attacks. The proposed method
leverages a deep tree network to route the unknown attacks
to the most proper leaf node for spoof detection. The tree is
trained in an unsupervised fashion to ﬁnd the feature base
with the largest variation to split the spoof data. We collect
SiW-M that contains more subjects and spoof types than
any previous databases. Finally, we experimentally show
superior performance of the proposed method.
Acknowledgment This research is based upon work sup-
ported by the Ofﬁce of the Director of National Intelli-
gence (ODNI), Intelligence Advanced Research Projects
Activity (IARPA), via IARPA R&D Contract No. 2017-
17020200004. The views and conclusions contained herein
are those of the authors and should not be interpreted as nec-
essarily representing the ofﬁcial policies or endorsements,
either expressed or implied, of the ODNI, IARPA, or the
U.S. Government. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.

4687

References

[1] ISO/IEC JTC 1/SC 37 Biometrics. information technol-
ogy biometric presentation attack detection part 1: Frame-
work. international organization for standardization, 2016.
https://www.iso.org/obp/ui/iso. 6
[2] A. Agarwal, R. Singh, and M. Vatsa. Face anti-spooﬁng us-
ing Haralick features. In BTAS, 2016. 2
[3] S. R. Arashloo, J. Kittler, and W. Christmas. An anomaly de-
tection approach to face spooﬁng detection: a new formula-
tion and evaluation protocol. IEEE Access, 5:13868–13882,
2017. 2, 7
[4] Y. Atoum, Y. Liu, A. Jourabloo, and X. Liu. Face anti-
spooﬁng using patch and depth-based CNNs. In IJCB, 2017.
1, 2
[5] W. Bao, H. Li, N. Li, and W. Jiang. A liveness detection
method for face recognition based on optical ﬂow ﬁeld. In
IEEE International Conference on Image Analysis and Sig-
nal Processing (IASP), 2009. 2
[6] S. Bharadwaj, T. I Dhamecha, M. Vatsa, and R. Singh.
Face anti-spooﬁng via motion magniﬁcation and multifea-
ture videolet aggregation. Technical report, 2014. 2
[7] Z. Boulkenafet, J. Komulainen, and A. Hadid. Face anti-
spooﬁng based on color texture analysis. In ICIP, 2015. 1,
2
[8] Z. Boulkenafet, J. Komulainen, and A. Hadid. Face anti-
spooﬁng using speeded-up robust features and ﬁsher vector
encoding. IEEE Signal Processing Letters, 2017. 2
[9] Z. Boulkenafet, J. Komulainen, L. Li, X. Feng, and A. Hadid.
OULU-NPU: A mobile face presentation attack database
with real-world variations. In FG, 2017. 3, 6, 7, 8
[10] Q. Cao, X. Liang, B. Li, G. Li, and L. Lin. Visual question
reasoning on general dependency tree. In CVPR, 2018. 3
[11] H. Chang, J. Lu, F. Yu, and A. Finkelstein. PairedCycle-
GAN: Asymmetric style transfer for applying and removing
makeup. In CVPR, 2018. 1
[12] C. Chen, A. Dantcheva, and A. Ross. Automatic facial
makeup detection with application in face recognition.
In
ICB, 2013. 1
[13] C. Chen, A. Dantcheva, and A. Ross. Impact of facial cos-
metics on automatic gender and age estimation algorithms.
In IEEE International Conference on Computer Vision The-
ory and Applications (VISAPP), 2014. 1
[14] X. Chen, C. Liu, and D. Song. Tree-to-tree neural networks
for program translation. arXiv preprint arXiv:1802.03691,
2018. 3
[15] I. Chingovska, A. Anjos, and S. Marcel. On the effectiveness
of local binary patterns in face anti-spooﬁng.
In BIOSIG,
2012. 3, 6, 7
[16] T. de Freitas Pereira, A. Anjos, J. M. De Martino, and S. Mar-
cel. LBP-TOP based countermeasure against face spooﬁng
attacks. In ACCV, 2012. 2
[17] T. de Freitas Pereira, A. Anjos, J. M. De Martino, and S.
Marcel. Can face anti-spooﬁng countermeasures work in a
real world scenario? In ICB, 2013. 2
[18] L. Feng, L. Po, Y. Li, X. Xu, F. Yuan, T. C. Cheung, and K.
Cheung.
Integration of image quality and motion cues for

face anti-spooﬁng: A neural network approach. Journal of
Visual Communication and Image Representation, 2016. 1,
2
[19] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T.
Mikolov, et al. Devise: A deep visual-semantic embedding
model. In NIPS, 2013. 2
[20] A. Jourabloo, Y. Liu, and X. Liu. Face de-spooﬁng: Anti-
spooﬁng via noise modeling. In ECCV, 2018. 1, 2
[21] T. Kaneko, K. Hiramatsu, and K. Kashino. Generative ad-
versarial image synthesis with decision tree latent controller.
In CVPR, 2018. 3
[22] N. Karessli, Z. Akata, B. Schiele, A. Bulling, et al. Gaze em-
beddings for zero-shot image classiﬁcation. In CVPR, 2017.
2
[23] V. Kazemi and J. Sullivan. One millisecond face alignment
with an ensemble of regression trees. In CVPR, 2014. 3
[24] K. Kollreider, H. Fronthaler, M. I. Faraj, and J. Bigun. Real-
time face detection and motion analysis with application in
liveness assessment. In TIFS, 2007. 2
[25] J. Komulainen, A. Hadid, and M. Pietikainen. Context based
face anti-spooﬁng. In BTAS, 2013. 2
[26] C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to
detect unseen object classes by between-class attribute trans-
fer. In CVPR, 2009. 2
[27] L. Li, X. Feng, Z. Boulkenafet, Z. Xia, M. Li, and A. Hadid.
An original face anti-spooﬁng approach using partial convo-
lutional neural network. In IEEE International Conference
on Image Processing Theory Tools and Applications (IPTA),
2016. 2
[28] X. Li, J. Komulainen, G. Zhao, P. C. Yuen, and M.
Pietik ¨ainen. Generalized face anti-spooﬁng by detecting
pulse from face videos. In ICPR, 2016. 2
[29] S. Liu, X. Lan, and P. C. Yuen. Remote photoplethysmog-
raphy correspondence feature for 3D mask face presentation
attack detection. In ECCV, 2018. 1
[30] S. Liu, B. Yang, P. C. Yuen, and Guoying Zhao. A 3D mask
face anti-spooﬁng database with real world variations.
In
CVPRW, 2016. 1, 2, 3
[31] S. Liu, P. C. Yuen, S. Zhang, and G. Zhao. 3D mask face
anti-spooﬁng with remote photoplethysmography. In ECCV,
2016. 1
[32] Y. Liu, A. Jourabloo, and X. Liu. Learning deep models
for face anti-spooﬁng: Binary or auxiliary supervision.
In
CVPR, 2018. 1, 2, 3, 5, 6, 7, 8
[33] Y. Liu, A. Jourabloo, W. Ren, and X. Liu. Dense face align-
ment. In ICCVW, 2017. 5
[34] L. Maaten and G. Hinton. Visualizing data using t-SNE.
Journal of machine learning research, 9(Nov):2579–2605,
2008. 8
[35] J. M ¨a ¨att ¨a, A. Hadid, and M. Pietik ¨ainen. Face spooﬁng de-
tection from single images using micro-texture analysis. In
IJCB, 2011. 1, 2
[36] G. Pan, L. Sun, Z. Wu, and S. Lao. Eyeblink-based anti-
spooﬁng in face recognition from a generic webcamera. In
ICCV, 2007. 2
[37] K. Patel, H. Han, and A. K. Jain. Cross-database face anti-
spooﬁng with robust feature representation. In CCBR, 2016.
2

4688

[38] K. Patel, H. Han, and A. K. Jain. Secure face unlock: Spoof
detection on smartphones. In TIFS, 2016. 1, 2
[39] R. Shao, X. Lan, and P. C. Yuen.
Deep convolu-
tional dynamic texture learning with adaptive channel-
discriminability for 3D mask face anti-spooﬁng.
In IJCB,
2017. 2
[40] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot
learning through cross-modal transfer. In NIPS, 2013. 2
[41] R. Valle and M. Jos ´e. A deeply-initialized coarse-to-ﬁne
ensemble of regression trees for face alignment. In ECCV,
2018. 3
[42] D. Wen, H. Han, and A. K. Jain. Face spoof detection with
image distortion analysis. In TIFS, 2015. 6, 7
[43] Y. Wu and K. He. Group normalization. In ECCV, 2018. 5
[44] C. Xiong, X. Zhao, D. Tang, K. Jayashree, S. Yan, and
T. Kim.
Conditional convolutional neural network for
modality-aware face recognition. In ICCV, 2015. 3, 7
[45] F. Xiong and W. Abdalmageed. Unknown presentation at-
tack detection with face RGB images.
In BTAS, 2018. 2,
7
[46] Z. Xu, S. Li, and W. Deng. Learning temporal features using
LSTM-CNN architecture for face anti-spooﬁng.
In ACPR,
2015. 2
[47] J. Yang, Z. Lei, S. Liao, and S. Z. Li. Face liveness detection
with component dependent descriptor. In ICB, 2013. 2
[48] Z. Yang, J.and Lei and S. Z. Li.
Learn convolutional
neural network for face anti-spooﬁng.
arXiv preprint
arXiv:1408.5601, 2014. 2
[49] L. Zhang, T. Xiang, and S. Gong. Learning a deep embed-
ding model for zero-shot learning. In CVPR, 2017. 2
[50] Z. Zhang, J. Yan, S. Liu, Z. Lei, D. Yi, and S. Z. Li. A face
antispooﬁng database with diverse attacks. In ICB, 2012. 3,
6, 7

4689

DeepCO3 : Deep Instance Co-segmentation by Co-peak Search and Co-saliency
Detection

Kuang-Jui Hsu1
1Academia Sinica, Taiwan

2

,

Yen-Yu Lin1
Yung-Yu Chuang1
2National Taiwan University, Taiwan

2

,

Abstract

In this paper, we address a new task called instance co-
segmentation. Given a set of images jointly covering object
instances of a speciﬁc category, instance co-segmentation
aims to identify all of these instances and segment each of
them, i.e. generating one mask for each instance. This task
is important since instance-level segmentation is preferable
for humans and many vision applications. It is also chal-
lenging because no pixel-wise annotated training data are
available and the number of instances in each image is un-
known. We solve this task by dividing it into two sub-tasks,
co-peak search and instance mask segmentation. In the for-
mer sub-task, we develop a CNN-based network to detect
the co-peaks as well as co-saliency maps for a pair of im-
ages. A co-peak has two endpoints, one in each image, that
are local maxima in the response maps and similar to each
other. Thereby, the two endpoints are potentially covered by
a pair of instances of the same category. In the latter sub-
task, we design a ranking function that takes the detected
co-peaks and co-saliency maps as inputs and can select the
object proposals to produce the ﬁnal results. Our method
for instance co-segmentation and its variant for object co-
localization are evaluated on four datasets, and achieve fa-
vorable performance against the state-of-the-art methods.
The source codes and the collected datasets are available
at https://github.com/KuangJuiHsu/DeepCO3/.

1. Introduction

Object co-segmentation aims to segment the common
objects repetitively appearing in a set of images. It is a fun-
damental and active research topic in computer vision. As
an important component of image content understanding, it
is essential to many vision applications, such as semantic
segmentation [48], image matching [4, 19, 25, 52, 60, 61],
object skeletonization [8, 27], and 3D reconstruction [42].
Object co-segmentation has recently gained signiﬁcant
progress owing to the fast development of convolutional
neural networks (CNNs). The CNN-based methods [21, 33,
62] learn the representation of common objects in an end-

Figure 1. Two examples of instance co-segmentation on categories
bird and sheep, respectively. An instance here refers to an object
appearing in an image. In each example, the top row gives the
input images while the bottom row shows the instances segmented
by our method. The instance-speciﬁc coloring indicates that our
method produces a segmentation mask for each instance.

to-end manner and can produce object-level results of high
quality. However, they do not explore instance-aware infor-
mation, i.e. one segmentation mask for each instance rather
than each class, which is more consistent with human per-
ception and offers better image understanding, such as the
locations and shapes of individual instances.
In this work, we present a new and challenging task
called instance-aware object co-segmentation (or instance
co-segmentation for short). Two examples of this task are
shown in Figure 1 for a quick start. Given a set of images of
a speciﬁc object category with each image covering at least
one instance of that category,
instance co-segmentation
aims to identify all of these instances and segment each of
them out, namely one mask for each instance. Note that un-
like semantic [18] or instance segmentation [65], no pixel-
wise data annotations are collected for learning. The object
category can be arbitrary and unknown, which means that
no training images of that category are available in advance.
Instance-level segments that can be obtained by solving this
task are valuable to many vision applications, such as au-
tonomous driving [2, 64], instance placement [31], image
and sentence matching [26] or amodal segmentation [23].

18846

Therefore, instance co-segmentation has a practical setting
in input collection and better accomplishing it potentially
advances the ﬁeld of computer vision.
In this paper, we develop a CNN-based method for in-
stance co-segmentation. Based on the problem setting, our
method has no access to annotated instance masks for learn-
ing and cannot involve any pre-training process. Inspired
by Zhou et al. [65]’s observation that object instances often
cover the peaks in a response map of a classier, we design a
novel co-peak loss to detect the common peaks (or co-peaks
for short) in two images. The co-peak loss is built upon a
4D tensor that is learned to encode the inter-image similar-
ity at every location. The co-peaks inferred from the learned
4D tensor correspond to two locations, one in each of the
two images, where discriminative and similar features are
present. Therefore, the two locations are potentially cov-
ered by two object instances. Using the co-peak loss alone
may lead to unfavorable false positives and negatives. Thus,
we develop the afﬁnity loss and the saliency loss to comple-
ment the co-peak loss. The former carries out discrimina-
tive feature learning for the 4D tensor construction by sep-
arating the foreground and background features. The lat-
ter estimates the co-saliency maps to localize the co-salient
objects in an image, and can make our model focus on co-
peak search in co-salient regions. The three loss functions
work jointly and can detect co-peaks of high quality. We
design a ranking function taking the detected co-peaks and
co-saliency maps as inputs and accomplish instance mask
segmentation by selecting object proposals.
We make the following contributions in this work. First,
we introduce a new and interesting task called instance co-
segmentation. Its input is a set of images containing object
instances of a speciﬁc category, and hence is easy to col-
lect. Its output is instance-aware segments, which are de-
sired in many vision applications. Thus, we believe instance
co-segmentation worth exploring. Second, a simple and ef-
fective method is developed for instance co-segmentation.
The proposed method learns a model based on the fully con-
volutional network (FCN) [40] by optimizing three losses,
including the co-peak, afﬁnity, and saliency losses. The
learned model can reliably detect co-peaks and co-saliency
maps for instance mask segmentation. Third, we collect
four datasets for evaluating instance co-segmentation. The
proposed method for instance co-segmentation and its vari-
ant for object co-localization [5, 6, 51, 58, 59] are extensively
evaluated on the four datasets. Our method performs favor-
ably against the state-of-the-art methods.

2. Related Work

Object co-segmentation. This task [13, 28, 45, 46, 54, 56,
57] aims to segment the common objects in images. Its ma-
jor difﬁculties lie in large intra-class variations and back-
ground clutter. Most methods rely on robust features, such

as handcrafted and deep learning based features, for ad-
dressing these difﬁculties. In addition, saliency evidence,
including single-image saliency [12, 20, 27, 28, 46, 53] or
multi-image co-saliency [3, 54, 57], has been explored to
localize the salient and common objects. Recently, CNN-
based methods [21, 33, 62] achieve better performance by
joint representation learning and co-segmentation.
Despite effectiveness, the aforementioned methods do
not provide instance-level results. In this work, we go be-
yond object co-segmentation and investigate instance co-
segmentation. Our method can determine the number, lo-
cations, and contours of common instances in each image,
and offers instance-aware image understanding.

Object co-localization. This task [5, 6, 51, 58, 59] discov-
ers the common instances in images. Different from object
co-segmentation, it is instance-aware. It detects and outputs
the bounding box of a single instance in each image even if
multiple instances are present in the image. Compared with
object co-localization, instance co-segmentation identiﬁes
all instances in an image in the form of instance segments.

Instance-aware segmentation.
Instance-aware segmen-
tation includes class-aware [1, 7, 15, 17, 65] and class-
agnostic [11, 24, 32] methods. Given training data of pre-
deﬁned categories, class-aware instance segmentation, aka
instance segmentation, learns a model to seek each object
instance belonging to one of these categories. A widely
used way for instance segmentation is to ﬁrst detect instance
bounding boxes and then segment the instances within the
bounding boxes [7, 15–17, 35, 38, 43]. Another way is to di-
rectly segment each instance without bounding box detec-
tion [1, 30, 36, 39, 65]. While most methods for instance seg-
mentation are supervised, Zhou et al. [65] present a weakly
supervised one. All these methods for instance segmenta-
tion rely on training data to learn the models. Despite the
effectiveness and efﬁciency in testing, their learned models
are not applicable to unseen object categories.
In practice, it is difﬁcult to enumerate all object cat-
egories of interest in advance and prepare class-speciﬁc
training data, which limits the applicability of class-aware
instance segmentation. Class-agnostic instance segmenta-
tion [11, 24, 32] aims at segmenting object instances of arbi-
trary categories, and has drawn recent attention. It is chal-
lenging because it involves both generic object detection
and segmentation. Instance co-segmentation is highly re-
lated to class-agnostic instance segmentation in the sense
that both of them can be applied to arbitrary and even un-
seen object categories. However, existing class-agnostic
methods require annotated training data in the form of ob-
ject contours. On the contrary, our method for instance
co-segmentation explores the mutual information regarding
the common instances in given images, and does not need
any pre-training procedure on additional data annotations.
Thus, our method has better generalization.

8847

(cid:38)(cid:82)(cid:16)(cid:83)(cid:72)(cid:68)(cid:78)(cid:3)(cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)

(cid:1835)(cid:3041)

(cid:1835)(cid:3040)

(cid:44)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:48)(cid:68)(cid:86)(cid:78)(cid:3)(cid:54)(cid:72)(cid:74)(cid:80)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:1859)

(cid:1832)(cid:3041)

(cid:1875) (cid:1860) (cid:1856)

(cid:1832)(cid:3040)

(cid:1875) (cid:1860) (cid:1856)

(cid:883) (cid:3400) (cid:883) (cid:70)(cid:82)(cid:81)(cid:89)(cid:17)
(cid:38)(cid:82)(cid:85)(cid:85)(cid:72)(cid:79)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:1846)(cid:3041)(cid:3040)

(cid:1845)(cid:4634)(cid:3041)

(cid:1845)(cid:4634)(cid:3040)

(cid:39)(cid:72)(cid:70)(cid:82)(cid:81)(cid:89)(cid:17)

(cid:48)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:38)(cid:82)(cid:16)(cid:83)(cid:72)(cid:68)(cid:78)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86)

(cid:48)(cid:68)(cid:91)(cid:76)(cid:80)(cid:76)(cid:93)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)

(cid:36)(cid:73)(cid:73)(cid:76)(cid:81)(cid:76)(cid:87)(cid:92)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86)

(cid:1845)(cid:3041)

(cid:1845)(cid:4632)(cid:3041)

(cid:54)(cid:68)(cid:79)(cid:76)(cid:72)(cid:81)(cid:70)(cid:92)(cid:16)(cid:74)(cid:88)(cid:76)(cid:71)(cid:72)(cid:71)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86)

following object function

3.2.2 Afﬁnity loss ℓa

L♣wq ✏ λt

  λa

N

➳

n✏1

N

➳

n✏1

➳

m✘n

➳

m✘n

ℓt ♣In , Im ; wq

ℓa ♣In , Im ; wq  

(1)

N

➳

n✏1

ℓs ♣In ; wq,

where w is the set of learnable parameters of the network.
Nonnegative weights λt and λa control the relative impor-
tance among the three losses. They are ﬁxed to 0.5 and
0.1 in this work, respectively. The co-peak loss ℓt stimu-
lates co-peak detection. The afﬁnity loss ℓa refers to the co-
saliency maps and enables discriminative feature learning.
The saliency loss ℓs working with the other two losses car-
ries out co-saliency detection and hence facilitates instance
co-segmentation. The three losses are elaborated in the fol-
lowing.

3.2.1 Co-peak loss ℓt

This loss aims to stimulate co-peak detection. A co-peak
consists of to two points, one in each of In and Im . Since
a co-peak covered by a pair of instances of the same object
category is desired, the two points of the co-peak must be
inside the object and similar to each other. Therefore, both
intra-image saliency and inter-image correlation are taken
into account in this loss.
As shown in Figure 2, our two-stream network produces
the intra-image saliency maps ˜Sn and ˜Sm in one stream and
inter-image correlation map Tnm in the other stream. To
jointly consider the two types of information, a saliency-
guided correlation tensor T s
nm P Rw✂h✂w✂h is constructed
with its elements deﬁned below
T s
nm ♣p, qq ✏ ˜Sn ♣pq ˜Sm ♣qqTnm ♣p, qq,
where p P P , q P P , and P is the set of all spatial coordi-
nates of the feature maps. In Eq. (2), ˜Sn ♣pq is the saliency
value of ˜Sn at point p, and ˜Sm ♣qq is similarly deﬁned.
To have more reliable keypoints to reveal object in-
stances, we deﬁne a co-peak as a local maximum in T s
within a 4D local window of size 3 ✂ 3 ✂ 3 ✂ 3. Suppose
that ♣p, qq is a peak in T s
nm . Both point p in Fn and point
q in Fm are salient, and they are the most similar to each
other in a local region. The former property implies that the
two points probably reside in two salient object instances.
The latter one reveals that the two instances are likely of the
same class, since they have similar parts. Based on above
discussion, the co-peak loss used to stimulate reliable co-
peaks is deﬁned by
ℓt ♣In , Im q ✏ ✁log ☎

(2)

nm

1

nm ♣p, qq☞
T s
✌, (3)

⑤Mnm ⑤ ➳

♣p,qqPMnm

✆

where Mnm is the set of co-peaks.

The co-peak loss refers to the feature maps of the images,
so discriminative features that can separate instances from
background are preferable. Besides, the co-peak loss is ap-
plied to the locations of co-peaks, and features on other lo-
cations are ignored. The afﬁnity loss is introduced to ad-
dress the two issues.
It aims to derive the features with
which pixels in the salient regions are similar to each other
while being distinct from those in the background. For a
pair of images In and Im , a loss ˜ℓa ♣In , Im q is deﬁned by
˜ℓa ♣In , Im q ✏ ➳

˜Sn ♣pq ˜Sn ♣qq ♣1 ✁ Tnm ♣p, qqq

➳

pPP

qPP

(4)

  α♣ ˜Sn ♣pq ✁ ˜Sn ♣qqq2Tnm ♣p, qq,

where constant α is empirically set to 4.
In Eq. (4), the
ﬁrst term penalizes the case of low similarity between two
salient pixels, while the second term prevents high similar-
ity between a salient pixel and a non-salient pixel. The pro-
posed afﬁnity loss generalizes ˜ℓa in Eq. (4) to consider both
inter-image and intra-image afﬁnities and is deﬁned by

ℓa ♣In , Im q ✏ ˜ℓa ♣In , Im q   ˜ℓa ♣In , In q   ˜ℓa ♣Im , Im q. (5)

3.2.3 Saliency loss ℓs

This term aims to identify the salient regions and can guide
the training of our model. Following the studies of ob-
ject co-segmentation [27, 28, 46, 53], we utilize an off-the-
shelf method for saliency detection. The resultant saliency
maps can serve as the object prior. In this work, we adopt
the unsupervised method, SVFSal [63], which produces the
saliency map ˆSn for image In . Note that the resolutions of
ˆSn and In are the same. Thus, the deconvolutional layers
are employed to increase the resolution. Following [22], the
saliency loss ℓs applied to image In is deﬁned by

ℓs ♣In q ✏ ➳

pPIn

ρn ♣pq⑥Sn ♣pq ✁ ˆSn ♣pq⑥2
2 ,

(6)

where p indexes the pixels of In , ρn ♣pq is a weight repre-
senting the importance of pixel p, and Sn is the predicted
saliency map for In by our model. The weight ρn ♣pq deals
with the imbalance between the salient and non-salient ar-
eas. It is set to 1 ✁ ε if pixel p resides in the salient region,
and ε otherwise, where ε is the ratio of the salient area to the
whole image. The mean value of ˆSn is used as the thresh-
old to divide ˆSn into the salient and non-salient regions.
In this way, the salient and non-salient regions contribute
equally in Eq. (6). As shown in Figure 2, except for the de-
convolutional layers, our model used to produce maps tSn ✉
is derived by the three losses jointly. Thus, tSn ✉ derived
with both intra- and inter-image cues are called co-saliency
maps. This prior term is helpful as it compensates for the
lack of supervisory signals in instance co-segmentation.

8849

3.3. Instance mask segmentation

n ✉M

After optimizing Eq. (1), we simply use the detected
peaks on the estimated co-saliency maps as the ﬁnal co-
peaks, because detecting the co-peaks on all possible image
pairs is complicated. Thus, the peaks tpi
i✏1 of each im-
age In are collected, where M is the number of the peaks.
We adopt the method called peak back-propagation [65] to
infer an instance-aware heat map O i
n for each peak pi
n . The
map O i
n is supposed to highlight the instance covering pi
n .
An example is given in Figure 2.
For instance mask generation, we utilize an unsuper-
vised method, called multi-scale combinatorial grouping
(MCG) [44], to produce a set of instance proposals for im-
age In . With the heat maps tO i
i✏1 and the co-saliency
map Sn , we extend the proposal ranking function in [65]
by further taking the co-saliency cues into account, and se-
lect the top-ranked proposal as the mask for each detected
peak. Speciﬁcally, given the maps O i
n and Sn , the ranking
function R applied to an instance proposal P is deﬁned by

n ✉M

R♣P q ✏ β ♣O i
n ✝Sn q✝P  ♣O i
n ✝Sn q✝ ˆP ✁γ ♣1✁Sn q✝P , (7)
where ˆP is the contour of the proposal P and operator ✝
is the Frobenius inner product between two matrices. The
coefﬁcients β and γ are set to 0.8 and 10✁5 , respectively.
In Eq. (7), three terms, i.e. the instance-aware, contour-
preserving, and object-irrelevant terms, are included. The
instance-aware term prefers the proposals that cover the re-
gions with high responses in O i
n and high saliency in Sn .
The contour-preserving term focuses on the ﬁne-detailed
boundary information. The background map, 1 ✁ Sn , is
used in the object-irrelevant term to suppress background
regions. Compared with the ranking function in [65], ours
further exploits the properties of instance co-segmentation,
i.e. the high co-saliency values in object instances, and can
select more accurate proposals. Following a standard pro-
tocol of instance segmentation, we perform non-maximum
suppression (NMS) to remove the redundancies.

3.4. Implementation details

We implement the proposed method using MatCon-
vNet [55]. VGG-16 [49] is adopted as the feature extrac-
tor g . It is pre-trained on the ImageNet [47] dataset, and is
updated during optimizing Eq. (1). The same network ar-
chitecture is used in all experiments. Note that the objective
in Eq. (1) involves all image pairs. Direct optimization is
not feasible due to the limited memory size. Thereby, we
adopt the piecewise training scheme [50]. Namely, only a
subset of images is considered in each epoch, and the subset
size is set to 6 in this work. The learning rate, weight decay,
and momentum are set to 10✁6 , 0.0005, and 0.9, respec-
tively. The optimization procedure stops after 40 epochs.
We choose ADAM [29] as the optimization solver. All im-
ages are resized to the resolution 448 ✂ 448 in advance. We

dataset
COCO-VOC
COCO-NONVOC
VOC12
SOC

(a)
12
32
18
5

(b)
1281
3130
891
522

(c)
3151
8303
2214
835

(d)
106.8
91.8
178.2
29.0

(e)
2.5
2.7
2.5
1.6

Table 1. Some statistics of the four collected datasets, including (a)
the number of classes, (b) the number of images, (c) the number
of instances, (d) the average number of images per class, and (e)
the average number of instances per image.

resize the instance co-segmentation results back to the orig-
inal image resolution for performance evaluation.

4. Experimental Results

In this section, our method for instance co-segmentation
and its variant for co-localization are evaluated. First,
the adopted datasets and evaluation metrics are described.
Then, the competing methods are introduced. Finally, the
comparison results are reported and analyzed.

4.1. Dataset collection

As instance co-segmentation is a new task, no public
benchmarks exist. Therefore, we establish four datasets
with pixel-wise instance annotations by collecting im-
ages from three public benchmarks,
including the MS
COCO [37], PASCAL VOC 2012 [9, 14], and SOC [10]
datasets. The following pre-processing is applied to each
dataset. First, we remove the images where objects of more
than one category are present. Second, we discard the cate-
gories that contain less than 10 images. The details of col-
lecting images from each dataset are described below.

MS COCO dataset. We collect images from the training
and validation sets of the MS COCO 2017 object detection
task. As MS CCCO is a large-scale dataset, we further re-
move the images that do not contain at least two instances.
Total 44 categories remain. Some competing methods are
pre-trained on PASCAL VOC 2012 dataset. For the ease
of comparison, we divide the 44 categories into two dis-
joint sets, COCO-VOC and COCO-NONVOC. The former
contains 12 categories covered by the PASCAL VOC 2012
dataset, while the latter contains the rest.

PASCAL VOC 2012 dataset. Because few pixel-wise
instance annotations are available in the PASCAL VOC
2012 dataset, we adopt the augmented VOC12 dataset [14],
which has 18 object categories after dataset preprocessing.

SOC dataset. SOC [10] is a newly collected dataset
for saliency detection. It provides image-level labels and
instance-aware annotations. After preprocessing, only ﬁve
object categories remain because many images contain ob-
ject instances of multiple categories and some categories
have less than 10 images.

8850

method

year

trained

CLRW [51] CVPR 2014 ✂
UODL [5] CVPR 2015 ✂
DDT [58]
IJCAI 2017 ✂
DDT  [59]
arXiv 2017
DFF [6]
ECCV 2018 ✂
NLDF [41] CVPR 2017 ❵
C2S-Net [34] ECCV 2018 ❵
CVPR 2018 ❵
PRM [65]
Ours
-

✂

✂

COCO-VOC
mAPr
33.3
9.6
31.4
31.7
30.8
39.1
39.6
44.9
52.6

13.7
2.2
10.1
10.6
11.6
18.2
13.4
14.6
21.1

COCO-NONVOC

VOC12

SOC

0.25 mAPr
0.5 mAPr
0.25 mAPr
0.5 mAPr
0.25 mAPr
0.5 mAPr
0.25 mAPr
0.5

24.6
8.5
25.7
26.0
22.6
23.9
25.1
-
35.3

10.7
1.8
9.7
10.1
7.3
8.5
7.6
-
12.3

29.2
9.4
30.7
33.6
27.7
34.3
30.1
45.3
45.6

10.5
2.0
8.8
9.4
13.7
12.7
10.7
14.8
16.7

34.9
11.0
43.0
39.6
42.3
49.5
37.0
-
54.2

15.6
2.7
25.7
22.4
17.0
21.6
12.5
-
26.0

Table 2. Performance of instance co-segmentation on the four collected datasets. The numbers in red and green show the best and the
second best results, respectively. The column “trained” indicates whether additional training data are used.

The statistics and the abbreviations of the four collected
datasets are given in Table 1. Note that our method can work
on images containing one or multiple instances of the com-
mon object category. The SOC dataset helps test this issue.
As shown in Table 1, the average number of instances in
SOC is 1.6, less than 2. It shows that there exist many im-
ages in this dataset with only one object instance. Please re-
fer to the supplementary material for more details and some
image samples of the four collected datasets.

4.2. Evaluation metrics

For instance co-segmentation, mean average precision
(mAP) [15] is adopted as the performance measure. Follow-
ing [65], we report mAP using the IoU thresholds at 0.25
and 0.5, denoted as mAPr
0.25 and mAPr
0.5 , respectively.
For object co-localization,
the performance measure
CorLoc [5, 6, 51, 58, 59] is used as the evaluation metric. The
measure CorLoc is designed for evaluating the results in the
form of object bounding boxes. For comparing with meth-
ods whose output is object or instance segments, we extend
CorLoc to CorLocr to evaluate the results in the form of
object segments.

4.3. Competing methods

As instance co-segmentation is a new task, there are
no existing methods for performance comparison. We
adopt two strategies for comparing our method with exist-
ing ones. First, we consider competing methods of three
categories, including object co-localization, class-agnostic
saliency segmentation, and weakly supervised instance seg-
mentation. For methods of the three categories, we convert
their predictions into the results in the form of instance co-
segmentation, namely one segment mask for each detected
instance.
In this way, our method can be compared with
these methods on the task of instance co-segmentation.
Second, we compare our method with methods of all the
aforementioned three categories on the task of object co-
localization. To this end, we need to convert the output of
each compared method into the results in the form of object

co-localization, namely the object bounding box with the
highest conﬁdence in each image.
In the two strategies of method comparison, two types
of prediction conversion are required, including converting
a bounding box to an instance segment and its inverse di-
rection. Unless further speciﬁed, we adopt the following
way to convert a bounding box prediction to an instance
segment. Given a bounding box in an image, we apply
MCG [44] to that image to generate a set of instance pro-
posals, and retrieve the proposal with the highest IoU with
the bounding box to represent it. On the other hand, it is
easy to concert a given instance segment to a bounding box.
We simply use the bounding box of that instance segment to
represent it. In the following, the selected competing meth-
ods from each of the three categories are speciﬁed.

Object co-localization. We choose the state-of-the-art
methods of
this category for comparison,
including
CLRW [51], UODL [5], DDT [58], DDT  [59], and
DFF [6]. The ﬁrst two methods, CLRW and UODL, out-
put all bounding boxes with their scores, but cannot deter-
mine the number of instances in each image. Thus, we pick
the top-scored bounding boxes as many as the instances de-
tected by our method, and similarly apply NMS to remove
redundancies. The last three methods, DDT, DDT , and
DFF, ﬁrst produce the heat maps to highlight objects, then
convert the heat maps into the binary masks by using their
proposed mechanisms, and ﬁnally take the bounding boxes
of the connected components on the binary masks.

Class-agnostic instance segmentation (CAIS). We se-
lect two powerful methods, NLDF [41] and C2S-Net [34],
of this category as the competing methods. The algorithm
proposed in [32] is used to convert the saliency contours
generated by NLDF and C2S-Net into the results in the form
of instance co-segmentation.

Weakly supervised instance segmentation (WSIS). The
WSIS method, PRM [65], is trained on the PASCAL VOC
2012 dataset, and it cannot be applied to the images whose
categories are not covered by the PASCAL VOC 2012

8851

cow

sheep

horse

train

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Figure 3. Results of instance co-segmentation on four object categories, i.e.cow, sheep, horse, and train, of the COCO-VOC dataset. (a)
Input images. (b) Ground truth. (c) ✒ (g) Results with instance-speciﬁc coloring generated by different methods including (c) our method,
(d) CLRW [51], (e) DFF [6], (f) NLDF [41], and (g) PRM [65], respectively.

(cid:954)(cid:3046) (cid:3397) (cid:954)(cid:3047)
(cid:954)(cid:3046)
(cid:954)(cid:3046) (cid:3397) (cid:954)(cid:3047) (cid:3397) (cid:954)(cid:3028)
(cid:954)(cid:3046)
(cid:954)(cid:3046) (cid:3397) (cid:954)(cid:3047) (cid:3397) (cid:954)(cid:3028)
(cid:954)(cid:3046) (cid:3397) (cid:954)(cid:3047)

Figure 4. Performance in mAPr
0.25 with different loss function
combinations on the COCO-VOC and COCO-NONVOC datasets.

dataset. Therefore, PRM is compared with our method only
on the COCO-VOC and VOC12 datasets.

4.4. Instance co-segmentation

i.e. trained

and non-trained.

For
the ease of performance analysis, we divide
the evaluated methods into two groups,
The group trained includes
NLDF [41], C2S-Net [34] and PRM [65]. Methods
of this group require additional training data other than
the input to instance co-segmentation. The other group
non-trained contains our method and the rest of the
competing methods. Methods of group non-trained
have access to only the input to instance co-segmentation.
Our method and all competing methods are evaluated
on the four collected datasets. Their performance is re-
ported in Table 2. The proposed method outperforms the
competing methods of group non-trained by large mar-
gins even though all of them access the same data. We at-

w/o co-saliency map
w co-saliency map

25.3
35.3

8.3
12.3

COCO-NONVOC

0.25 mAPr
0.5 mAPr
0.25 mAPr
0.5

COCO-VOC
mAPr
33.5
52.6

12.4
21.1

Table 3. Performance of our method working with the proposal
ranking function without or with the co-saliency information on
the COCO-VOC and COCO-NONVOC datasets.

tribute the performance gain yielded by our method to fea-
ture learning enabled CNNs. The competing methods of
group non-trained adopt pre-deﬁned features, and can-
not well deal with complex and diverse intra-class varia-
tions and background clutters. On the contrary, our method
leverages CNNs to carry out feature learning and instance
co-segmentation simultaneously,
leading to much better
performance. Although the methods of group trained
have access to additional training data, ours still reaches
more favorable results. The main reason is that our method
explores co-occurrent patterns via co-peak detection when
images for instance co-segmentation are available, while the
methods of group trained ﬁx their models after training
on additional data and cannot adapt themselves to newly
given images for instance co-segmentation.

To gain the insight into the quantitative results, Fig-
ure 3 visualizes the qualitative results generated by our
method, CLRW [51], DFF [6], NLDF [41], and PRM [65].
The major difﬁculties of instance segmentation lie in in-
stance mutual occlusions, intra-class variations, and clut-

8852

r

a
e
b

e
n
o
h
p

l
l

e
c

t

n
a
h
p
e

l

e

r

a
e
b
y
d
d
e

t

a

r

b
e
z

a

r

b
e
z

a

r

b
e
z

Figure 5. Seven examples, one in each row, of the co-localization
results by our method on the COCO-NONVOC dataset.

tered scene. As shown in Figure 3(c), our method still
works well when instance mutual occlusions occur on cate-
gories cow, sheep, and horse and large intra-class variations
and cluttered scene are present on category train. In Fig-
ure 3(d), CLRW yields some false alarms in the background
while has false negatives on category train. In Figure 3(e),
DFF cannot well address instance mutual occlusions due
to computing connected components for instance identiﬁ-
cation. In Figure 3(f) and Figure 3(g), NLDF and CRP per-
form favorably against other competing methods, but still
suffer from over-segmentation and misses, respectively.

Ablation studies. We analyze the proposed objective con-
sisting of three loss functions in Eq. (1) on the COCO-VOC
and COCO-NONVOC datasets, and report the results in
Figure 4. Except loss ℓs , the other two losses, ℓt and ℓa , are
added one by one. When ℓt is included, the performance
gains are signiﬁcant on both datasets. It implies that ℓt for
reliable co-peak search is important in our method. Once ℓa
is added, the performance is moderately enhanced, which
means that discriminative feature learning is helpful for in-
stance co-segmentation.
In addition to the objective, the
effect of referring to co-saliency maps in proposal ranking
is analyzed in Table 3. The results clearly point out that in-
formation from co-saliency detection is crucial to proposal
ranking.
It is not surprised. Since co-peaks identify the
keypoints within instances, we still need the evidence from
co-saliency maps to reveal the corresponding instances.

method
year
trained COCO-VOC COCO-NONVOC VOC12 SOC
CLRW [51] CVPR 2014 ✂
33.4
31.6
29.9
30.9
UODL [5] CVPR 2015 ✂
12.3
12.7
9.5
10.3
DDT [58]
IJCAI 2017 ✂
30.0
27.4
25.0
16.7
DDT  [59]
PR 2019
29.5
25.8
23.7
18.4
DFF [6]
ECCV 2018 ✂
32.3
30.5
28.7
22.9
NLDF [41] CVPR 2017 ❵
51.2
31.0
39.2
42.0
C2S-Net [34] ECCV 2018 ❵
39.0
28.4
31.1
32.9
PRM [65]
CVPR 2018 ❵
18.1
-
23.3
-
Ours
-
49.6
34.3
39.2
43.1

✂

✂

Table 4. Performance of object co-localization on the four datasets.
The numbers in red and green indicate the best and the second
best results, respectively. The column “trained” indicates whether
additional training data are used.

4.5. Object co-localization

We evaluate our method and the competing methods
for object co-localization in the four datasets we collected.
For our method, we pick the top-ranked proposal in each
image when evaluating the performance in CorLocr . Ta-
ble 4 reports the performance of all the compared methods.
Our method achieves the comparable or even better perfor-
mance, even though it is not originally designed for object
co-localization. Seven examples of object co-localization
by our method are shown in Figure 5, where accurate in-
stance masks and the corresponding bounding boxes are
discovered by our method.

5. Conclusions

In this paper, we present an interesting and challeng-
ing task called instance co-segmentation, and propose a
CNN-based method to effectively solve it without using
additional training data. We decompose this task into two
sub-tasks,
including co-peak search and instance mask
segmentation. In the former sub-task, we design three novel
losses, co-peak, afﬁnity, and saliency losses, for joint co-
peak and co-saliency map detection. In the latter sub-task,
we develop an effective proposal ranking algorithm, and
can retrieve high-quality proposals to accomplish instance
co-segmentation. Our method for instance co-segmentation
and its variant for object co-localization are extensively
evaluated on the four collected datasets. Both quantitative
and qualitative results show that our method and its variant
perform favorably against
the state-of-the-arts.
In the
future, we plane to integrate the proposed method into
more high-level tasks, such as autonomous driving, visual
question answering, image and sentence matching where
instance-aware annotations are valuable.

Acknowledgments. This work was supported in part by
Ministry of Science and Technology (MOST) under grants
107-2628-E-001-005-MY3 and 108-2634-F-007-009, and
MOST Joint Research Center for AI Technology and All
Vista Healthcare under grant 108-2634-F-002-004.

8853

References

[1] Min Bai and Raquel Urtasun. Deep watershed transform for
instance segmentation. In CVPR, 2017.
[2] Bert De Brabandere, Davy Neven, and Luc Van Gool. Se-
mantic instance segmentation for autonomous driving.
In
CVPR Workshop, 2017.
[3] Kai-Yueh Chang, Tyng-Luh Liu, and Shang-Hong Lai. From
co-saliency to co-segmentation: An efﬁcient and fully unsu-
pervised energy minimization model. In CVPR, 2011.
[4] Hsin-I Chen, Yen-Yu Lin, and Bing-Yu Chen.
Co-
segmentation guided hough transform for robust feature
matching. TPAMI, 2015.
[5] Minsu Cho, Suha Kwak, Cordelia Schmid, and Jean Ponce.
Unsupervised object discovery and localization in the wild:
Part-based matching with bottom-up region proposals.
In
CVPR, 2015.
[6] Edo Collins, Radhakrishna Achanta, and Sabine S ¨usstrunk.
Deep feature factorization for concept discovery. In ECCV,
2018.
[7] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-
mantic segmentation via multi-task network cascades.
In
CVPR, 2016.
[8] Jifeng Dai, Ying Nian Wu, Jie Zhou, and Song-Chun Zhu.
Cosegmentation and cosketch by unsupervised learning. In
ICCV, 2013.
[9] Mark Everingham, Luc Van Gool, Christopher K.
I.
Williams, John Winn, and Andrew Zisserman. The pascal
visual object classes (VOC) challenge. IJCV, 2010.
[10] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-
Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clut-
ter: Bringing salient object detection to the foreground. In
ECCV, 2018.
[11] Ruochen Fan, Qibin Hou, Ming-Ming Cheng, Tai-Jiang Mu,
and Shi-Min Hu. S4Net: Single stage salient-instance seg-
mentation. In CVPR, 2019.
[12] H. Fu, D. Xu, B. Zhang, S. Lin, and R. Ward. Object-based
multiple foreground video co-segmentation via multi-state
selection graph. TIP, 2015.
[13] Junwei Han, Rong Quan, Dingwen Zhang, and Feiping Nie.
Robust object co-segmentation using background prior. TIP,
2018.
[14] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev,
Subhransu Maji, and Jitendra Malik. Semantic contours from
inverse detectors. In ICCV, 2011.
[15] Bharath Hariharan, Pablo Arbelaez, Ross Girshick, and Ji-
tendra Malik. Simultaneous detection and segmentation. In
ECCV, 2014.
[16] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.
Boundary-aware instance segmentation. In CVPR, 2017.
[17] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-
shick. Mask R-CNN. In ICCV, 2017.
[18] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Aug-
mented multiple instance regression for inferring object con-
tours in bounding boxes. TIP, 2014.
[19] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Ro-
bust image alignment with multiple feature descriptors and
matching-guided neighborhoods. In CVPR, 2015.

[20] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Weakly
supervised saliency detection with a category-driven map
generator. In BMVC, 2017.
[21] Kuang-Jui Hsu, Yen-Yu Lin, and Yung-Yu Chuang. Co-
attention CNNs for unsupervised object co-segmentation. In
IJCAI, 2018.
[22] Kuang-Jui Hsu, Chung-Chi Tsai, Yen-Yu Lin, Xiaoning
Qian, and Yung-Yu Chuang. Unsupervised CNN-based co-
saliency detection with graphical optimization.
In ECCV,
2018.
[23] Yuan-Ting Hu, Hong-Shuo Chen, Kexin Hui, Jia-Bin Huang,
and Alexander Schwing. SAIL-VOS: Semantic amodal in-
stance level video object segmentation - a synthetic dataset
and baselines. In CVPR, 2019.
[24] Yuan-Ting Hu, Jia-Bin Huang, and Alexander Schwing.
MaskRNN: Instance level video object segmentation.
In
NIPS, 2017.
[25] Yuan-Ting Hu and Yen-Yu Lin. Progressive feature match-
ing with alternate descriptor selection and correspondence
enrichment. In CVPR, 2016.
[26] Yan Huang, Wei Wang, and Liang Wang. Instance-aware im-
age and sentence matching with selective multimodal LSTM.
In CVPR, 2017.
[27] Koteswar Rao Jerripothula, Jianfei Cai, Jiangbo Lu, and Jun-
song Yuan. Object co-skeletonization with co-segmentation.
In CVPR, 2017.
[28] Koteswar Rao Jerripothula, Jianfei Cai, and Junsong Yuan.
Image co-segmentation via saliency co-fusion. TMM, 2016.
[29] Diederik Kingma and Jimmy Ba. ADAM: A method for
stochastic optimization. In ICLR, 2014.
[30] Shu Kong and Charless Fowlkes. Recurrent pixel embedding
for instance grouping. In CVPR, 2018.
[31] Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-
Hsuan Yang, and Jan Kautz. Context-aware synthesis and
placement of object instances. In NIPS, 2018.
[32] Guanbin Li, Yuan Xie, Liang Lin, and Yizhou Yu. Instance-
level salient object segmentation. In CVPR, 2017.
[33] Weihao Li, Omid Hosseini Jafari, and Carsten Rother. Deep
object co-segmentation. In ACCV, 2018.
[34] Xin Li, Fan Yang, Hong Cheng, Wei Liu, and Dinggang
Shen. Contour knowledge transfer for salient object detec-
tion. In ECCV, 2018.
[35] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
In CVPR, 2017.
[36] Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang,
Liang Lin, and Shuicheng Yan. Proposal-free network for
instance-level object segmentation. TPAMI, 2018.
[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir
Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva
Ramanan, C. Lawrence Zitnick, and Piotr Doll ´ar. Microsoft
COCO: Common objects in context. In ECCV, 2014.
[38] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
Path aggregation network for instance segmentation.
In
CVPR, 2018.
[39] Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu,
Houqiang Li, and Yan Lu. Afﬁnity derivation and graph
merge for instance segmentation. In ECCV, 2018.

8854

[59] Xiu-Shen Wei, Chen-Lin Zhang, Jianxin Wu, Chunhua Shen,
and Zhi-Hua Zhou. Unsupervised object discovery and co-
localization by deep descriptor transforming. PR, 2019.
[60] Tsun-Yi Yang, Jo-Han Hsu, Yen-Yu Lin, and Yung-Yu
Chuang. DeepCD: Learning deep complementary descrip-
tors for patch representations. In ICCV, 2017.
[61] Tsun-Yi Yang, Yen-Yu Lin, and Yung-Yu Chuang. Accumu-
lated stability voting: A robust descriptor from descriptors of
multiple scales. In CVPR, 2016.
[62] Zehuan Yuan, Tong Lu, and Yirui Wu. Deep-dense condi-
tional random ﬁelds for object co-segmentation. In IJCAI,
2017.
[63] Dingwen Zhang, Junwei Han, and Yu Zhang. Supervision by
fusion: Towards unsupervised learning of deep salient object
detector. In ICCV, 2017.
[64] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun.
Instance-
level segmentation for autonomous driving with deep
densely connected mrfs. In CVPR, 2016.
[65] Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu, and Jianbin
Jiao. Weakly supervised instance segmentation using class
peak response. In CVPR, 2018.

[40] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional models for semantic segmentation. In CVPR,
2015.
[41] Zhiming Luo, Akshaya Mishra, Andrew Achkar, Justin
Eichel, Shaozi Li, and Pierre-Marc Jodoin. Non-local deep
features for salient object detection. In CVPR, 2017.
[42] Armin Mustafa and Adrian Hilton. Semantically coherent
co-segmentation and reconstruction of dynamic scenes.
In
CVPR, 2017.
[43] David Novotny, Samuel Albanie, Diane Larlus, and Andrea
Vedaldi. Semi-convolutional operators for instance segmen-
tation. In ECCV, 2018.
[44] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T. Barron, Fer-
ran Marques, and Jitendra Malik. Multiscale combinatorial
grouping for image segmentation and object proposal gener-
ation. TPAMI, 2017.
[45] Rong Quan, Junwei Han, Dingwen Zhang, and Feiping Nie.
Object co-segmentation via graph optimized-ﬂexible mani-
fold ranking. In CVPR, 2016.
[46] Michael Rubinstein, Armand Joulin, Johannes Kopf, and Ce
Liu. Unsupervised joint object discovery and segmentation
in internet images. In CVPR, 2013.
[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Fei-Fei Li.
ImageNet large scale visual recognition chal-
lenge. IJCV, 2015.
[48] Tong Shen, Guosheng Lin, Lingqiao Liu, Chunhua Shen, and
Ian Reid. Weakly supervised semantic segmentation based
on co-segmentation. In BMVC, 2017.
[49] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015.
[50] Charles Sutton and Andrew McCallum. Piecewise training
for structured prediction. ML, 2009.
[51] Kevin Tang, Armand Joulin, Li-Jia Li, and Fei-Fei Li. Co-
localization in real-world images. In CVPR, 2014.
[52] Tatsunori Taniai, Sudipta N. Sinha, and Yoichi Sato. Joint re-
covery of dense correspondence and cosegmentation in two
images. In CVPR, 2016.
[53] Zhiqiang Tao, Hongfu Liu, Huazhu Fu, and Yun Fu.
Im-
age cosegmentation via saliency-guided constrained cluster-
ing with cosine similarity. In AAAI, 2017.
[54] Chung-Chi Tsai, Weizhi Li, Kuang-Jui Hsu, Xiaoning Qian,
and Yen-Yu Lin.
Image co-saliency detection and co-
segmentation via progressive joint optimization. TIP, 2018.
[55] Andrea Vedaldi and Karel Lenc. MatConvNet – Convolu-
tional neural networks for MATLAB. In ACMMM, 2015.
[56] Chuan Ping Wang, Hua Zhang, Liang Yang, Xiaochun Cao,
and Hongkai Xiong. Multiple semantic matching on aug-
mented n-partite graph for object co-segmentation. TIP,
2017.
[57] Wenguan Wang, Jianbing Shen, Hanqiu Sun, and Ling Shao.
Video co-saliency guided co-segmentation. TCSVT, 2018.
[58] Xiu-Shen Wei, Chen-Lin Zhang, Yao Li, Chen-Wei Xie,
Jianxin Wu, Chunhua Shen, and Zhi-Hua Zhou. Deep de-
scriptor transforming for image co-localization.
In IJCAI,
2017.

8855

DeepSDF: Learning Continuous Signed Distance Functions
for Shape Representation

Jeong Joon Park1,3

Peter Florence 2,3

Julian Straub3

Richard Newcombe3

Steven Lovegrove3

1University of Washington

2Massachusetts Institute of Technology

3Facebook Reality Labs

Figure 1: DeepSDF represents signed distance functions (SDFs) of shapes via latent code-conditioned feed-forward decoder networks.
Above images are raycast renderings of DeepSDF interpolating between two shapes in the learned shape latent space. Best viewed digitally.

Abstract

1. Introduction

Computer graphics, 3D computer vision and robotics
communities have produced multiple approaches to rep-
resenting 3D geometry for rendering and reconstruction.
These provide trade-offs across ﬁdelity, efﬁciency and com-
pression capabilities. In this work, we introduce DeepSDF,
a learned continuous Signed Distance Function (SDF) rep-
resentation of a class of shapes that enables high qual-
ity shape representation, interpolation and completion from
partial and noisy 3D input data. DeepSDF, like its clas-
sical counterpart, represents a shape’s surface by a con-
tinuous volumetric ﬁeld: the magnitude of a point in the
ﬁeld represents the distance to the surface boundary and the
sign indicates whether the region is inside (-) or outside (+)
of the shape, hence our representation implicitly encodes a
shape’s boundary as the zero-level-set of the learned func-
tion while explicitly representing the classiﬁcation of space
as being part of the shapes’ interior or not. While classical
SDF’s both in analytical or discretized voxel form typically
represent the surface of a single shape, DeepSDF can repre-
sent an entire class of shapes. Furthermore, we show state-
of-the-art performance for learned 3D shape representation
and completion while reducing the model size by an order
of magnitude compared with previous work.

Work performed while Park and Florence were interns at Facebook.

Deep convolutional networks which are a mainstay of
image-based approaches grow quickly in space and time
complexity when directly generalized to the 3rd spatial di-
mension, and more classical and compact surface repre-
sentations such as triangle or quad meshes pose problems
in training since we may need to deal with an unknown
number of vertices and arbitrary topology. These chal-
lenges have limited the quality, ﬂexibility and ﬁdelity of
deep learning approaches when attempting to either input
3D data for processing or produce 3D inferences for object
segmentation and reconstruction.

In this work, we present a novel representation and ap-
proach for generative 3D modeling that is efﬁcient, expres-
sive, and fully continuous. Our approach uses the concept
of a SDF, but unlike common surface reconstruction tech-
niques which discretize this SDF into a regular grid for eval-
uation and measurement denoising [14], we instead learn a
generative model to produce such a continuous ﬁeld.

The proposed continuous representation may be intu-
itively understood as a learned shape-conditioned classiﬁer
for which the decision boundary is the surface of the shape
itself, as shown in Fig. 2. Our approach shares the genera-
tive aspect of other works seeking to map a latent space to
a distribution of complex shapes in 3D [52], but critically
differs in the central representation. While the notion of an

1165

Decision 
boundary
of implicit 
surface

(a)

(b)

(c)

Figure 2: Our DeepSDF representation applied to the Stanford
Bunny: (a) depiction of the underlying implicit surface SDF = 0
trained on sampled points inside SDF < 0 and outside SDF > 0
the surface, (b) 2D cross-section of the signed distance ﬁeld, (c)
rendered 3D surface recovered from SDF = 0. Note that (b) and
(c) are recovered via DeepSDF.

implicit surface deﬁned as a SDF is widely known in the
computer vision and graphics communities, to our knowl-
edge no prior works have attempted to directly learn contin-
uous, generalizable 3D generative models of SDFs.
Our contributions include: (i) the formulation of gen-
erative shape-conditioned 3D modeling with a continuous
implicit surface, (ii) a learning method for 3D shapes based
on a probabilistic auto-decoder, and (iii) the demonstration
and application of this formulation to shape modeling and
completion. Our models produce high quality continuous
surfaces with complex topologies, and obtain state-of-the-
art results in quantitative comparisons for shape reconstruc-
tion and completion. As an example of the effectiveness
of our method, our models use only 7.4 MB (megabytes)
of memory to represent entire classes of shapes (for exam-
ple, thousands of 3D chair models) – this is, for example,
less than half the memory footprint (16.8 MB) of a single
uncompressed 5123 3D bitmap.

2. Related Work

We review three main areas of related work: 3D rep-
resentations for shape learning (Sec. 2.1), techniques for
learning generative models (Sec. 2.2), and shape comple-
tion (Sec. 2.3).

2.1. Representations for 3D Shape Learning

Representations for data-driven 3D learning approaches
can be largely classiﬁed into three categories: point-based,
mesh-based, and voxel-based methods. While some appli-
cations such as 3D-point-cloud-based object classiﬁcation
are well suited to these representations, we address their
limitations in expressing continuous surfaces with complex
topologies.
Point-based. A point cloud is a lightweight 3D representa-

tion that closely matches the raw data that many sensors (i.e.
LiDARs, depth cameras) provide, and hence is a natural ﬁt
for applying 3D learning. PointNet [36], for example, uses
max-pool operations to extract global shape features, and
the technique is widely used as an encoder for point genera-
tion networks [55, 1]. There is a sizable list of related works
for learning on point clouds [37, 51, 56]. A primary limi-
tation, however, of learning with point clouds is that they
do not describe topology and are not suitable for producing
watertight surfaces.

Mesh-based. Various approaches represent classes of sim-
ilarly shaped objects, such as morphable human body parts,
with predeﬁned template meshes and some of these models
demonstrate high ﬁdelity shape generation results [2, 31].
Other recent works [3] use poly-cube mapping [48] for
shape optimization. While the use of template meshes is
convenient and naturally provides 3D correspondences, it
can only model shapes with ﬁxed mesh topology.

Other mesh-based methods use existing [45, 33] or
learned [19, 20] parameterization techniques to describe 3D
surfaces by morphing 2D planes. The quality of such repre-
sentations depends on parameterization algorithms that are
often sensitive to input mesh quality and cutting strategies.
To address this, recent data-driven approaches [55, 19] learn
the parameterization task with deep networks. They report,
however, that (a) multiple planes are required to describe
complex topologies but (b) the generated surface patches
are not stitched, i.e. the produced shape is not closed. To
generate a closed mesh, sphere parameterization may be
used [19, 20], but the resulting shape is limited to the topo-
logical sphere. Other works related to learning on meshes
propose to use new convolution and pooling operations for
meshes [16, 50] or general graphs [8].

Voxel-based. Voxels, which non-parametrically describe
volumes with 3D grids of values, are perhaps the most natu-
ral extension into the 3D domain of the well-known learning
paradigms (i.e., convolution) that have excelled in the 2D
image domain. The most straightforward variant of voxel-
based learning is to use a dense occupancy grid (occupied /
not occupied). Due to the cubically growing compute and
memory requirements, however, current methods are only
able to handle low resolutions (1283 or below). As such,
voxel-based approaches do not preserve ﬁne shape details
[54, 13], and additionally voxels visually appear signiﬁ-
cantly different than high-ﬁdelity shapes, since when ren-
dered their normals are not smooth. Octree-based methods
[49, 41, 22] alleviate the compute and memory limitations
of dense voxel methods, extending for example the ability to
learn at up to 5123 resolution [49], but even this resolution
is far from producing shapes that are visually compelling.

Aside from occupancy grids, and more closely related to
our approach, it is also possible to use a 3D grid of vox-
els to represent a signed distance function. This inherits

2166

from the success of fusion approaches that use a truncated
SDF (TSDF), pioneered in [14, 35], to combine noisy depth
maps into a single 3D model. Voxel-based SDF represen-
tations have been extensively used for 3D shape learning
[57, 15, 46], but their use of discrete voxels is expensive
in memory. As a result, these approaches generally present
low resolution shapes. Wavelet transform-based methods
[27] and dimensionality reduction techniques [27] for dis-
tance ﬁelds were reported, but they encode the SDF volume
of each individual scene rather than a dataset of shapes.
Recently, concurrently to our work, binary implicit sur-
face representations were used by [12, 34], where they train
deep networks across classes of shapes to classify 3D points
as inside or outside of a shape. Note that the binary occu-
pancy function is a special case of SDF, considering only
‘sign’ of SDF values. As DeepSDF models metric signed
distance to the surface, it can be used to raycast against sur-
faces and compute surface normals with its gradients.

2.2. Representation Learning Techniques

Modern representation learning techniques aim at auto-
matically discovering a set of features that compactly but
expressively describe data. For a more extensive review of
the ﬁeld, we refer to Bengio et al. [4].
Generative Adversial Networks. GANs [18] and their
variants [11, 39, 26, 28] learn deep embeddings of target
data, from which realistic images are sampled, by training
discriminators adversarially against generators. In the 3D
domain, Wu et al. [52] trains a GAN to generate objects in a
voxel form, while Hamu et al. [20] uses multiple parameter-
ization planes to generate shapes of topological spheres. On
the downside, training for GANs is known to be unstable.
Auto-encoders. The ability of auto-encoders as a represen-
tation learning tool has been evidenced by the vast variety of
3D shape learning works in the literature [15, 46, 2, 19, 53]
who adopt auto-encoders for representation learning. Re-
cent 3D vision works [5, 2, 31] often adopt a variational
auto-encoder (VAE) learning scheme, in which bottleneck
features are perturbed with Gaussian noise to encourage
smooth and complete latent spaces. The regularization on
the latent vectors enables exploring the embedding space
with gradient descent or random sampling.
Optimizing Latent Vectors. Instead of using the full auto-
encoder, an alternative is to learn compact data represen-
tations by training decoder-only networks. This idea goes
back to at least the work of Tan et al. [47] which simulta-
neously optimizes the latent vectors assigned to each data
point and the decoder weights through back-propagation.
For inference, an optimal latent vector is searched to match
the new observation with ﬁxed decoder parameters. Similar
approaches have been extensively studied in [40, 7, 38], for
applications including noise reduction, missing measure-
ment completions, and fault detections. Recent approaches

[6, 17] extend the technique by applying deep architectures.
Throughout the paper we refer to this class of networks as
auto-decoders, for they are trained with self-reconstruction
loss on decoder-only architectures.

2.3. Shape Completion

3D shape completion related works aim to infer unseen
parts of the original shape given sparse or partial input ob-
servations. This task is analogous to image-inpainting in 2D
computer vision.
Classical surface reconstruction methods complete a
point cloud into a dense surface by ﬁtting radial basis func-
tion (RBF) [9] to approximate implicit surface functions, or
by casting the reconstruction from oriented point clouds as
a Poisson problem [29]. These methods only model a single
shape rather than a dataset.
Various recent methods use data-driven approaches for
the 3D completion task. Most of these methods adopt
encoder-decoder architectures to reduce partial inputs of oc-
cupancy voxels [54], discrete SDF voxels [15], depth maps
[42], RGB images [13, 53] or point clouds [46] into a la-
tent vector and subsequently generate a prediction of full
volumetric shape based on learned priors.

3. Modeling SDFs with Neural Networks

In this section we present DeepSDF, our continuous
shape learning approach. We describe modeling shapes
as the zero iso-surface decision boundaries of feed-forward
networks trained to represent SDFs. A signed distance func-
tion is a continuous function that, for a given spatial point,
outputs the point’s distance to the closest surface, whose
sign encodes whether the point is inside (negative) or out-
side (positive) of the watertight surface:

SDF (x) = s : x ∈ R3 , s ∈ R .

(1)

The underlying surface is implicitly represented by the iso-
surface of SDF (·) = 0. A view of this implicit surface can
be rendered through raycasting or rasterization of a mesh
obtained with, for example, Marching Cubes [32].
Our key idea is to directly regress the continuous SDF
from point samples using deep neural networks. The result-
ing trained network is able to predict the SDF value of a
given query position, from which we can extract the zero
level-set surface by evaluating spatial samples. Such sur-
face representation can be intuitively understood as a spa-
tial classiﬁer for which the decision boundary is the surface
of the shape itself (Fig. 2). As a universal function approxi-
mator [24], deep feed-forward networks in theory can learn
continuous SDFs with arbitrary precision. Yet, the preci-
sion of the approximation in practice is limited by the ﬁnite
number of point samples that guide the decision boundaries
and the ﬁnite capacity of the network due to restricted com-
pute power.

3167

(a) Single Shape DeepSDF

(b) Coded Shape DeepSDF

Figure 3: DeepSDF network outputs SDF value at a 3D query lo-
cation. While (a) the network can memorize a single shape, (b)
conditioning the network with a code vector allows DeepSDF to
model a large space of shapes, where the shape information is con-
tained in the code vector that is concatenated with the query point.

The most direct application of this approach is to train a
single deep network for a given target shape as depicted in
Fig. 3a. Given a target shape, we prepare a set of pairs X
composed of 3D point samples and their SDF values:

X := {(x, s) : SDF (x) = s} .

(2)

We train the parameters θ of a multi-layer fully-connected
neural network fθ on the training set S to make fθ a good
approximator of the given SDF in the target domain Ω:

fθ (x) ≈ SDF (x), ∀x ∈ Ω .

(3)

The training is done by minimizing the sum over losses
between the predicted and real SDF values of points in X
under the following L1 loss function:

L(fθ (x), s) = | clamp(fθ (x), δ) − clamp(s, δ) |,

(4)

where clamp(x, δ) := min(δ, max(−δ, x)) introduces the

parameter δ to control the distance from the surface over
which we expect to maintain a metric SDF. Larger values
of δ allow for fast ray-tracing since each sample gives in-
formation of safe step sizes [23]. Smaller δ can be used
to concentrate network capacity on details near the surface.
We used δ = 0.1 in practice. (see supplementary for details)
Once trained, the surface is implicitly represented as the
zero iso-surface of fθ (x), which can be visualized through
raycasting or marching cubes. Another nice property of this
approach is that accurate normals can be analytically com-
puted by calculating the spatial derivative ∂ fθ (x)/∂x via
back-propogation through the network.

4. Learning the Latent Space of Shapes

Training a speciﬁc neural network for each shape is nei-
ther feasible nor very useful. Instead, we want a model that
can represent a wide variety of shapes, discover their com-
mon properties, and embed them in a low dimensional latent
space. To this end, we introduce a latent vector z , which can
be thought of as encoding the desired shape, as a second in-
put to the neural network as depicted in Fig. 3b. Concep-
tually, we map this latent vector to a 3D shape represented

(a) Auto-encoder

(b) Auto-decoder

Figure 4: Different from an auto-encoder whose latent code is
produced by the encoder, an auto-decoder directly accepts a la-
tent vector as an input. A randomly initialized latent vector is
assigned to each data point in the beginning of training, and the la-
tent vectors are optimized along with the decoder weights through
standard backpropagation. During inference, decoder weights are
ﬁxed, and an optimal latent vector is estimated.

by a continuous SDF. Formally, for some shape indexed by
i, fθ is now a function of a latent code zi and a query 3D
location x, which outputs the shape’s approximate SDF:

fθ (zi , x) ≈ SDF i (x).

(5)

By conditioning the network output on a latent vector, this
formulation allows modeling multiple SDFs with a single
neural network. Given the decoding model fθ , the contin-
uous surface associated with a latent vector z is similarly
represented with the zero iso-surface of fθ (z , x), and the
shape can again be discretized for visualization by, for ex-
ample, raycasting or Marching Cubes.
Throughout the paper we use the coded shape DeepSDF
model of Fig. 3b whose decoder is a feed-forward network
composed of eight fully connected layers, each of them ap-
plied with dropouts. All internal layers are 512-dimensional
and have ReLU non-linearities. The output non-linearity
regressing the SDF value is tanh. We found training with
batch-normalization [25] to be unstable and applied the
weight-normalization technique instead [43]. For training,
we use the Adam optimizer [30].
In the next section we explain training the decoding
model fθ (z , x) and introduce the ‘auto-decoder’ formula-
tion for encoder-less training of shape-coded DeepSDF.

4.1. Motivating Encoder(cid:173)less Learning

Auto-encoders
and encoder-decoder networks
are
widely used for representation learning as their bottleneck
features tend to form natural latent variable representations.
Recently, in applications such as modeling depth maps
[5], faces [2], and body shapes [31] a full encoder-decoder
network is trained but only the decoder is retained for infer-
ence, where they search for an optimal latent vector given
some input observation. Since the trained encoder is unused
at test time, it is unclear whether (1) training encoder is an
effective use of computational resources and (2) it is neces-

4168

sary for researchers to design encoders for various 3D input
representations (e.g. points, meshes, octrees, etc).
This motivates us to use an auto-decoder for learning a
shape embedding without an encoder as depicted in Fig. 4b.
We show that learning continuous SDFs with auto-decoder
leads to high quality 3D generative models. Further, we
develop a probabilistic formulation for training and test-
ing the auto-decoder that naturally introduces latent space
regularization for improved generalization. To the best of
our knowledge, this work is the ﬁrst to introduce the auto-
decoder learning method to the 3D learning community.

4.2. Auto(cid:173)decoder(cid:173)based DeepSDF Formulation

To derive the auto-decoder-based shape-coded DeepSDF
formulation we adopt a probabilistic perspective. Given a
dataset of N shapes represented with signed distance func-
tion SDF iN
i=1 , we prepare a set of K point samples and
their signed distance values:

Xi = {(xj , sj ) : sj = SDF i (xj )} .

(6)

For an auto-decoder, as there is no encoder, each latent
code zi is paired with training shape Xi . The posterior over
shape code zi given the shape SDF samples Xi can be de-
composed as:

pθ (zi |Xi ) = p(zi ) Q(xj ,sj )∈Xi

pθ (sj |zi ; xj ) ,

(7)

where θ parameterizes the SDF likelihood.
In the latent
shape-code space, we assume the prior distribution over
codes p(zi ) to be a zero-mean multivariate-Gaussian with
a spherical covariance σ2 I . This prior encapsulates the no-
tion that the shape codes should be concentrated and we
empirically found it was needed to infer a compact shape
manifold and to help converge to good solutions.
In the auto-decoder-based DeepSDF formulation we ex-
press the SDF likelihood via a deep feed-forward network
fθ (zi , xj ) and, without loss of generality, assume that the
likelihood takes the form:

pθ (sj |zi ; xj ) = exp(−L(fθ (zi , xj ), sj )) .

(8)

The SDF prediction ˜sj = fθ (zi , xj ) is represented using a
fully-connected network. L(˜sj , sj ) is a loss function penal-
izing the deviation of the network prediction from the actual
SDF value sj . One example for the cost function is the stan-
dard L2 loss function which amounts to assuming Gaussian
noise on the SDF values. In practice we use the clamped L1
cost from Eq. 4 for reasons outlined previously.
At training time we maximize the joint log posterior over
all training shapes with respect to the individual shape codes
i=1 and the network parameters θ :

{zi }N

arg min

θ,{zi }N

i=1

N

X

i=1

K


X


j=1

L(fθ (zi , xj ), sj ) +

1
σ2 ||zi ||2

2


 .

(9)

Figure 5: Compared to car shapes memorized using OGN [49]
(right), our models (left) preserve details and render visually pleas-
ing results as DeepSDF provides oriented surace normals.

At inference time, after training and ﬁxing θ , a shape
code zi for shape Xi can be estimated via Maximum-a-
Posterior (MAP) estimation as:

ˆz = arg min

z X

(xj ,sj )∈X

L(fθ (z , xj ), sj ) +

1
σ2 ||z ||2

2 . (10)

Crucially, this formulation is valid for SDF samples X
of arbitrary size and distribution because the gradient of the
loss with respect to z can be computed separately for each
SDF sample. This implies that DeepSDF can handle any
form of partial observations such as depth maps. This is
a major advantage over the auto-encoder framework whose
encoder expects a test input similar to the training data, e.g.
shape completion networks of [15, 56] require preparing
training data of partial shapes.
To incorporate the latent shape code, we stack the code
vector and the sample location as depicted in Fig. 3b and
feed it into the same fully-connected NN described previ-
ously at the input layer and additionally at the 4th layer. We
again use the Adam optimizer [30]. The latent vector z is
initialized randomly from N (0, 0.012 ).
Note that while both VAE and the proposed auto-decoder
formulation share the zero-mean Gaussian prior on the la-
tent codes, we found that the the stochastic nature of the
VAE optimization did not lead to good training results.

5. Data Preparation

To train our continuous SDF model, we prepare the SDF
samples X (Eq. 2) for each mesh, which consists of 3D
points and their SDF values. While SDF can be computed
through a distance transform for any watertight shapes from
real or synthetic data, we train with synthetic objects, (e.g.
ShapeNet [10]), for which we are provided complete 3D
shape meshes. To prepare data, we start by normalizing
each mesh to a unit sphere and sampling 500,000 spatial
points x’s: we sample more aggressively near the surface
of the object as we want to capture a more detailed SDF
near the surface. For an ideal oriented watertight mesh,
computing the signed distance value of x would only in-
volve ﬁnding the closest triangle, but we ﬁnd that human
designed meshes are commonly not watertight and con-
tain undesired internal structures. To obtain the shell of a

5169

Complex
topologies

Closed
surfaces

Surface
normals

Model
size (GB)

Inf.
time (s)

X

X

X

X

0.42
0.54
0.015

-
0.32
0.01

Method

Type

3D-EPN [15]
OGN [49]
AtlasNet
-Sphere [19]
AtlasNet
-25 [19]
DeepSDF
(ours)

Voxel SDF
Octree
Parametric
mesh
Parametric
mesh
Continuous
SDF

Discretization
323 voxels
2563 voxels
1 patch

25 patches

none

X

X

X

X

Eval.
tasks

C
K
K, U

0.172

0.32

K, U

X

X

0.0074

9.72

K, U, C

Table 1: Overview of the benchmarked methods. AtlasNet-Sphere can only describe topological-spheres, voxel/octree occupancy methods
(i.e. OGN) only provide 8 directions for normals, and AtlasNet does not provide oriented normals. Our tasks evaluated are: (K) representing
known shapes, (U) representing unknown shapes, and (C) shape completion.

mesh with proper orientation, we set up equally spaced vir-
tual cameras around the object, and densely sample surface
points, denoted Ps , with surface normals oriented towards
the camera. Double sided triangles visible from both orien-
tations (indicating that the shape is not closed) cause prob-
lems in this case, so we discard mesh objects containing too
many of such faces. Then, for each x, we ﬁnd the closest
point in Ps , from which the SDF (x) can be computed. We
refer readers to supplementary material for further details.

6. Results

We conduct a number of experiments to show the repre-
sentational power of DeepSDF, both in terms of its ability
to describe geometric details and its generalization capabil-
ity to learn a desirable shape embedding space. Largely, we
propose four main experiments designed to test its ability to
1) represent training data, 2) use learned feature representa-
tion to reconstruct unseen shapes, 3) apply shape priors to
complete partial shapes, and 4) learn smooth and complete
shape embedding space from which we can sample new
shapes. For all experiments we use the popular ShapeNet
[10] dataset.
We select a representative set of 3D learning approaches
to comparatively evaluate aforementioned criteria: a recent
octree-based method (OGN) [49], a mesh-based method
(AtlasNet) [19], and a volumetric SDF-based shape comple-
tion method (3D-EPN) [15] (Table 1). These works show
state-of-the-art performance in their respective representa-
tions and tasks, so we omit comparisons with the works
that have already been compared: e.g. OGN’s octree model
outperforms regular voxel approaches, while AtlasNet com-
pares itself with various points, mesh, or voxel based meth-
ods and 3D-EPN with various completion methods.

6.1. Representing Known 3D Shapes

First, we evaluate the capacity of the model to represent
known shapes, i.e. shapes that were in the training set, from
only a restricted-size latent code — testing the limit of ex-
pressive capability of the representations.

Method \metric

OGN
AtlasNet-Sph.
AtlasNet-25
DeepSDF

CD,
mean

0.167
0.210
0.157
0.084

CD,
median

0.127
0.185
0.140
0.058

EMD,
mean

0.043
0.046
0.060
0.043

EMD,
median

0.042
0.045
0.060
0.042

Table 2: Comparison for representing known shapes (K) for cars
trained on ShapeNet. CD = Chamfer Distance (30, 000 points)
multiplied by 103 , EMD = Earth Mover’s Distance (500 points).

Quantitative comparison in Table 2 shows that the pro-
posed DeepSDF signiﬁcantly beats OGN and AtlasNet in
Chamfer distance against the true shape computed with a
large number of points (30,000). The difference in earth
mover distance (EMD) is smaller because 500 points do not
well capture the additional precision. Figure 5 shows a qual-
itative comparison of DeepSDF to OGN.

6.2. Representing Test 3D Shapes (auto(cid:173)encoding)

For encoding unknown shapes, i.e. shapes in the held-out
test set, DeepSDF again signiﬁcantly outperforms AtlasNet
on a wide variety of shape classes and metrics as shown
in Table 3. Note that AtlasNet performs reasonably well
at classes of shapes that have mostly consistent topology
without holes (like planes) but struggles more on classes
that commonly have holes, like chairs. This is shown in
Fig. 6 where AtlasNet fails to represent the ﬁne detail of the
back of the chair. Figure 7 shows more examples of detailed
reconstructions on test data from DeepSDF as well as two
example failure cases.

6.3. Shape Completion

A major advantage of the proposed DeepSDF approach
for representation learning is that inference can be per-
formed from an arbitrary number of SDF samples. In the
DeepSDF framework, shape completion amounts to solving
for the shape code that best explains a partial shape obser-
vation via Eq. 10. Given the shape-code a complete shape
can be rendered using the priors encoded in the decoder.

6170

(a) Ground-truth

(b) Our Result

(c) [19]-25 patch

(d) [19]-sphere

(e) Our Result

(f) [19]-25 patch

Figure 6: Reconstruction comparison between DeepSDF and AtlasNet [19] (with 25-plane and sphere parameterization) for test shapes.
Note that AtlasNet fails to capture the ﬁne details of the chair, and that (f) shows holes on the surface of sofa and the plane.

Figure 7: Reconstruction of test shapes. From left to right alternating: ground truth shape and our reconstruction. The two right most
columns show failure modes of DeepSDF. These failures are likely due to lack of training data and failure of minimization convergence.

CD, mean
AtlasNet-Sph.
AtlasNet-25
DeepSDF

CD, median
AtlasNet-Sph.
AtlasNet-25
DeepSDF

EMD, mean
AtlasNet-Sph.
AtlasNet-25
DeepSDF

Mesh acc., mean
AtlasNet-Sph.
AtlasNet-25
DeepSDF

chair
0.752
0.368
0.204

0.511
0.276
0.072

0.071
0.064
0.049

0.033
0.018
0.009

plane
0.188
0.216
0.143

0.079
0.065
0.036

0.038
0.041
0.033

0.013
0.013
0.004

table
0.725
0.328
0.553

0.389
0.195
0.068

0.060
0.073
0.050

0.032
0.014
0.012

lamp
2.381
1.182
0.832

2.180
0.993
0.219

0.085
0.062
0.059

0.054
0.042
0.013

sofa
0.445
0.411
0.132

0.330
0.311
0.088

0.050
0.063
0.047

0.017
0.017
0.004

Table 3: Comparison for representing unknown shapes (U) for
various classes of ShapeNet. Mesh accuracy as deﬁned in [44]
is the minimum distance d such that 90% of generated points are
within d of the ground truth mesh. Lower is better for all metrics.

We test our completion scheme using single view depth
observations which is a common use-case and maps well
to our architecture without modiﬁcation. Note that we cur-
rently require the depth observations in the canonical shape
frame of reference.

To generate SDF point samples from the depth image ob-
servation, we sample two points for each depth observation,
each of them located η distance away from the measured

Method
\Metric

chair
3D-EPN
DeepSDF
plane
3D-EPN
DeepSDF
sofa
3D-EPN
DeepSDF

lower is better
CD,
CD,
med. mean

EMD

Mesh
acc.

higher is better
Mesh
Cos
comp.
sim.

2.25
1.28

1.63
0.37

2.03
0.82

2.83
2.11

2.19
1.16

2.18
1.59

0.084
0.071

0.059
0.049

0.209
0.500

0.752
0.766

0.063
0.049

0.040
0.032

0.165
0.722

0.710
0.823

0.071
0.059

0.049
0.041

0.254
0.541

0.742
0.810

Table 4: Comparison for shape completion (C) from partial range
scans of unknown shapes from ShapeNet.

surface point (along surface normal estimate). With small
η we approximate the signed distance value of those points
to be η and −η , respectively. We solve for Eq. 10 with
loss function of Eq. 4 using clamp value of η . Additionally,
we incorporate free-space observations, (i.e. empty-space
between surface and camera), by sampling points along
the freespace-direction and enforce larger-than-zero con-
straints. The freespace loss is |fθ (z , xj )| if fθ (z , xj ) < 0
and 0 otherwise.
Given the SDF point samples and empty space points,
we similarly optimize the latent vector using MAP estima-
tion. Tab. 4 and Figs. (8, 9) respectively shows quantitative
and qualitative shape completion results. Compared to one
of the most recent completion approaches [15] using volu-

7171

(a) Input Depth

(b) Completion (ours)

(c) Second View (ours)

(d) Ground truth

(e) 3D-EPN

Figure 8: For a given depth image visualized as a green point cloud, we show a comparison of shape completions from our DeepSDF
approach against the true shape and 3D-EPN.

(a) Noisy Input Point Cloud

(b) Shape Completion

Figure 9: Demonstration of DeepSDF shape completion from a
partial noisy point cloud. Input here is generated by perturbing the
3D point cloud positions generated by the ground truth depth map
by 1.5% of the plane length. We provide a comprehensive analysis
of robustness to noise in the supplementary material.

metric shape representation, our continuous SDF approach
produces more visually pleasing and accurate shape recon-
structions. While a few recent shape completion methods
were presented [21, 53], we could not ﬁnd the code to run
the comparisons, and their underlying 3D representation is
voxel grid which we extensively compare against.

6.4. Latent Space Shape Interpolation

To show that our learned shape embedding is complete
and continuous, we render the results of the decoder when
a pair of shapes are interpolated in the latent vector space
(Fig. 1). The results suggests that the embedded continuous
SDF’s are of meaningful shapes and that our representation
extracts common interpretable shape features, such as the
arms of a chair, that interpolate linearly in the latent space.

7. Conclusion & Future Work

DeepSDF signiﬁcantly outperforms
the applicable
benchmarked methods across shape representation and
completion tasks and simultaneously addresses the goals
of representing complex topologies, closed surfaces, while
providing high quality surface normals of the shape. How-
ever, while point-wise forward sampling of a shape’s SDF is
efﬁcient, shape completion (auto-decoding) takes consider-
ably more time during inference due to the need for explicit
optimization over the latent vector. We look to increase per-
formance by replacing ADAM optimization with more ef-
ﬁcient Gauss-Newton or similar methods that make use of
the analytic derivatives of the model.

DeepSDF models enable representation of more com-
plex shapes without discretization errors with signiﬁcantly
less memory than previous state-of-the-art results as shown
in Table 1, demonstrating an exciting route ahead for 3D
shape learning. The clear ability to produce quality latent
shape space interpolation opens the door to reconstruction
algorithms operating over scenes built up of such efﬁcient
encodings. However, DeepSDF currently assumes models
are in a canonical pose and as such completion in-the-wild
requires explicit optimization over a SE (3) transformation
space increasing inference time. Finally, to represent the
true space-of-possible-scenes including dynamics and tex-
tures in a single embedding remains a major challenge, one
which we continue to explore.

8172

References

[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas.
Learning representations and generative models for 3d point
clouds. 2018.
[2] T. Bagautdinov, C. Wu, J. Saragih, P. Fua, and Y. Sheikh.
Modeling facial geometry using compositional vaes. 1:1.
[3] P. Baqu ´e, E. Remelli, F. Fleuret, and P. Fua. Geodesic
convolutional
shape optimization.
arXiv preprint
arXiv:1802.04016, 2018.
[4] Y. Bengio, A. Courville, and P. Vincent.
Representa-
tion learning: A review and new perspectives. TPAMI,
35(8):1798–1828, 2013.
[5] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and
A. J. Davison. Codeslam-learning a compact, optimis-
able representation for dense visual slam. arXiv preprint
arXiv:1804.00874, 2018.
[6] P. Bojanowski, A. Joulin, D. Lopez-Pas, and A. Szlam. Op-
timizing the latent space of generative networks.
In J. Dy
and A. Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 600–609. PMLR, 10–
15 Jul 2018.
[7] M. Bouakkaz and M.-F. Harkat. Combined input training and
radial basis function neural networks based nonlinear princi-
pal components analysis model applied for process monitor-
ing. In IJCCI, pages 483–492, 2012.
[8] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral
networks and locally connected networks on graphs. arXiv
preprint arXiv:1312.6203, 2013.
[9] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell, W. R.
Fright, B. C. McCallum, and T. R. Evans. Reconstruction
and representation of 3d objects with radial basis functions.
In SIGGRAPH, pages 67–76. ACM, 2001.
[10] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015.
[11] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. Infogan: Interpretable representation learning
by information maximizing generative adversarial nets.
In
NIPS, pages 2172–2180, 2016.
[12] Z. Chen and H. Zhang. Learning implicit ﬁelds for generative
shape modeling. arXiv preprint arXiv:1812.02822, 2018.
[13] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d-
r2n2: A uniﬁed approach for single and multi-view 3d object
reconstruction. In ECCV, pages 628–644. Springer, 2016.
[14] B. Curless and M. Levoy. A volumetric method for building
complex models from range images. In SIGGRAPH, pages
303–312. ACM, 1996.
[15] A. Dai, C. Ruizhongtai Qi, and M. Niessner. Shape comple-
tion using 3d-encoder-predictor cnns and shape synthesis. In
CVPR, pages 5868–5877, 2017.
[16] M. Defferrard, X. Bresson, and P. Vandergheynst. Convolu-
tional neural networks on graphs with fast localized spectral
ﬁltering. In NIPS, pages 3844–3852, 2016.
[17] J. Fan and J. Cheng. Matrix completion by deep matrix fac-
torization. Neural Networks, 98:34–41, 2018.

[18] I. Goodfellow,
J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In NIPS, pages 2672–2680, 2014.
[19] T. Groueix, M. Fisher, V. G. Kim, B. C. Russell, and
M. Aubry. Atlasnet: A papier-m\ˆ ach\’e approach to learn-
ing 3d surface generation. arXiv preprint arXiv:1802.05384,
2018.
[20] H. B. Hamu, H. Maron, I. Kezurer, G. Avineri, and Y. Lip-
man. Multi-chart generative surface modeling.
arXiv
preprint arXiv:1806.02143, 2018.
[21] X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu. High-
resolution shape completion using deep neural networks for
global structure and local geometry inference.
[22] C. H ¨ane, S. Tulsiani, and J. Malik. Hierarchical surface pre-
diction for 3d object reconstruction.
In 3D Vision (3DV),
2017 International Conference on, pages 412–420. IEEE,
2017.
[23] J. C. Hart. Sphere tracing: A geometric method for the an-
tialiased ray tracing of implicit surfaces. The Visual Com-
puter, 12(10):527–545, 1996.
[24] K. Hornik, M. Stinchcombe, and H. White. Multilayer feed-
forward networks are universal approximators. Neural net-
works, 2(5):359–366, 1989.
[25] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
arXiv preprint arXiv:1502.03167, 2015.
[26] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
Image-
to-image translation with conditional adversarial networks.
arXiv preprint, 2017.
[27] M. W. Jones. Distance ﬁeld compression. Journal of WSCG,
12(2):199–204, 2004.
[28] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of gans for improved quality, stability, and variation.
arXiv preprint arXiv:1710.10196, 2017.
[29] M. Kazhdan and H. Hoppe. Screened poisson surface recon-
struction. ACM TOG, 32(3):29, 2013.
[30] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
[31] O. Litany, A. Bronstein, M. Bronstein, and A. Makadia. De-
formable shape completion with graph convolutional autoen-
coders. CVPR, 2017.
[32] W. E. Lorensen and H. E. Cline. Marching cubes: A high
resolution 3d surface construction algorithm. In SIGGRAPH,
volume 21, pages 163–169. ACM, 1987.
[33] H. Maron, M. Galun, N. Aigerman, M. Trope, N. Dym,
E. Yumer, V. G. Kim, and Y. Lipman. Convolutional neu-
ral networks on surfaces via seamless toric covers. 2017.
[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and
A. Geiger. Occupancy networks: Learning 3d reconstruction
in function space. arXiv preprint arXiv:1812.03828, 2018.
[35] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohi, J. Shotton, S. Hodges, and
A. Fitzgibbon. Kinectfusion: Real-time dense surface map-
ping and tracking. In ISMAR, pages 127–136. IEEE, 2011.
[36] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep
learning on point sets for 3d classiﬁcation and segmentation.
In CVPR, pages 652–660, 2017.

9173

[55] Y. Yang, C. Feng, Y. Shen, and D. Tian. Foldingnet: In-
terpretable unsupervised learning on 3d point clouds. arXiv
preprint arXiv:1712.07262, 2017.
[56] W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert. Pcn:
Point completion network. In 3DV, 2018.
[57] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and
T. Funkhouser. 3dmatch: Learning local geometric descrip-
tors from rgb-d reconstructions. In CVPR, pages 199–208.
IEEE, 2017.

[37] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep
hierarchical feature learning on point sets in a metric space.
In NIPS, pages 5099–5108, 2017.
[38] Z. Qunxiong and L. Chengfei. Dimensionality reduction
with input training neural network and its application in
chemical process modelling. Chinese Journal of Chemical
Engineering, 14(5):597–603, 2006.
[39] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-
sentation learning with deep convolutional generative adver-
sarial networks. arXiv preprint arXiv:1511.06434, 2015.
[40] V. Reddy and M. Mavrovouniotis. An input-training neu-
ral network approach for gross error detection and sensor
replacement. Chemical Engineering Research and Design,
76(4):478–489, 1998.
[41] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learning
deep 3d representations at high resolutions. In CVPR, pages
6620–6629. IEEE, 2017.
[42] J. Rock, T. Gupta, J. Thorsen, J. Gwak, D. Shin, and
D. Hoiem. Completing 3d object shape from one depth im-
age. In CVPR, pages 2484–2493, 2015.
[43] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In NIPS, pages 901–909, 2016.
[44] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and
R. Szeliski. A comparison and evaluation of multi-view
stereo reconstruction algorithms.
pages 519–528. IEEE,
2006.
[45] A. Sinha, J. Bai, and K. Ramani. Deep learning 3d shape
surfaces using geometry images. In ECCV, pages 223–240.
Springer, 2016.
[46] D. Stutz and A. Geiger. Learning 3d shape completion from
laser scan data with weak supervision.
In CVPR, pages
1955–1964, 2018.
[47] S. Tan and M. L. Mayrovouniotis. Reducing data dimen-
sionality through optimizing neural network inputs. AIChE
Journal, 41(6):1471–1480, 1995.
[48] M. Tarini, K. Hormann, P. Cignoni, and C. Montani.
Polycube-maps. In ACM TOG, volume 23, pages 853–860.
ACM, 2004.
[49] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3d outputs. In ICCV, 2017.
[50] N. Verma, E. Boyer, and J. Verbeek. Feastnet: Feature-
steered graph convolutions for 3d shape analysis.
[51] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and
J. M. Solomon. Dynamic graph cnn for learning on point
clouds. arXiv preprint arXiv:1801.07829, 2018.
[52] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.
Learning a probabilistic latent space of object shapes via
3d generative-adversarial modeling. In NIPS, pages 82–90,
2016.
[53] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman,
and J. B. Tenenbaum. Learning shape priors for single-
view 3d completion and reconstruction.
arXiv preprint
arXiv:1809.05068, 2018.
[54] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3d shapenets: A deep representation for volumetric
shapes. In CVPR, pages 1912–1920, 2015.

10174

Efﬁcient Online Multi-Person 2D Pose Tracking with
Recurrent Spatio-Temporal Afﬁnity Fields

Yaadhav Raaj
Haroon Idrees
Gines Hidalgo
Yaser Sheikh
The Robotics Institute, Carnegie Mellon University

{raaj@cmu.edu, hidrees@cs.cmu.edu, gines@cmu.edu, yaser@cs.cmu.edu}

Abstract

We present an online approach to efﬁciently and simul-
taneously detect and track 2D poses of multiple people in
a video sequence. We build upon Part Afﬁnity Fields (PAF)
representation designed for static images, and propose an
architecture that can encode and predict Spatio-Temporal
Afﬁnity Fields (STAF) across a video sequence.
In par-
ticular, we propose a novel temporal topology cross-linked
across limbs which can consistently handle body motions
of a wide range of magnitudes. Additionally, we make the
overall approach recurrent in nature, where the network in-
gests STAF heatmaps from previous frames and estimates
those for the current frame. Our approach uses only online
inference and tracking, and is currently the fastest and the
most accurate bottom-up approach that is runtime-invariant
to the number of people in the scene and accuracy-invariant
to input frame rate of camera. Running at ∼30 fps on a
single GPU at single scale, it achieves highly competitive
results on the PoseTrack benchmarks. 1

s

F

A

T

t

n

i

o
p
y
e

K

s

F

A

T

b

m

i

L

t
t+1

t
t+1

t
t+1

t
t+1

Large Motion

Negligible Motion

Figure 1: We solve multi-person human pose tracking by
encoding change in position and orientation of keypoints
or limbs across time as Temporal Afﬁnity Fields (TAFs) in
a recurrent fashion. Top: Modeling TAFs (blue arrows)
through keypoints works when motion occurs but fails dur-
ing limited motion making temporal association difﬁcult.
Bottom: Cross-linked TAFs across limbs perform consis-
tently for all kinds of motions providing redundancy and
smoother encoding for further reﬁnement and prediction.

1. Introduction

Multi-person human pose estimation has received con-
siderable attention in the past few years assisted by deep
convolutional learning as well as COCO [21] and MPII [3]
datasets. The recently introduced PoseTrack dataset [17]
has provided the community with a large scale corpus of
video data with multiple people in the scenes. In this paper,
our aim is to utilize these towards building a truly online
and real-time multi-person 2D pose estimator and tracker
that is deployable and scalable while achieving high perfor-
mance and requiring minimal post-processing. The poten-
tial uses include real-time and closed-loop applications with
low latency where the execution is in sync with frame rate
of camera such as self-driving cars and augmented reality.
The real-time and online nature of such an approach in-
troduces several challenges:
i) scenes with multiple peo-

1 Project Page

ple demand handling of occlusion, proximity and contact
as well as limb articulation, and ii) it should be runtime-
invariant to the number of people in the scene. Further-
more, iii) it must be capable of handling challenges in-
duced from video data, such as large camera motion and
motion blur across frames. We build upon the Part Afﬁn-
ity Fields (PAFs) [6] to overcome these challenges, which
represent connections across body keypoints in static im-
ages as normalized 2D vector ﬁelds with position and ori-
entation.
In this work, we propose Temporal Afﬁnity
Fields (TAFs) which encode connections between keypoints
across frames, including a unique cross-linked limb topol-
ogy as seen in bottom row of Figure 1. In the absence of mo-
tion or when there is not enough data from previous frames,
TAFs constructed between same keypoints, e.g., wrist-wrist
or elbow-elbow across frames lose all associative properties
(see top row of Fig. 1). In this case, the nulliﬁcation of mag-
nitude and orientation provides no useful information to dis-

14620

cern between the case where a new person appears or where
an existing person stops moving. This effect is compounded
if these two cases occur in proximity together. However,
the longer limb TAF connections allow information preser-
vation even in the absence of motion or appearance of new
people by preventing corruption of valid information with
noise as the magnitude of motion becomes small.
In the
limiting case of zero motion, the TAF effectively collapses
to a PAF. From the perspective of a network, TAF between
keypoints destroys spatial information about keypoints as
motion ceases, whereas TAF across keypoints simply learns
to propagate the PAF, which is a much simpler task.
Furthermore, we work on videos in a recurrent manner
to make the approach real-time, where computation of each
frame leverages information from previous frames thereby
reducing overall computation. Where the single-image pose
estimation methods use multiple stages to reﬁne heatmaps
[6, 24], we exploit the redundant information in the video
frames and divert the resources towards efﬁcient computa-
tion of both poses and tracks across multiple frames. Thus,
the multi-stage computation over images is divided over
multiple frames in a video. Overall, we call this Recur-
rent Spatio-Temporal Afﬁnity Fields (STAF) and it achieves
highly competitive results on the PoseTrack benchmarks:
[64.6% mAP, 58.4% MOTA] on single scale at ∼30 FPS,
and [71.5% mAP, 61.3% MOTA] on multiple scales at ∼7
FPS on the PoseTrack 2017 validation set using one GTX
1080 Ti. As of writing, our approach currently ranks second
for accuracy and at third place for tracking on the 2017 chal-
lenge [1]. Note that, our tracking approach is truly online
on a per-frame basis with no post processing.
The rest of the paper is organized as follows. In Sec. 2,
we discuss related work and situate the paper in the litera-
ture. In Sec. 3, we present details of our approach, training
procedure as well as tracking and inference algorithm. Fi-
nally, we present results and ablation experiments in Sec. 4
and conclude the paper in Sec. 5.

2. Related Work

Early methods for human pose estimation localized key-
points or body parts of individuals but did not consider mul-
tiple people simultaneously [4, 28, 36, 20, 33]. Hence, these
methods were not adept at localizing keypoints of highly
articulated or interacting people. Person detection was typ-
ically used which followed single-person keypoint detec-
tion [29, 11, 32, 16]. With deep learning, human detec-
tion methods such as Mask-RCNN [10, 14] were employed
to directly predict multiple human bounding boxes through
ROI-pooling followed by pose estimation per person [12].
However, these methods suffered when people were in close
proximity as bounding boxes got grouped together. Further-
more, these top-down methods required more computation
as the number of people increased in the image, making

them inadequate for real-time pose estimation and tracking.

The bottom-up Part Afﬁnity Fields (PAF) method [6]
produced a spatial encoding of pair-wise body part connec-
tions in the image space, followed by greedy bipartite graph
matching for inference permitting consistent computation
speed irrespective of the number of people. Person Lab [25]
built upon these ideas to incorporate redundant connections
on people with a less greedy inference approach getting
highly competitive results on the COCO [22] and MPII [3]
datasets. These methods work on single images and do not
incorporate any keypoint tracking or past information.

Many ofﬂine methods have been proposed to enforce
temporal consistency of poses in videos [15, 17, 34]. These
require solving spatio-temporal graphs or incorporating
data from future frames making them inadequate for on-
line operation. Alternatively, Song et al. and Pﬁster et al.
[27, 31] demonstrate how optical ﬂow ﬁelds could be pre-
dicted per keypoint by formulating the input to be multi-
framed. LSTM Pose Machines [23] built upon previous
work demonstrating use of single stage per frame for video
sequences. However, these networks did not model spatial
relationship between keypoints and were evaluated on the
single person Penn Action [37] and JHMDB [18] datasets.

A different line of works explored maintaining tempo-
ral graphs in neural networks for handling multiple peo-
ple [9, 8]. Rohit et al. demonstrated that a 3D extension
of Mask-RCNN, called person tubes, can connect people
across time. However, this required applying grouped con-
volutions over a stack of frames reducing speed, and did not
achieve better results for tracking than the Hungarian Algo-
rithm baseline. Joint Flow [8] used the concept of Temporal
Flow Field which connected keypoints across two frames.
However, it did not use a recurrent structure and explicitly
required a pair of images as input increasing run-time sig-
niﬁcantly. The ﬂow representation also suffered from am-
biguity when subjects moved slowly or were stationary and
required special handling of such cases during tracking.

Top-down pose and tracking methods [34, 33, 7, 26, 14]
have dominated the detection and tracking tasks [34] [35]
in PoseTrack but their speed suffered due to explicit human
detection and follow-up keypoint detection for each per-
son. Moreover, modeling long-term spatio-temporal graphs
for tracking in an ofﬂine manner hurts real-time applica-
tions. None of these methods are able to report any signif-
icant runtime-to-performance measures as they cannot run
in real time. In this work, we demonstrate this problem can
be solved in a simple elegant single-stage network that in-
corporates recurrence by using the previous pose heatmaps
to predict both keypoints and their spatio-temporal asso-
ciations. We call this Recurrent Spatio-Temporal Afﬁnity
Fields (STAF) which not only represents the prediction of
Spatial (PAF) and Temporal (TAF) Afﬁnity Fields but also
how they are reﬁned through past information.

24621

Figure 2: Left: Training architecture for one of our models which ingests video sequences in a recurrent manner across time
while generating keypoints and connections across keypoints in each frame as Part Afﬁnity Fields (PAFs), and connections
between keypoints across frames as Temporal Afﬁnity Fields (TAFs). Together, we call this Recurrent Spatio-Temporal
Afﬁnity Fields (STAF). Each module ingests outputs from other modules in both previous and current frames (shown with
arrows) and reﬁnes it. Center: During inference, our network operates on a single video frame at each time step using
past information. Right: During inference, we use the predicted heatmaps to detect and track people. Keypoints (red) are
extracted ﬁrst, then associated into poses and tracklets using PAFs (green), TAFs (blue), and tracklets from previous frames.

3. Proposed Approach

ground truth between pairs of keypoints for each person:

Our approach aims to solve the problems of keypoint es-
timation and tracking simultaneously in videos. We em-
ploy Recurrent Convolutional Neural Networks which we
construct from four essential building blocks. Let Pt rep-
resent the pose of a person in a particular frame or time
t, consisting of keypoints K = {K1 , K2 , . . . KK }. The
Part Afﬁnity Fields (PAFs) L = {L1 , L2 , . . . LL } are syn-
thesized from keypoints in each frame. For tracking key-
points across frames a video, we propose Temporal Afﬁnity
Fields (TAFs) given by R = {R1 , R2 , . . . RR } which cap-
ture the recurrence and connect the keypoints across frames.
Together, they are referred to as Spatio-Temporal Afﬁnity
Fields (STAF). These blocks are visualized in Fig. 2 where
each block is shown with a different color:
the raw con-
volutional feature from VGG backbone [30] are shown in
amber, PAFs in green, keypoints in red and TAFs in blue.

Thus, the output of VGG backbone, PAFs, keypoints and
TAFs are given by V, L, K and R, respectively, and com-
puted through CNNs by ψV , ψL , ψK and ψR , respectively.
The keypoint heatmaps are constructed from ground truth
by placing a Gaussian kernel at the location of the annotated
keypoint, whereas the PAFs and TAFs are constructed from

k→k′ := Ω(cid:0) eKt
k′ (cid:1), eRt
k→k′ := Ω(cid:0) eKt−1
eLt
k , eKt

k′ (cid:1),
, eKt
where ∼ denotes the ground truth and the function Ω(·)
places a directional unit vector at every pixel within a pre-
deﬁned radius of the line connecting the two keypoints.

k

(1)

3.1. Video Models for Pose Estimation and Tracking

Next, we present the three models comprising the four
blocks capable of estimating keypoints and STAF. The in-
put to each network consists of a set of consecutive frames
of a video. Each block in each network consists of ﬁve 7 × 7
and two 1 × 1 convolution layers. Each 7 × 7 layer is re-
placeable with the concatenation of three 3 × 3 convolution
layers providing the same receptive ﬁeld. The ﬁrst stage has
a unique set of weights from subsequent frames as it cannot
incorporate any previous data and also has a lower depth
which was found to improve results (see Sec. 4). The VGG
features are computed for each frame. For frame It at time
t of the video, they are computed as Vt = ψV (It ).

Model I: Given Vt−1 and Vt , the the following equations

34622

describe the ﬁrst model:

Lt = ψL (cid:0)Vt , ψq−1
Kt = ψK (cid:0)Vt , ψ q
Rt = ψR (cid:0)Vt−1 , Vt , Lt−1 , Lt , Rt−1 (cid:1),

K (·)(cid:1),

L (·)(cid:1),

L (·), ψ q−1

(2)

where ψq means q recursive applications of ψ . In our ex-
periments, we found that performance plateaus at q = 5. In
Model I, PAFs are obtained by recursive application of ψL
on concatenated input from VGG features and PAFs from
previous stage. Similarly, keypoints depend on VGG fea-
tures, keypoints from the previous stage and PAFs from the
current stage. Finally, TAFs are dependent on VGG fea-
tures and PAFs from both the previous and current frames,
as well as TAFs from previous frame. This model produces
good results but is the slowest due to recursive stages.

Model II: Unlike Model I with multiple applications of
CNNs for PAFs and keypoints, Model II computes the PAFs
and keypoints in a single pass as visualized in Fig. 2:

Lt = ψL (cid:0)Vt , Lt−1 (cid:1),
Kt = ψK (cid:0)Vt , Lt , Kt−1 (cid:1),
Rt = ψR (cid:0)Vt−1 , Vt , Lt−1 , Lt , Rt−1 (cid:1).

(3)

Replacing ﬁve stages with a single stage is expected to
drop performance. Therefore, the multi-stage computation
of PAFs and keypoints in Model II is supplanted with out-
put of PAFs and keypoints from the previous frames. This
boosts up the speed signiﬁcantly without major loss in per-
formance as it takes advantage of the redundant information
in videos, i.e., the PAFs and keypoints from previous frame
are a reliable guide to the location of PAFs and keypoints in
the current frame.

Model III: Finally, the third model attempts to estimate Part
and Temporal Afﬁnity Fields through a single CNN:

[L, R]t = ψ[L,R] (cid:0)Vt−1 , Vt , [L, R]t−1 (cid:1),
Kt = ψK (cid:0)Vt , Lt , Kt−1 (cid:1),

(4)

where [L, R] implies simultaneous computation of Part
and Temporal Afﬁnity Fields through a single CNN. For
Model III, the channels corresponding to PAFs are then
passed for keypoint estimation along with VGG features
from current frame and keypoints from previous frame. As
Model III consists of only three blocks, it has the fastest in-
ference, however it proved to be the most difﬁcult to train.

3.2. Topology of Spatio(cid:173)Temporal Afﬁnity Fields

For our body model, we deﬁne K = 21 body parts or
keypoints which is the union of body parts in COCO and
MPII pose datasets. They include ears, nose and eyes from
COCO; and head and neck from MPII. Next, there are sev-
eral possible ways to associate and track the keypoints and

(a)                      (b)                     (c)

Figure 3: This ﬁgure illustrates the three possible topology
variations for Spatio-Temporal Afﬁnity Fields including the
new cross-linked limb topology (b). Keypoints, PAFs and
TAFs are represented by solid circles, straight lines and ar-
rows, respectively.

STAF across frames as illustrated in Figure 3. In this ﬁgure,
solid circles represent keypoints while straight lines and ar-
rows stand for PAFs and TAFs, respectively. Figure 3(a)
consists of TAFs between same keypoints as well as PAFs.
For this topology, the number of TAFs and PAFs is 21 and
48, respectively. The TAFs capture temporal connections
directly across keypoints similar to [8].
On the other hand, Figure 3(b) consists of TAFs between
different limbs in a cross-linked manner across frames. The
number of PAFs and TAFs is 48 and 96, respectively. We
also tested the topology in Figure 3(c) which consists of 69
keypoints and limb TAFs only. This does not model any
spatial links within frames across keypoints.

3.3. Model Training

During training, we unroll each model to handle multi-
ple frames at once. Each model is ﬁrst pre-trained in Im-
age Mode where we present a single image or frame at
each time instant to the model. This implies multiple appli-
cations of PAF and keypoint stages to the same frame. We
train with COCO, MPII and PoseTrack datasets with a batch
distribution of 0.7, 0.2 and 0.1, respectively, which corre-
sponds to dataset sizes where each batch consists of images
or frames from one dataset exclusively. For masking out
un-annotated keypoints, we use the head bounding boxes
available in MPII and PoseTrack datasets, and location of
annotated keypoints for batches from COCO dataset. The
net takes in 368 × 368 images and has scaling, rotation and
translation augmentations. Heatmaps are computed with an
ℓ2 loss with a stride of 8 resulting in 46 × 46 dimensional
heatmaps. We initialize the limb TAFs with PAFs in topol-
ogy 3(b,c), and keypoint TAFs with zeros in topology 3(a,c).
We train the net for a maximum of 400k iterations.

44623

Next, we proceed training in the Video Mode where
we expose the network to video sequences. For static im-
age datasets including COCO and MPII, we augment data
with video sequences that have length equal to number of
times the network is unrolled by synthesizing motion with
scaling, rotation and translation. We train COCO, MPII
and PoseTrack in Video Mode with a batch distribution of
of 0.4, 0.1 and 0.5, respectively. Moreover, we also use
skip-frame augmentation for video-based PoseTrack dataset
where some of the randomly selected sequences skip up to
3 frames. We lock the weights of VGG module in Video
Mode. For Model I, we only train the TAF module when
training on videos. For Model II, we train keypoint, PAF
and TAF modules for 5000 epochs, then lock all modules
except TAF. In Model III, both STAF and keypoints remain
unlocked throughout the 300k iterations.

3.4. Inference and Tracking

The method described till now predicts heatmaps of key-
points and STAF at every frame. Next, we present the
framework to perform pose inference and tracking across
frames given the predicted heatmaps. Let the inferred poses
at time t be given by {Pt,1 , Pt,2 , . . . , Pt,N } where the
second superscript indexes over people at frame t. Each
pose at a particular time consists of up to K keypoints
that become part of a pose post inference, i.e., Pt,n =
2 , . . . , K
K }.
The detection and tracking procedure begins with local-
ization of keypoints at time t. The inferred keypoints K
are
obtained by rescaling the heatmaps to original image resolu-
tion followed by non-maximal suppression. Then, we infer

1 , K

{K

t,n

t,n

t,n

t

t

t

PAF weights, L
, and those for TAF, R
, between all pairs
of keypoints in each frame deﬁned by the given topology,
i.e.,

t

k→k′ = ω(cid:0)K

t

k , K

t

k′ (cid:1), R

t

k→k′ = ω(cid:0)K

t−1
k

L

t

k′ (cid:1),

, K

(5)

where the function ω(·) samples points between the two
keypoints, computes the dot product between the the mean
vector of the sampled points and the directional vector from
the ﬁrst to the second keypoint.
Both the inferred PAFs and TAFs are sorted by their
scores before inferring the complete poses and associating
them across frames with unique ids. We perform this in a
bottom-up style where we utilize poses and inferred PAFs
from the previous frame to determine the update, addition or
deletion of tracklets. Going through each PAF in the sorted
list, (i) we initialize a new pose if both keypoints in the PAF
are unassigned, (ii) add to existing pose if one of the key-
points is assigned, (iii) update score of PAF in pose if both
are assigned to the same pose, and (iv) merge two poses if
keypoints belong to different poses with opposing keypoints
unassigned. Finally, we assign id to each pose in the current

F

H

G

E

A

B

C

D

(a)

(b)

(c)

(d)

Figure 4: (a) Ambiguity when selecting between two wrist
locations B and E is resolved by reweighing PAFs through
TAFs. (b)-(d): With transitivity, incorrect PAFs containing
ankles (c) are resolved with past pose (b) resulting in (d).

Assume we are constructing a person, starting at Node A. We are confused about 
moving either to B or E, since their scores were sorted closely in PAF. 

We first select E, and select the best TAF linking it, going to F. We know F 
belongs to person A, so we go to G. Then we sample the TAF between G and A, 
since transitivity only exists between those limbs. We see the score is lower

We then select B, and select the best TAF linking it, going to C. We know C 
belongs to person B, so we go to D. Then we sample TAF between D and A. We 
see the score is higher.

frame with the most frequent id of keypoints from the pre-
vious frame. For cases where we have ambiguous PAFs,
i.e., multiple equally likely possibilities as seen in Figure 4,
we use transitivity that reweighs PAFs with TAFs to disam-
biguate between them, using α as a biasing weight. In this
ﬁgure, keypoint {A} - an elbow - is under consideration
with wrists {B } and {E } as two possibilities. We select the
strongest TAF where {A, B , C, D , A} has a higher weight
than {A, E , F , G, A}, computed as:

Hence, we select B to be our next point in the graph.

L

t,n

k→k′ = (1 − α) ∗ ω(K

t−1,n
k

, K

t,n

k′ ) + α ∗ ω(K

t,n

k , K

t,n

k′ ).

4. Experiments

In this section, we present results of our experiments. In-
put images to networks are resized at W×368 maintaining
aspect ratio for single scale (SS); and W×736, W×368 and
W×184 for multiple scales (MS). The heatmaps for multi-
ple scales are re-sized back to W×736 and merged through
averaging. This is followed by inference and tracking.

4.1. Ablation Study

We conducted a series of ablation studies to determine
the construction of our network architecture:

Filter Sizes: As discussed in Sec. 3, each block either con-
sists of ﬁve 7 × 7 layers followed by two 1 × 1 layers [6],
or each 7 × 7 layer is replaced with three 3 × 3 layers simi-
lar to [5] in the alternate experiment. The results are shown
in Table 1. We run single frame inference on Model I and
ﬁnd the 3 × 3 ﬁlter size to be 2% more accurate than 7 × 7,
with signiﬁcant boosts in average precision of knee and an-
kle keypoints.
It is also 40% faster while requiring 40%
more memory.

Video Mode / Depth of First Stage: Next, we report results
when training in Image Mode (Im) using single images, and
when we continue training beyond images while exposing
the network to videos and augmenting with synthetic mo-
tion in the Video Mode (Vid). During testing, the network
is run recurrently on video sequences with one frame per

54624

Method
Hea Sho Elb Wri Hip Kne Ank mAP fps
Model I - 3x3 75.7 73.9 67.8 56.3 66.8 62.3 56.9
66.3
14
Model I - 7x7 76.0 73.3 66.4 54.0 63.4 59.2 52.2
64.3
10

Table 1: This table shows results for experiments with the
two ﬁlter sizes on PoseTrack 2017 validation set.

y
c
a

c
c

A

r

u

88
86
84
82
80
78
76
74

(a) Validation Subset

(b) Validation Set

65

60

y
c
a

55

r

u

c
c

A

50

45

6Hz

12Hz 
Frame Rate

24Hz

6Hz

12Hz 
Frame Rate

24Hz

*Model II: 1 s  / 35 fps
*Model II: 2 s / 26 fps
*Model II: 3 s / 20 fps
*Model II: 4 s / 17  fps

Model II: 1 s   / 35 fps 
Model II: 2 s  / 26 fps 
Model  I: 5 s  /  14 fps

Figure 5: Improvement in quality of heatmaps before (a,c)
and after (b,d) the network is exposed to videos and syn-
thetic motion augmentation. We observe better peaks and
less noise across both PAF and keypoint heatmaps.

stage. Model II is deployed for these experiments. We ﬁnd
that by exposing the network to video sequences for 5000
iterations, we were able to boost the mAP as seen in Table 2
and Fig. 5. We also ﬁnd that if we use the same depth,
i.e., number of channels for the ﬁrst frame as the other
frames (128-128), the network was not able to generalize
well to recurrent execution (56.6 mAP) when trained with
Image Mode. When reducing the depth for the ﬁrst frame
to one-half, i.e. (64-128), we found that the generalization
to videos was better (62.6 mAP). When trained with Video
Mode, mAP increased further to 64.1. We reason that the
64-depth modules produced relatively vague outputs which
gave sufﬁcient room for the subsequent modules in the fol-
lowing frames to process and reﬁne the heatmaps yielding
a boost in performance. Furthermore, this also highlights
the importance of incorporating shot change detection and
running the ﬁrst stage at each shot change.

Method
Hea Sho Elb Wri Hip Kne Ank mAP fps
Im - 7x7 - 128-128 74.6 69.6 55.5 40.2 56.4 47.2 44.0 56.6 27
Vid - 7x7 - 128-128 76.2 71.6 64.5 51.9 62.6 59.3 52.5 63.6 27
Im - 7x7 - 64-128
73.5 72.2 63.8 52.1 62.7 57.3 51.1 62.6 27
Vid - 7x7 - 64-128
75.8 73.4 65.5 53.8 64.2 58.4 51.4 64.1 27
Im - 3x3 - 64-128
73.5 72.5 65.0 52.7 63.7 57.7 53.2 63.4 35
Vid - 3x3 - 64-128
75.4 73.2 67.4 55.0 63.9 58.4 53.5 64.6 35

Table 2: This table shows single-scale performance using
Model II before and after training with videos, ﬁlter sizes,
as well as different depths for ﬁrst stage.

Effect of Camera Frame Rate on mAP: For these exper-
iments, we studied how the frame rate of the camera and
number of stages affect the accuracy of pose estimation.
With a high frame rate, the apparent motion between frames
is smooth, which becomes relatively abrupt at low frame-
rates. Therefore, the heatmaps from previous frames would

Figure 6: These graphs show mAP curves as a function
of frame rates of camera, i.e., the rate at which an origi-
nal 24Hz video is input to the method. The ﬂat black line
shows the performance of ﬁve-stage Model I, while ‘*’ in
the legend indicates training using Image Mode only.

not be as useful at low frame-rates. We tested this hypoth-
esis with Model I (ﬁve stages of the same modules without
ingesting previous frame heatmaps), and Model II (different
number of stages with each ingesting heatmaps from previ-
ous frame). We also evaluate the inﬂuence of training with
Image and Video modes in Figure 6.
Fig. 6(a) shows results on a subset of ten sequences
where the human subjects comprised at least 30% of the
frame height in the PoseTrack 2017 validation set. Fig. 6(b)
presents results on the entire validation set. The original
videos were assumed to run at the ﬁlm-standard 24 Hz,
hence we ran experiments by varying frame rates at 24, 12
and 6 Hz through sub-sampling. The ground truth has been
annotated at 6 Hz. As expected, accuracy is proportional to
video frame rate and number of stages. When the Model II
was trained in Image Mode, we observed small increments
in accuracy until at four stages, it peaks at the same level as
Model I. Upon training with Video Mode, it surpasses this
accuracy peaking earlier at two stages.
When considering the entire validation set, the approach
is still able to reap the beneﬁts of more stages and train-
ing in Video Mode as can be seen in Fig. 6(b). However,
it was barely able to reach the accuracy of the much slower
Model I. For the validation set, the accuracy reduced when
including sequences with smaller apparent size of humans.
These sequences usually were more crowded as well, and
passing in the previous heatmaps seemed to hurt the perfor-
mance. The body parts of small-sized humans only occu-
pied a few pixels in the heatmaps and the normalized direc-
tion vectors were inconsistent and random across frames.

Inﬂuence of Topology / Model Type in Tracking: Next,
we report experiments on different combinations of topol-
ogy deﬁned in Fig. 3 with the three models presented in
Sec. 3.1, both for pose estimation and tracking evaluated

64625

using mean Average Precision (mAP) and Multiple Object
Tracking Accuracy (MOTA) metrics in Table 3. We found
an improvement in tracking using limb TAFs in Topology
B versus keypoint TAFs in Topology A. As highlighted
in Fig. 1, Topology A lacks associative properties when a
keypoint has minimal motion or when a new person ap-
pears. Although we enforced spatial constraint that joint lo-
cations should be close in consecutive frames, and adjusted
it according to scale (similar to [8]), this still resulted in
false positives since it is difﬁcult to disambiguate between
a newly detected person and some nearby stationary per-
son. Furthermore, where motion of a person tended to be
small, Topology A resulted in jittery and noisy vectors caus-
ing more reliance on pixel distances. This was further exac-
erbated by recurrence where accumulation of noisy vectors
from previous frame heatmaps deteriorated associative abil-
ity of Temporal Afﬁnity Fields. Table 3 also shows results
for Topology C which signiﬁcantly under-performed com-
pared to Topology B. Since it exclusively consists of limb
and joint TAFs without any spatial components, this makes
keypoint localization and association rather difﬁcult.
Topology B solves all of these problems elegantly. The
longer cross-linked limb TAF connections preserve infor-
mation even in the absence of motion or appearance of
new people since the TAF effectively collapses to a PAF
in such cases. This allows us to avoid association heuristics
and makes the problem of new person identiﬁcation trivial.
With this representation, recurrence was observably bene-
ﬁcial due to true and consistent representation irrespective
of magnitude of motion. As a side-advantage, this also al-
lowed us to warm-start the TAF input with PAF providing
more reliable initialization for tracking in the ﬁrst frame.
For Model III, training beyond 5000 iterations gradually
begins to harm the accuracy of the pose estimation resulting
in reduced tracking performance as well. This is primarily
due to the disparity in the amount of diverse data between
COCO / MPII and PoseTrack datasets. For Model II, if we
train on keypoints and PAFs modules and lock their weights
afterwards, then follow with training only the TAF, this re-
sults in better performance with a signiﬁcant boost in speed
as well. Although Model I outperformed the other models
with ﬁve stages for keypoints and PAFs; and a single recur-
rent stage for TAFs, however this comes at the expense of
speed. Furthermore, we observe that an increase in mAP
ends up sub-linearly increasing the MOTA as well.

Effect of Video Rate and Number of People on Tracking:
Finally, we performed a study on how the frame rate of the
camera affects tracking accuracy, since a lower frame rate
would require longer associations in pixel space.
We ran Lukas Kanade (LK) as a baseline tracker by re-
placing the TAF Module in Model I with LK (21 × 21
window size; 3 pyramid levels). Initially, we observe that
there is roughly 2.0% improvement in MOTA as seen in

Method
Model I-A
Model I-B
Model II-A
Model II-B
Model III-B
Model III-C

Wrist-AP Ankles-AP mAP MOTA fps
56.2
56.4
66.0
58.5
14
56.3
56.9
66.3
59.4
13
54.9
53.0
64.4
57.4
28
55.0
53.5
64.6
58.4
27
51.9
49.5
61.6
57.8
30
42.5
40.5
55.2
49.9
36

Table 3: This table shows pose estimation and tracking per-
formance for combinations of model types and topologies.

A
T

O

M

57
56
55
54
53
52
51
50

(a)

Model I: TAFs
Model I: LK

6Hz

   12Hz
Camera Frame Rate

24Hz

(b)

30

28

26

24

22

d
n
o

c
e
S

r

e

p

s
e

m

a

r

F

20
0

10
20
30
# people tracked

40

Figure 7: (a) This graph shows MOTA as a function of video
frame rate for Temporal Afﬁnity Fields (TAFs) and Lukas-
Kanade (LK) tracker. The performance of TAFs is virtu-
ally invariant to frame rate or alternatively to the amount
of motion between frames. (b) Our approach is effectively
runtime-invariant to the number of people in the scene.

Fig. 7(a). However, we note that around 20% of the se-
quences have signiﬁcant articulation and camera movement,
where TAFs outperformed LK as the latter was not able to
match keypoints across large displacements whereas TAFs
found matches due to stronger descriptive power. TAFs
were able to maintain tracking accuracy even with low
frame-rate cameras, but with LK the MOTA drops off sig-
niﬁcantly (see Fig. 7(a)). Furthermore, Fig. 7(b) suggests
that our approach is nearly runtime-invariant to number of
people in the frame making it suitable for crowded scenes.

4.2. Comparison

We present results on PoseTrack dataset in Table 4 for
2017 validation set (top), 2017 test set (middle) and 2018
validation set (bottom). FlowTrack, JointFlow and Pose-
Flow are included as comparison in this table. FlowTrack
is a top-down approach which means human detection is
performed ﬁrst followed by pose estimation. Due to this
reason, it is signiﬁcantly slower than bottom-up approaches
such as ours. Model II-B with single scale is competitive
with other bottom-up approaches while being 270% faster.
However, multi-scale (MS) processing boosts performance
by ∼6% and ∼1.5% for mAP and MOTA, respectively. We
are also able to achieve competitive results on the Pose-
Track 2018 Validation set while maintaining the best speeds
amongst all reported results. Note that PoseTrack 2018 Test
set was not released to public at the time of submission of
this paper. Figure 8 shows some qualitative results.

74626

Figure 8: Three example cases of tracking at ∼30 FPS on multiple targets. Top / Middle: Observe that tracking continues
to function despite large motion displacements and occlusions. Bottom: A failure case where abrupt scene change causes
ghosting, where previously tracked person appears in the new frame. This issue can be rectiﬁed through a warm-start.

5. Conclusion

In this paper, we ﬁrst motivated recurrent Spatio-
Temporal Afﬁnity Fields (STAF) as the right approach for
detection and tracking of articulated human pose in videos,
especially for real-time reactive systems. We showed that
leveraging the previous frame data within a recurrent struc-
ture and training on video sequences yields as good results
as a multi-stage network albeit at much lower computation
cost. We also demonstrated the stability of tracking accu-
racy at reduced frame rates for the TAF formulation, due to
its ability to correlate keypoints over large pixel distances.
This implies that our method can be deployed on low-power
embedded systems which may not be able to run large net-
works at high frame rates, yet are able to maintain reason-
able accuracy. Our new cross-linked limb temporal topol-
ogy is able to generalize better than previous approaches
due to strong associative power with PAF being a special
case of TAF. We are also able to operate at the same con-
sistent speed irrespective of the number of people due to
bottom-up formulation. For future work, we plan to embed
a re-identiﬁcation module to handle cases of people leaving
and reappearing in a camera view. Furthermore, detecting
and triggering warm-start at every shot change has the po-
tential to boost pose estimation and tracking performance.

Acknowledgment: Supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via Depart-
ment of Interior/ Interior Business Center (DOI/IBC) con-
tract number D17PC00340. The U.S. Government is autho-
rized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright annotation/herein.
Disclaimer: The views and conclusions contained herein

Method

n

w

o

D

-

p
o

T

p

-

U
m

o

t
t

o

B

n

w

o

D

-

p
o

T

p

-

U
m

o

t
t

o

B

p

-

U
m

o

t
t

o

B

55.2
65.4
62.9
66.0
58.3
59.8
58.4
59.4
61.3
62.7

Wrist-AP Ankles-AP mAP MOTA fps
PoseTrack 2017 Validation
Detect-and-track [9]
51.7
49.8
FlowTrack - 152 [34]
72.4
67.1
FlowTrack - 50 [34]
66.0
61.7
MDPN - 152 [13]
77.5
71.4
PoseFlow [35]
61.1
61.3
JointFlow [8]
-
-
Model II-B (SS)
55.0
53.5
Model I-B (SS)
56.8
56.8
Model II-B (MS)
62.9
60.9
Model I-B (MS)
65.0
62.7
PoseTrack 2017 Testing
Detect-and-track [9]
-
-
Flowtrack - 152 [34]
70.7
64.9
Flowtrack - 50 [34]
65.1
60.3
PoseTrack [2]
54.3
49.2
BUTD [19]
52.9
42.6
PoseFlow [35]
59.0
57.9
JointFlow [8]
53.1
50.4
Model II-B (MS)
62.8
59.5
Model I-B (MS)
65.0
60.7
PoseTrack 2018 Validation
56.2
54.2
58.3
56.7
62.7
60.6
64.7
62.0

Model II-B (SS)
Model I-B (SS)
Model II-B (MS)
Model I-B (MS)

1.2
-
-
-
10*
0.2
27
13
7
2

1.2
-
-
-
-
10*
0.2
7
2

51.8
57.6
56.4
48.4
50.6
51.0
53.1
52.4
53.8

60.6
76.7
72.4
80.7
66.5
69.3
64.6
66.3
71.5
72.6

59.6
73.9
70.0
59.4
59.1
63.0
63.3
69.6
70.3

63.7
64.9
69.9
70.4

58.4
59.6
59.8
60.9

27
13
7
3

Table 4: This table shows comparison on the PoseTrack
datasets. For our approach, we report results with Models I
/ II and Top. B. The last column shows the speed in frames
per second (* excludes pose inference time). FlowTrack is
a top-down approach using ResNet-152 (or 50); whereas
JointFlow, PoseFlow and our approach are bottom-up.

are those of the authors and should not be interpreted as nec-
essarily representing the ofﬁcial policies or endorsements,
either expressed or implied, of IARPA, DOI/IBC, or the
U.S. Government.

84627

[21] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B.
Girshick, J. Hays, P. Perona, D. Ramanan, P. Doll ´ar, and
C. L. Zitnick. Microsoft COCO: common objects in context.
CoRR, abs/1405.0312, 2014. 1
[22] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll ´ar, and C. L. Zitnick. Microsoft COCO: Com-
mon objects in context. In ECCV, 2014. 2
[23] Y. Luo, J. Ren, Z. Wang, W. Sun, J. Pan, J. Liu, J. Pang, and
L. Lin. LSTM pose machines. In CVPR, 2018. 2
[24] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. CoRR, abs/1603.06937,
2016. 2
[25] G. Papandreou, T. Zhu, L.-C. Chen, S. Gidaris, J. Tompson,
and K. Murphy. PersonLab: Person pose estimation and in-
stance segmentation with a bottom-up, part-based, geometric
embedding model. arXiv:1803.08225, 2018. 2
[26] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tomp-
son, C. Bregler, and K. Murphy. Towards accurate multi-
person pose estimation in the wild. In CVPR, 2017. 2
[27] T. Pﬁster, J. Charles, and A. Zisserman. Flowing convnets
for human pose estimation in videos. In ICCV, 2015. 2
[28] L. Pishchulin, M. Andriluka, P. Gehler, and B. Schiele. Pose-
let conditioned pictorial structures. In CVPR, 2013. 2
[29] L. Pishchulin, A. Jain, M. Andriluka, T. Thormahlen, and . B.
Schiele. Articulated people detection and pose estimation:
Reshaping the future. In CVPR, 2012. 2
[30] K. Simonyan and A. Zisserman.
Very deep con-
volutional networks for
large-scale image recognition.
arXiv:1409.1556, 2014. 3
[31] J. Song, L. Wang, L. Van Gool, and O. Hilliges. Thin-slicing
network: A deep structured model for pose estimation in
videos. In CVPR, 2017. 2
[32] M. Sun and S. Savarese. Articulated part-based model for
joint object detection and pose estimation. In ICCV, 2011. 2
[33] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
volutional pose machines. In CVPR, 2016. 2
[34] B. Xiao, H. Wu, and Y. Wei. Simple baselines for human
pose estimation and tracking. CoRR, abs/1804.06208, 2018.
2, 8
[35] Y. Xiu, J. Li, H. Wang, Y. Fang, and C. Lu. Pose ﬂow: Efﬁ-
cient online pose tracking. In BMVC, 2018. 2, 8
[36] Y. Yang and D. Ramanan. Articulated human detection with
ﬂexible mixtures of parts. TPAMI, 2013. 2
[37] W. Zhang, M. Zhu, and K. Derpanis. From actemes to ac-
tion: A strongly-supervised representation for detailed action
understanding. In ICCV, 2013. 2

References

[1] Posetrack leaderboard.
leaderboard.php. 2

https://posetrack.net/

[2] M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, and
A. Milan. PoseTrack: A benchmark for human pose estima-
tion and tracking. In CVPR, 2018. 8

[3] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2D
human pose estimation: new benchmark and state of the art
analysis. In CVPR, 2014. 1, 2

[4] M. Andriluka, S. Roth, and B. Schiele. Monocular 3D pose
estimation and tracking by detection. In CVPR, 2010. 2

[5] Z. Cao, G. Hidalgo, T. Simon, S. Wei, and Y. Sheikh. Open-
pose: Realtime multi-person 2D pose estimation using part
afﬁnity ﬁelds. CoRR, abs/1812.08008, 2018. 5

[6] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime
multi-person 2D pose estimation using part afﬁnity ﬁelds. In
CVPR, 2017. 1, 2, 5

[7] Y. Chen, Z. Wang, Y. Peng, Z. Zhang, G. Yu, and J. Sun.
Cascaded pyramid network for multi-person pose estimation.
arXiv:1711.07319, 2017. 2

[8] A. Doering, U. Iqbal, and J. Gall.
Joint ﬂow: Temporal
ﬂow ﬁelds for multi person tracking. CoRR, abs/1805.04596,
2018. 2, 4, 7, 8

[9] R. Girdhar, G. Gkioxari, L. Torresani, M. Paluri, and D. Tran.
Detect-and-track: Efﬁcient pose estimation in videos. CoRR,
abs/1712.09184, 2017. 2, 8

[10] R. Girshick,
I. Radosavovic, G. Gkioxari, P. Doll ´ar,
and K. He.
Detectron.
https://github.com/
facebookresearch/detectron, 2018. 2

[11] G. Gkioxari, B. Hariharan, R. Girshick, and J. Malik. Us-
ing k-poselets for detecting people and localizing their key-
points. In CVPR, 2014. 2

[12] R. A. Guler, N. Neverova, and I. Kokkinos. DensePose:
Dense human pose estimation in the wild. In CVPR, 2018. 2

[13] H. Guo, T. Tang, G. Luo, R. Chen, and Y. Lu. Multi-domain
pose network for multi-person pose estimation and tracking.
In ECCV, 2018. 8

[14] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick. Mask R-
CNN. In ICCV, 2017. 2

[15] E. Insafutdinov, M. Andriluka, L. Pishchulin, S. Tang,
E. Levinkov, B. Andres, and B. Schiele. ArtTrack: Artic-
ulated multi-person tracking in the wild. In CVPR, 2017. 2

[16] U. Iqbal and J. Gall. Multi-person pose estimation with lo-
cal joint-to-person associations. In ECCVW: Crowd Under-
standing, 2016. 2

[17] U. Iqbal, A. Milan, and J. Gall. PoseTrack: Joint multi-
person pose estimation and tracking. In CVPR, 2017. 1, 2

[18] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. J. Black.
Towards understanding action recognition. In ICCV, 2013. 2

[19] S. Jin, X. Ma, Z. Han, Y. Wu, and W. Yang. Towards multi-
person pose tracking: Bottom-up and top-down methods.
ICCV, 2017. 8

[20] S. Johnson and M. Everingham. Clustered pose and nonlin-
ear appearance models for human pose estimation. In BMVC,
2010. 2

94628

Estimating 3D Motion and Forces of Person-Object Interactions
from Monocular Video

Zongmian Li1,2
Ivan Laptev1,2
1DI ENS PSL
2 Inria

Jiri Sedlar3
Justin Carpentier1,2
Nicolas Mansard4
Josef Sivic1,2,3
3CIIRC, CTU in Prague
4LAAS-CNRS

Abstract

In this paper, we introduce a method to automatically
reconstruct the 3D motion of a person interacting with an
object from a single RGB video. Our method estimates the
3D poses of the person and the object, contact positions,
and forces and torques actuated by the human limbs. The
main contributions of this work are three-fold. First, we in-
troduce an approach to jointly estimate the motion and the
actuation forces of the person on the manipulated object by
modeling contacts and the dynamics of their interactions.
This is cast as a large-scale trajectory optimization prob-
lem. Second, we develop a method to automatically recog-
nize from the input video the position and timing of contacts
between the person and the object or the ground, thereby
signiﬁcantly simplifying the complexity of the optimization.
Third, we validate our approach on a recent MoCap dataset
with ground truth contact forces and demonstrate its perfor-
mance on a new dataset of Internet videos showing people
manipulating a variety of tools in unconstrained environ-
ments.

1. Introduction

People can easily learn how to break concrete with a
sledgehammer or cut hay using a scythe by observing other
people performing such tasks in instructional videos, for ex-
ample. They can also easily perform the same task in a dif-
ferent context. This involves advanced visual intelligence
capabilities such as recognizing and interpreting complex
person-object interactions that achieve a speciﬁc goal. Un-
derstanding such complex interactions is a key to building
autonomous machines that learn how to interact with the
physical world by observing people.

* Please see our project webpage [2] for trained models, data and code.
1D ´epartement d’informatique de l’ENS, ´Ecole normale sup ´erieure,
CNRS, PSL Research University, 75005 Paris, France.
3Czech Institute of Informatics, Robotics and Cybernetics at the
Czech Technical University in Prague, Czech Republic.

Figure 1: Our method automatically reconstructs in 3D the object
manipulation action captured by a single RGB video. Top: Frames
from the input video. Bottom: the output human and object 3D
motion including the recovered contact forces (yellow arrows) and
moments (white arrows).

This work makes a step in this direction and describes a
method to estimate the 3D motion and actuation forces of a
person manipulating an object given a single unconstrained
video as input, as shown in Figure 1. This is an extremely
challenging task. First, there are inherent ambiguities in the
2D-to-3D mapping from a single view: multiple 3D human
poses correspond to the same 2D input. Second, human-
object interactions often involve contacts, resulting in dis-
continuities in the motion of the object and the human body
part in contact. For example, one must place a hand on the
hammer handle before picking the hammer up. The contact
motion strongly depends on the physical quantities such as
the mass of the object and the contact forces exerted by the
hand, which renders modeling of contacts a very difﬁcult
task. Finally, the tools we consider in this work, such as
hammer, scythe, or spade, are particularly difﬁcult to recog-
nize due to their thin structure, lack of texture, and frequent
occlusions by hands and other human parts.

To address these challenges, we propose a method to
jointly estimate the 3D trajectory of both the person and
the object by visually recognizing contacts in the video

8640

and modeling the dynamics of the interactions. We focus
on rigid stick-like hand tools (e.g. hammer, barbell, spade,
scythe) with no articulation and approximate them as 3D
line segments. Our key idea is that, when a human joint
is in contact with an object, the object can be integrated as
a constraint on the movement of the human limb. For ex-
ample, the hammer in Figure 1 provides a constraint on the
relative depth between the person’s two hands. Conversely,
3D positions of the hands in contact with the hammer pro-
vide a constraint on the hammer’s depth and 3D rotation.
To deal with contact forces, we integrate physics in the es-
timation by modeling dynamics of the person and the ob-
ject. Inspired by recent progress in humanoid locomotion
research [16], we formulate person-object trajectory estima-
tion as an optimal control problem given the contact state of
each human joint. We show that contact states can be au-
tomatically recognized from the input video using a deep
neural network.

2. Related work

Here we review the key areas of related work in both
computer vision and robotics literature.
Single-view 3D pose estimation aims to recover the 3D
joint conﬁguration of the person from the input image. Re-
cent human 3D pose estimators either attempt to build a
direct mapping from image pixels to the 3D joints of the
human body or break down the task into two stages: es-
timating pixel coordinates of the joints in the input image
and then lifting the 2D skeleton to 3D. Existing direct ap-
proaches either rely on generative models to search the state
space for a plausible 3D skeleton that aligns with the im-
age evidence [59, 26, 25] or, more recently, extract deep
features from images and learn a discriminative regressor
from the 2D image to the 3D pose [37, 49, 53, 62]. Build-
ing on the recent progress in 2D human pose estimation
[51, 50, 34, 14], two-stage methods have been shown to be
very effective [5, 71, 9, 19] and achieve state-of-the-art re-
sults [47] on 3D human pose benchmarks [35]. To deal with
depth ambiguities, these estimators rely on good pose pri-
ors, which are either hand-crafted or learnt from large-scale
MoCap data [71, 9, 37]. However, unlike our work, these
methods do not consider explicit models for 3D person-
object interactions with contacts.
Understanding human-object interactions involves both
recognition of actions and modeling of interactions.
In
action recognition, most existing approaches that model
human-object
interactions do not consider 3D,
instead
model interactions and contacts in the 2D image space [29,
20, 70, 55]. Recent works in scene understanding [36, 24]
consider interactions in 3D but have focused on static
scene elements rather than manipulated objects as we do
in this work. Tracking 3D poses of people interacting with
the environment has been demonstrated for bipedal walk-

ing [12, 13] or in sports scenarios [66]. However, these
works do not consider interactions with objects. Further-
more, [66] requires manual annotation of the input video.

There is also related work on modeling person-object
interactions in robotics [60] and computer animation [10].
Similarly to people, humanoid robots interact with the envi-
ronment by creating and breaking contacts [32], for exam-
ple, during walking. Typically, generating artiﬁcial motion
is formulated as an optimal control problem, transcribed
into a high-dimensional numerical optimization problem,
seeking to minimize an objective function under contact
and feasibility constraints [21, 58]. A known difﬁculty is
handling the non-smoothness of the resulting optimization
problem introduced by the creation and breaking of con-
tacts [67]. Due to this difﬁculty, the sequence of contacts is
often computed separately and not treated as a decision vari-
able in the optimizer [38, 64]. Recent work has shown that
it may be possible to decide both the continuous movement
and the contact sequence together, either by implicitly for-
mulating the contact constraints [54] or by using invariances
to smooth the resulting optimization problem [48, 68].

In this paper, we take advantage of rigid-body models in-
troduced in robotics and formulate the problem of estimat-
ing 3D person-object interactions from monocular video as
an optimal control problem under contact constraints. We
overcome the difﬁculty of contact irregularity by ﬁrst iden-
tifying the contact states from the visual input, and then
localizing the contact points in 3D via our trajectory esti-
mator. This allows us to treat multi-contact sequences (like
walking) without manually annotating the contact phases.

Object 3D pose estimation methods often require depth or
RGB-D data as input [61, 22, 33], which is restrictive since
depth information is not always available (e.g. for outdoor
scenes or specular objects), as is the case of our instructional
videos. Recent work has also attempted to recover object
pose from RGB input only [11, 56, 69, 39, 52, 28, 57].
However, we found that the performance of these methods
is limited for the stick-like objects we consider in this work.
Instead, we recover the 3D pose of the object via localizing
and segmenting the object in 2D, and then jointly recover-
ing the 3D trajectory of both the human limbs and the ob-
ject. As a result, both the object and the human pose help
each other to improve their joint 3D trajectory by leveraging
the contact constraints.

Instructional videos. Our work is also related to recent ef-
forts in learning form Internet instructional videos [46, 6, 6]
that aim to segment input videos into clips containing con-
sistent actions.
In contrast, we focus on extracting a de-
tailed representation of the object manipulation in the form
of a 3D person-object trajectory with contacts and underly-
ing manipulation forces.

8641

Recognition stage

Estimation stage

Input video

Human 2D pose

Human 2D pose 
with contact states

Object 2D endpoints

Reconstructed 
motion and forces

Figure 2: Overview of the proposed method. In the recognition stage, the system estimates from the input video the person’s 2D joints, the
hammer’s 2D endpoints and the contact states of the individual joints. The human joints and the object endpoints are visualized as colored
dots in the image. Human joints recognized as in contact are shown in green, joints not in contact in red. In the estimation stage, these
image measurements are fused in a trajectory estimator to recover the human and object 3D motion together with the contact positions and
forces.

3. Approach overview

We are given a video clip of a person manipulating an
object or in another way interacting with the scene. Our ap-
proach, illustrated in Figure 2, receives as input a sequence
of frames and automatically outputs the 3D trajectories of
the human body, the manipulated object, and the ground
plane. At the same time, it localizes the contact points and
recovers the contact forces that actuate the motion of the
person and the object. Our approach proceeds along two
stages. In the ﬁrst, recognition stage, we extract 2D mea-
surements from the input video. These consist of 2D loca-
tions of human joints, 2D locations of a small number of
predeﬁned object endpoints, and contact states of selected
joints over the course of the video. In the second, estimation
stage, these image measurements are then fused in order to
estimate the 3D motion, 3D contacts, and the controlling
forces of both the person and the object. The person and
object trajectories, contact positions, and contact forces are
constrained jointly by our carefully designed contact mo-
tion model, force model, and dynamics equations. Finally,
the reconstructed object manipulation sequence can be ap-
plied to control a humanoid robot via behavior cloning.
In the following, we start in Section 4 by describing the
estimation stage giving details of the formulation as an op-
timal control problem. Then, in Section 5 we give details of
the recognition stage including 2D human pose estimation,
contact recognition, and object 2D endpoint estimation. Fi-
nally, we describe results in Section 6.

4. Estimating person-object trajectory under
contact and dynamics constraints

We assume that we are provided with a video clip of du-
ration T depicting a human subject manipulating an object.
We encode the 3D poses of the human and the object, in-
cluding joint translations and rotations, in the conﬁguration

vectors qh and qo , for the human and the object respectively.
We deﬁne a constant set of K contact points between the hu-
man body and the object (or the ground plane). Each contact
point corresponds to a human joint, and is activated when-
ever that human joint is recognized as in contact. At each
contact point, we deﬁne a contact force fk , whose value is
non-zero whenever the contact point k is active. The state
of the complete dynamical system is then obtained by con-
catenating the human and the object joint conﬁgurations q
and velocities ˙q as x := (cid:0)qh , qo , ˙qh , ˙qo (cid:1). Let τ h
m be the joint
torque vector describing the actuation by human muscles.
This is a nq − 6 dimensional vector where nq is the dimen-
sion of the human body conﬁguration vector. We deﬁne
the control variable u as the combination of the joint torque
vector together with contact forces at the K contact joints,
m , fk , k = 1, ..., K (cid:1). To deal with sliding contacts,
we further deﬁne a contact state c that consists of the rel-
ative positions of all the contact points with respect to the
object (or ground) in the 3D space.

u := (cid:0)τ h

Our goal is two-fold. We wish to (i) estimate smooth
and consistent human-object and contact trajectories x and
c, while (ii) recovering the control u which gives rise to the
observed motion1 . This is achieved by jointly optimizing
the 3D trajectory x, contacts c, and control u given the mea-
surements (2D positions of human joints and object end-
points together with contact states of human joints) obtained
from the input video. The intuition is that the human and
the object’s 3D poses should match their respective projec-
tions in the image while their 3D motion is linked together
by the recognized contact points and the corresponding con-
tact forces. In detail, we formulate person-object interaction
estimation as an optimal control problem with contact and

1 In this paper, trajectories are denoted as underlined variables, e.g.

x, u or c.

8642

dynamics constraints:

minimize

x,u,c Xe∈{h,o}

Z T

0

le (x, u, c) dt,

subject to κ(x, c) = 0 (contact motion model),
˙x = f (x, c, u)
u ∈ U (force model),

(full-body dynamics),

(1)

(2)

(3)

(4)

where e denotes either ‘h’ (human) or ‘o’ (object), and the
constraints (2)-(4) must hold for all t ∈ [0, T ]. The loss
function le is a weighted sum of multiple costs capturing
(i) the data term measuring the discrepancy between the ob-
served and re-projected 2D joint and object endpoint po-
sitions, (ii) the prior on the estimated 3D poses, (iii) the
physical plausibility of the motion and (iv) the smoothness
of the trajectory. Next, we in turn describe these cost terms
as well as the insights leading to their design choices. For
simplicity, we ignore the superscript e when introducing a
cost term that exists for both the human lh and the object lo
component of the loss. We describe the individual terms us-
ing continuous time notation as used in the overall problem
formulation (1). A discrete version of the problem as well
as the optimization and implementation details are relegated
to Section 4.5.

4.1. Data term: 2D re(cid:173)projection error

j

We wish to minimize the re-projection error of the es-
timated 3D human joints and 3D object endpoints with re-
spect to the 2D measurements obtained in each video frame.
In detail, let j = 1, ..., N be human joints or object end-
points and p2D
their 2D position observed in the image. We
aim to minimize the following data term
ρ (cid:0)p2D
j − Pcam (pj (q))(cid:1),
where Pcam is the camera projection matrix and pj the 3D
position of joint or object endpoint j induced by the person-
object conﬁguration vector q . To deal with outliers, we use
the robust Huber loss, denoted by ρ.

ldata = Xj

(5)

4.2. Prior on 3D human poses

A single 2D skeleton can be a projection of multiple 3D
poses, many of which are unnatural or impossible exceeding
the human joint limits. To resolve this, we incorporate into
the human loss function lh a pose prior similar to [9]. The
pose prior is obtained by ﬁtting the SMPL human model
[43] to the CMU MoCap data [1] using MoSh [44] and
ﬁtting a Gaussian Mixture Model (GMM) to the resulting
SMPL 3D poses. We map our human conﬁguration vec-
tor qh to a SMPL pose vector θ and compute the likelihood
under the pre-trained GMM
lh
pose = − log (cid:0)p(qh ; GMM)(cid:1) .

(6)

During optimization, lh
pose is minimized in order to fa-
vor more plausible human poses against rare or impossible
ones.

4.3. Physical plausibility of the motion

Human-object interactions involve contacts coupled with
interaction forces, which are not included in the data-driven
cost terms (5) and (6). Modeling contacts and physics is
thus important to reconstruct object manipulation actions
from the input video. Next, we outline models for describ-
ing the motion of the contacts and the forces at the con-
tact points. Finally, the contact motions and forces, together
with the system state x, are linked by the laws of mechan-
ics via the dynamics equations, which constrain the esti-
mated person-object interaction. This full body dynamics
constraint is detailed at the end of this subsection.

(7)

(cid:13)(cid:13)ph

Contact motions.
In the recognition stage, our contact
recognizer predicts, given a human joint (for example, left
hand, denoted by j ), a sequence of contact states δj : t −→
{1, 0}. Similarly to [16], we call a contact phase any time
segment in which j is in contact, i.e., δj = 1. Our key idea
is that the 3D distance between human joint j and the active
contact point on the object (denoted by k) should remain
zero during a contact phase:
j (qh ) − pc
k (x, c)(cid:13)(cid:13) = 0 (point contact),
where ph
j and pc
k are the 3D positions of joint j and ob-
ject contact point k , respectively. Note that position of the
object contact point pc
k (x, c) depends on the state vector x
describing the human-object conﬁguration and the relative
position c of the contact along the object. The position of
contact pc
k is subject to a feasible range denoted by C . For
stick-like objects such as hammer, C is approximately the
3D line segment representing the handle. For the ground,
the feasible range C is a 3D plane. In practice, we imple-
ment pc
k ∈ C by putting a constraint on the trajectory of
relative contact positions c.
Equation (7) applies to most common cases where the
contact area can be modeled as a point. Examples in-
clude the hand-handle contact and the knee-ground contact.
To model the planar contact between the human sole and
ground, we approximate each sole surface as a planar poly-
gon with four vertices, and apply the point contact model at
each vertex. In our human model, each sole is attached to
its parent ankle joint, and therefore the four vertex contact
points of the sole are active when δankle = 1.
The resulting overall contact motion function κ in prob-
lem (1) is obtained by unifying the point and the planar con-
tact models:
κ(x, c) = Xj Xk∈φ(j )

j (qh )(cid:1) − pc

T (kj ) (cid:0)ph

δj (cid:13)(cid:13)(cid:13)

, (8)

k (x, c)(cid:13)(cid:13)(cid:13)

8643

where the external sum is over all human joints. The in-
ternal sum is over the set of active object contact points
mapped to their corresponding human joint j by mapping
φ(j ). The mapping T (kj ) translates the position of an ankle
joint j to its corresponding k-th sole vertex; it is an identity
mapping for non-ankle joints.

Contact forces. During a contact phase of the human
joint j , the environment exerts a contact force fk on each
of the active contact points in φ(j ). fk is always expressed
in contact point k’s local coordinate frame. We distinguish
two types of contact forces: (i) 6D spatial forces exerted by
objects and (ii) 3D linear forces due to ground friction. In
the case of object contact, fk is an unconstrained 6D spatial
force with 3D linear force and 3D moment. In the case of
ground friction, fk is constrained to lie inside a 3D friction
cone K3 (also known as the quadratic Lorentz “ice-cream”
cone [16]) characterized by a positive friction coefﬁcient µ.
In practice, we approximate K3 by a 3D pyramid spanned
by a basis of N = 4 generators, which allows us to rep-
resent fk as the convex combination fk = PN
n ,
where λkn ≥ 0 and g (3)
n with n = 1, 2, 3, 4 are the 3D
generators of the contact force. We sum the contact forces
induced by the four sole-ground contact points and express
a uniﬁed contact force in the ankle’s frame:

n=1 λkn g (3)

fj =

4
Xk=1

(cid:18) fk
pk × fk(cid:19) =

4
Xk=1

N

Xn=1

λjkn g (6)

kn ,

(9)

where pk is the position of contact point k expressed in joint
j ’s (left/right ankle) frame, × is the cross product operator,
λjkn ≥ 0, and g (6)
kn are the 6D generators of fj . Please see
the appendix in the extended version of this paper available
at [40] for additional details including the expressions of
n and g (6)

g (3)

kn .

Full body dynamics. The full-body movement of the per-
son and the manipulated object is described by the Lagrange
dynamics equation:

M (q) ¨q + b(q , ˙q) = g(q) + τ ,

(10)

where M is the generalized mass matrix, b covers the cen-
trifugal and Coriolis effects, g is the generalized gravity
vector and τ represents the joint torque contributions.
and ¨q are the joint velocities and joint accelerations, respec-
tively. Note that (10) is a uniﬁed equation which applies
to both human and object dynamics, hence we drop the su-
perscript e here. Only the expression of the joint torque τ
differs between the human and the object and we give the
two expressions next.
For human, it is the sum of two contributions: the ﬁrst
one corresponds to the internal joint torques (exerted by the

˙q

muscles for instance) and the second one comes from the
contact forces:

K

fk ,

k (cid:1)T

(11)

m(cid:19) +

Xk=1 (cid:0)J h

τ h = (cid:18) 06
τ h
where τ h
m is the human joint torque exerted by muscles, fk
is the contact force at contact point k and J h
k is the Jacobian
mapping human joint velocities ˙qh to the Cartesian velocity
of contact point k expressed in k’s local frame. Let nh
q de-
note the dimension of qh , ˙qh and ¨qh , then τ h
m and J h
k are of
dimension nh
q − 6 and 3 × nh
q , respectively. We model the
human body and the object as free-ﬂoating base systems. In
the case of human body, the six ﬁrst entries in the conﬁgura-
tion vector q correspond to the 6D pose of the free-ﬂoating
base (translation + orientation), which is not actuated by any
internal actuators such as human muscles. This constraint
is taken into consideration by adding the zeros in Eq. (11).
In the case of the manipulated object, there is no actu-
ation other than the contact forces exerted by the human.
Therefore, the object torque is expressed as
(J o
k )T fk ,

(12)

τ o = − Xobject contact k

where the sum is over the object contact points, fk is the
contact force, and J o
k denotes the object Jacobian, which
maps from the object joint velocities ˙qo to the Cartesian ve-
locity of the object contact point k expressed in k’s local
frame. J o
k is a 3 × no
q matrix where no
q is the dimension of
object conﬁguration vectors qo , ˙qo and ¨qo .
We concatenate the dynamics equations of both human
and object to form the overall dynamics in Eq. (3) in prob-
lem (1), and include a muscle torque term lh
in the overall cost. Minimizing the muscle torque acts as
a regularization over the energy consumption of the human
body.

torque = kτ h
m k2

4.4. Enforcing the trajectory smoothness

Regularizing human and object motion. Taking advan-
tage of the temporal continuity of video, we minimize the
sum of squared 3D joint velocities and accelerations to im-
prove the smoothness of the person and object motion and
to remove incorrect 2D poses. We include the following
motion smoothing term to the human and object loss in (1):
lsmooth = Xj (cid:16)kνj (q , ˙q)k2 + kαj (q , ˙q , ¨q)k2(cid:17),
where νj and αj are the spatial velocity and the spatial
acceleration2 of joint j , respectively.
In the case of ob-
ject, j represents an endpoint on the object. By minimiz-
ing lsmooth , both the linear and angular movements of each
joint/endpoint are smoothed simultaneously.

(13)

2 Spatial velocities (accelerations) are minimal and uniﬁed representa-
tions of linear and angular velocities (accelerations) of a rigid body [23].
They are of dimension 6.

8644

Regularizing contact motion and forces.
In addition to
regularizing the motion of the joints, we also regularize the
contact states and control by minimizing the velocity of
the contact points and the temporal variation of the con-
tact force. This is implemented by including the following
contact smoothing term in the cost function in problem (1):
δj (cid:16)ωk k ˙ck k2 + γk k ˙fk k2(cid:17) dt,

smooth = Xj Xk∈φ(j )

lc

(14)

where ˙ck and ˙fk represent respectively the temporal vari-
ation of the position and the contact force at contact point
k . ωk and γk are scalar weights of the regularization terms
˙ck and ˙fk . Note that some contact points, for example the
four contact points of the human sole during the sole-ground
contact, should remain ﬁxed with respect to the object or the
ground during the contact phase. To tackle this, we adjust
ωk to prevent contact point k form sliding while being in
contact.

4.5. Optimization

Conversion to a numerical optimization problem. We
convert the continuous problem (1) into a discrete nonlinear
optimization problem using the collocation approach [8].
All trajectories are discretized and constraints (2), (3), (4)
are only enforced on the “collocation” nodes of a time grid
matching the discrete sequence of video frames. The op-
timization variables are the sequence of human and object
poses [x0 ...xT ], torque and force controls [u1 ...uT ], con-
tact locations [c0 ...cT ], and the scene parameters (ground
plane and camera matrix). The resulting problem is nonlin-
ear, constrained and sparse (due to the sequential structure
of trajectory optimization). We rely on the Ceres solver [4],
which is dedicated to solving sparse estimation problems
(e.g. bundle adjustment [65]), and on the Pinocchio soft-
ware [17, 18] for the efﬁcient computation of kinematic and
dynamic quantities and their derivatives [15]. Additional
details are given in the appendix available in the extended
version of this paper [40].

Initialization. Correctly initializing the solver is key to
escape from poor local minima. We warm-start the opti-
mization by inferring the initial conﬁguration vector qk at
each frame using the human body estimator HMR [37] that
estimates the 3D joint angles from a single RGB image.

5. Extracting 2D measurements from video

In this section, we describe how 2D measurements are
extracted from the input video frames during the ﬁrst,
recognition stage of our system. In particular, we extract
the 2D human joint positions, the 2D object endpoint posi-
tions and the contact states of human joints.

Estimating 2D positions of human joints. We use the
state-of-the-art Openpose [14] human 2D pose estimator,

which achieved excellent performance on the MPII Multi-
Person benchmark [7]. Taking a pretrained Openpose
model, we do a forward pass on the input video in a frame-
by-frame manner to obtain an estimate of the 2D trajectory
of human joints, ph,2D
.

j

Recognizing contacts. We wish to recognize and local-
ize contact points between the person and the manipulated
object or the ground. This is a challenging task due to the
large appearance variation of the contact events in the video.
However, we demonstrate here that a good performance can
be achieved by training a contact recognition CNN mod-
ule from manually annotated contact data that combine both
still images and videos harvested from the Internet. In de-
tail, the contact recognizer operates on the 2D human joints
predicted by Openpose. Given 2D joints at video frame i,
we crop ﬁxed-size image patches around a set of joints of
interest, which may be in contact with an object or ground.
Based on the type of human joint, we feed each image patch
to the corresponding CNN to predict whether the joint ap-
pearing in the patch is in contact or not. The output of the
contact recognizer is a sequence δj i encoding the contact
states of human joint j at video frame i, i.e. δj i = 1 if joint
j is in contact at frame i and zero otherwise. Note that δj i
is the discretized version of the contact state trajectory δj
presented in Sec. 4.
Our contact recognition CNNs are built by replacing the
last layer of an ImageNet pre-trained Resnet model [31]
with a fully connected layer that has a binary output. We
have trained separate models for ﬁve types of joints: hands,
knees, foot soles, toes, and neck. To construct the training
data, we collect still images of people manipulating tools
using Google image search. We also collect short video
clips of people manipulating tools from Youtube in order to
also have non-contact examples. We run Openpose pose es-
timator on this data, crop patches around the 2D joints, and
annotate the resulting dataset with contact states.

Estimating 2D object pose. The objective is to estimate
the 2D position of the manipulated object in each video
frame. To achieve this, we build on instance segmentation
obtained by Mask R-CNN [30]. We train the network on
shapes of object models from different viewpoints and ap-
ply the trained network on the test videos. The output masks
and bounding boxes are used to estimate object endpoints in
each frame. The resulting 2D endpoints are used as input to
the trajectory optimizer. Details are given next.
In the case of barbell, hammer and scythe, we created a
single 3D model for each tool, roughly approximating the
shapes of the instances in the videos, and rendered it from
multiple viewpoints using a perspective camera. For spade,
we annotated 2D masks of various instances of the tool in
thirteen different still images. The shapes of the rendered
3D models or 2D masks are used to train Mask R-CNN for
instance segmentation of each tool. The training set is aug-

8645

mented by 2D geometric transformations (translation, rota-
tion, scale) to handle the changes in shapes of tool instances
in the videos. In addition, domain randomization [42, 63]
is applied to handle the variance of instances and changes
in appearance in the videos caused by illumination: the ge-
ometrically transformed shape is ﬁlled with pixels from a
random image (foreground) and pasted on another random
image (background). We utilized random images from the
MS COCO dataset [41] for this purpose. We use a Mask
R-CNN (implementation [3]) model pre-trained on the MS
COCO dataset and re-train the head layers for each tool.

At test time, masks and bounding boxes obtained by the
re-trained Mask R-CNN are used to estimate the coordi-
nates of tool endpoints. Proximity to coordinates of esti-
mated wrist joints is used to select the mask and bound-
ing box in case multiple candidates are available in the
frame. To estimate the main axis of the object, a line is
ﬁtted through the output binary mask. The endpoints are
calculated as the intersection of the ﬁtted line and bound-
aries of the bounding box. Using the combination of the
output mask and the bounding box compensates for errors
in the segmentation mask caused by occlusions. The rela-
tive orientation of the tool (i.e. the head vs. the handle of
the tool) is determined by spatial location of endpoints in
the video frames as well as by their proximity to the esti-
mated wrist joints.

6. Experiments

In this section we present quantitative and qualitative
evaluation of the reconstructed 3D person-object interac-
tions. Since we recover not only human poses but also ob-
ject poses and contact forces, evaluating our results is dif-
ﬁcult due to the lack of ground truth forces and 3D object
poses in standard 3D pose benchmarks such as [35]. Conse-
quently, we evaluate our motion and force estimation quan-
titatively on a recent Biomechanics video/MoCap dataset
capturing challenging dynamic parkour motions [45].
In
addition, we report joint errors and show qualitative results
on our newly collected dataset of videos depicting handtool
manipulation actions.

6.1. Parkour dataset

This dataset contains videos capturing human subjects
performing four typical parkour actions:
two-hand jump,
moving-up, pull-up and a single-hand hop. These are highly
dynamic motions with rich contact interactions with the en-
vironment. The ground truth 3D motion and contact forces
are captured with a Vicon motion capture system and sev-
eral force plates. The 3D motion and forces are recon-
structed with frame rates of 400Hz and 2200Hz, respec-
tively, whereas the RGB videos are captured in a relatively
lower rate of 25Hz, making this dataset a challenge for com-
puter vision algorithms due to motion blur.

Method
SMPLify [9]
HMR [37]
Ours

Jump Move-up
121.75
147.41
111.36
140.16
98.42
125.21

Pull-up
120.48
132.44
119.92

Hop
169.36
149.64
138.45

Avg
139.69
135.65
122.11

Table 1: Mean per joint position error (in mm) of the recovered
3D motion for each action on the Parkour dataset.

Force (N)
Moment (N·m)

L. Sole R. Sole
144.23
138.21
23.71
22.32

L. Hand R. Hand
107.91
113.42
131.13
134.21

Table 2: Estimation errors of the contact forces exerted on soles
and hands on the Parkour dataset.

Evaluation set-up. We evaluate both the estimated hu-
man 3D motion and the contact forces. For evaluating the
accuracy of the recovered 3D human poses, we follow the
common approach of computing the mean per joint posi-
tion error (MPJPE) of the estimated 3D pose with respect
to the ground truth after rigid alignment [27]. We evalu-
ate contact forces without any alignment: we express both
the estimated and the ground truth 6D forces at the position
of the contact aligned with the world coordinate frame pro-
vided in the dataset. We split the 6D forces into linear and
moment components and report the average Euclidean dis-
tance of the linear force and the moment with respect to the
ground truth.

Results. We report joint errors for different actions in Ta-
ble 1 and compare the results with the HMR 3D human pose
estimator [37], which is used to warm-start our method. To
make it a fair comparison, we use the same Openpose 2D
joints as input. In addition, we evaluate the recent SMPLify
[9] 3D pose estimation method. Our method outperforms
both baselines by more than 10mm on average on this chal-
lenging data. Finally, Table 2 summarizes the force esti-
mation results. To estimate the forces we assume a generic
human physical model of mass 74.6 kg for all the subjects.
Despite the systematic error due to the generic human mass
assumption, the results in Table 2 validate the quality of our
force estimation at the soles and the hands during walking
and jumping. We observe higher errors of the estimated mo-
ments at hands, which we believe is due to the challenging
nature of the Parkour sequences where the entire person’s
body is often supported by hands.
In this case, the hand
may exert signiﬁcant force and torque to support the body,
and a minor shift in the force direction may lead to signiﬁ-
cant errors.

6.2. Handtool dataset

In addition to the Parkour data captured in a controlled
set-up, we would like to demonstrate generalization of our
approach to the “in the wild” Internet instructional videos.
For this purpose, we have collected a new dataset of ob-
ject manipulation videos, which we refer to as the Handtool
dataset. The dataset contains videos of people manipulat-
ing four types of tools: barbell, hammer, scythe, and spade.
For each type of tool, we chose among the top videos re-

8646

Figure 3: Example qualitative results on the Handtool dataset. Each example shows the input frame (left) and two different views of the
output 3D pose of the person and the object (middle, right). The yellow and the white arrows in the output show the contact forces and
moments, respectively. Note how the proposed approach recovers from these challenging unconstrained videos the 3D conﬁguration of the
person-object interaction together with the contact forces and moments. For additional video results please see the project webpage [2].

turned by YouTube ﬁve videos covering a range of actions.
We then cropped short clips from each video showing the
whole human body and the tool.

Evaluation of 3D human poses. For each video in the
Handtool dataset, we have annotated the 3D positions of
the person’s left and right shoulders, elbows, wrist, hips,
knees, and ankles, for the ﬁrst, the middle, and the last
frame. We evaluate the accuracy of the recovered 3D hu-
man poses by computing their MPJPE after rigid alignment.
Quantitative evaluation of the recovered 3D poses is shown
table 3. On average, our method outperforms the strong
HMR [37] and SMPLify [9] baselines. However, the dif-
ferences between the methods are reaching the limits of the
accuracy of the manually provided 3D human pose annota-
tions on this dataset. Videos available at project website [2]
demonstrate that our model produces smooth 3D motion,
respecting person-object contacts and capturing the interac-
tion of the person with the tool. This is not the case for the
HMR [37] and SMPLify [9] baselines that are applied to in-
dividual frames and do not model the interaction between
the person and the tool. Example results for our method are
shown in Figure 3. For additional results including exam-
ples of the main failure modes please see the appendix in
the extended version of this paper available at [40].

Evaluation of 2D object poses. To evaluate the quality
of estimated object poses, we manually annotated 2D ob-
ject endpoints in every 5th frame of each video in the Hand-
tool dataset and calculated the 2D Euclidean distance (in
pixels) between each manually annotated endpoint and its
estimated 2D location provided by our method. The 2D lo-
cation is obtained by projecting the estimated 3D tool posi-
tion back to the image plane. We compare our results to the
output of the Mask R-CNN instance segmentation baseline
[30] (which provides initialization for our person-object in-
teraction model). In Table 4 we report for both methods the
percentage of endpoints for which the estimated endpoint
location lies within 25, 50, and 100 pixels from the anno-

Method
SMPLify [9]
HMR [37]
Ours

Barbell
130.69
105.04
104.23

Spade
135.03
97.18
95.21

Hammer
93.43
96.34
95.87

Scythe
112.93
115.42
114.22

Avg
118.02
103.49
102.02

Table 3: Mean per joint position error (in mm) of the recovered
3D human poses for each tool type on the Handtool dataset.

Method
Mask R-CNN [30]
Ours

Barbell
33/42/54
38/71/98

Spade
54/79/93
57/86/99

Hammer
35/44/45
61/91/99

Scythe
63/72/76
69/88/98

Table 4: The percentage of endpoints for which the estimated 2D
location lies within 25/50/100 pixels (in 600×400 pixel image)
from the manually annotated ground truth location.

tated ground truth endpoint location. The results demon-
strate that our approach provides more accurate and stable
object endpoint locations compared to the Mask R-CNN
baseline thanks to modeling the interaction between the ob-
ject and the person.

7. Conclusion

We have developed a visual recognition system that takes
as input video frames together with a simple object model,
and outputs a 3D motion of the person and the object includ-
ing contact forces and torques actuated by the human limbs.
We have validated our approach on a recent MoCap dataset
with ground truth contact forces. Finally, we have collected
a new dataset of unconstrained instructional videos depict-
ing people manipulating different objects and have demon-
strated beneﬁts of our approach on this data. Our work
opens up the possibility of large-scale learning of human-
object interactions from Internet instructional videos [6].

Acknowledgments. We warmly thank Bruno Watier (Universit ´e
Paul Sabatier and LAAS-CNRS) and Galo Maldonado (ENSAM Paris-

Tech) for setting up the Parkour dataset. This work was partly sup-

ported by the ERC grant LEAP (No. 336845),

the H2020 Memmo

project, CIFAR Learning in Machines&Brains program, and the Euro-

pean Regional Development Fund under the project IMPACT (reg. no.

CZ.02.1.01/0.0/0.0/15 003/0000468).

8647

References

[1] CMU Graphics Lab Motion Capture Database. http://
mocap.cs.cmu.edu.

[2] Project
webpage
(code/dataset).
//www.di.ens.fr/willow/research/
motionforcesfromvideo/.

https:

[3] W. Abdulla. Mask R-CNN for object detection and in-
stance segmentation on Keras and TensorFlow. https:
//github.com/matterport/Mask_RCNN, 2017.

[4] S. Agarwal, K. Mierle, and Others. Ceres solver. http:
//ceres-solver.org.

[5] I. Akhter and M. J. Black. Pose-conditioned joint angle lim-
its for 3d human pose reconstruction. In CVPR, 2015.

[6] J.-B. Alayrac, P. Bojanowski, N. Agrawal, I. Laptev, J. Sivic,
and S. Lacoste-Julien. Unsupervised learning from narrated
instruction videos. In CVPR, 2016.

[7] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2d
human pose estimation: New benchmark and state of the art
analysis. In CVPR, 2014.

[8] L. T. Biegler. Nonlinear programming: concepts, algo-
rithms, and applications to chemical processes, volume 10,
chapter 10. Siam, 2010.

[9] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero,
and M. J. Black. Keep it smpl: Automatic estimation of 3d
human pose and shape from a single image. In ECCV, 2016.

[10] R. Boulic, N. M. Thalmann, and D. Thalmann. A global hu-
man walking model with real-time kinematic personiﬁcation.
The Visual Computer, 6(6):344–358, Nov 1990.

[11] E. Brachmann, F. Michel, A. Krull, M. Ying Yang,
S. Gumhold, et al. Uncertainty-driven 6d pose estimation
of objects and scenes from a single rgb image.
In CVPR,
2016.

[12] M. A. Brubaker, D. J. Fleet, and A. Hertzmann. Physics-
based person tracking using simpliﬁed lower-body dynam-
ics. In CVPR, 2007.

[13] M. A. Brubaker, L. Sigal, and D. J. Fleet. Estimating contact
dynamics. In CVPR, 2009.

[14] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-
person 2d pose estimation using part afﬁnity ﬁelds. In CVPR,
2017.

[15] J. Carpentier and N. Mansard. Analytical derivatives of rigid
body dynamics algorithms. In Robotics: Science and Sys-
tems (RSS 2018), 2018.

[16] J. Carpentier and N. Mansard. Multi-contact locomotion of
legged robots. IEEE Transactions on Robotics, 2018.

[17] J. Carpentier, G. Saurel, G. Buondonno, J. Mirabel, F. Lami-
raux, O. Stasse, and N. Mansard. The Pinocchio C++
library–A fast and ﬂexible implementation of rigid body dy-
namics algorithms and their analytical derivatives. In Inter-
national Symposium on System Integrations, 2019.

[18] J. Carpentier, F. Valenza, N. Mansard, et al.
Pinoc-
chio: fast forward and inverse dynamics for poly-articulated
systems. https://stack-of-tasks.github.io/
pinocchio, 2015–2017.

[19] C.-H. Chen and D. Ramanan. 3d human pose estimation= 2d
pose estimation+ matching. In CVPR, 2017.

[20] V. Delaitre, J. Sivic, and I. Laptev. Learning person-object
interactions for action recognition in still images. In NIPS,
2011.
[21] M. Diehl, H. Bock, H. Diedam, and P.-B. Wieber. Fast Direct
Multiple Shooting Algorithms for Optimal Robot Control. In
Fast Motions in Biomechanics and Robotics. 2006.
[22] A. Doumanoglou, R. Kouskouridas, S. Malassiotis, and T.-
K. Kim. 6d object detection and next-best-view prediction in
the crowd. In CVPR, 2016.
[23] R. Featherstone. Rigid body dynamics algorithms. Springer,
2008.
[24] D. F. Fouhey, V. Delaitre, A. Gupta, A. A. Efros, I. Laptev,
and J. Sivic. People watching: Human actions as a cue for
single view geometry. IJCV, 110(3):259–274, 2014.
[25] J. Gall, B. Rosenhahn, T. Brox, and H.-P. Seidel. Optimiza-
tion and ﬁltering for human motion capture.
IJCV, 87(1-
2):75, 2010.
[26] S. Gammeter, A. Ess, T. J ¨aggli, K. Schindler, B. Leibe, and
L. Van Gool. Articulated multi-body tracking under egomo-
tion. In ECCV, 2008.
[27] J. C. Gower. Generalized procrustes analysis. Psychome-
trika, 40(1):33–51, 1975.
[28] A. Grabner, P. M. Roth, and V. Lepetit. 3D Pose Estimation
and 3D Model Retrieval for Objects in the Wild. In CVPR,
2018.
[29] A. Gupta, A. Kembhavi, and L. S. Davis. Observing human-
object interactions: Using spatial and functional compatibil-
ity for recognition. PAMI, 31(10):1775–1789, 2009.
[30] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick. Mask
R-CNN. CoRR, abs/1703.06870, 2017.
[31] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, 2016.
[32] A. Herdt, N. Perrin, and P.-B. Wieber. Walking without
thinking about it. In International Conference on Intelligent
Robots and Systems (IROS), 2010.
[33] S. Hinterstoisser, V. Lepetit, N. Rajkumar, and K. Konolige.
Going further with point pair features. In ECCV, 2016.
[34] E. Insafutdinov, L. Pishchulin, B. Andres, M. Andriluka, and
B. Schiele. Deepercut: A deeper, stronger, and faster multi-
person pose estimation model. In ECCV, 2016.
[35] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Hu-
man3.6m: Large scale datasets and predictive methods for 3d
human sensing in natural environments. PAMI, 36(7):1325–
1339, jul 2014.
[36] Y. Jiang, H. Koppula, and A. Saxena. Hallucinated humans
as the hidden context for labeling 3d scenes. In CVPR, 2013.
[37] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
to-end recovery of human shape and pose. In CVPR, 2018.
[38] J. Kuffner, K. Nishiwaki, S. Kagami, M. Inaba, and H. Inoue.
Motion planning for humanoid robots. In Robotics Research.
The Eleventh International Symposium, 2005.
[39] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox. DeepIM: Deep
Iterative Matching for 6D Pose Estimation. In ECCV, 2018.
[40] Z. Li, J. Sedlar, J. Carpentier, I. Laptev, N. Mansard, and
J. Sivic.
Estimating 3D motion and forces of person-
object interactions from monocular video. arXiv preprint
arXiv:1904.02683, 2019.

8648

[59] H. Sidenbladh, M. J. Black, and D. J. Fleet. Stochastic track-
ing of 3d human ﬁgures using 2d image motion. In ECCV,
2000.
[60] Y. Tassa, T. Erez, and E. Todorov. Synthesis and stabilization
of complex behaviors through online trajectory optimization.
In IEEE International Conference on Intelligent Robots and
Systems (IROS), 2012.
[61] A. Tejani, D. Tang, R. Kouskouridas, and T.-K. Kim. Latent-
class hough forests for 3d object detection and pose estima-
tion. In ECCV, 2014.
[62] B. Tekin, A. Rozantsev, V. Lepetit, and P. Fua. Direct predic-
tion of 3d body poses from motion compensated sequences.
In CVPR, 2016.
[63] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and
P. Abbeel. Domain randomization for transferring deep
neural networks from simulation to the real world. CoRR,
abs/1703.06907, 2017.
[64] S. Tonneau, A. Del Prete, J. Pettr ´e, C. Park, D. Manocha,
and N. Mansard. An Efﬁcient Acyclic Contact Planner for
Multiped Robots.
IEEE Transactions on Robotics (TRO),
2018.
[65] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgib-
bon. Bundle adjustmenta modern synthesis. In International
workshop on vision algorithms, 1999.
[66] X. Wei and J. Chai. Videomocap: Modeling physically real-
istic human motion from monocular video sequences. ACM
Trans. Graph., 29(4):42:1–42:10, July 2010.
[67] E. R. Westervelt, J. W. Grizzle, and D. E. Koditschek. Hybrid
zero dynamics of planar biped walkers. 2003.
[68] A. W. Winkler, C. D. Bellicoso, M. Hutter, and J. Buchli.
Gait and trajectory optimization for legged systems through
phase-based end-effector parameterization.
IEEE Robotics
and Automation Letters, 3(3):1560–1567, 2018.
[69] Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox. Posecnn:
A convolutional neural network for 6d object pose estimation
in cluttered scenes. CoRR, abs/1711.00199, 2017.
[70] B. Yao and L. Fei-Fei. Recognizing human-object interac-
tions in still images by modeling the mutual context of ob-
jects and human poses. PAMI, 34(9):1691–1703, 2012.
[71] X. Zhou, M. Zhu, S. Leonardos, K. G. Derpanis, and
K. Daniilidis. Sparseness meets deepness: 3d human pose
estimation from monocular video. In CVPR, 2016.

[41] T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B.
Girshick, J. Hays, P. Perona, D. Ramanan, P. Doll ´ar, and
C. L. Zitnick. Microsoft COCO: common objects in context.
CoRR, abs/1405.0312, 2014.

[42] V. Loing, R. Marlet, and M. Aubry. Virtual training for a
real application: Accurate object-robot relative localization
without calibration. IJCV, Jun 2018.

[43] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J.
Black. Smpl: A skinned multi-person linear model. ACM
Transactions on Graphics (TOG), 34(6):248, 2015.

[44] M. M. Loper, N. Mahmood, and M. J. Black. MoSh: Motion
and shape capture from sparse markers. ACM Transactions
on Graphics, (Proc. SIGGRAPH Asia), 33(6):220:1–220:13,
Nov. 2014.
[45] G. Maldonado, F. Bailly, P. Sou `eres, and B. Watier. An-
gular momentum regulation strategies for highly dynamic
landing in Parkour. Computer Methods in Biomechanics and
Biomedical Engineering, 20(sup1):123–124, 2017.

[46] J. Malmaud, J. Huang, V. Rathod, N. Johnston, A. Rabi-
novich, and K. Murphy. What’s cookin’? interpreting cook-
ing videos using text, speech and vision. arXiv preprint
arXiv:1503.01558, 2015.

[47] J. Martinez, R. Hossain, J. Romero, and J. J. Little. A sim-
ple yet effective baseline for 3d human pose estimation. In
ICCV, 2017.
[48] I. Mordatch, E. Todorov, and Z. Popovi ´c. Discovery of com-
plex behaviors through contact-invariant optimization. ACM
Transactions on Graphics (TOG), 31(4):43, 2012.

[49] F. Moreno-Noguer. 3d human pose estimation from a single
image via distance matrix regression. In CVPR, 2017.

[50] A. Newell, Z. Huang, and J. Deng. Associative embedding:
End-to-end learning for joint detection and grouping.
In
NIPS, 2017.

[51] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In ECCV, 2016.

[52] M. Oberweger, M. Rad, and V. Lepetit. Making Deep
Heatmaps Robust to Partial Occlusions for 3D Object Pose
Estimation. In ECCV, 2018.

[53] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis.
Coarse-to-ﬁne volumetric prediction for single-image 3d hu-
man pose. In CVPR, 2017.

[54] M. Posa, C. Cantu, and R. Tedrake. A direct method for tra-
jectory optimization of rigid bodies through contact. The
International Journal of Robotics Research, 33(1):69–81,
2014.

[55] A. Prest, V. Ferrari, and C. Schmid. Explicit modeling
of human-object interactions in realistic videos. PAMI,
35(4):835–848, 2013.

[56] M. Rad and V. Lepetit. Bb8: A scalable, accurate, robust
to partial occlusion method for predicting the 3d poses of
challenging objects without using depth. In ICCV, 2017.

[57] M. Rad, M. Oberweger, and V. Lepetit. Feature Mapping for
Learning Fast and Accurate 3D Pose Inference from Syn-
thetic Images. In CVPR, 2018.

[58] G. Schultz and K. Mombaur. Modeling and optimal control
of human-like running. IEEE/ASME Transactions on mecha-
tronics, 15(5):783–792, 2010.

8649

FilterReg: Robust and Efﬁcient Probabilistic Point-Set Registration
using Gaussian Filter and Twist Parameterization

Wei Gao
Massachusetts Institute of Technology

Russ Tedrake
Massachusetts Institute of Technology

weigao@mit.edu

russt@mit.edu

Abstract

Probabilistic point-set registration methods have been
gaining more attention for their robustness to noise, out-
liers and occlusions. However, these methods tend to be
much slower than the popular iterative closest point (ICP)
algorithms, which severely limits their usability. In this pa-
per, we contribute a novel probabilistic registration method
that achieves state-of-the-art robustness as well as substan-
tially faster computational performance than modern ICP
implementations. This is achieved using a rigorous yet
computationally-efﬁcient probabilistic formulation. Point-
set registration is cast as a maximum likelihood estimation
and solved using the EM algorithm. We show that with a
simple augmentation, the E step can be formulated as a ﬁl-
tering problem, allowing us to leverage advances in efﬁcient
Gaussian ﬁltering methods. We also propose a customized
permutohedral ﬁlter [1] for improved efﬁciency while re-
taining sufﬁcient accuracy for our task. Additionally, we
present a simple and efﬁcient twist parameterization that
generalizes our method to the registration of articulated and
deformable objects. For articulated objects, the complexity
of our method is almost independent of the Degrees Of Free-
dom (DOFs). The results demonstrate the proposed method
consistently outperforms many competitive baselines on a
variety of registration tasks. The video demo and source
code are available on our project page.

1. Introduction

Point-set registration is the task of aligning two point
clouds by estimating their relative transformation. This
problem is an essential component for many practical vision
systems, such as SLAM [27], object pose estimation [20],
dense 3d reconstruction [40], and interactive tracking of ar-
ticulated [24] and deformable [12] objects.
The ICP [3] algorithm is the most widely used method
for this task. ICP alternatively establishes nearest-neighbor
correspondences and minimizes the point-pair distances.

With spatial indices such as the KD-tree, ICP provides rel-
atively fast performance. The literature contains many vari-
ants of the ICP algorithm; [28] and [29] provide a thorough
review and comparison.
Despite its popularity, the ICP algorithm is suscepti-
ble to noise, outliers and occlusions. These limitations
have been widely documented in the literature [9, 25, 15].
Thus, a great deal of research has been done on the use of
probabilistic models for point-set registration [25, 14, 13],
which can in principle provide better outlier-rejection. Ad-
ditionally,
if each point
is given a Gaussian variance,
the point cloud can be interpreted as a Gaussian Mixture
Model (GMM). Most statistical registration methods are
built on the GMM and empirically provide improved ro-
bustness [25, 15, 10]. However, these methods tend to be
much slower than the ICP and can hardly scale to large point
clouds, which severely limits their practical usability.
In this paper, we present a novel probabilistic registra-
tion algorithm that achieves state-of-the-art robustness as
well as substantially faster computational performance than
modern ICP implementations. To achieve it, we propose
a computationally-efﬁcient probabilistic model and cast the
registration as a maximum likelihood estimation, which can
be solved using the EM algorithm. With a simple augmen-
tation, we formulate the E step as a ﬁltering problem and
solve it using advances in efﬁcient Gaussian ﬁlters [1, 5, 2].
We also present a customized permutohedral ﬁlter [1] with
improved efﬁciency while retaining sufﬁcient accuracy for
our task. Empirically our method is as robust as state-of-
the-art GMM-based methods, such as [25]. In terms of the
speed, our method with CPU is 3-7 times faster than modern
ICP implementations and orders of magnitude faster than
typical robust GMM-based methods. Furthermore, the pro-
posed method can be GPU-parallelized and is 7 times faster
than the CPU implementation.
Additionally, we propose a simple and efﬁcient twist pa-
rameterization that extends our method to articulated and
node-graph [17] deformable objects. Our method is easy
to implement and achieves substantial speedup over direct
parameterization. For articulated objects, the complexity

111095

of our method is almost independent of the DOFs, which
makes it highly efﬁcient even for high-DOF systems. Com-
bining these components, we present a robust, efﬁcient and
general registration method that outperforms many compet-
itive baselines on a variety of registration tasks. The video
demo, supplemental document and source code are avail-
able on our project page.

2. Related Work

The problem of point set registration is extensively pur-
sued in computer vision and an exhaustive review is pro-
hibitive.
In the following text, we limit our discussion
to GMM-based probabilistic registration and review them
roughly according to their underlying probabilistic models.
The earliest statistical methods [30, 21, 23, 14] implic-
itly assumed the model points, which is controlled by the
motion parameters (such as the rigid transformation or joint
angles), induce a GMM distribution over the 3d space.
The observation points are independently sampled from this
distribution. Later, several contributions [25, 32, 15] de-
rived the EM procedure rigorously from the aforementioned
probabilistic model. This formulation has also been applied
to the registration of multi-rigid [10], articulated [42, 15]
and deformable [25, 32] objects.
Another type of algorithms is known as the correlation-
based methods [38, 16, 4, 33]. These algorithms treat both
observation points and model points as probabilistic dis-
tributions. The point-cloud registration can be interpreted
as minimizing some distance between distributions, for in-
stance the KL-divergence. To improve the efﬁciency, tech-
niques such as voxelization [33] or Support Vector Ma-
chine [4] are used to create compact GMM representations.
In this paper, we assume that the observation points
induce a probabilistic distribution over the space.
Intu-
itively, the registration is to move the model points to po-
sitions with large posterior probability, subject to kinematic
constraints. This formulation is related to several existing
works [9, 22, 25], and a more technical comparison is pre-
sented in Sec. 3.2. In addition to the formulation, the key
contribution of our work includes the introduction of the
ﬁlter-based correspondence and twist parameterization built
on the probabilistic model, as mentioned in Sec. 1. Combin-
ing these components, the proposed method is general, ro-
bust and efﬁcient that outperform various competitive base-
lines.

3. Probabilistic Model for Registration

3.1. Probabilistic Formulation

the motion parameter θ . Another point set Y is deﬁned as
the observation, which is ﬁxed during the registration.

We are interested in the joint distribution p(X, Y , θ). We
assume given model geometry X , the observation Y is in-
dependent of θ , and the joint distribution p(X, Y , θ) can be
factored as

p(X, Y , θ) ∝ φkinematic (X, θ)φgeometric (X, Y )

(1)

where φgeometric (X, Y ) is the potential function that encodes
the geometric relationship, and the potential φkinematic (X, θ)
encodes the kinematic model. The φkinematic (X, θ) can en-
code hard constraints such as X = X (θ) and/or soft motion
regularizers, for instance the smooth terms in [25, 26] and
the non-penetration term in [32].

We further assume the kinematic model φkinematic (X, θ)
has already captured the dependency within model points
X . Thus, conditioned on the motion parameter θ , the points
in X are independent of each other. The distribution can be
further factored as

p(X, Y , θ) ∝ φkinematic (X, θ)

M

Y

i=1

φgeometric (xi , Y )

(2)

A factor graph representation of our model is shown in
Fig. 1. With these factorization schemes, the conditional
distribution can be written as

p(X, θ |Y ) ∝ φkinematic (X, θ)

M

Y

i=1

φgeometric (xi |Y )

(3)

Following several existing work [9, 25], we let the geomet-
ric distribution of each model point φgeometric (xi |Y ) be a
GMM,

φgeometric (xi |Y ) =

N +1

X

j=1

P (yj )p(xi |yj )

(4)

where p(xi |yj ) = N (xi ; yj , Σxyz ) is the Probability Den-
sity Function (PDF) of the Gaussian distribution, yj is the
Gaussian centroid and Σxyz = diag(σ2
z ) is the diag-
onal covariance matrix. An additional uniform distribution
M is added to account for the noise and out-
liers. Similar to [25], we use equal membership probabili-
ties P (yj ) = 1
N for all GMM components, and introduce a
parameter 0 ≤ w ≤ 1 to represent the ratio of outliers.

p(xi |yN +1 ) = 1

x , σ2
y , σ2

We estimate the motion parameter θ and model points X
by maximizing the following log-likelihood,

L =

M

X

i=1

log(

N +1

X

j=1

P (yj )p(xi (θ)|yj ))

(5)

In this subsection, we present our probabilistic model for
point-set registration. We use X, Y to denote the two point
sets, x1 , x2 , ..xM and y1 , y2 , ..., yN are points in X and Y .
We deﬁne the model X as the point set that is controlled by

here we restrict ourselves to the kinematic model X =
X (θ) and leave the general case to supplemental materi-
als. We use the EM [7] algorithm to solve this optimization.
The EM procedure is

11096

E step: For each xi , compute

M 0
xi = X
M 1
xi = X

yk

yk

N (xi (θold ); yk , Σxyz )

N (xi (θold ); yk , Σxyz )yk

M step: minimize the following objective function

xi

M 0
M 0
xi + c

X

xi

(xi (θ) −

xi

M 1
M 0

xi

)T Σ−1

xyz (xi (θ) −

xi

M 1
M 0

xi

(6)

)

(7)

N

xi

xi

w
1−w

where M 0
and M 1
are computed in the E step (6), c =
M is a constant, and w is the parameter that represents
the ratio of outliers.
The EM procedure is conceptually related to ICP. The
weight-averaged target point (M 1
xi ) replaces the near-
est neighbour in ICP, and each model point is weighted by

xi /M 0

M 0
M 0

Xi

Xi

+c . Intuitively, the averaged target provides robustness
to noise in observation, while the weight for each model
point should reject outliers in the model. Please refer to
supplemental materials for the complete derivation.

3.2. Discussion and Comparison

At a high level, the proposed formulation can be viewed
as an “inverse” of Coherent Point Drift (CPD) [25] and
many similar formulations [10, 32, 15], as shown in Fig. 1.
CPD [25] assumes the observation points are independently
distributed according to a GMM introduced by model
points, while the proposed formulation directly assumes the
observation points induce a GMM over the space. Empiri-
cally, both methods are very robust to noise and outliers and
signiﬁcantly outperform ICP.
On the perspective of computation, the proposed method
is much more simple and efﬁcient than CPD [25] and sim-
ilar formulations [10, 32, 15]. The proposed method only
requires sum over Y (6), while CPD [25] requires sum over
both Y and X . Moreover, if a spatial index is used to per-
form this sum, CPD [25] must rebuild the index every EM
iteration as the model points X are updated. In our formu-
lation, we only need to build the index once if the variance
is ﬁxed during EM iterations, which is sufﬁcient for many
applications [32, 39].
Several existing works [9, 22] also build a GMM rep-
resentation of the observation points. Compared with our
method, they do not explicitly account for the outlier dis-

M 0
M 0

Xi

Xi

tribution and miss the weight

+c . Furthermore, these
methods assume each model point is only correlated with
one or several “nearest” GMM centroids, while conceptu-
ally we assume each model point is correlated with all ob-
servation GMM centroids. Additionally, combined with the
ﬁlter-based correspondence and twist parameterization in
Sec. 4 and Sec. 5, our method tends to be much faster than
these works, as demonstrated by our experiments.

Figure 1. An illustration of the proposed probabilistic model. Top:
at a high level, the proposed formulation assumes the observation
Y introduces a probabilistic distribution, while CPD [25] assumes
the model X introduces a distribution controlled by the motion
parameter θ . Bottom: factor graph representations of both our
formulation and the formulation of CPD [25].

3.3. Several Extensions

The presented probabilistic formulation can be extended
to incorporate many well-established GMM-based registra-
tion techniques. Additionally, these extensions can be ef-
ﬁciently computed in a uniﬁed framework using the ﬁlter-
based E step in Sec. 4 and the twist-based M step in Sec. 5.
We select the optimized variance proposed in [25], feature
correspondence in [32] and point-to-plane residual in [6] as
three practically important examples, although many other
methods can also be integrated in a very similar way.

Features: Currently in the E step (6), only the 3d position
is used to measure the similarity between the model and ob-
servation points. Similar to [32], the E step can be extended
to incorporate features such as normal, SHOT [37], learned
features [31] or their concatenation. The E step for arbitrary
feature is

M 0
xi = X
M 1
xi = X

yk

yk

N (fxi ; fyk , Σf )

N (fxi ; fyk , Σf )yk

(8)

where fxi and fyk are the feature value for point xi and yk ,
Σf is the diagonal covariance for the feature.

Optimized Variance: In our previous formulation, the vari-
ance of Gaussian kernel Σxyz is used as a ﬁxed parameter.
Similar to CPD [25], if Σxyz = diag(σ2 , σ2 , σ2 ), the vari-
ance σ can be interpreted as a decision variable and opti-
mized analytically. Please refer to supplemental materials
for the detailed formula and derivation.
Point-to-Plane Distance: The objective in our M step (7) is
similar to the point-to-point distance in ICP, which doesn’t
capture the planar structure. A simple solution is to com-
pute a normal direction to characterize the local planar

11097

structure in the vicinity of the target (M 1

xi /M 0

xi )

Nxi = (X

yk

N (xi ; yk , Σxyz )Nyk )/M 0

xi

(9)

where Nyk is the normal of the observation point yk . The
objective in the M step is then a point-to-plane error

xi

M 0
M 0
xi + c

X

xi

dot(Nxi , xi (θ) −

xi

M 1
M 0

xi

)2

(10)

4. E Step: Filter-based Correspondence

4.1. General Formulation

In this section, we discuss the method to compute the E
step (6) and several extensions (8 and 9). These speciﬁc E
steps can be written into the following generalized form

G(fxi ) = X

yk

1

2 (fxi

−fyk )2

e−

vyk

(11)

xi

xi

M 0

where vyk generalizes the 3d position yk and the unit weight
in (6, 8) and the normal Nyk in (9). The G(fxi ) generalizes
and M 1
in (6, 8) and the normal Nxi in (9). The
features fxi and fyk generalize 3d positions xi and yk in the
Gaussian PDF N (xi ; yk , Σxyz ). The features fxi and fyk
are normalized to have identity covariance. We also omit
the normalization constant det(2πΣxyz )−
2 of the Guassian
PDF N (xi ; yk , Σxyz ) for notational simplicity.

1

Equ. (11) is known as the general Gaussian Transform
and the Improved Fast Gaussian Transform (IFGT) [41] is
proposed for it. However, IFGT [41] uses a k-means tree
internally and there would be too many k-means centroids
for typical parameters in the registration. Practically, [41]
is not much faster than brute-force evaluation for our task.

We instead propose to compute (11) using Gaussian ﬁl-
tering algorithms [5, 1, 2], which demonstrate promising
accuracy and efﬁciency on image processing. The ﬁltering
operation that these algorithms accelerated is

G(fyi ) = X

yk

1

2 (fyi

−fyk )2

e−

vyk

(12)

which is a subset of the general Gaussian transform:
the
feature fyi used to retrieve the ﬁltered value G(fyi ) must
be included in the input point set Y .

In our case, we would like to retrieve the value G(fxi )
using feature fxi from another point cloud X , which cannot
be directly expressed in (12). To resolve it, we propose the
following augmented input:

Fﬁlter-input = [FX , FY ]
Vﬁlter-input = [0,

(13)

VY ]
where FX = [fx1 , fx2 , ..., fxM ], FY = [fy1 , fy2 , ..., fyN ]

and VY = [vy1 , vy2 , ..., vyN ]. The new input feature
Fﬁlter-input and value Vﬁlter-input are suitable for these ﬁltering

Figure 2. An illustration of the permutohedral lattice ﬁlter [1].
Splat: The input features are interpolated to permutohedral lat-
tice using barycentric weights. Blur: lattice points exchange their
values with nearby lattice points. Slice: The ﬁltered signal is in-
terpolated back onto the input signal.

algorithms [5, 1, 2], and the ﬁltered output is

G(fxi ) = X

zk ∈Fﬁlter-input

1

2 (fxi

−fzk )2

e−

vzk

= X

yk ∈FY

1

2 (fxi

−fyk )2

e−

vyk

(14)

With this augmentation, we can apply these ﬁltering al-
gorithms [1, 2, 5] as a black box to our problem. However,
by exploiting the structure of these methods, we can make
them more efﬁcient for our tasks. In the following text, the
permutohedral lattice ﬁlter [1] is discussed as an example,
which is used in our experiments.

4.2. Permutohedral Lattice Filter

We brieﬂy review the ﬁltering process of [1], an illustra-
tion is shown in Fig. 2. The d-dimension feature f is ﬁrst
embedded in (d + 1)-dimensional space, where the permu-
tohedral lattice lives.
In the embedded space, each input
value v Splats onto the vertices of its enclosing simplex
with barycentric weights. Next, lattice points Blur their val-
ues with nearby lattice points. Finally, the space is Sliced
at each input position using the same barycentric weights to
interpolate output values.
Although the permutohedral ﬁlter [1] has demonstrated
promising performance on a variety of tasks, it is still not
optimal for our problem. In particular, the index building
in [1] can be inefﬁcient when the variance Σxyz is too small.
Additionally, naively apply [1] to the E step (6) requires re-
building the index every EM iteration as the model point X
is updated. To resolve these problems, we propose a cus-
tomization of the permutohedral ﬁlter [1] that is more efﬁ-
cient while retaining sufﬁcient accuracy for our task. The
detailed method is presented in the supplemental material.

5. M Step: Efﬁcient Twist Parameterization

In this section, we present methods to solve the optimiza-
tions (7, 10) with the twist parameterization. We ﬁrst dis-
cuss the twist in the general kinematic model, then special-
ize it to articulated and node-graph deformable objects.

11098

We focus on the following general kinematic model,

xi = Ti (θ)xi reference

(15)
where Ti (θ) ∈ SE (3) is a rigid transformation, xi reference is
the ﬁxed reference point for the xi . Note that Ti (θ) depends
on i and the kinematic model is not necessarily a global
rigid transformation.
Twist is a 6-vector that represents the locally linearized
“change” of SE (3). Let the twist ζi = (wi , ti ) =
(αi , βi , γi , ai , bi , ci ) be the local linearization of Ti , we
have

Algorithm 1 The A matrix for articulated kinematic
1: for all bodyj do
⊲ can be parallelized

2:

3:

4:

J T J twist j = 06×6

for all point xi in bodyj do

⊲ can be parallelized

J T J twist j += ( ∂ rxi
∂ ζi

)T ∂ rxi
∂ ζi

5: A = 0Njoint×Njoint

7:

6: for all bodyj do
⊲ The spatial velocity Jacobian can be computed
⊲ using off-the-shelf simulators such as [36, 34]
Jspatial j = spatial velocity Jacobian of bodyj

8:

9:

T new

i ≈

Ti

(16)

10:

A += J T

spatial j (J T J twist j )Jspatial j




1
γi
−βi
0

−γi
1
αi
0

βi
−αi
1
0

ai
bi
ci
1




∂ ζi

Thus, the Jacobian ∂xi
= [skew(xi ), I3×3 ] is a 3× 6 matrix,
where I3×3 is identity matrix, and skew(xi ) is a 3×3 matrix
such that skew(xi )b = cross(xi , b) for arbitrary b ∈ R3 .
The optimization (7, 10) are least squares problems, and
we focus on the following generalized form of them

rT

xi rxi

X

xi

(17)

where rxi is the concatenated least-squares residuals that
depends on xi . We use the Gauss-Newton (GN) algorithm
to solve (17). In each GN iteration we need to compute the
following A and b matrices by

A = X

xi

(

∂ rxi
∂ θ

)T ∂ rxi
∂ θ

,

b = X

xi

(

∂ rxi
∂ θ

)T (rxi )

(18)

and the update of the motion parameters is ∆θ = −A−1 b.
Thus, the primary computational bottleneck is to assemble
the matrices A and b. In the following text, we only discuss
the computation of the A matrix, while the computation of
the b vector is similar and easier. The computation of the A
matrix can be written as

A = X

xi

(

∂ ζi
∂ θ

)T ((

∂ rxi
∂ ζi

)T ∂ rxi
∂ ζi

)(

∂ ζi
∂ θ

)

(19)

∂ rxi
∂ ζi

= ∂ rxi
∂xi

where ∂ ζi
∂ θ is the Jacobian that maps the change of motion
parameter θ to the change of the rigid transformation Ti ,
while the change of Ti is expressed as its twist. Note that
the term
is very easy to compute, as both
and ∂xi
are only dependent on xi .
If the kinematic model is a global rigid transformation,
we have ∂ ζi
∂ θ = I6×6 and A = Pxi
In
the following subsections, we proceed to the articulated and
node-graph deformable kinematic models.

)T ∂ rxi
∂ ζi

(( ∂ rxi
∂ ζi

∂ rxi
∂xi

∂xi
∂ ζi

∂ ζi

).

5.1. Articulated Model

Articulated objects consist of rigid bodies connected
through joints in a kinematic tree. A broad set of real-world
objects, including human bodies, hands and robots are artic-
ulated objects. If the kinematic model (15) is an articulated

model, the motion parameter θ ∈ RNjoint would be the joint
angles, where Njoint is the number of joints. The Ti (θ) is the
rigid transform of the rigid body that the point xi is attached
to. The computation of the A matrix can be factored as

A = X

bodyj

(

∂ ζj
∂ θ

)T ( X

xi in bodyj

(

∂ rxi
∂ ζi

)T ∂ rxi
∂ ζi

)(

∂ ζj
∂ θ

)

(20)

∂ θ

∂ ζi
∂ ζj

where ζj is the twist of rigid body j , and we have exploited
= I6×6 if point i is on rigid body j . Importantly, ∂ ζj
is known as the spatial velocity Jacobian and is provided by
many off-the-shelf rigid body simulators [34, 36, 19]. The
algorithm that uses (20) is shown in Algorithm 1.
The lines 1-4 of Algorithm 1 dominates the overall per-
formance and the complexity is O(62M ), where M is the
number of model points and usually M ≫ Njoint . Thus,
the complexity of this algorithm is almost independent of
Njoint . As a comparison, previous articulated registration
methods [19, 35] need O(N 2
jointM ) time to assemble the A
matrix, and Njoint is usually much larger than 6. Further-
more, lines 1-4 of Algorithm 1 is very simple to implement
and can be easily GPU parallelized. Combined with an off-
the-shelf simulator, the overall pipeline can achieve promis-
ing efﬁciency. On the contrary, previous methods [19, 35]
typically need a customized kinematic tree implementation
for real-time performance, while requires substantial soft-
ware engineering effort to realize.

5.2. Node(cid:173)Graph Deformable Model

To capture the motion of objects such as rope or cloth,
we need a kinematic model which allows large deforma-
tion while preventing unrealistic collapsing or distortion.
In this paper, we follow [17] to represent the general de-
formable kinematic model as a node graph. Intuitively, the
node graph deﬁnes a motion ﬁeld in the 3D space and the
reference vertex in Equ. (15) is deformed according to the
motion ﬁeld. More speciﬁcally, the node graph is deﬁned as
a set {[pj ∈ R3 , Tj ∈ SE (3)]}, where j is the node index,
pj is the position of the j th node, and Tj is the SE (3) mo-
tion deﬁned on the j th node. The kinematic equation (15)

11099

can be written as

Ti (θ) = normalized(Σk∈Ni (xi reference )wkiTk )

(21)

where Ni (xi reference ) is the nearest neighbor nodes of model
point xi reference , and wki is the ﬁxed skinning weight. The
interpolation of the rigid transformation Tk is performed us-
ing the DualQuaternion [18] representation of the SE (3).
The A matrices for this kinematic model can be con-
structed using an algorithm very similar to Algorithm 1.
The detailed method is provided in supplemental materials.

6. Results

We conduct a variety of experiments to test the robust-
ness, accuracy and efﬁciency of the proposed method. Our
hardware platform is an Intel i7-3960X CPU except for
Sec. 6.5, where the proposed method is implemented with
CUDA on a Nivida Titan Xp GPU. The video demo and the
source code are available on our project page.

Figure 3. A comparison of the robustness of various algorithms
with respect to outliers. Top: (a) shows an example initialization
with 0.2 outlier ratio; (b) and (c) are the ﬁnal alignment by the
proposed method and TrICP [6] initialized from (a), respectively.
Bottom: the alignment error (22) of each algorithm for different
numbers of outliers.

6.1. Robustness Test on Synthetic Data

We follow CPD [25] to setup an experiment on syn-
thetic data. We use a subsampled Stanford bunny with 3500
points. The initial rotation discrepancy is 50 degrees with a
random axis. The proposed method is compared with two
baselines: CPD [25], a representative GMM-based algo-
rithm; TrICP [6], a widely used robust ICP variant. Param-
eters for all methods are well tuned and provided in supple-
mental materials. We use the following metric to measure
the pose estimation error

error(T ) =

1
M

ΣM

i=1 |(T − Tgt )xi reference |2

(22)

where Tgt is the known ground truth pose, xi reference deﬁned
in (15) is the reference position. We terminate the algo-
rithm when the twist (change of transformation) is less than
a threshold. In this way, the ﬁnal alignment error (22) is
about 1 [mm] for all methods. All of the statistical results
are the averaged value of 30 independent runs.
Fig. 3 shows the robustness of different algorithms with
respect to outliers in the point sets. We add different num-
ber of points randomly to both the model and observation
clouds. An example of such point sets with initial align-
ment is shown in Fig. 3 (a), the converged alignment by the
proposed method and TrICP [6] are shown in Fig. 3 (b) and
Fig. 3 (c), respectively. The proposed method and CPD [25]
signiﬁcantly outperform the robust ICP.
Fig. 4 shows the robustness of different algorithms with
respect to noise in the point sets. We corrupt each point in
both model and observation clouds with a Gaussian noise.
The unit of the noise is the diameter of the Bunny. An ex-
ample of such point sets with initial alignment are shown in
Fig. 4 (a). Fig. 4 (b) and (c) are the ﬁnal alignment by the
proposed method and TrICP [6] initialized from (a). Note

time[ms]
per iteration

#iterations

overall
time[ms]

Proposed
ﬁxed σ
Proposed
updated σ
CPD
Robust ICP

0.96

1.16

228
3.10

40.4

27.6

26.8
70.2

38.4

32.1

6110
217.6

Table 1. The performance of different algorithms for the registra-
tion of the Stanford Bunny.

that we use clean point clouds for better visualization. Our
method and CPD [25] are more accurate than the robust ICP.
Table. 6.1 summarizes the computational performance of
each method. The running time is measured on clean point
cloud. Our method is about 7 times faster than TrICP [6]
and two orders of magnitude faster than CPD [25]. The pro-
posed method with ﬁxed σ is faster per iteration, but need
more iterations to converge. Overall the proposed method is
as robust as the state-of-the-art statistical registration algo-
rithm CPD [25], and runs substantially faster than the mod-
ern ICP implementation.

6.2. Rigid Registration on Real(cid:173)World Data

We follow [9] to setup this experiment:
the algorithm
is used to compute the frame-to-frame rigid transformation
on the Stanford Lounge dataset [43]. We register every 5th
frame for the ﬁrst 400 frames, each downsampled to about
5000 points. The average Euler angle deviation from the
ground truth is used as the estimation error.
Fig. 5 (a) shows an example registration by the proposed
method. Fig. 5 (b) shows the accuracy and speed of vari-
ous algorithms. The results of baseline methods are from

11100

Figure 4. A comparison of the robustness of various algorithms
with respect to noise. Top: (a) shows an example initialization
with 0.03 relative noise; (b) and (c) are the ﬁnal alignment by the
proposed method and TrICP [6] initialized from (a). Note that
we use clean point clouds for better visualization. Bottom: the
alignment error (22) of each algorithm for different levels of noise.

Figure 5. Rigid registration on the Stanford Lounge dataset [43].
The results for most baselines are from [9]. (a) shows an example
registration by the proposed method. (b) shows the accuracy and
performance of various algorithms.

[9]1 except for CPD [25]. For CPD [25] we use σinit =
20 [cm] instead of the data-based initialization of [25], with
which we observed improved performance. As the point-to-
point error doesn’t capture the planar structure, the point-
to-point version of the proposed method as well as many
other point-to-point algorithms [25, 22, 3, 6] are less accu-
rate on this dataset. The proposed method with point-to-

1Our CPU (i7-3960X) is slightly inferior to [9] (i7-5920K), and we
observe similar accuracy and slightly worse speed using CPD [25] and
TrICP [6]. Thus, we think our speed result are comparable to [9] despite
hardware difference.

Figure 6. A feature-based global registration under ambiguous
outliers and strong occlusion. (a) shows the initialization of the
registration colored by the feature [11]. (b) and (c) are the feature-
based registration by our method and the baseline. (d) and (e) show
the ﬁnal alignment using 3d local reﬁnement initialized from (b)
and (c). The proposed method converges to the correct pose while
the baseline method is trapped to bad alignment.

plane error achieves state-of-the-art accuracy. On the per-
spective of computation, the proposed method signiﬁcantly
outperforms all the baselines, including GMMTree [9] and
EMICP [14] that rely on a high-end Titan X GPU.

6.3. Global Pose Estimation using Learned Features

We demonstrate global pose estimation using motion-
invariant features. The task is to align a pre-built geomet-
ric model to observation clouds from RGBD images, where
both the model and observation clouds are colored by the
learned feature [11]. We use the proposed method with
feature correspondence in Sec. 3.3 and ﬁxed σ = 0.05 as
the feature has unit norm. The proposed method is com-
pared with a modiﬁed TrICP [6]:
the nearest neighbour
is searched in feature space (instead of 3d-space). After
feature-based registration, we apply 3d-space local reﬁne-
ment to get the ﬁnal alignment.

Fig. 6 shows an example registration. Note that we treat
the background as outliers. As shown in Fig. 6 (a), the
observation (RGBD cloud) is under severe occlusion and
contains very ambiguous outliers. Fig. 6 (b) and (c) show
feature-based registration by our method and the “feature”
TrICP. The proposed method is more robust to the outliers
and occlusion. Fig. 6 (d) and (e) show the ﬁnal alignment
using local reﬁnement initialized from (b) and (c). The pro-
posed method converges to correct pose while the baseline
is trapped to bad alignment. Table. 2 summaries the suc-
cess rate of both methods on 30 RGBD images with differ-
ent view points and lighting conditions. Our method has a
higher success rate and is more efﬁcient than the baseline.

11101

Proposed
Feature ICP

success rate
29/30
25/30

time [ms]
13
34

Table 2. The success rate and speed on the feature-based global
registration.

6.4. Articulated Tracking

The proposed method with articulated kinematic model
is used to track a robot manipulating a box. The robot and
box model has 20 DOFs (12 for the ﬂoating bases of the box
and the robot, 8 for robot joints). We use drake [34] for the
kinematic computation in (20). We use ﬁxed σ = 1 [cm]
and set the maximum EM iterations to be 15. Our template
has 7500 points and the depth cloud has about 10000 points.
Fig. 7 (a) shows the snapshots of the tracked manipula-
tion scenario. Fig. 7 (b) shows the live geometric model
and the observation clouds. Points from observation are in
black, while the geometric model is colored according to
rigid bodies. Note that points from the table are treated as
outliers. Fig. 7 (c) summaries the averaged per-frame per-
formance of various algorithms. The proposed twist param-
eterization is an order of magnitude faster than direct pa-
rameterization. Combining the ﬁlter-based correspondence
and twist parameterization leads to a real-time tracking al-
gorithm and substantial performance improvement over ar-
ticulated ICP and [42].

6.5. Application to Dynamic Reconstruction

The proposed method with node-graph deformable kine-
matic is implemented on GPU and used as the internal non-
rigid tracker of DynamicFusion [26] (our implementation).
The proposed method is compared with the projective ICP,
the original non-rigid tracker of [26]. We use ﬁxed σ = 2
[cm]. Fig. 8 shows both methods operate on a RGBD se-
quence with relative fast motions. The proposed method
tracks it correctly, while the projective ICP fails to track the
right hand of the actor. The proposed method is more robust
to fast and tangential motion than the projective ICP.
To test the efﬁciency of the proposed twist parameteriza-
tion on node-graph deformable objects, we compare it with
Opt [8], a highly optimized GPU least squares solver using
direct parameterization. The per-frame computational per-
formance of various algorithms is summarized in Table. 3.
The GPU parallelization of our ﬁlter-based E step achieves
8 times speedup over the CPU version, and the proposed
twist parameterization is about 20 times faster than [8].

Proposed
(GPU)
7.8
21.6

Proposed
(CPU)
62
Not implemented

Proposed
(Opt [8])
7.8
382

E step [ms]
M step [ms]

Table 3. The per-frame performance of various algorithms for
deformable tracking on the sequence in Fig. 8.

Figure 7. The proposed method is applied to track a robot ma-
nipulating a box. (a): the snapshots of the tracked manipulation
scenario. (b) the observation point clouds (black) and the live ge-
ometric model (colored according to rigid bodies). (c): the per-
frame performance of various algorithms on this dataset. Ye &
Yang stands for our CPU implementation of [42].

Figure 8. The proposed method with node-graph deformable kine-
matic is implemented on GPU and used as the internal non-rigid
tracker of DynamicFusion [26]. For a relative fast motion, the pro-
posed method tracks it correctly while the projective ICP used by
DynamicFusion [26] fails to track the right hand of the actor.

7. Conclusion

To conclude, we present a probabilistic registration
method that achieves state-of-the-art robustness, accuracy
and efﬁciency. We show that the correspondence search
can be formulated as a ﬁltering problem, and employ ad-
vances in efﬁcient Gaussian ﬁltering methods to solve it.
In addition, we present a simple and efﬁcient twist param-
eterization that generalizes our method to articulated and
deformable objects. Extensive empirical evaluation demon-
strates the effectiveness our method.

Acknowledgments This work was supported by NSF Award IIS-1427050

and Amazon Research Award. The views expressed in this paper are those

of the authors themselves and are not endorsed by the funding agencies.

11102

References

[1] A. Adams, J. Baek, and M. A. Davis. Fast high-dimensional
ﬁltering using the permutohedral
lattice.
In Computer
Graphics Forum, volume 29, pages 753–762. Wiley Online
Library, 2010. 1, 4

[2] A. Adams, N. Gelfand, J. Dolson, and M. Levoy. Gaussian
kd-trees for fast high-dimensional ﬁltering. In ACM Trans-
actions on Graphics (ToG), volume 28, page 21. ACM, 2009.
1, 4

[3] P. J. Besl and N. D. McKay. Method for registration of 3-d
shapes. In Sensor Fusion IV: Control Paradigms and Data
Structures, volume 1611, pages 586–607. International Soci-
ety for Optics and Photonics, 1992. 1, 7

[4] D. Campbell and L. Petersson. An adaptive data represen-
tation for robust point-set registration and merging. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 4292–4300, 2015. 2

[5] J. Chen, S. Paris, and F. Durand. Real-time edge-aware im-
age processing with the bilateral grid. In ACM Transactions
on Graphics (TOG), volume 26, page 103. ACM, 2007. 1, 4

[6] D. Chetverikov, D. Stepanov, and P. Krsek. Robust euclidean
alignment of 3d point sets: the trimmed iterative closest point
algorithm.
Image and Vision Computing, 23(3):299–309,
2005. 3, 6, 7

[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum
likelihood from incomplete data via the em algorithm. Jour-
nal of the royal statistical society. Series B (methodological),
pages 1–38, 1977. 2

[8] Z. DeVito, M. Mara, M. Zollh ¨ofer, G. Bernstein, J. Ragan-
Kelley, C. Theobalt, P. Hanrahan, M. Fisher, and M. Nießner.
Opt: A domain speciﬁc language for non-linear least squares
optimization in graphics and imaging. ACM Transactions on
Graphics (TOG), 36(5):171, 2017. 8

[9] B. Eckart, K. Kim, and J. Kautz. Fast and accurate point
cloud registration using trees of gaussian mixtures. arXiv
preprint arXiv:1807.02587, 2018. 1, 2, 3, 6, 7

[10] G. D. Evangelidis, D. Kounades-Bastian, R. Horaud, and
E. Z. Psarakis. A generative model for the joint registration
of multiple point sets. In European Conference on Computer
Vision, pages 109–122. Springer, 2014. 1, 2, 3

[11] P. R. Florence, L. Manuelli, and R. Tedrake. Dense ob-
ject nets: Learning dense visual object descriptors by and
for robotic manipulation. arXiv preprint arXiv:1806.08756,
2018. 7

[12] W. Gao and R. Tedrake.
Surfelwarp: Efﬁcient non-
volumetric single view dynamic reconstruction. 1

[13] S. Gold, A. Rangarajan, C.-P. Lu, S. Pappu, and E. Mjol-
sness. New algorithms for 2d and 3d point matching:
Pose estimation and correspondence. Pattern recognition,
31(8):1019–1031, 1998. 1

[14] S. Granger and X. Pennec. Multi-scale em-icp: A fast and
robust approach for surface registration. In European Con-
ference on Computer Vision, pages 418–432. Springer, 2002.
1, 2, 7

[15] R. Horaud, F. Forbes, M. Yguel, G. Dewaele, and J. Zhang.
Rigid and articulated point registration with expectation con-

ditional maximization. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 33(3):587–602, 2011. 1, 2,
3
[16] B. Jian and B. C. Vemuri. A robust algorithm for point set
registration using mixture of gaussians. In Computer Vision,
2005. ICCV 2005. Tenth IEEE International Conference on,
volume 2, pages 1246–1251. IEEE, 2005. 2
[17] L. Kavan, S. Collins, C. OSullivan, and J. Zara. Dual quater-
nions for rigid transformation blending. 1, 5
[18] L. Kavan, S. Collins, J. ˇZ ´ara, and C. O’Sullivan. Geometric
skinning with approximate dual quaternion blending. ACM
Transactions on Graphics (TOG), 27(4):105, 2008. 6
[19] J. Lee, M. X. Grey, S. Ha, T. Kunz, S. Jain, Y. Ye, S. S. Srini-
vasa, M. Stilman, and C. K. Liu. Dart: Dynamic animation
and robotics toolkit. The Journal of Open Source Software,
3(22):500, 2018. 5
[20] F. Lu and E. Milios. Robot pose estimation in unknown envi-
ronments by matching 2d range scans. Journal of Intelligent
and Robotic systems, 18(3):249–275, 1997. 1
[21] B. Luo and E. R. Hancock. A uniﬁed framework for align-
ment and correspondence. Computer Vision and Image Un-
derstanding, 92(1):26–55, 2003. 2
[22] M. Magnusson. The three-dimensional normal-distributions
transform: an efﬁcient representation for registration, sur-
face analysis, and loop detection. PhD thesis, ¨Orebro uni-
versitet, 2009. 2, 3, 7
[23] G. McNeill and S. Vijayakumar. A probabilistic approach
to robust shape matching. In Image Processing, 2006 IEEE
International Conference on, pages 937–940. IEEE, 2006. 2
[24] L. Mundermann, S. Corazza, and T. P. Andriacchi. Accu-
rately measuring human movement using articulated icp with
soft-joint constraints and a repository of articulated mod-
els.
In Computer Vision and Pattern Recognition, 2007.
CVPR’07. IEEE Conference on, pages 1–6. IEEE, 2007. 1
[25] A. Myronenko and X. Song. Point set registration: Coherent
point drift. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 32(12):2262–2275, Dec 2010. 1, 2, 3, 6,
7
[26] R. A. Newcombe, D. Fox, and S. M. Seitz. Dynamicfusion:
Reconstruction and tracking of non-rigid scenes in real-time.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 343–352, 2015. 2, 8
[27] A. N ¨uchter, K. Lingemann, J. Hertzberg, and H. Surmann.
6d slam3d mapping outdoor environments. Journal of Field
Robotics, 24(8-9):699–722, 2007. 1
[28] F. Pomerleau, F. Colas, R. Siegwart, et al. A review of point
cloud registration algorithms for mobile robotics. Founda-
tions and Trends R(cid:13) in Robotics, 4(1):1–104, 2015. 1
[29] F. Pomerleau, F. Colas, R. Siegwart, and S. Magnenat. Com-
paring ICP Variants on Real-World Data Sets. Autonomous
Robots, 34(3):133–148, Feb. 2013. 1
[30] A. Rangarajan, H. Chui, E. Mjolsness, S. Pappu, L. Davachi,
P. Goldman-Rakic, and J. Duncan. A robust point-matching
algorithm for autoradiograph alignment. Medical image
analysis, 1(4):379–398, 1997. 2
[31] T. Schmidt, R. Newcombe, and D. Fox. Self-supervised
visual descriptor learning for dense correspondence. IEEE
Robotics and Automation Letters, 2(2):420–427, 2017. 3

11103

[32] J. Schulman, A. Lee, J. Ho, and P. Abbeel. Tracking de-
formable objects with point clouds. In Robotics and Automa-
tion (ICRA), 2013 IEEE International Conference on, pages
1130–1137. IEEE, 2013. 2, 3
[33] T. Stoyanov, M. Magnusson, and A. J. Lilienthal. Point set
registration through minimization of the l2 distance between
3d-ndt models. 2
[34] R. Tedrake and the Drake Development Team. Drake: A
planning, control, and analysis toolbox for nonlinear dynam-
ical systems, 2016. 5, 8
[35] A. Tkach, M. Pauly, and A. Tagliasacchi. Sphere-meshes for
real-time hand modeling and tracking. ACM Transactions on
Graphics (TOG), 35(6):222, 2016. 5
[36] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine
for model-based control. In Intelligent Robots and Systems
(IROS), 2012 IEEE/RSJ International Conference on, pages
5026–5033. IEEE, 2012. 5
[37] F. Tombari, S. Salti, and L. Di Stefano. Unique signatures of
histograms for local surface description. In European con-
ference on computer vision, pages 356–369. Springer, 2010.
3
[38] Y. Tsin and T. Kanade. A correlation-based approach to ro-
bust point set registration. In European conference on com-
puter vision, pages 558–569. Springer, 2004. 2
[39] B. Wang, L. Wu, K. Yin, U. Ascher, L. Liu, and H. Huang.
Deformation capture and modeling of soft objects. ACM
Transactions on Graphics (TOG), 34(4):94, 2015. 3
[40] T. Whelan, R. F. Salas-Moreno, B. Glocker, A. J. Davison,
and S. Leutenegger. Elasticfusion: Real-time dense slam
and light source estimation. The International Journal of
Robotics Research, 35(14):1697–1716, 2016. 1
[41] C. Yang, R. Duraiswami, N. A. Gumerov, and L. Davis. Im-
proved fast gauss transform and efﬁcient kernel density esti-
mation. In null, page 464. IEEE, 2003. 4
[42] M. Ye and R. Yang. Real-time simultaneous pose and shape
estimation for articulated objects using a single depth cam-
era.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2345–2352, 2014. 2,
8
[43] Q.-Y. Zhou and V. Koltun. Dense scene reconstruction with
points of interest. ACM Transactions on Graphics (ToG),
32(4):112, 2013. 6, 7

11104

GA-Net: Guided Aggregation Net for End-to-end Stereo Matching

Feihu Zhang1∗
Victor Prisacariu1
1 University of Oxford

Ruigang Yang2
Philip H.S. Torr1
2 Baidu Research, Baidu Inc.

Abstract

In the stereo matching task, matching cost aggregation is
crucial in both traditional methods and deep neural network
models in order to accurately estimate disparities. We pro-
pose two novel neural net layers, aimed at capturing local
and the whole-image cost dependencies respectively. The
ﬁrst is a semi-global aggregation layer which is a differen-
tiable approximation of the semi-global matching, the sec-
ond is the local guided aggregation layer which follows a
traditional cost ﬁltering strategy to reﬁne thin structures.
These two layers can be used to replace the widely
used 3D convolutional
layer which is computationally
costly and memory-consuming as it has cubic computa-
tional/memory complexity.
In the experiments, we show
that nets with a two-layer guided aggregation block eas-
ily outperform the state-of-the-art GC-Net which has nine-
teen 3D convolutional
layers. We also train a deep
guided aggregation network (GA-Net) which gets better
accuracies than state-of-the-art methods on both Scene
Flow dataset and KITTI benchmarks. Code is available at

https://github.com/feihuzhang/GANet.

1. Introduction

Stereo reconstruction is a major research topic in com-
puter vision, robotics and autonomous driving. It aims to
estimate 3D geometry by computing disparities between
matching pixels in a stereo image pair.
It is challenging
due to a variety of real-world problems, such as occlusions,
large textureless areas (e.g. sky, walls etc.), reﬂective sur-
faces (e.g. windows), thin structures and repetitive textures.
Traditionally, stereo reconstruction is decomposed into
three important steps: feature extraction (for matching cost
computation), matching cost aggregation and disparity pre-
diction [9, 21]. Feature-based matching is often ambiguous,
with wrong matches having a lower cost than the correct
ones, due to occlusions, smoothness, reﬂections, noise etc.
Therefore, cost aggregation is a key step needed to obtain
accurate disparity estimations in challenging regions.
Deep neural networks have been used for matching cost

∗ Part of the work was done when working in Baidu Research.

(a) Input image

(b) GC-Net [13]

(c) Our GA-Net-2

(d) Ground truth

Figure 1: Performance illustrations. (a) a challenging input im-
age. (b) Result of the state-of-the-art method GC-Net [13] which
has nineteen 3D convolutional layers for matching cost aggrega-
tion. (c) Result of our GA-Net-2, which only uses two proposed
GA layers and two 3D convolutional layers.
It aggregates the
matching information into the large textureless region and is an
order of magnitude faster than GC-Net. (d) Ground truth.

computation in, e.g, [30, 33], with (i) cost aggregation based
on traditional approaches, such as cost ﬁltering [10] and
semi-global matching (SGM) [9] and (ii) disparity com-
putation with a separate step. Such methods considerably
improve over traditional pixel matching, but still struggle
to produce accurate disparity results in textureless, reﬂec-
tive and occluded regions. End-to-end approaches that link
matching with disparity estimation were developed in e.g.
DispNet [15], but it was not until GC-Net [13] that cost ag-
gregation, through the use of 3D convolutions, was incorpo-
rated in the training pipeline. The more recent work of [3],
PSMNet, further improves accuracy by implementing the
stacked hourglass backbone [17] and considerably increas-
ing the number of 3D convolutional layers for cost aggrega-
tion. The large memory and computation cost incurred by
using 3D convolutions is reduced by down-sampling and
up-sampling frequently, but this leads to a loss of precision
in the disparity map.
Among these approaches, traditional semi-global match-
ing (SGM) [9] and cost ﬁltering [10] are all robust and ef-
ﬁcient cost aggregation methods which have been widely

1185

used in many industrial products. But, they are not differen-
tiable and cannot be easily trained in an end-to-end manner.
In this work, we propose two novel cost aggregation lay-
ers for end-to-end stereo reconstruction to replace the use of
3D convolutions. Our solution considerably increases accu-
racy, while decreasing both memory and computation costs.
First, we introduce a semi-global guided aggregation
layer (SGA) which implements a differentiable approxima-
tion of semi-global matching (SGM) [9] and aggregates the
matching cost in different directions over the whole image.
This enables accurate estimations in occluded regions or
large textureless/reﬂective regions.
Second, we introduce a local guided aggregation layer
(LGA) to cope with thin structures and object edges in order
to recover the loss of details caused by down-sampling and
up-sampling layers.
As illustrated in Fig. 1, a cost aggregation block with
only two GA layers and two 3D convolutional layers eas-
ily outperforms the state-of-the-art GC-Net [13], which has
nineteen 3D convolutional layers. More importantly, one
GA layer has only 1/100 computational complexity in terms
of FLOPs (ﬂoating-point operations) as that of a 3D convo-
lution. This allows us to build a real-time GA-Net model,
which achieves better accuracy compared with other exist-
ing real-time algorithms and runs at a speed of 15∼20 fps.
We further increase the accuracy by improving the net-
work architectures used for feature extraction and matching
cost aggregation. The full model, which we call “GA-Net”,
achieves the state-of-the-art accuracy on both the Scene
Flow dataset [15] and the KITTI benchmarks [7, 16].

2. Related Work

Feature based matching cost is often ambiguous, as
wrong matches can easily have a lower cost than correct
ones, due to occlusions, smoothness, reﬂections, noise etc.
To deal with this, many cost aggregation approaches have
been developed to reﬁne the cost volume and achieve bet-
ter estimations. This section brieﬂy introduces related work
in the application of deep neural networks in stereo recon-
struction with a focus on the existing matching cost aggre-
gation strategies, and brieﬂy reviews approaches for tradi-
tional local and semi-global cost aggregations.

2.1. Deep Neural Networks for Stereo Matching

Deep neural networks were used to compute patch-wise
similarity scores in [4, 6, 29, 33], with traditional cost ag-
gregation and disparity computation/reﬁnement methods
[9, 10] used to get the ﬁnal disparity maps. These ap-
proaches achieved state-of-the-art accuracy, but, limited by
the traditional matching cost aggregation step, often pro-
duced wrong predictions in occluded regions, large texture-
less/reﬂective regions and around object edges. Some other
methods looked to improve the performance of traditional

cost aggregation, with, e.g. SGM-Nets [23] predicting the
penalty-parameters for SGM [9] using a neural net, whereas
Sch ¨onberger et al. [22] learned to fuse proposals by opti-
mization in stereo matching and Yang et al. proposed to ag-
gregate costs using a minimum spanning tree [28].
Recently, end-to-end deep neural network models have
become popular. Mayer et al. created a large synthetic
dataset to train end-to-end deep neural network for disparity
estimation (e.g. DispNet) [15]. Pang et al. [19] built a two-
stage convolutional neural network to ﬁrst estimate and then
reﬁne the disparity maps. Tulyakov et al. proposed end-to-
end deep stereo models for practical applications [26]. GC-
Net [13] incorporated the feature extraction, matching cost
aggregation and disparity estimation into a single end-to-
end deep neural model to get state-of-the-art accuracy on
several benchmarks. PSMNet [3] used pyramid feature ex-
traction and a stacked hourglass block [18] with twenty-ﬁve
3D convolutional layers to further improve the accuracy.

2.2. Cost Aggregation

Traditional stereo matching algorithms [1,9,27] added an
additional constraint to enforce smoothness by penalizing
changes of neighboring disparities. This can be both local
and (semi-)global, as described below.

2.2.1 Local Cost Aggregation

The cost volume C is formed of matching costs at each
pixel’s location for each candidate disparity value d . It has
a size of H × W × Dmax (with H : image height, W : image
width, Dmax : maximum of the disparities) and can be sliced
into Dmax slices for each candidate disparity d . An efﬁ-
cient cost aggregation method is the local cost ﬁlter frame-
work [10, 31], where each slice of the cost volume C(d ) is
ﬁltered independently by a guided image ﬁlter [8, 25, 31].
The ﬁltering for pixel’s location p = (x, y) at disparity d is a
weighted average of all neighborhoods q ∈ Np in the same
slice C(d ):

CA (p, d ) = ∑

q∈Np

ω (p, q) · C(q, d )

(1)

Where C(q, d ) means the matching cost at location p for
candidate disparity d . CA (p, d ) represents the aggregated
matching cost. Different image ﬁlters [8, 25, 31] can be used
to produce the guided ﬁlter weights ω . Since these methods
only aggregate the cost in a local region Np , they can run at
fast speeds and reach real-time performance.

2.2.2 Semi-Global Matching

When enforcing (semi-)global aggregation, the matching
cost and the smoothness constraints are formulated into one
energy function E (D) [9] with the disparity map of the in-
put image as D. The problem of stereo matching can now
be formulated as ﬁnding the best disparity map D∗ that min-

186

imizes the energy E (D):
E (D) = ∑p{Cp (Dp ) + ∑q∈Np P1 · δ (|Dp − Dq | = 1)

(2)

+ ∑q∈Np P2 · δ (|Dp − Dq | > 1)}.

The ﬁrst term ∑p Cp (Dp ) is the sum of matching costs at all
pixel locations p for disparity map D. The second term is
a constant penalty P1 for locations q in the neighborhood
of p if they have small disparity discontinuities in disparity
map D (|Dp − Dq | = 1). The last term adds a larger constant
penalty P2 , for all larger disparity changes (|Dp − Dq | > 1).
Hirschmuller proposed to aggregate matching costs in
1D from sixteen directions to get a approximate solution
with O(KN ) time complexity, which is well known as semi-
global matching (SGM) [9]. The cost CA
r (p, d ) of a location
p at disparity d aggregates along a path over the whole im-
age in the direction r, and is deﬁned recursively as:
CA
CA
CA
min

r (p − r, d ),
r (p − r, d − 1) + P1 ,
r (p − r, d + 1) + P1 ,
r (p − r, i) + P2 .

CA
r (p, d ) = C(p, d ) + min

CA

(3)

i

Where r is a unit direction vector. The same aggregation
steps were used in MC-CNN [23, 30], and similar iterative
steps were employed in [1, 2, 14].
In the following section, we detail our much more efﬁ-
cient guided aggregation (GA) strategies, which include a
semi-global aggregation (SGA) layer and a local guided ag-
gregation (LGA) layer. Both GA layers can be implemented
with back propagation in end-to-end models to replace the
low-efﬁcient 3D convolutions and obtain higher accuracy.




3. Guided Aggregation Net

In this section, we describe our proposed guided aggre-
gation network (GA-Net), including the guided aggregation
(GA) layers and the improved network architecture.

3.1. Guided Aggregation Layers

State-of-the-art end-to-end stereo matching neural nets
such as [3, 13] build a 4D matching cost volume (with size
of H × W × Dmax × F , H : height, W : width, Dmax : max
disparity, F :
feature size) by concatenating features be-
tween the stereo views, computed at different disparity val-
ues. This is next reﬁned by a cost aggregation stage, and
ﬁnally used for disparity estimation. Different from these
approaches, and inspired by semi-global and local match-
ing cost aggregation methods [9, 10], we propose our semi-
global guided aggregation (SGA) and local guided aggrega-
tion (LGA) layers, as outlined below.

3.1.1 Semi-Global Aggregation

Traditional SGM [9] aggregates the matching cost itera-
tively in different directions (Eq.
(3)). There are several

difﬁculties in using such a method in end-to-end trainable
deep neural network models.
First, SGM has many user-deﬁned parameters (P1 , P2 ),
which are not straightforward to tune. All of these param-
eters become unstable factors during neural network train-
ing. Second, the cost aggregations and penalties in SGM are
ﬁxed for all pixels, regions and images without adaptation
to different conditions. Third, the hard-minimum selection
leads to a lot of fronto parallel surfaces in depth estimations.

i

(4)

We design a new semi-global cost aggregation step
which supports backpropagation. This is more effective
than the traditional SGM and can be used repetitively in
a deep neural network model to boost the cost aggregation
effects. The proposed aggregation step is:
CA

r (p, d ) = C(p, d )

CA

+ sum




r (p − r, d ),
r (p − r, d − 1),
r (p − r, d + 1),
r (p − r, i).

w1 (p, r) · CA
w2 (p, r) · CA
w3 (p, r) · CA
w4 (p, r) · max
This is different from the SGM in three ways. First, we
make the user-deﬁned parameters learnable and add them
as penalty coefﬁcients/weights of the matching cost terms.
These weights would therefore be adaptive and more ﬂex-
ible at different locations for different situations. Second,
we replace the ﬁrst/external minimum selection in Eq. (3)
with a weighted sum, without any loss in accuracy. This
change was proven effective in [24], where convolutions
with strides were used to replace the max-pooling layers to
get an all convolutional network without loss of accuracy.
Third, the internal/second minimum selection is changed
to a maximum. This is because the learning target in our
models is to maximize the probabilities at the ground truth
depths instead of minimizing the matching costs. Since
CA
r (p − r, i) in Eq. (4) can be shared by CA
max
r (p, d ) for
d different locations, here, we do not use another weighted
summation to replace it in order to reduce the computational
complexity.
For both Eq. (3) and Eq. (4), the values of CA
r (p, d ) in-
crease along the path, which may lead to very large values.
We normalize the weights of the terms to avoid such a prob-
lem. This leads to our new semi-global aggregation:

i

CA
r (p, d ) = sum

s.t .




w0 (p, r) · C(p, d )
r (p − r, d ),
r (p − r, d − 1),
r (p − r, d + 1).
r (p − r, i).

w1 (p, r) · CA
w2 (p, r) · CA
w3 (p, r) · CA
w4 (p, r) · max
∑
wi (p, r) = 1

CA

i

i=0,1,2,3,4

(5)

C(p, d ) is known as the cost volume (with a size of H ×W ×
Dmax × F ). Same as the traditional SGM [9], the cost vol-
ume can be sliced into Dmax slices at the third dimension for

187

(

a

)

G
A
N

-

e

t

A

r

c

h

i
t

e
c

t

u

r

e

(b) Semi-Global Guided Aggregation (SGA)

(c) Local Guided Aggregation (LGA)

Figure 2: (a) Architecture overview. The left and right images are fed to a weight-sharing feature extraction pipeline. It consists of a
stacked hourglass CNN and is connected by concatenations. The extracted left and right image features are then used to form a 4D cost
volume, which is fed into a cost aggregation block for regularization, reﬁnement and disparity regression. The guidance subnet (green)
generates the weight matrices for the guided cost aggregations (SGA and LGA). (b) SGA layers semi-globally aggregate the cost volume
in four directions. (c) The LGA layer is used before the disparity regression and locally reﬁnes the 4D cost volume for several times.

each candidate disparity d and each of these slices repeats
the aggregation operation of Eq. (5) with the shared weight
matrices (w0...4 ). All the weights w0...4 can be achieved by
a guidance subnet (as shown in Fig. 2). Different to the
original SGM which aggregates in sixteen directions, in or-
der to improve the efﬁciency, the proposed aggregations are
done in totally four directions (left, right, up and down)
along each row or column over the whole image, namely

r ∈ {(0, 1), (0, −1), (1, 0), (−1, 0)}.

The ﬁnal aggregated output CA (p) is obtained by select-
ing the maximum between the four directions:
CA (p, d ) = max

CA

(6)

r (p, d )

r

The last maximum selection keeps the best message from
only one direction. This guarantees that the aggregation ef-
fects are not blurred by the other directions. The backprop-
agation for w and C(p, d ) in the SGA layer can be done in-
versely as Eq. (5) (details are available in the supplementary
materials). Our SGA layer can be repeated several times in
the neural network model to obtain better cost aggregation
effects (as illustrated in Fig. 2).

3.1.2 Local Aggregation

We now introduce the local guided aggregation (LGA) layer
which aims to reﬁne the thin structures and object edges.
Down-sampling and up-sampling are widely used in stereo
matching models which blurs thin structures and object
edges. The LGA layer learns several guided ﬁlters to reﬁne
the matching cost and aid in the recovery of thin structure
information. The local aggregation follows the cost ﬁlter

deﬁnition [10] (Eq. (1)) and can be written as:
CA (p, d ) = sum 
s.t . ∑

∑q∈Np ω0 (p, q) · C(q, d ),
∑q∈Np ω1 (p, q) · C(q, d − 1),
∑q∈Np ω2 (p, q) · C(q, d + 1).
ω0 (p, q) + ω1 (p, q) + ω2 (p, q) = 1



q∈Np

(7)

Different slices (totally Dmax slices) of cost volume share
the same ﬁltering/aggregation weights in LGA. This is the
same as the original cost ﬁlter framework [10] and the SGA
(Eq.(5)) in this paper. While, different with the traditional
cost ﬁlter [10] which uses a K × K ﬁlter kernel to ﬁlter the
cost volume in a K × K local/neighboor region Np , the pro-
posed LGA layer has three K × K ﬁlters (ω0 , ω1 and ω2 )
at each pixel location p for disparities d , d − 1 and d + 1
respectively. Namely, it aggregates with a K × K × 3 weight
matrix in a K × K local region for each pixel location p.
The setting of the weight matrix is also similar to [11], but,
weights and ﬁlters are shared during the aggregation as de-
signed in [10].

3.1.3 Efﬁcient Implementation

We use several 2D convolutional layers to build a fast guid-
ance subnet (as illustrated in Fig. 2). The implementation is
similar to [32].
It uses the reference image as input and
outputs the aggregation weights w (Eq.
(5)). For a 4D
cost volume C with size of H × W × D × F (H : height, W :
width, D: max disparity, F : feature size), the output of the
guidance subnet is split, reshaped and normalized as four
H ×W × K × F (K = 5) weight matrices for four directions’
aggregation using Eq. (5). Note that aggregations for dif-

188

ferent disparities corresponding to a slice d share the same
aggregation weights. Similarly, the LGA layer need to learn
a H × W × 3K 2 × F (K = 5) weight matrix and aggregates
using Eq. (7).
Even though the SGA layer involves an iterative ag-
gregation across the width or the height, the forward and
backward can be computed in parallel due to the indepen-
dence between elements in different feature channels or
rows/columns. For example, when aggregating in the left
direction, the elements in different channels or rows are in-
dependent and can be computed simultaneously. The ele-
ments of the LGA layer can also be computed in parallel by
simply decomposing it into element-wise matrix multipli-
cations and summations. In order to increase the receptive
ﬁeld of the LGA layer, we repeat the computation of EQ. (7)
twice with the same weight matrix, which is similar to [5].

3.2. Network Architecture

As illustrated in Fig.2, the GA-Net consists of four parts:
the feature extraction block, the cost aggregation for the 4D
cost volume, the guidance subnet to produce the cost ag-
gregation weights and the disparity regression. For the fea-
ture extraction, we use a stacked hourglass network which is
densely connected by concatenations between different lay-
ers. The feature extraction block is shared by both left and
right views. The extracted features for left and right images
are then used to form a 4D cost volume. Several SGA layers
are used for the cost aggregation and LGA layers can be im-
plemented before and after the softmax layer of the disparity
regression. It reﬁnes the thin-structures and compensate for
the accuracy loss caused by the down-sampling done for the
cost volume. The weight matrices (in Eq.(5) and Eq.(7)) are
generated by an extra guidance subnet which uses the refer-
ence view (e.g. the left image) as input. The guidance sub-
net consists of several fast 2D convolutional layers and the
outputs are reshaped and normalized into required weight
matrices for these GA layers.1

3.3. Loss Function

We adopt the smooth L1 loss function to train our mod-
els. Smooth L1 is robust at disparity discontinuities and has
low sensitivity to outliers or noises, as compared to L2 loss.
The loss function for training our models is deﬁned as:

L( ˆd , d ) = 1

N

∑

N

n=1

l (| ˆd − d |)
l (x) = (cid:26) x − 0.5,

x ≥ 1
x2 /2,
x < 1
where, | ˆd − d | measures the absolute error of the disparity
predictions, N is the number of valid pixels with ground
truths for training.

(8)

1 The parameter settings of “GA-Net-15” used in our experiments are
detailed in the supplementary material (available at www.feihuzhang.com).

For the disparity estimation, we employ the disparity re-
gression proposed in [13]:

ˆd =

Dmax

∑

d=0

d × σ (−CA (d ))

(9)

The disparity prediction ˆd is the sum of each disparity
candidate weighted by its probability. The probability of
each disparity d is calculated after cost aggregation via the
softmax operation σ (·). The disparity regression is shown
more robust than classiﬁcation based methods and can gen-
erate sub-pixel accuracy.

4. Experiments

In this section, we evaluate our GA-Nets with different
settings using Scene Flow [15] and KITTI [7, 16] datasets.
We implement our architectures using pytorch or caffe [12]
(only for real-time models’ implementation). All models
are optimized with Adam (β1 = 0.9, β2 = 0.999). We train
with a batch size of 16 on eight GPUs using 240 × 576 ran-
dom crops from the input images. The maximum of the
disparity is set as 192. Before training, we normalize each
channel of the image by subtracting their means and divid-
ing their standard deviations. We train the model on Scene
Flow dataset for 10 epochs with a constant learning rate of
0.001. For the KITTI datasets, we ﬁne-tune the models pre-
trained on Scene Flow dataset for a further 640 epochs. The
learning rate for ﬁne-tuning begins at 0.001 for the ﬁrst 300
epochs and decreases to 0.0001 for the remaining epochs.

4.1. Ablation Study

We evaluate the performance of GA-Nets with differ-
ent settings, including different architectures and different
number (0-4) of GA layers. As listed in Table 1, The guided
aggregation models signiﬁcantly outperform the baseline
setting which only has 3D convolutional layers for cost ag-
gregation. The new architectures for feature extraction and
cost aggregation improve the accuracy by 0.14% on KITTI
dataset and 0.9% on Scene Flow dataset. Finally, the best
setting of GA-Net with three SGA layers and one LGA
layer gets the best 3-pixel threshold error rate of 2.71% on
KITTI 2015 validation set. It also achieves the best average
EPE of 0.84 pixel and the best 1-pixel threshold error rate
of 9.9% on the Scene Flow test set.

4.2. Effects of Guided Aggregations

In this section, we compare the guided aggregation
strategies with other matching cost aggregation methods.
We also analyze the effects of the GA layers by observing
the post-softmax probabilities output by different models.
Firstly, our proposed GA-Nets are compared with the
cost aggregation architectures in GC-Net (with nineteen 3D
convolutions) and PSMNet (with twenty-ﬁve 3D convolu-
tions). We ﬁxed the feature extraction architecture as pro-

189

Table 1: Evaluations of GA-Nets with different settings. Average end point error (EPE) and threshold error rate are used for evaluations.

Feature Extraction
Stacked Block

Densely Concatenate

Cost Aggregation
SGA Layer

LGA Layer

Scene Flow
EPE Error
Error Rates (%)

KITTI 2015
Error Rates (%)

√
√
√
√
√
√

+1
+2
+3
+4
+3

√

1.26
1.19
1.14
1.05
0.97
0.90
0.89
0.84

13.4
13.0
12.5
11.7
11.0
10.5
10.4
9.9

3.39
3.31
3.25
3.09
2.96
2.85
2.83
2.71

√
√
√
√
√
√
√

r

o

r
r

e

t

n

i

o
p

d
n

E

Table 2: Comparisons of different cost aggregation methods. Av-
erage end point error (EPE) and 1-pixel threshold error rate are
used for evaluations on Scene Flow dataset.

Models

GC-Net
PSMNet
GA-Net-1
GA-Net-2
GA-Net-3
GA-Net-7
GA-Net-11
GA-Net-15

3D Conv
Number

Param Time(s)

19
25
1
2
3
7
11
15

2.9M
3.5M
0.5M
0.7M
0.8M
1.3M
1.8M
2.3M

4.4
2.1
0.17
0.35
0.42
0.62
0.95
1.5

EPE
Error

1.80
1.09
1.82
1.51
1.36
1.07
0.95
0.84

Error
Rates

15.6
12.1
16.5
15.0
13.9
11.9
10.8
9.9

ﬁrst model (ﬁrst row of Fig. 4) only has 3D convolutions
(without any GA layers), the second model (second row of
Fig. 4) has SGA layers and the last model (last row of Fig.
4) has both SGA layers and LGA layer.
As illustrated in Fig. 4(a), for large textureless regions,
there would be a lot of noise since there is no any distinc-
tive features in these regions for correct matching. The
SGA layers successfully suppress these noise in the prob-
abilities by aggregating surrounding matching information.
The LGA layer further concentrates the probability peak on
the ground truth value. It could reﬁne the matching results.
Similarly, in the sample of reﬂective region (Fig. 4(b)), the
SGA and LGA layers correct the wrong matches and con-
centrate the peak on the correct disparity value. For the sam-
ples around the objects edges (Fig. 4(c)), there are usually
two peaks in the probability distribution which are inﬂu-
enced by the background and the foreground respectively.
The SGA and LGA use spatial aggregation along with ap-
propriate maximum selection to cut down the aggregation of
the wrong matching information from the background and
therefore suppress the false probability peak appeared at the
background’s disparity value.

4.3. Comparisons with SGMs and 3D Convolutions

The SGA layer is a differentiable approximation of the
SGM [9]. But, it produces far better results compared with
both the original SGM with handcrafted features and the
MC-CNN [30] with CNN based features (as shown in Table
5). This is because 1) SGA does not have any user-deﬁned
parameters that are all learned in an end-to-end fashion. 2)
The aggregation of SGA is fully guided and controlled by

190

Number of 3D conv

Figure 3: Illustration of the effects of guided aggregations. GA-
Nets are compared with the same architectures without GA Layers.
Evaluations are on Scene Flow dataset using average EPE.

posed above. As shown in Table 2, GA-Nets have fewer pa-
rameters, run at a faster speed and achieve better accuracy.
E.g., with only two GA layers and two 3D convolutions, our
GA-Net-2 outperforms the GC-Net by 0.29 pixel in average
EPE. Also, the GA-Net-7 with three GA layers and seven
3D convolutions outperforms the current best PSMNet [3]
which has twenty-ﬁve 3D convolutional layers.

We also study the effects of the GA layers by comparing
with the same architectures without GA steps. These base-
line models “GA-Nets∗ ” have the same network architec-
tures and all other settings except that there is no GA layer
implemented. As shown in Fig. 3, for all these models, GA
layers have signiﬁcantly improved the models’ accuracy (by
0.5-1.0 pixels in average EPE). For example, the GA-Net-
2 with two 3D convolutions and two GA layers produces
lower EPE (1.51) compared with GA-Net∗ -11 (1.54) which
utilizes eleven 3D convolutions. This implies that two GA
layers are more effective than nine 3D convolutional layers.

Finally, in order to observe and analyze the effects of
GA layers, in Fig. 4, we plot the post-softmax probabili-
ties with respect to a range of candidate disparities. These
probabilities are directly used for disparity estimation using
Eq. (9) and can reﬂect the effectiveness of the cost aggrega-
tion strategies. The data samples are all selected from some
challenging regions, such as a large textureless region (sky),
the reﬂective region (window of a car) and pixels around the
object edges. Three different models are compared. The

v
n
o
c

D

3
y

l

n
o

A
G

S
h

t
i

w

A
G

L
+

C
G

S

(a) large textureless region (sky)

(b) reﬂective region (car window)

(c) object edges

Figure 4: Post-softmax probability distributions with respect to disparity values. Red lines illustrate the ground truth disparities. Samples
are selected from three challenging regions: (a) the large smooth region (sky), (b) the reﬂective region from one car window and (c) one
region around the object edges. The ﬁrst row shows the probability distributions without GA layers. The second row shows the effects of
semi-global aggregation (SGA) layers and the last row is the reﬁned probabilities with one extra local guided aggregation (LGA) layer.

(a) Input view

(b) large textureless region

(c) result of traditional SGM

(d) result of our GA-Net-15

Figure 5: Comparisons with traditional SGM. More results and
comparisons are avaiable at GA-Net-15 and SGM.

the weight matrices. The guidance subnet learns effective
geometrical and contextual knowledge to control the direc-
tions, scopes and strengths of the cost aggregations.
Moreover, compared with original SGM, most of the
fronto-parallel approximations in large textureless regions
have been avoided. (Example is in Fig. 5.) This might be
beneﬁted from: 1) the use of the soft weighted sum in Eq.
(5) (instead of the hard min/max selection in Eq. (3)); and
2) the regression loss of Eq. (9) which helps achieve the
subpixel accuracy.
Our SGA layer is also more efﬁcient and effective than
the 3D convolutional layer. This is because the 3D convolu-
tional layer could only aggregate in a local region restricted
by the kernel size. As a result, a series of 3D convolutions
along with encoder and decoder architectures are indispens-
able in order to achieve good results. As a comparison,
our SGA layer aggregates semi-globally in a single layer
which is more efﬁcient. Another advantage of the SGA is
that the aggregation’s direction, scope and strength are fully
guided by variable weights according to different geometri-
cal and contextual information in different locations. E.g.,
the SGA behaves totally different in the occlusions and the
large smoothness regions. But, the 3D convolutional layer
has ﬁxed weights and always perform the same for all loca-
tions in the whole image.

Table 3: Comparisons with existing real-time algorithms

Methods

End point error

Error rates

Speed (fps)

Our GA-Net
DispNet [15]
Toast [20]

0.7 px
1.0 px
1.4 px

3.21 %
4.65 %
7.42 %

15 (GPU)
22 (GPU)
25 (CPU)

4.4. Complexity and Real(cid:173)time Models

The computational complexity of one 3D convolutional
layer is O(K 3CN ), where N is the elements number of the
output blob. K is the size of the convolutional kernel and
C is the channel number of the input blob. As a compari-
son, the complexity of SGA is O(4KN ) or O(8KN ) for four
or eight-direction aggregations. In GC-Net [13] and PSM-
Net [3], K = 3, C = 32, 64 or 128 and in our GA-Nets, K
is used as 5 (for SGA layer). Therefore, the computational
complexity in terms of ﬂoating-point operations (FLOPs) of
the proposed SGA step is less than 1/100 of one 3D convo-
lutional layer.
The SGA layer are much faster and more effective than
3D convolutions. This allows us to build an accurate real-
time model. We implement one caffe [12] version of the
GA-Net-1 (with only one 3D convolutional layer and with-
out LGA layers). The model is further simpliﬁed by us-
ing 4× down-sampling and up-sampling for cost volume.
The real-time model could run at a speed of 15∼20 fps for
300 × 1000 images on a TESLA P40 GPU. We also compare
the accuracy of the results with the state-of-the-art real-time
models. As shown in Table 3, the real-time GA-Net far out-
performs other existing real-time stereo matching models.

4.5. Evaluations on Benchmarks

For the benchmark evaluations, we use the GA-Net-15
with full settings for evaluations. We compare our GA-Net
with the state-of-the-art deep neural network models on the
Scene Flow dataset and the KITTI benchmarks.

191

Figure 6: Results visualization and comparisons. First row: input image. Second row: Results of GC-Net [13]. Third row: Results of
PSMNet [3]. Last row: Results of our GA-Net. Signiﬁcant improvements are pointed out by blue arrows. The guided aggregations can
effectively aggregate the disparity information to the large textureless regions (e.g. the cars and the windows) and give precise estimations.
It can also aggregate the object knowledge and preserve the depth structure very well (last column).

Table 4: Evaluation Results on KITTI 2012 Benchmark

Models

error rates
(2 pixels)

error rates
(3 pixels)

Reﬂective
regions

Avg-All
(end point)

Our GA-Net
PSMNet [3]
GC-Net [13]
MC-CNN [30]

2.18 %
2.44 %
2.71 %
3.90 %

1.36 %
1.49 %
1.77 %
2.43 %

7.87%
8.36%
10.80%
17.09%

0.5 px
0.6 px
0.7 px
0.9 px

Table 5: Evaluation Results on KITTI 2015 Benchmark

Models

Non Occlusion
Foreground Avg All

All Areas
Foreground Avg All

Our GA-Net-15
PSMNet [3]
GC-Net [13]
SGM-Nets [23]
MC-CNN [30]
SGM [9]

3.39%
4.31%
5.58%
7.43%
7.64%
11.68%

1.84%
2.14 %
2.61%
3.09%
3.33%
5.62%

3.91%
4.62%
6.16%
8.64%
8.88%
13.00%

2.03%
2.32%
2.87%
3.66%
3.89%
6.38%

4.5.1 Scene Flow Dataset

The Scene Flow synthetic dataset [15] contains 35,454
training and 4,370 testing images. We use the “ﬁnal” set
for training and testing. GA-Nets are compared with other
state-of-the-art DNN models by evaluating with the average
end point errors (EPE) and 1-pixel threshold error rates on
the test set. The results are presented in Table 2. We ﬁnd
that our GA-Net outperforms the state-of-the-arts on both of
the two evaluation metrics by a noteworthy margin (2.2%
improvement in error rate and 0.25 pixel improvement in
EPE compared with the current best PSMNet [3].).

4.5.2 KITTI 2012 and 2015 Datasets

After training on Scene Flow dataset, we use the GA-Net-
15 to ﬁne-tune on the KITTI 2015 and KITTI 2012 data sets

respectively. The models are then evaluated on the test sets.
According to the online leader board, as shown in Table 4
and Table 5, our GA-Net has fewer low-efﬁcient 3D con-
volutions but achieves better accuracy. It surpasses current
best PSMNet in all the evaluation metrics. Examples are
shown in Fig. 6. The GA-Nets can effectively aggregate
the correct matching information into the challenging large
textureless or reﬂective regions to get precise estimations.
It also keeps the object structures very well.

5. Conclusion

In this paper, we developed much more efﬁcient and ef-
fective guided matching cost aggregation (GA) strategies,
including the semi-global aggregation (SGA) and the lo-
cal guided aggregation (LGA) layers for end-to-end stereo
matching. The GA layers signiﬁcantly improve the accu-
racy of the disparity estimation in challenging regions, such
as occlusions, large textureless/reﬂective regions and thin
structures. The GA layers can be used to replace computa-
tionally costly 3D convolutions and get better accuracy.

Acknowledgement

Research is mainly supported by Baidu’s Robotics
and Auto-driving Lab and in part by the ERC grant
ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte
EP/M013774/1 and EPSRC/MURI grant EP/N019474/1.
We would also like to acknowledge the Royal Academy of
Engineering. Victor Adrian Prisacariu would like to thank
the European Commission Project Multiple-actOrs Virtual
Empathic CARegiver for the Elder (MoveCare).

192

References

[1] F. Besse, C. Rother, A. Fitzgibbon, and J. Kautz. Pmbp:
Patchmatch belief propagation for correspondence ﬁeld esti-
mation. International Journal of Computer Vision, 110(1):2–
13, Oct 2014. 2, 3
[2] M. Bleyer, C. Rhemann, and C. Rother. Patchmatch stereo-
stereo matching with slanted support windows.
In British
Machine Vision Conference (BMVC), pages 1–11, 2011. 3
[3] J.-R. Chang and Y.-S. Chen. Pyramid stereo matching net-
work. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 5410–5418,
2018. 1, 2, 3, 6, 7, 8
[4] Z. Chen, X. Sun, L. Wang, Y. Yu, and C. Huang. A deep
visual correspondence embedding model for stereo matching
costs. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pages 972–980, 2015. 2
[5] X. Cheng, P. Wang, and R. Yang. Depth estimation via afﬁn-
ity learned with convolutional spatial propagation network.
In Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 103–119, 2018. 5
[6] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 5515–5524,
2016. 2
[7] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? the kitti vision benchmark suite. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 3354–3361. IEEE, 2012.
2, 5
[8] K. He, J. Sun, and X. Tang. Guided image ﬁltering. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
(6):1397–1409, 2013. 2
[9] H. Hirschmuller. Stereo processing by semiglobal matching
and mutual information. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 30(2):328–341, 2008. 1, 2, 3,
6, 8
[10] A. Hosni, C. Rhemann, M. Bleyer, C. Rother, and
M. Gelautz. Fast cost-volume ﬁltering for visual correspon-
dence and beyond. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 35(2):504–511, 2013. 1, 2, 3, 4
[11] X. Jia, B. De Brabandere, T. Tuytelaars, and L. V. Gool. Dy-
namic ﬁlter networks.
In Advances in Neural Information
Processing Systems, pages 667–675, 2016. 4
[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding.
In Proceedings of
the ACM International Conference on Multimedia, pages
675–678. ACM, 2014. 5, 7
[13] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry,
R. Kennedy, A. Bachrach, and A. Bry. End-to-end learning
of geometry and context for deep stereo regression. CoRR,
vol. abs/1703.04309, 2017. 1, 2, 3, 5, 7, 8
[14] S. Liu, S. De Mello, J. Gu, G. Zhong, M.-H. Yang, and
J. Kautz. Learning afﬁnity via spatial propagation net-
works. In Advances in Neural Information Processing Sys-
tems (NIPS), pages 1520–1530, 2017. 3

[15] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers,
A. Dosovitskiy, and T. Brox. A large dataset to train convo-
lutional networks for disparity, optical ﬂow, and scene ﬂow
estimation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 4040–
4048, 2016. 1, 2, 5, 7, 8
[16] M. Menze and A. Geiger. Object scene ﬂow for autonomous
vehicles. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 3061–
3070, 2015. 2, 5
[17] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation.
In Proceedings of the
European Conference on Computer Vision, pages 483–499.
Springer, 2016. 1
[18] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 483–
499. Springer, 2016. 2
[19] J. Pang, W. Sun, J. S. Ren, C. Yang, and Q. Yan. Cas-
cade residual learning: A two-stage convolutional neural net-
work for stereo matching. IEEE International Conference on
Computer Vision Workshops (ICCVW), 2017. 2
[20] B. Ranft and T. Strauß. Modeling arbitrarily oriented slanted
planes for efﬁcient stereo vision based on block matching. In
IEEE International Conference on Intelligent Transportation
Systems (ITSC), pages 1941–1947. IEEE, 2014. 7
[21] D. Scharstein and R. Szeliski. A taxonomy and evaluation of
dense two-frame stereo correspondence algorithms. Interna-
tional Journal of Computer Vision, 47(1-3):7–42, 2002. 1
[22] J. L. Sch ¨onberger, S. N. Sinha, and M. Pollefeys. Learning to
fuse proposals from multiple scanline optimizations in semi-
global matching. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 739–755, 2018. 2
[23] A. Seki and M. Pollefeys. Sgm-nets: Semi-global matching
with neural networks. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 6640–6649, 2017. 2, 3, 8
[24] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-
miller. Striving for simplicity: The all convolutional net.
arXiv preprint arXiv:1412.6806, 2014. 3
[25] C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and
color images. In Proceedings of the IEEE International Con-
ference on Computer Vision (ICCV), pages 839–846. IEEE,
1998. 2
[26] S. Tulyakov, A. Ivanov, and F. Fleuret. Practical deep stereo
(pds): Toward applications-friendly deep stereo matching.
arXiv preprint arXiv:1806.01677, 2018. 2
[27] S. Xu, F. Zhang, X. He, X. Shen, and X. Zhang. Pm-pm:
Patchmatch with potts model for object segmentation and
stereo matching. IEEE Transactions on Image Processing,
24(7):2182–2196, July 2015. 2
[28] Q. Yang. A non-local cost aggregation method for stereo
matching. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 1402–
1409. IEEE, 2012. 2
[29] S. Zagoruyko and N. Komodakis. Learning to compare im-
age patches via convolutional neural networks. In Proceed-

193

ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4353–4361, 2015. 2
[30] J. Zbontar and Y. LeCun. Computing the stereo matching
cost with a convolutional neural network.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1592–1599, 2015. 1, 3, 6, 8
[31] F. Zhang, L. Dai, S. Xiang, and X. Zhang. Segment graph
based image ﬁltering: fast structure-preserving smoothing.
In Proceedings of the IEEE International Conference on
Computer Vision (ICCV), pages 361–369, 2015. 2
[32] F. Zhang and B. W. Wah. Supplementary meta-learning: To-
wards a dynamic model for deep neural networks. In Pro-
ceedings of the IEEE International Conference on Computer
Vision, pages 4344–4353, 2017. 4
[33] F. Zhang and B. W. Wah. Fundamental principles on learn-
ing new features for effective dense matching. IEEE Trans-
actions on Image Processing, 27(2):822–836, 2018. 1, 2

194

Geometry-Consistent Generative Adversarial Networks for One-Sided
Unsupervised Domain Mapping

Huan Fu∗ 1 Mingming Gong∗ 2,3
Chaohui Wang4
Kayhan Batmanghelich2
Kun Zhang3
Dacheng Tao1
UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney, Darlington, NSW 2008, Australia

1

2

Department of Biomedical Informatics, University of Pittsburgh

3

Department of Philosophy, Carnegie Mellon University

4

Universit ´e Paris-Est, LIGM (UMR 8049), CNRS, ENPC, ESIEE Paris, UPEM, Marne-la-Vall ´ee, France

{hufu6371@uni., dacheng.tao@}sydney.edu.au

{mig73, kayhan}@pitt.edu

chaohui.wang@u-pem.fr

kunz1@cmu.edu

Abstract

Unsupervised domain mapping aims to learn a function
GX Y to translate domain X to Y in the absence of paired
examples. Finding the optimal GX Y without paired data
is an ill-posed problem, so appropriate constraints are re-
quired to obtain reasonable solutions. While some promi-
nent constraints such as cycle consistency and distance
preservation successfully constrain the solution space, they
overlook the special properties of images that simple geo-
metric transformations do not change the image’s seman-
tic structure. Based on this special property, we develop
a geometry-consistent generative adversarial network (Gc-
GAN), which enables one-sided unsupervised domain map-
ping. GcGAN takes the original image and its counterpart
image transformed by a predeﬁned geometric transforma-
tion as inputs and generates two images in the new domain
coupled with the corresponding geometry-consistency con-
straint. The geometry-consistency constraint reduces the
space of possible solutions while keep the correct solutions
in the search space. Quantitative and qualitative compar-
isons with the baseline (GAN alone) and the state-of-the-art
methods including CycleGAN [66] and DistanceGAN [5]
demonstrate the effectiveness of our method.

1. Introduction

Domain mapping or image-to-image translation, which
targets at translating an image from one domain to another,
has been intensively investigated over the past few years.
Let X ∈ X denote a random variable representing source
domain images and Y ∈ Y represent target domain images.
According to whether we have access to a paired sample
i=1 , domain mapping can be studied in a super-

{(xi , yi )}N

∗ equal contribution

vised or unsupervised manner. While several works have
successfully produced high-quality translations by focusing
on supervised domain mapping with constraints provided
by cross-domain image pairs [46, 26, 59, 58], the progress
of unsupervised domain mapping is relatively slow.

In unsupervised domain mapping, the goal is to model
the joint distribution PX Y given samples drawn from the
marginal distributions PX and PY in individual domains.
Since the two marginal distributions can be inferred from
an inﬁnite set of possible joint distributions, it is difﬁcult
to guarantee that an individual input x ∈ X and the output
GX Y (x) are paired up in a meaningful way without addi-
tional assumptions or constraints.

To address this problem, recent approaches have ex-
ploited the cycle-consistency assumption, i.e., a mapping
GX Y and its inverse mapping GY X should be bijections
[66, 28, 61]. Speciﬁcally, when feeding an example x ∈ X
into the networks GX Y ◦ GY X : X → Y → X , the output
should be a reconstruction of x and vise versa for y , i.e.,
GY X (GX Y (x)) ≈ x and GX Y (GY X (y)) ≈ y . Further,
DistanceGAN [5] showed that maintaining the distances
between images within domains allows one-sided unsuper-
vised domain mapping.

Existing constraints overlook the special properties of
images that simple geometric transformations (global geo-
metric transformations without shape deformation) do not
change the image’s semantic structure. Here, semantic
structure refers to the information that distinguishes differ-
ent object/staff classes, which can be easily perceived by
humans regardless of trivial geometric transformations such
as rotation. Based on this property, we develop a geometry-
consistency constraint, which helps in reducing the search
space of possible solutions while still keeping the correct set
of solutions under consideration, and results in a geometry-
consistent generative adversarial network (GcGAN).

Our geometry-consistency constraint is motivated by the

12427

Input

Ground Truth

GAN alone

GAN alone (rot)

GcGAN

GcGAN (rot)

Figure 1: Geometry consistency. The original input image is denoted by x, and the predeﬁned function f (·) is a 90◦ clockwise rotation
(rot). GAN alone: G1
X Y (x). GAN alone (rot): f −1 (G1
˜X ˜Y (f (x))). GcGAN: G2
X Y (x). GcGAN (rot): f −1 (G2
˜X ˜Y (f (x)). It can be seen
that GAN alone produces geometrically-inconsistent output images, indicating that the learned GX Y and G ˜X ˜Y are far away from the
correct mapping functions. By enforcing geometry consistency, our method results in more sensible domain mapping.

fact that a given geometric transformation f (·) between
the input images should be preserved by related transla-
tors GX Y and G ˜X ˜Y , if ˜X and ˜Y are the domains obtained
by applying f (·) on the examples of X and Y , respec-
tively. Mathematically, given a random example x from the
source domain X and a predeﬁned geometric transforma-
tion function f (·), geometry consistency can be expressed

as f (GX Y (x)) ≈ G ˜X ˜Y (f (x)) and f −1 (G ˜X ˜Y (f (x))) ≈

GX Y (x), where f −1 (·) is the inverse function of f (·). Be-
cause it is unlikely that GX Y and G ˜X ˜Y always fail in the
same location, GX Y and G ˜X ˜Y co-regularize each other by
the geometry-consistency constraint and thus correct each
others’ failures in local regions of their respective trans-
lations (see Figure 1 for an illustration). Our geometry-
consistency constraint allows one-sided unsupervised do-
main mapping, i.e., GX Y can be trained independently from
GY X . In this paper, we employ two simple but represen-
tative geometric transformations as examples, i.e., vertical
ﬂipping (vf ) and 90 degrees clockwise rotation (rot), to il-
lustrate geometry consistency. Quantitative and qualitative
comparisons with the baseline (GAN alone) and the state-
of-the-art methods including CycleGAN [66] and Distance-
GAN [5] demonstrate the effectiveness of our method.

2. Related Work

Generative Adversarial Networks. Generative adver-
sarial networks (GANs) [21, 45, 14, 47, 51, 3] learn two
networks, i.e., a generator and a discriminator, in a staged
zero-sum game fashion to generate images from inputs.
Many tasks have recently been developed based on deep
convolutional GANs, such as image inpainting, style trans-
fer, and domain adaptation [7, 62, 46, 48, 31, 60, 9, 52, 23,
53, 64, 27, 50, 19, 18, 35, 63]. The key component enabling
GANs is the adversarial constraint, which enforces the
generated images to be indistinguishable from real images.

Domain Mapping. Recent adversarial domain mapping
has been studied in a supervised or unsupervised manner
with respect to paired or unpaired inputs. There are a va-

riety of literatures [46, 31, 26, 59, 56, 58, 25, 37, 4, 10]
on supervised domain mapping. One representative exam-
ple is conditional GAN [26], which learned the discrimina-
tor to distinguish (x, y) and (x, GX Y (x)) instead of y and
GX Y (x), where (x, y) is a meaningful pair across domains.
Further, Wang et al. [59] showed that conditional GANs can
be used to generate high-resolution images with a novel fea-
ture matching loss, as well as multi-scale generator and dis-
criminator architectures. While there has been signiﬁcant
progress in supervised domain mapping, many real-word
applications can not provide aligned images across domains
because data preparation is expensive. Thus, different con-
straints and frameworks have been proposed for image-to-
image translation in the absence of training pairs.
In unsupervised domain mapping, only unaligned exam-
ples in individual domains are provided, making the task
more practical but more difﬁcult. Unpaired domain map-
ping has a long history, and some successes in adversarial
networks have recently been presented [40, 66, 5, 39, 42,
38, 6, 11]. For example, Liu and Tuzel [40] introduced
coupled GAN (CoGAN) to learn cross-domain represen-
tations by enforcing a weight-sharing constraint. Subse-
quently, CycleGAN [66], DiscoGAN [28], and DualGAN
[61] enforced that translators GX Y and GY X should be
bijections. Thus, jointly learning GX Y and GY X by en-
forcing cycle consistency can help to produce convinc-
ing mappings. Since then, many constraints and assump-
tions have been proposed to improve cycle consistency
[8, 17, 24, 32, 34, 11, 2, 67, 20, 44, 39, 36, 1]. Recently, Be-
naim and Wolf [5] reported that maintaining the distances
between samples within domains allows one-sided unsuper-
vised domain mapping. GcGAN is also a one-sided frame-
work coupled with our geometry-consistency constraint,
and produces competitive and even better translations than
the two-sided CycleGAN in various applications.

3. Preliminaries

Let X and Y be two domains with unpaired training ex-
amples {xi }N
i=1 and {yj }M
j=1 , where xi and yj are drawn

2428

IM

"EF

JN

"FE

&

JM

&

IN

"FE

"EF

&&

IM

&&

JN

IM

"EF

&

JM

IM

"EF

&

JM

=(JM
&)

P(IM , IN )

P(JM
& , JN
&)

=(IM )

share
parameters

IN

"EF

&

JN

IKM

"EO FO

&

JKM

= -. (JKM
&)

	cyclic	reconstruction
	for	cycle	consistency

&

JM

&

IN

/F

/E

*/1

*/1

preserving	P 3 	
for	distance	consistency

preserving	= 3 	
for	geometry	consistency

&

JM

&

JN

/F

/F

*/1

*/1

&

JM

&

JKM

/F

/FO

*/1

*/1

CycleGAN

DistanceGAN

GcGAN

Figure 2: An illustration of the differences between CycleGAN [66], DistanceGAN [5], and our GcGAN. x and y are random examples
from domain X and Y , respectively. d(xi , xj ) is the distance between images xi and xj . f (·) is a predeﬁned geometric transformation
function for images, which satisﬁes f −1 (f (x)) = f (f −1 (x)) = x. GX Y and G ˜X ˜Y are the generators (or translators) which target
the domain translation tasks from X to Y and ˜X to ˜Y , where ˜X and ˜Y are two domains obtained by applying f (·) on all the images
in X and Y , respectively. DY is an adversarial discriminator in domain Y . The red dotted lines denote the unsupervised constraints
with respect to cycle consistency (x ≈ GY X (GX Y (x))), distance consistency (x ≈ GY X (GX Y (x))), and our geometry consistency
(f (GX Y (x)) ≈ G ˜X ˜Y (f (x))), respectively.

from the marginal distributions PX and PY , where X and
Y are two random variables associated with X and Y , re-
spectively. In the paper, we exploit style transfer without
undesirable semantic distortions, and have two goals. First,
we need to learn a mapping GX Y such that GX Y (X ) has
the same distribution as Y , i.e., PGXY (X ) ≈ PY . Second,
the learned mapping function only changes the image style
without distorting the semantic structures.
While many works have modeled the invertibility be-
tween GX Y and GY X for convincing mappings since the
success of CycleGAN, here we propose to enforce geom-
etry consistency as a constraint that allows one-sided do-
main mapping. Let f (·) be a predeﬁned geometric trans-
formation. We can obtain two extra domains ˜X and ˜Y
with examples { ˜xi }N
i=1 and { ˜yj }M
j=1 by applying f (·) on
X and Y , respectively. We learn an additional transla-
tor G ˜X ˜Y : ˜X → ˜Y while learning GX Y : X → Y ,
and introduce our geometry-consistency constraint based
on the predeﬁned transformation such that the two net-
works can co-regularize each other. Our framework en-
forces that GX Y (x) and G ˜X ˜Y ( ˜x) should keep the same ge-
ometric transformation with the one between x and ˜x, i.e.,
f (GX Y (x)) ≈ G ˜X ˜Y ( ˜x), where ˜x = f (x). We denote the
two adversarial discriminators as DY and D ˜Y with respect
to domains Y and ˜Y , respectively.

4. Proposed Method

We present our geometry-consistency constraint and Gc-
GAN beginning with a review of the cycle-consistency con-
straint and the distance constraint. An illustration of the dif-
ferences between these constraints is shown in Figure 2.

4.1. Unsupervised Constraints

Cycle-consistency constraint.
Following the cycle-
consistency assumption [28, 66, 61], through the translators

GX Y ◦ GY X : X → Y → X and GY X ◦ GX Y : Y →

X → Y , the examples x and y in domain X and Y should
recover the original images, i.e., x ≈ GY X (GX Y (x)) and
y ≈ GX Y (GY X (y)). Cycle consistency is implemented
by a bidirectional reconstruction process that requires GX Y
and GY X to be jointly learned, as shown in Figure 2 (Cycle-
GAN). The cycle consistency loss Lcyc (GX Y , GY X , X, Y )
takes the form as:

Lcyc = Ex∼PX [kGY X (GX Y (x)) − xk1 ]
+ Ey∼PY [kGX Y (GY X (y)) − yk1 ].

(1)

Distance constraint. The assumption behind the distance
constraint is that the distance between two examples xi and
xj in domain X should be preserved after mapping to do-

main Y , i.e., d(xi , xj ) ≈ a · d(GX Y (xi ), GX Y (xj )) + b,

where d(·) is a predeﬁned function to measure the distance
between two examples and a and b are the linear coefﬁcient
and bias. In DistanceGAN [5], the distance consistency loss
Ldis (GX Y , X, Y ) is the exception to the absolute differ-
ences between distances:

φ(xi , xj ) =

Ldis = Exi ,xj ∼PX [|φ(xi , xj ) − ψ(xi , xj )|],
1
σX
1
σY

(kxi − xj k1 − µX ),

ψ(xi , xj ) =

(kGX Y (xi ) − GX Y (xj )k1 − µY ),

(2)

where µX , µY (σX , σY ) are the means (standard devia-
tions) of distances of all the possible pairs of (xi , xj ) within

2429

domain X and (yi , yj ) within domain Y , respectively.

4.2. Geometry(cid:173)consistent Generative Adversarial
Networks

Adversarial constraint. Taking GX Y as an example, an
adversarial loss Lgan (GX Y , DY , X, Y ) [21] enforces GX Y
and DY to simultaneously optimize each other in an mini-
max game, i.e., minGXY maxDY Lgan (GX Y , DY , X, Y ). In
other words, DY aims to distinguish real examples {y}
from translated samples {GX Y (x)}. By contrast, GX Y
aims to fool DY so that DY can label a fake example
y ′ = GX Y (x) as a sample satisfying y ′ ∼ PY . The ob-
jective can be expressed as:

Lgan = Ey∼PY [log DY (y)]
+ Ex∼PX [log(1 − DY (GX Y (x)))].

(3)

In the transformed domains ˜X and ˜Y , we employ the
adversarial loss Lgan (G ˜X ˜Y , D ˜Y , ˜X , ˜Y ) that has the same
form to Lgan (GX Y , DY , X, Y ).

Geometry-consistency constraint. As shown in Figure 2
(GcGAN), given a predeﬁned geometric transformation
function f (·), we feed the images x ∈ X and ˜x = f (x) into
the translators GX Y and G ˜X ˜Y , respectively. Following our
geometry-consistency constraint, the outputs y ′ = GX Y (x)
and ˜y ′ = G ˜X ˜Y ( ˜x) should also satisfy ˜y ′ ≈ f (y ′ ) like x and
˜x. Considering both f (·) and the inverse geometric transfor-
mation function f −1 (·), our complete geometry consistency
loss Lgeo (GX Y , G ˜X ˜Y , X, Y ) has the following form:

Lgeo = Ex∼PX [kGX Y (x) − f −1 (G ˜X ˜Y (f (x)))k1 ]
+ Ex∼PX [kG ˜X ˜Y (f (x)) − f (GX Y (x))k1 ].

(4)

This geometry-consistency loss can be seen as a recon-
struction loss that relies on the predeﬁned geometric
transformation function f (·).
In this paper, we only take
two common geometric transformations as examples,
namely vertical ﬂipping (vf ) and 90◦ clockwise rotation
(rot), to demonstrate the effectiveness of our geometry-
consistency constraint. Note that, GX Y and G ˜X ˜Y have the
same architecture and share all the parameters.

Full objective. By combining our geometry-consistency
constraint with the standard adversarial constraint, a re-
markable one-sided unsupervised domain mapping can
be targeted.
The full objective for our GcGAN
LGcGAN (GX Y , G ˜X ˜Y , DY , D ˜Y , X, Y ) will be:

LGcGAN = Lgan (GX Y , DY , X, Y )
+ Lgan (G ˜X ˜Y , D ˜Y , X, Y )
+ λLgeo (GX Y , G ˜X ˜Y , X, Y ),

(5)

where λ (λ = 20.0 in all the experiments) is a trade-off
hyperparameter to weight the contribution of Lgan and

Lgeo during the model training. Carefully tuning λ may
give preferable results to speciﬁc translation tasks.

Network architecture. The full framework of our GcGAN
is illustrated in Figure 2. Our experimental settings,
network architectures, and learning strategies follow Cy-
cleGAN. We employ the same discriminator and generator
as CycleGAN depending on the speciﬁc tasks. Speciﬁcally,
the generator is a standard encoder-decoder, where the
encoder contains two convolutional layers with stride 2 and
several residual blocks [22] (6 / 9 blocks with respect to
128 × 128 / 256 × 256 of input resolution), and the decoder
contains two deconvolutional layers also with stride 2.
The discriminator distinguishes images at the patch level
following PatchGANs [26, 33]. Like CycleGAN, we also
use an identity mapping loss [55] in all of our experiments
(except SVHN → MNIST), including our baseline (GAN
alone). For other details, we use LeakyReLU as nonlinear-
ity for the discriminators and instance normalization [57]
to normalize convolutional feature maps.

Learning and inference. We use the Adam solver [29] with
a learning rate of 0.0002 and coefﬁcients of (0.5, 0.999),
where the latter is used to compute running averages of gra-
dients and their squares. The learning rate is ﬁxed in the
initial 100 epochs, and linearly decays to zero over the next
100 epochs. Following CycleGAN, the negative log likeli-
hood objective is replaced with a more stable and effective
least-squares loss [43] for Lgan . The discriminator is up-
dated with random samples from a history of generated im-
ages stored in an image buffer [54] of size 50. The generator
and discriminator are optimized alternately. In the inference
phase, we feed an image only into the learned generator
GX Y to obtain a translated image.

5. Experiments

We apply our GcGAN to a wide range of applications
and make both quantitative and qualitative comparisons
with the baseline (GAN alone) and previous state-of-the-
art methods including DistanceGAN and CycleGAN. We
also study different ablations (based on rot) to analyze
our geometry-consistency constraint. Since adversarial net-
works are not always stable, every independent experiment
could result in slightly different scores. The scores in the
quantitative analysis are computed by the average on three
independent experiments.

5.1. Quantitative Analysis

The results demonstrate that our geometry-consistency
constraint can not only partially ﬁlter out the candidate so-
lutions having mode collapse or semantic distortions and
thus produce more sensible translations, but also compati-

2430

Input

Ground Truth

GAN alone

CycleGAN

GcGAN

Input

Ground Truth

GAN alone

GcGAN

Input

Ground Truth

GAN alone

GcGAN

Figure 3: Qualitative comparison on Cityscapes (Parsing ⇋ Image) and Google Maps (Map ⇋ Aerial photo). GAN alone suffers from
mode collapse. Translated images by GcGAN contain more details. GcGAN = GAN alone + geometry consistency .

ble with other unsupervised constraints such as cycle con-
sistency [66] and distance preservation [5].

Cityscapes. Cityscapes [12] contains 3975 image-label
pairs, with 2975 used for training and 500 for validation
(test in this paper). For a fair comparison with CycleGAN,
the translators are trained at a resolution of 128 × 128 in
an unaligned fashion. We evaluate our domain mappers us-
ing FCN scores and scene parsing metrics following pre-
vious works [41, 12, 66]. Speciﬁcally, for parsing → im-
age, we assume that a high-quality translated image should
produce qualitative semantic segmentation like real images
when feeding it into a scene parser. Thus, we employ the
pretrained FCN-8s [41] provided by pix2pix [26] to pre-
dict semantic labels for the 500 translated images. The
label maps are then resized to the original resolution of
1024 × 2048 and compared against the ground truth labels
using some standard scene parsing metrics including pixel
accuracy, class accuracy, and mean IoU [41]. For image
→ parsing, since the fake labels are in the RGB format, we
simply convert them into class-level labels using the nearest
neighbor search strategy. In particular, we have 19 (cate-
gory labels) + 1 (ignored label) categories for Cityscapes,
each with a corresponding color value (RGB). For a pixel i
in a translated parsing, we compute the distances between
the 20 groundtruth color values and the color value of pixel
i. The label of pixel i should be the one with the small-

est distance. Then, the aforementioned metrics are used to
evaluate our mapping on the 19 category labels.
The parsing scores are presented in Table 1. Our Gc-
GAN outperforms the baseline (GAN alone) by a large mar-
gin. We take the average of pixel accuracy, class accu-
racy, and mean IoU as the ﬁnal score for analysis [65], i.e.,
score = (pixel acc + class acc + mean IoU)/3. For im-
age → parsing, GcGAN (32.6%) yields a slightly higher
score than CycleGAN (32.0%). For parsing → image, Gc-
GAN (29.0% ∼ 29.5%) obtains a convincing improvement
of 1.3% ∼ 1.8% over distanceGAN (27.7%).
We next perform ablation studies to further discuss Gc-
GAN. The scores are reported in Table 1. Speciﬁcally,
GcGAN-rot-Seperate shows that the generator GX Y em-
ployed in GcGAN is sufﬁcient to handle both the style trans-
fers (without shape deformation) X → Y and ˜X → ˜Y .
GcGAN-Mix-{comb, rand} demonstrate that persevering a
geometric transformation can ﬁlter out most of the candi-
date solutions having mode collapse or undesired shape de-
formation, but preserving more ones can not leach more.
Besides, GcGAN-Mix-rand performs slightly worse than
GcGAN-Mix-comb. One of the possible reasons is that nei-
ther Xrot→Yrot nor Xvf→Yvf are sufﬁciently trained in the
random case, which would decrease the effect of the afore-
mentioned co-regularization mechanism. For GcGAN-rot
+ Cycle, we set the trade-off parameter for Lcyc to 10.0 as
published in CycleGAN. The consistent improvement is a

2431

method

image → parsing
parsing → image
pixel acc
class acc mean IoU pixel acc
class acc mean IoU

CoGAN [40]
BiGAN/ALI [15, 16]
SimGAN [54]
CycleGAN (Cycle) [66]
DistanceGAN [5]
GAN alone (baseline)
GcGAN-rot
GcGAN-vf

Benchmark Performance
0.11
0.08
0.13
0.07
0.11
0.07
0.22
0.16
-
-
0.160
0.104
0.234
0.170
0.232
0.171

0.45
0.41
0.47
0.58
-
0.514
0.574
0.576

0.40
0.19
0.20
0.52
0.53
0.437
0.551
0.548

Ablation Studies (Robustness & Compatibility)
LGcGAN w/o Lgeo (λ = 0)
0.486
0.163
0.102
0.396
LGcGAN w/o Lgan ( ˜X , ˜Y )
0.549
0.199
0.139
0.526
GcGAN-rot-Seperate
0.575
0.233
0.170
0.545
GcGAN-Mix-comb
0.573
0.229
0.168
0.545
GcGAN-Mix-rand
0.564
0.217
0.156
0.547
GcGAN-rot + Cycle
0.587
0.246
0.182
0.557

0.10
0.06
0.10
0.17
0.19
0.161
0.197
0.196

0.148
0.184
0.196
0.197
0.192
0.201

0.06
0.02
0.04
0.11
0.11
0.098
0.129
0.127

0.088
0.111
0.124
0.128
0.123
0.132

Table 1: Parsing scores on Cityscapes. LGcGAN : The objective in Eqn. 5 with rot. GcGAN-rot-Separate: GX Y and G ˜X ˜Y do not share
parameters. GcGAN-Mix-comb: Training GcGAN with both vf and rot in each iteration. GcGAN-Mix-rand: Training GcGAN with
randomly chosen vf and rot in each iteration. GcGAN-rot + Cycle: GcGAN-rot with the cycle-consistency constraint.

method

class acc (%)

erate preferable translations.

Benchmark Performance
DistanceGAN (Dist.) [5]
CycleGAN (Cycle) [66]
Self-Distance [5]
GcGAN-rot
GcGAN-vf

26.8
26.1
25.2
32.5
33.3

Ablation Studies (Compatibility)
Cycle + Dist. [5]
18.0
GcGAN-rot + Dist.
34.0
GcGAN-rot + Cycle
33.8
GcGAN-rot + Dist. + Cycle
33.2

Table 2: Quantitative scores for SVHN → MNIST.

credible support that our geometry-consistency constraint
is compatible with the widely-used cycle-consistency con-
straint. Moreover, when setting λ = 0 in LGcGAN , both
GX Y and GY X perform badly. One of the possible rea-
sons is that, without the geometry consistency constraint,
jointly modeling X→Y and ˜X→ ˜Y with the shared gen-
erator GX Y would decrease the performance due to do-
main diversities caused by the geometric transformations.
When removing Lgan (G ˜X ˜Y , D ˜Y ), the obtained scores are
much higher than baseline (GAN alone) because Y ′ can
partially correct ˜Y ′ so that GX Y is able to handle the map-
ping ˜X→ ˜Y , and then ˜Y ′ can constrain the mapping X→Y .
As the analysis, when learning both Lgan (GX Y , DY ) and
Lgan (G ˜X ˜Y , D ˜Y ) with Lgeo , the co-regularization help gen-

SVHN → MNIST. We apply our approach to the SVHN →
MNIST translation task. The translation models are trained
on 73257 and 60000 training images of resolution 32 × 32
contained in the SVHN and MNIST training sets, respec-
tively. The experimental settings follow DistanceGAN [5],
including the default trade-off parameters for Lcyc and Ldis .
We compare our GcGAN with both DistanceGAN and Cy-
cleGAN in this translation task. To obtain quantitative re-
sults, we feed the translated images into a pretrained classi-
ﬁer trained on the MNIST training split, as done in [5].

Classiﬁcation accuracies are reported in Table 2. Both
GcGAN-rot and GcGAN-vf outperform DistanceGAN and
CycleGAN by a large margin (about 6% ∼ 7%). From
the ablations, adding our geometry-consistency constraint
to current unsupervised domain mapping frameworks will
achieve different levels of improvements against the origi-
nal ones. Note that, it seems that the distance-preservation
constraint is not compatible with the cycle-consistency con-
straint on this task, but our geometry-consistency constraint
can improve both ones.

Google Maps. We obtain 2194 (map, aerial photo) pairs of
images in and around New York City from Google Maps
[26], and split them into training and test sets with 1096
and 1098 pairs, respectively. We train Map ⇋ Aerial photo
translators with an image size of 256×256 using the training
set in an unsupervised manner (unpaired) by ignoring the
pair information. For Aerial photo → Map, we make com-

2432

DistanceGAN [5]

GcGAN

Figure 4: Qualitative comparison for SVHN → MNIST.

max(|ri − r ′
i |, |gi − g ′
i |, |bi − b′

parisons with CycleGAN using average RMSE and pixel
accuracy (%). Given a pixel i with the ground-truth RGB
value (ri , gi , bi ) and the predicted RGB value (r ′
i ), if
i |) < δ , we consider this is
an accurate prediction. Since maps only contain a limited
number of different RGB values, it is reasonable to compute
pixel accuracy using this strategy (δ1 = 5 and δ2 = 10 in
this paper). For Map → Aerial photo, we only show some
qualitative results in Figure 3.

i , g ′
i , b′

method

RMSE acc (δ1 )
Benchmark Performance
CycleGAN [66]
28.15
41.8
GAN alone (baseline)
33.27
19.3
GcGAN-rot
28.31
41.2
GcGAN-vf
28.50
37.3

acc (δ2 )

63.7
42.0
63.1
58.9

Ablation Studies (Robustness & Compatibility)
GcGAN-rot-Separate
30.25
40.7
60.8
GcGAN-Mix-comb
27.98
42.8
64.6
GcGAN-rot + Cycle
28.21
40.6
63.5

Table 3: Quantitative scores for Aerial photo → Map.

From the scores presented in Table 3, GcGAN produces
superior translations to the baseline (GAN alone). In partic-
ular, GcGAN yields an 18.0% ∼ 21.9% improvement over
the baseline with respect to pixel accuracy when δ = 5.0,
demonstrating that the fake maps obtained by our GcGAN
contain more details. In addition, GcGANs achieve com-
petitive scores compared with CycleGAN.

5.2. Qualitative Evalutation

The qualitative results are shown in Figure 3, Figure 4,
and Figure 5. Our geometry-consistency constraint improve
the training of GAN alone, and helps to generate empiri-
cally more impressive translations on various applications.
The following applications are trained in the image size of
256 × 256 with the rot geometric transformation.

Horse → Zebra. We apply GcGAN to the widely studied
object transﬁguration application task, i.e., Horse → Zebra.
The images are randomly sampled from ImageNet [13] us-
ing the keywords (i.e., wild horse and zebra). The numbers
of training images are 939 and 1177 for horse and zebra, re-

spectively. We ﬁnd that training GcGAN without parameter
sharing would produce preferable translations for the task.
Synthetic ⇋ Real. We employ the 2975 training images
from Cityscapes as the real-world scenes, and randomly se-
lect 3060 images from SYNTHIA-CVPR16 [49], which is
a virtual urban scene benchmark, as the synthetic images.
Summer ⇋ Winter. The images used for the season trans-
lation tasks are provided by CycleGAN. The training set
sizes for Summer and Winter are 1273 and 854.
Photo ⇋ Artistic Painting. We translate natural images to
artistic paintings with different art styles, including Monet,
Cezanne, Van Gogh, and Ukiyo-e. We also perform Gc-
GAN on the translation task of Monet’s paintings → pho-
tographs. We use the photos and paintings (Monet: 1074,
Cezanne: 584, Van Gogh: 401, Ukiyo-e: 1433, and Pho-
tographs: 6853) collected by CycleGAN for training.
Day ⇋ Night. We randomly extract 4500 training images
for both Day and Night from the 91 webcam sequences cap-
tured by [30].

6. Conclusion

In this paper, we propose to enforce geometry consis-
tency as a constraint for unsupervised domain mapping,
which can be viewed as a predeﬁned geometric transforma-
tion f (·) preserving the geometry of a scene. The geometry-
consistency constraint makes the translation networks on
the original images and transformed images co-regularize
each other, which not only provides an effective remedy to
the mode collapse problem suffered by standard GANs, but
also reduces the semantic distortions in the translation. We
evaluate our model, i.e., the geometry-consistent generative
adversarial network (GcGAN), both qualitatively and quan-
titatively in various applications. The experimental results
demonstrate that GcGAN achieves competitive and some-
times even better translations than the state-of-the-art meth-
ods including DistanceGAN and CycleGAN. Finally, our
geometry-consistency constraint is compatible with other
well-studied unsupervised constraints.

7. Acknowledgement

This research was supported by Australian Research
Council Projects FL-170100117, DP-180103424, and IH-
180100002. This work was partially supported by SAP
SE and CNRS INS2I-JCJC-INVISANA. This work is par-
tially supported by NIH Award Number 1R01HL141813-
01, NSF 1839332 Tripod+X, and SAP SE. We gratefully
acknowledge the support of NVIDIA Corporation with the
donation of the Titan X Pascal GPU used for this research.
We were also grateful for the computational resources pro-
vided by Pittsburgh Super Computing grant number TG-
ASC170024.

2433

Horse → Zebra

Monet → Photo

Input

CycleGAN

GcGAN

Input

CycleGAN

GcGAN

Real → Synthetic

Synthetic → Real

Input

GAN alone

GcGAN

Input

GAN alone

GcGAN

Winter → Summer

Summer → Winter

Input

GAN alone

GcGAN

Input

GAN alone

GcGAN

Photo → Artistic Painting

Photographs

Monet

Cezanne

Van Gogh

Ukiyo-e

Day → Night

Night → Day

Input

GcGAN

Input

GcGAN

Input

GcGAN

Input

GcGAN

Figure 5: Qualitative results on different applications, including Horse → Zebra, Monet → Photo, Synthetic ⇋ Real, Summar ⇋ Winter,
Photo → Artist Painting, and Day ⇋ Night. GcGAN has the potential to produce realistic images. Zoom in for better view.

2434

References

[1] Amjad Almahairi, Sai Rajeswar, Alessandro Sordoni, Philip
Bachman, and Aaron Courville. Augmented cyclegan:
Learning many-to-many mappings from unpaired data.
ICML, 2018. 2

[2] Asha Anoosheh, Eirikur Agustsson, Radu Timofte, and Luc
Van Gool. Combogan: Unrestrained scalability for image
domain translation. In CVPRW, 2018. 2

[3] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
2

[4] Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen
Wang, Eli Shechtman, and Trevor Darrell. Multi-content gan
for few-shot font style transfer. In CVPR, 2018. 2

[5] Sagie Benaim and Lior Wolf. One-sided unsupervised do-
main mapping. In NIPS, 2017. 1, 2, 3, 5, 6, 7

[6] Sagie Benaim and Lior Wolf. One-shot unsupervised cross
domain translation. NIPS, 2018. 2

[7] Konstantinos Bousmalis, George Trigeorgis, Nathan Silber-
man, Dilip Krishnan, and Dumitru Erhan. Domain separa-
tion networks. In NIPS, 2016. 2

[18] Mingming Gong, Kun Zhang, Biwei Huang, Clark Gly-
mour, Dacheng Tao, and Kayhan Batmanghelich. Causal
generative domain adaptation networks.
arXiv preprint
arXiv:1804.04333, 2018. 2

[19] Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao,
Clark Glymour, and Bernhard Sch ¨olkopf. Domain adapta-
tion with conditional transferable components.
In ICML,
pages 2839–2848, 2016. 2

[20] Abel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Ben-
gio. Image-to-image translation for cross-domain disentan-
glement. NIPS, 2018. 2

[21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
2, 4

[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
2016. 4

[23] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adapta-
tion. ICML, 2018. 2

[8] Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkel-
stein. Pairedcyclegan: Asymmetric style transfer for apply-
ing and removing makeup. In CVPR, 2018. 2

[24] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan
Kautz. Multimodal unsupervised image-to-image transla-
tion. ECCV, 2018. 2

[9] Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang
Hua. Stereoscopic neural style transfer. In CVPR, 2018. 2

[10] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reﬁnement networks. In ICCV, 2017.
2

[11] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,
Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera-
tive adversarial networks for multi-domain image-to-image
translation. In CVPR, 2018. 2

[12] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR),
2016. 5

[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, 2009. 7

[14] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep
generative image models using a? laplacian pyramid of ad-
versarial networks. In NIPS, 2015. 2

[15] Jeff Donahue, Philipp Kr ¨ahenb ¨uhl, and Trevor Darrell. Ad-
versarial feature learning. arXiv preprint arXiv:1605.09782,
2016. 6

[16] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier
Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron
Courville. Adversarially learned inference. arXiv preprint
arXiv:1606.00704, 2016. 6

[17] Aaron Gokaslan, Vivek Ramanujan, Daniel Ritchie,
Kwang In Kim, and James Tompkin. Improving shape defor-
mation in unsupervised image-to-image translation. ECCV,
2018. 2

[25] Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Mat-
sushita, and Yasushi Yagi. Probabilistic plant modeling via
multi-view image-to-image translation. CVPR, 2018. 2

[26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros.
Image-to-image translation with conditional adver-
sarial networks. In CVPR, 2017. 1, 2, 4, 5, 6

[27] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution.
In
ECCV, 2016. 2

[28] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee,
and Jiwon Kim. Learning to discover cross-domain relations
with generative adversarial networks. In ICML, 2017. 1, 2, 3

[29] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 4

[30] Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao, Chao Qian,
and James Hays. Transient attributes for high-level un-
derstanding and editing of outdoor scenes. ACM TOG,
33(4):149, 2014. 7

[31] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In CVPR, 2017. 2

[32] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh
Singh, and Ming-Hsuan Yang. Diverse image-to-image
translation via disentangled representations. In ECCV, 2018.
2

[33] Chuan Li and Michael Wand. Precomputed real-time texture
synthesis with markovian generative adversarial networks. In
ECCV, 2016. 4

2435

[34] Minjun Li, Haozhi Huang, Lin Ma, Wei Liu, Tong Zhang,
and Yugang Jiang. Unsupervised image-to-image translation
with stacked cycle-consistent adversarial networks. ECCV,
2018. 2

[35] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang
Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza-
tion via conditional invariant adversarial networks. In ECCV,
pages 624–639, 2018. 2

[36] Xiaodan Liang, Hao Zhang, and Eric P Xing. Generative
semantic manipulation with contrasting gan. NIPS, 2017. 2

[37] Jianxin Lin, Yingce Xia, Tao Qin, Zhibo Chen, and Tie-
Yan Liu. Conditional image-to-image translation. In CVPR,
2018. 2

[38] Alexander Liu, Yen-Chen Liu, Yu-Ying Yeh, and Yu-
Chiang Frank Wang. A uniﬁed feature disentangler for multi-
domain image translation and manipulation. NIPS, 2018. 2

[39] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised
image-to-image translation networks. In NIPS, 2017. 2

[40] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversar-
ial networks. In NIPS, 2016. 2, 6

[41] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation.
In
CVPR, 2015. 5

[42] Shuang Ma, Jianlong Fu, Chang Wen Chen, and Tao Mei.
Da-gan: Instance-level image translation by deep attention
generative adversarial networks. In CVPR, 2018. 2

[43] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen
Wang, and Stephen Paul Smolley. Least squares generative
adversarial networks. In ICCV, 2017. 4

[44] Youssef A Mejjati, Christian Richardt, James Tompkin, Dar-
ren Cosker, and Kwang In Kim. Unsupervised attention-
guided image to image translation. NIPS, 2018. 2

[45] A ¨aron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse
Espeholt, Alex Graves, and Koray Kavukcuoglu. Condi-
tional image generation with pixelcnn decoders.
In NIPS,
2016. 2

[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR, 2016. 1, 2

[47] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gener-
ative adversarial networks. In ICLR, 2016. 2

[48] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In ICML, 2016. 2

[49] German Ros, Laura Sellart, Joanna Materzynska, David
Vazquez, and Antonio M. Lopez. The synthia dataset: A
large collection of synthetic images for semantic segmenta-
tion of urban scenes. In CVPR, 2016. 7

[50] Am ´elie Royer, Konstantinos Bousmalis, Stephan Gouws,
Fred Bertsch, Inbar Moressi, Forrester Cole, and Kevin Mur-
phy. Xgan: Unsupervised image-to-image translation for
many-to-many mappings. ICLR, 2018. 2

[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In NIPS, 2016. 2

[52] Falong Shen, Shuicheng Yan, and Gang Zeng. Neural style
transfer via meta networks. In CVPR, 2018. 2
[53] Lu Sheng, Ziyi Lin, Jing Shao, and Xiaogang Wang. Avatar-
net: Multi-scale zero-shot style transfer by feature decora-
tion. In CVPR, 2018. 2
[54] Ashish Shrivastava, Tomas Pﬁster, Oncel Tuzel, Joshua
Susskind, Wenda Wang, and Russell Webb.
Learning
from simulated and unsupervised images through adversarial
training. In CVPR, 2017. 4, 6
[55] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised
cross-domain image generation, 2016. 4
[56] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Vic-
tor Lempitsky. Texture networks: feed-forward synthesis of
textures and stylized images. In ICML, 2016. 2
[57] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-
stance normalization: The missing ingredient for fast styliza-
tion. arXiv preprint arXiv:1607.08022, 2016. 4
[58] Chao Wang, Haiyong Zheng, Zhibin Yu, Ziqiang Zheng,
Zhaorui Gu, and Bing Zheng. Discriminative region pro-
posal adversarial networks for high-quality image-to-image
translation. In ECCV, 2018. 1, 2
[59] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image
synthesis and semantic manipulation with conditional gans.
arXiv preprint arXiv:1711.11585, 2017. 1, 2
[60] Yaxing Wang, Joost van de Weijer, and Luis Herranz. Mix
and match networks: encoder-decoder alignment for zero-
pair image translation. In CVPR, 2018. 2
[61] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan:
Unsupervised dual learning for image-to-image translation.
In CVPR, 2017. 1, 2, 3
[62] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In ICCV, 2017. 2
[63] Kun Zhang, Bernhard Sch ¨olkopf, Krikamol Muandet, and
Zhikun Wang. Domain adaptation under target and condi-
tional shift. In ICML, pages 819–827, 2013. 2
[64] Yexun Zhang, Ya Zhang, and Wenbin Cai. Separating style
and content for generalized style transfer. In CVPR, 2018. 2
[65] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba.
Scene parsing through
ade20k dataset. In CVPR, 2017. 5
[66] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks.
In CVPR, 2017. 1, 2, 3,
5, 6, 7
[67] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation. In NIPS, 2017.
2

2436

Image Deformation Meta-Networks for One-Shot Learning

Zitian Chen1
Yanwei Fu1 *
Yu-Xiong Wang2
Lin Ma3
Wei Liu3
Martial Hebert2
1Schools of Computer Science, and Data Science, Fudan University; Jilian Technology Group (Video++)
2Robotics Institute, Carnegie Mellon University
3Tencent AI Lab

{chenzt15,yanweifu}@fudan.edu.cn

yuxiongw@cs.cmu.edu

forest.linma@gmail.com

wl2223@columbia.edu

hebert@cs.cmu.edu

(a)

(b)

(c)

(d)

(e)

Figure 1. Illustration of a variety of image deformations: ghosted (a, b), stitched (c), montaged (d), and partially occluded (e) images.

Abstract

Humans can robustly learn novel visual concepts even
when images undergo various deformations and loose cer-
tain information. Mimicking the same behavior and syn-
thesizing deformed instances of new concepts may help vi-
sual recognition systems perform better one-shot learning,
i.e., learning concepts from one or few examples. Our key
insight is that, while the deformed images may not be vi-
sually realistic, they still maintain critical semantic infor-
mation and contribute signiﬁcantly to formulating classi-
ﬁer decision boundaries. Inspired by the recent progress of
meta-learning, we combine a meta-learner with an image
deformation sub-network that produces additional training
examples, and optimize both models in an end-to-end man-
ner. The deformation sub-network learns to deform images
by fusing a pair of images — a probe image that keeps the
visual content and a gallery image that diversiﬁes the de-
formations. We demonstrate results on the widely used one-
shot learning benchmarks (miniImageNet and ImageNet 1K
Challenge datasets), which signiﬁcantly outperform state-
of-the-art approaches.

1. Introduction

Deep architectures have made signiﬁcant progress in var-
ious visual recognition tasks, such as image classiﬁcation
and object detection. This success typically relies on super-

*Yanwei Fu is the corresponding author

vised learning from large amounts of labeled examples. In
real-world scenarios, however, one may not have enough re-
sources to collect large training sets or need to deal with rare
visual concepts. It is also unlike the human visual system,
which can learn a novel concept with very little supervision.
One-shot or low/few-shot learning [4], which aims to build
a classiﬁer for a new concept from one or very few labeled
examples, has thus attracted more and more attention.
Recent efforts to address this problem have leveraged a
learning-to-learn or meta-learning paradigm [25, 20, 28,
32, 31, 22, 33, 17, 5, 13]. Meta-learning algorithms train a
learner, which is a parameterized function that maps labeled
training sets to classiﬁers. Meta-learners are trained by sam-
pling a collection of one-shot learning tasks and the corre-
sponding datasets from a large universe of labeled examples
of known (base) categories, feeding the sampled small train-
ing set to the learner to obtain a classiﬁer, and then comput-
ing the loss of the classiﬁer on the sampled test set. The
goal is that the learner is able to tackle the recognition of
unseen (novel) categories from few training examples.
Despite their noticeable performance improvements,
these generic meta-learning algorithms typically treat im-
ages as black boxes and ignore the structure of the visual
world. By contrast, our biological vision system is very ro-
bust and trustable in understanding images that undergo var-
ious deformations [27, 1]. For instance, we can easily rec-
ognize the objects in Figure 1, despite ghosting (Figure 1(a,
b)), stitching (Figure 1(c)), montaging (Figure 1(d)), and
partially occluding (Figure 1(e)) the images. While these

8680

deformed images may not be visually realistic, our key in-
sight is that they still maintain critical semantic information
and presumably serve as “hard examples” that contribute
signiﬁcantly to formulating classiﬁer decision boundaries.
Hence, by leveraging such modes of deformations shared
across categories, the synthesized deformed images could
be used as additional training data to build better classiﬁers.
A natural question then arises: how could we produce in-
formative deformations? We propose a simple parametriza-
tion that linearly combines a pair of images to generate the
deformed image. We use a probe image to keep the vi-
sual content and overlay a gallery image on a patch level to
introduce appearance variations, which could be attributed
to semantic diversity, artifacts, or even random noise. Fig-
ure 5 shows some examples of our deformed images. Im-
portantly, inspired by [30], we learn to deform images that
are useful for a classiﬁcation objective by end-to-end meta-
optimization that includes image deformations in the model.
Our Image Deformation Meta-Network (IDeMe-Net)
thus consists of two components:
a deformation sub-
network and an embedding sub-network. The deforma-
tion sub-network learns to generate the deformed images
by linearly fusing the patches of probe and gallery images.
Speciﬁcally, we treat the given small training set as the
probe images and sample additional images from the base
categories to form the gallery images. We evenly divide
the probe and gallery images into nine patches, and the de-
formation sub-network estimates the combination weight of
each patch. The synthesized images are used to augment the
probe images and train the embedding sub-network, which
maps images to feature representations and performs one-
shot classiﬁcation. The entire network is trained in an end-
to-end meta-learning manner on base categories.
Our contributions are three-fold.
(1) We propose
a novel image deformation framework based on meta-
learning to address one-shot learning, which leverages the
rich structure of shared modes of deformations in the visual
world. (2) Our deformation network learns to synthesize di-
verse deformed images, which effectively exploits the com-
plementarity and interaction between the probe and gallery
image patches. (3) By using the deformation network, we
effectively augment and diversify the one-shot training im-
ages, leading to a signiﬁcant performance boost on one-shot
learning tasks. Remarkably, our approach achieves state-of-
the-art performance on both the challenging ImageNet1K
and miniImagenet datasets.

2. Related Work

Meta-Learning. Typically, meta-learning [25, 24, 20,
28, 32, 31, 22, 33, 17, 5, 13, 37, 15] aims at training
a parametrized mapping from a few training instances to
model parameters in simulated one-shot learning scenarios.
Other meta-learning strategies in one-shot learning include

graph CNNs [7] and memory networks [19, 2]. Attention is
also introduced in meta-learning, in ways of analyzing the
relation between visual and semantic representations [29]
and learning the combination of temporal convolutions and
soft attention [14]. Different from prior work, we focus on
exploiting the complementarity and interaction between vi-
sual patches through the meta-learning mechanism.

Metric Learning. This is another important line of work
in one-shot learning. The goal is to learn a metric space
which can be optimized for one-shot learning. Recent work
includes Siamese networks [11], matching networks [28],
prototypical networks [22], relation networks [23], and dy-
namic few-shot learning without forgetting [8].

Data Augmentation. The key limitation of one-shot learn-
ing is the lack of sufﬁcient training images. As a com-
mon practice, data augmentation has been widely used to
help train supervised classiﬁers [12, 3, 35]. The standard
techniques include adding Gaussian noise, ﬂipping, rotat-
ing, rescaling, transforming, and randomly cropping train-
ing images. However, the generated images in this way
are particularly subject to visual similarity with the origi-
nal images. In addition to adding noise or jittering, previ-
ous work seeks to augment training images by using semi-
supervised techniques [31, 18, 16], or directly synthesizing
new instances in the feature domain [9, 30, 21, 6] to transfer
knowledge of data distribution from base classes to novel
classes. By contrast, we also use samples from base classes
to help synthesize deformed images but directly aim at max-
imizing the one-shot recognition accuracy.

The most relevant to our approach is the work of [30, 36].
Wang et al. [30] introduces a generator to hallucinate novel
instances in the feature domain by adding noise, whereas
we focus on learning to deform two real images in the im-
age domain without introducing noise. Zhang et al. [36]
randomly sample image pairs and linearly combine them to
generate additional training images. In this mixup augmen-
tation, the combination is performed with weights randomly
sampled from a prior distribution and is thus constrained to
be convex. The label of the generated image is similarly
the linear combination of the labels (as one-hot label vec-
tors) of the image pairs. However, they ignore structural
dependencies between images as well as image patches. By
contrast, we learn classiﬁers to select images that are simi-
lar to the probe images from the unsupervised gallery image
set. Our combination weights are learned through a defor-
mation sub-network on the image patch level and the com-
bination is not necessarily convex. In addition, our gener-
ated image preserves the label of its probe image. Com-
paring with these methods, our approach learns to dynami-
cally fuse patches of two real images in an end-to-end man-
ner. The produced images maintain the important patches
of original images while being visually different from them,
thus facilitating training one-shot classiﬁers.

8681

3. One-Shot Learning Setup

Following recent work [28, 17, 5, 22, 30], we establish
one-shot learning in a meta-learning framework: we have a
base category set Cbase and a novel category set Cnovel , in
which Cbase

Figure 2. The overall architecture of our image deformation meta-network (IDeMe-Net).

port set ˜S . Following [22], we calculate the prototype vector
θ for each class c in ˜S as

pc

pc
θ =

1

Z X(Ii ,yi )∈ ˜S

fθemb (Ii ) · Jyi = cK ,

(2)

where Z = Σ(Ii ,yi )∈ ˜S Jyi = cK is the normalization factor.
J·K is the Iverson’s bracket notation: JxK = 1 if x is true, and
0 otherwise. Given any query image Ii ∈ Q, its probability
of belonging to class c is computed as

Pθ (yi = c|Ii ) =

, (3)

exp (− kfθemb (Ii ) − p c
θ k)
j=1 exp (cid:16)− (cid:13)(cid:13)(cid:13)

fθemb (Ii ) − pj

θ (cid:13)(cid:13)(cid:13)(cid:17)

PN

where k · k indicates the Euclidean distance. The one-shot
classiﬁer P thus predicts the class label of Ii as the highest
probability over N classes.

5. Training Strategy of IDeMe-Net

5.1. Training Loss

Training the entire IDeMe-Net includes two subtasks:
(1) training the deformation sub-network which maximally
improves the one-shot classiﬁcation accuracy; (2) building
the robust embedding sub-network which effectively deals
with various synthesized deformed images. Note that our
one-shot classiﬁer has no parameters, which does not need
to be trained. We use the prototype loss and the cross-
entropy loss to train these two sub-networks, respectively.
Update the deformation sub-network. We optimize the
following prototype loss function to endow the deformation
sub-network with the desired one-shot classiﬁcation ability:

minθ EL∼Dbase

ES,G,Q∼L 
 X(Ii ,yi )∈Q

−logPθ (yi | Ii )



, (4)

where Pθ (yi | Ii ) is the one-shot classiﬁer in Eq. (3). Using
the prototype loss encourages the deformation sub-network
to generate diverse instances to augment the support set.
Update the embedding sub-network. We use the cross-
entropy loss to train the embedding sub-network to directly
classify the augmented support set ˜S . Note that with the

Algorithm 1 Meta-training procedure of our IDeMe-Net
fθ . G is the ﬁxed gallery constructed from Cbase .

1: procedure M ETA -TRA IN E P I SODE

3:

8:

7:

6:

5:

4:

2: The procedure of one meta-training episode
L ← randomly sample N classes from Cbase
S ← randomly sample instances belonging to L
//sample the support set
Q ← randomly sample instances belonging to L
//sample the query set
train the prototype classiﬁer P from fθemb (S )
⊲ initialize the augment support set
for c in L do
⊲ enumerate the chosen classes
pool ←use P to select ǫ% images in G that have
the highest class probability of c
for (Iprob , c) in Sc do
for j = 1 to naug do
Igallery ← randomly sample instances

˜S ← S

9:

10:

11:

12:

13:

14:

from pool

15:

16:

Isyn ← fθdef (Iprob , Igallery )

˜S ← ˜S ∪ (Isyn , c)

21:

20:

19:

17:

18:

end for
end for
end for
train the prototype classiﬁer ˜P from fθemb ( ˜S )
use ˜P to classify fθemb (Q) and obtain the prototype
loss
use the softmax classiﬁer to classify fθemb ( ˜S ) and
obtain the CELoss
update θemb with the CELoss
update θdef with the prototype loss
25: end procedure

23:

24:

22:

augmented support set ˜S , we have relatively more training
instances to train this sub-network and the cross-entropy
loss is the standard loss function in training a supervised
classiﬁcation network. Empirically, we ﬁnd that using the
cross-entropy loss speeds up the convergence and improves
the recognition performance than using the prototype loss
only.

8683

5.2. Training Strategy

We summarize the entire training procedure of our
IDeMe-Net on the base dataset Dbase in Algorithm 1. We
have access to the same, predeﬁned gallery G from Dbase
for both meta-training and meta-testing. During meta-
training, we sample the N -way-m-shot training episode to
produce S and Q from Dbase . The embedding sub-network
learns an initial one-shot classiﬁer on S using Eq. (3). Given
a probe image Iprobe , we then sample the gallery images
Igallery ∼ G and train the deformation sub-network to gen-
erate the augmented support set ˜S using Eq. (1). ˜S is fur-
ther used to update the embedding sub-network and learn
a better one-shot classiﬁer. We then conduct the ﬁnal one-
shot classiﬁcation on the query set Q and back-propagate
the prediction error to update the entire network. During
meta-testing, we sample the N -way-m-shot testing episode
to produce S and Q from the novel dataset Dnovel .

6. Experiments

Our IDeMe-Net is evaluated on two standard bench-
marks: miniImageNet [28] and ImageNet 1K Challenge [9]
datasets. miniImageNet is a widely used benchmark in one-
shot learning, which includes 600 images per class and has
100 classes in total. Following the data split in [17], we use
64, 16, 20 classes as the base, validation, and novel category
set, respectively. The hyper-parameters are cross-validated
on the validation set. Consistent with [28, 17], we evaluate
our model in 5-way-5-shot and 5-way-1-shot settings.
For the large-scale ImageNet 1K dataset, we divide the
original 1K categories into 389 base (Dbase ) and 611 novel
(Dnovel ) classes following the data split in [9]. The base
classes are further divided into two disjoint subsets: base
validation set Dcv
base (193 classes) and evaluation set Df in
(196 classes) and the novel classes are divided into two sub-
sets as well: novel validation set Dcv
novel (300 classes) and
evaluation set Df in
novel (311 classes). We use the base/novel
validation set Dcv for cross-validating hyper-parameters
and use the base/novel evaluation set Df in to conduct the ﬁ-
nal experiments. The same experimental setup is used in [9]
and the reported results are averaged over 5 trails. Here we
focus on synthesizing novel instances and we thus evaluate
the performance primarily on novel classes, i.e., 331-way-
m-shot settings, which is also consistent with most of the
contemporary work [28, 22, 17].

base

6.1. Results on ImageNet 1K Challenge

Setup. We use ResNet-10 architectures for ANET and
BNET (i.e., the deformation sub-network). For a fair com-
parison with [9, 30], we evaluate the performance of using
ResNet-10 (Table 1) and ResNet-50 (Table 2) for the em-
bedding sub-network. Stochastic gradient descent (SGD)

is used to train IDeMe-Net in an end-to-end manner.
It
gets converged over 100 epochs. The initial learning rates
of ANET, BNET, and the embedding sub-network are set
as 3 × 10−3 , 3 × 10−3 , and 10−1 , respectively, and de-
creased by 1/10 every 30 epochs. The batch size is set as
32. We randomly sample 10 images per base category to
construct the gallery G and we set ǫ as 2. Note that G is
ﬁxed during the entire experiments. ANET, BNET, and the
embedding sub-network are trained from scratch on Dbase .
Our model is evaluated on Dnovel . naug is cross-validated
as 8, which balances between the computational cost and
the augmented training data scale. In practice, we perform
stage-wise training to overcome potential negative inﬂuence
caused by misleading training images synthesized by the
initial deformation sub-network. Speciﬁcally, in the ﬁrst
20 epochs, we ﬁx the deformation sub-network and train
the embedding sub-network with only real images to ob-
tain good initial classiﬁers. In the next 20 epochs, we ﬁx
the embedding sub-network and learn the deformation sub-
network to reduce the discrepancy between synthesized and
real images. Finally, we train the embedding and defor-
mation sub-networks jointly (i.e., the entire IDeMe-Net) to
allow them to cooperate with each other.

Baselines and Competitors. We compare against several
baselines and competitors as follows. (1) We directly train a
ResNet-10 feature extractor on Dbase and use it to compute
features on Dnovel . We then train standard supervised clas-
siﬁers on Dnovel , including neural network, support vec-
tor machine (SVM), logistic regression (LR), and prototype
classiﬁers. The neural network classiﬁer consists of a fully-
connected layer and a softmax layer. (2) We compare with
state-of-the-art approaches to one-shot learning, such as
matching networks [28], generation SGM [9], prototypical
networks [22], Cosine Classiﬁer & Att. Weight Gen (Cos
& Att.) [8], CP-ANN [6], PMN, and PMN w/ H [30]. (3)
The data augmentation methods are also compared — ﬂip-
ping: the input image is ﬂipped from left to right; Gaussian
noise: cross-validated Gaussian noise N (0, 10) is added
to each pixel of the input image; Gaussian noise (feature
level): cross-validated Gaussian noise N (0, 0.3) is added
to each dimension of the ResNet feature for each image;
Mixup: using mixup [36] to combine probe and gallery im-
ages. For fair comparisons, all theses augmentation meth-
ods use the prototype classiﬁer as the one-shot classiﬁer.

Results. Tables 1 and 2 summarize the results of using
ResNet-10 and ResNet-50 as the embedding sub-network,
respectively. For example, using ResNet-10, the top-5 accu-
racy of IDeMe-Net in Table 1 is superior to the prototypical
network by 7% when m = 1, 2, 5, showing the sample efﬁ-
ciency of IDeMe-Net for one-shot learning. With more data
(e.g., m = 10, 20), while the plain prototype classiﬁer base-
line performs worse than other baselines (e.g., PMN), our
deformed images coupled with the prototype classiﬁer still

8684

Baselines

Competitors

Augmentation

Method

Softmax
LR
SVM
Prototype Classiﬁer

Matching Network [28]
Prototypical Network [22]
Generation-SGM [9]
PMN [30]
PMN w/ H [30]
Cos & Att. [8]
CP-AAN [6]

Flipping
Gaussian Noise
Gaussian Noise (feature level)
Mixup [36]

m = 1

2

5

10

20

– /16.3
18.3/42.8
15.9/36.6
17.1/39.2

– / 43.0
16.9/41.7
– / 34.3
– / 43.3
– / 45.8
– / 46.0
– / 48.4

17.4/39.6
16.8/39.0
16.7/39.1
15.8/38.7

– /35.9
26.0/54.7
22.7/48.4
24.3/51.1

– / 54.1
24.0/53.6
– / 48.9
– / 55.7
– / 57.8
– / 57.5
– / 59.3

24.7/51.2
24.0/51.2
24.2/51.4
24.6/51.4

– /57.4
35.8/66.1
31.5/61.2
33.8/63.9

– / 64.4
33.5/63.7
– / 64.1
– / 68.4
– / 69.0
– / 69.1
– / 70.2

33.7/64.1
33.9/63.7
33.4/63.3
32.0/61.1

- / 67.3
41.1/71.3
37.9/69.2
38.4/69.9

– / 68.5
37.7/68.2
– / 70.5
– / 74.0
– / 74.3
– / 74.8
– / 76.5

38.7/70.2
38.0/69.7
38.2/69.5
38.5/69.2

– /72.1
44.9/74.8
43.9/74.6
44.1/74.7

– / 72.8
42.7/72.3
– /74.6
– / 77.0
– / 77.4
– / 78.1
– / 79.3

44.2/74.5
43.8/74.5
44.0/74.2
42.1/72.9

Ours
IDeMe-Net
23.1/51.0
30.1/60.9
39.3/70.4
42.7/73.4
45.0/75.1
Table 1. Top-1 / Top-5 accuracy (%) on novel classes of the ImageNet 1K Challenge dataset. We use ResNet-10 as the embedding
sub-network. m indicates the number of training examples per class. Our IDeMe-Net consistently achieves the best performance.

Method

Softmax
SVM
LR

Proto-Clsf

G-SGM [9]
PMN [30]
PMN w/ H [30]
IDeMe-Net (Ours)

m = 1

2

5

10

– /28.2
20.1/41.6
22.9/47.9
20.8/43.1

– /47.3
– / 53.3
– / 54.7
30.3/60.1

– /51.0
29.4/57.7
32.3/61.3
29.9/58.1

– /60.9
– / 65.2
– / 66.8
39.7/69.6

– / 71.0
42.6/72.8
44.3/73.6
42.4/72.3

– /73.7
– / 75.9
– / 77.4
47.5/77.4

– /78.4
49.9/79.1
50.9/78.8
49.5/79.0

– /79.5
– / 80.1
– / 81.4
51.3/80.2

Table 2. Top-1 / Top-5 accuracy (%) on novel classes of the Im-
agenet 1K Challenge dataset. We use ResNet-50 as the embed-
ding sub-network. m indicates the number of training examples
per class. Proto-Clsf and G-SGM denote the prototype classiﬁer
and generation SGM [9], respectively.

have signiﬁcant effect (e.g., 3.5 point boost when m = 10).
The top-1 accuracy demonstrates the similar trend. Using
ResNet-50 as the embedding sub-network, the performance
of all the approaches improves and our IDeMe-Net consis-
tently achieves the best performance, as shown in Table 2.
Figure 3(a) further highlights that our IDeMe-Net consis-
tently outperforms all the baselines by large margins.

6.2. Ablation Study on ImageNet 1K Challenge

We conduct extensive ablation study to evaluate the con-
tribution of each component in our model.
Variants of IDeMe-Net. We consider seven different vari-
ants of our IDeMe-Net, as shown in Figure 3(b) and Ta-
ble 3. (1) ‘IDeMe-Net - CELoss’: the IDeMe-Net is trained
using only the prototype loss without the cross-entropy loss
(CELoss). (2) ‘IDeMe-Net - Proto Loss’: the IdeMe-Net is
trained using only the cross-entropy loss without the pro-
totype loss.
(3) ‘IDeMe-Net - Predict’:
the gallery im-

ages are randomly chosen in IDeMe-Net without predict-
ing their class probability. (4) ‘IDeMe-Net - Aug. Testing’:
the deformed images are not produced in the meta-testing
phase. (5) ‘IDeMe-Net - Def. Network’: the combination
weights in Eq. (1) are randomly generated instead of us-
ing the learned deformation sub-network. (6) ‘IDeMe-Net -
Gallery’: the gallery images are directly sampled from the
support set instead of constructing an additional Gallery. (7)
‘IDeMe-Net - Deform’: we simply use the gallery images to
serve as the deformed images. As shown in Figure 3(b), our
full IDeMe-Net model outperforms all these variants, show-
ing that each component is essential and complementary to
each other.
We note that (1) Using CELoss and prototype loss to
update the embedding and deformation sub-networks,
respectively, achieves the best result. As shown in
Figure 3(b),
the accuracy of ‘IDeMe-Net - CELoss’ is
marginally lower than IDeMe-Net but still higher than the
prototype classiﬁer baseline, while ‘IDeMe-Net - Proto
Loss’ underperforms the baseline.
(2) Our strategy for
selecting the gallery images is the key to diversify the
deformed images. Randomly choosing the gallery im-
ages (‘IDeMe-Net - Predict’) or sampling the gallery im-
ages from the support set (‘IDeMe-Net - Gallery’) obtains
no performance improvement. One potential explanation
is that they only introduce noise or redundancy and do not
bring in useful information.
(3) Our improved perfor-
mance mainly comes from the diversiﬁed deformed im-
ages, rather than the embedding sub-network. Without
producing the deformed images in the meta-testing phase
(‘IDeMe-Net - Aug. Testing’), the performance is close
to the baseline, suggesting that training on the deformed
images does not obviously beneﬁt from the embedding

8685

Method

m = 1

2

5

10

20

Baselines

LR
Prototype Classiﬁer

IDeMe-Net - CELoss
IDeMe-Net - Proto Loss
IDeMe-Net - Predict
IDeMe-Net - Aug. Testing
IDeMe-Net - Def. Network
IDeMe-Net - Gallery
IDeMe-Net - Deform
IDeMe-Net (1 × 1)
IDeMe-Net (5 × 5)
IDeMe-Net (7 × 7)
IDeMe-Net (pixel level)

Variants

Patch Size

18.3/42.8
17.1/39.2

21.3/50.0
15.3/36.7
17.0/39.3
17.0/39.1
15.9/38.0
17.5/39.4
15.7/37.8

16.2/39.3
24.1/51.7
23.8/52.1
17.3/39.0

26.0/54.7
24.3/51.1

28.0/58.3
21.4/50.4
24.0/50.7
24.30/51.3
24.1/50.1
24.2/51.4
22.7/49.8

24.4/52.1
30.3/61.2
30.2/61.3
23.8/51.2

35.8/66.1
33.8/63.9

37.7/69.4
31.7/62.0
33.6/63.5
33.5/63.8
32.6/63.3
33.5/63.7
31.9/62.6

32.9/63.0
39.6/70.4
39.1/70.2
34.1/63.7

41.1/71.3
38.4/69.9

41.3/71.6
37.9/69.0
38.0/69.2
38.0/69.1
38.2/68.9
38.7/70.3
38.0/68.7

38.8/69.5
42.4/73.2
42.7/73.1
38.5/70.2

44.9/74.8
44.1/74.7

44.3/74.3
43.7/73.7
43.7/73.8
43.8/74.5
42.4/73.1
44.4/74.5
43.5/73.8

42.7/73.2
44.3/74.6
44.5/74.7
43.9/74.5

Ours
IDeMe-Net
23.1/51.0
30.4/60.9
39.3/70.4
42.7/73.4
45.0/75.1
Table 3. Top-1 / Top-5 accuracy (%) of the ablation study on novel classes of the ImageNet 1K Challenge dataset. We use ResNet-10
as the embedding sub-network. m indicates the number of training examples per class. Our full model achieves the best performance.

sub-network.
(4) Our meta-learned deformation sub-
network effectively exploits the complementarity and in-
teraction between the probe and gallery image patches,
producing the key information in the deformed images.
To show this point, we investigate two deformation strate-
gies: randomly generating the weight vector w (‘IDeMe-
Net - Def. Network’) and setting all the weights to be 0
(‘IDeMe-Net - Deform’); in the latter case, it is equiva-
lent to purely using the gallery images to serve as the de-
formed images. Both strategies perform worse than the
prototype classiﬁer baseline, indicating the importance of
meta-learning a deformation strategy.

Different division schemes.
In the deformation sub-
network and Eq. (1), we evenly split the image into 3 × 3
patches. Some alternative division schemes are compared in
Table 3 and Figure 3(c). Speciﬁcally, we consider the 1 × 1,
5 × 5, 7 × 7, and pixel-level division schemes and report the
results as IDeMe-Net (1 × 1), IDeMe-Net (5 × 5), IDeMe-
Net (7 × 7), and IDeMe-Net (pixel level), respectively. The
experimental results suggest the patch-level fusion, rather
than image-level or pixel-level fusion in our IDeMe-Net.
The image-level division (1 × 1) ignores the local image
structures and deforms through a global combination, thus
decreasing the diversity. The pixel-level division is particu-
larly subject to the disarray of the local information, while
the patch-level division (3 × 3, 5 × 5, and 7 × 7) consid-
ers image patches as the basic unit to maintain some local
information. In addition, the results show that using a ﬁne-
grained patch size (e.g., 5 × 5 division and 7 × 7 division)
may achieve slightly better results than our 3×3 division. In
brief, our patch-level division not only maintains the critical
region information but also increases diversity.

Number of synthesized deformed images. We also show
how the top-5 accuracy changes with respect to the number

(a)

(b)

(c)
(d)
Figure 3. Ablation study on ImageNet 1K Challenge dataset:
(a) highlights the comparison with several competitors; (b) shows
the impact of different components on our IDeMe-Net; (c) ana-
lyzes the impact of different division schemes; (d) shows how the
performance changes with respect to the number of synthesized
deformed images. Best viewed in color with zoom.

of synthesized deformed images in Figure 3(d). Speciﬁ-
cally, we change the number of synthesized deformed im-
ages naug in the deformation sub-network, and plot the 5-
shot top-5 accuracy on the Imagenet 1K Challenge dataset.
It shows that when naug is changed from 0 to 8, the per-
formance of our IDeMe-Net is gradually improved. The
performance saturates when enough deformed images are
generated (naug > 8).
Visualization of deformed images in feature space. Fig-

8686

(a) Gaussian Baseline

(b) IDeMe-Net - Deform

(c) IDeMe-Net

Figure 4. t-SNE visualization of 5 novel classes. Dots, stars,
and triangles represent the real examples, the probe images, and
the synthesized deformed images, respectively. (a) Synthesis by
adding Gaussian noise. (b) Synthesis by directly using the gallery
images. (c) Synthesis by our IDeMe-Net. Best viewed in color
with zoom.

Figure 5. Examples of the deformed images during meta-testing.
1st row: probe images of novel classes.
2nd: gallery im-
ages of base classes.
3rd:
synthesized images. The probe-
gallery image pairs from left
to right: vase–jellyﬁsh, vase–
oboe, vase–garbage bin, vase–soup pot, golden retriever–poodle,
golden retriever–walker hound, golden retriever–walker hound,
and golden retriever–poodle. Best viewed in color with zoom.

ure 4 shows the t-SNE [26] visualization of 5 novel classes
from our IDeMe-Net, the Gaussian noise baseline, and the
‘IDeMe-Net - Deform’ variant. For the Gaussian noise
baseline, the synthesized images are heavily clustered and
close to the probe images. By contrast, the synthesized
deformed images of our IDeMe-Net scatter widely in the
class manifold and tend to locate more around the class
boundaries. For ‘IDeMe-Net - Deform’, the synthesized
images are the same as the gallery images and occasionally
fall into manifolds of other classes. Interesting, comparing
Figure 4(b) and Figure 4(c), our IDeMe-Net effectively de-
forms those misleading gallery images back to the correct
class manifold.

Visualization of deformed images in image space. Here
we show some examples of our deformed images on novel
classes in Figure 5. We can observe that the deformed im-
ages (in the third row) are visually different from the probe
images (in the ﬁrst row) and the gallery images (in the sec-
ond row). For novel classes (e.g., vase and golden retriever),
our method learns to ﬁnd visual samples that are similar
in shape and geometry (e.g., jelly ﬁsh, garbage bin, and
soup pot) or similar in appearance (e.g., poodle and walker
hound). By doing so, the deformed images preserve im-
portant visual content from the probe images and introduce
new visual contents from the gallery images, thus diversify-

Method

miniImageNet (%)
1-shot

5-shot

MAML [5]

48.70±1.84

63.11±0.92

Meta-SGD [13]

50.47±1.87

64.03±0.94

Matching Network [28]

43.56±0.84

55.31±0.73

Prototypical Network [22]

49.42±0.78

68.20±0.66

Relation Network [23]

57.02±0.92

71.07±0.69

SNAIL [14]

55.71±0.99

68.88±0.92

Delta-Encoder [21]

58.7

73.6

Cos & Att. [8]

55.45±0.89

70.13 ±0.68

Prototype Classiﬁer

52.54±0.81

72.71±0.73

IDeMe-Net (Ours)
5 9 .1 4±0.86
74.63±0.74
Table 4. Top-1 accuracy (%) on novel classes of
the
miniImageNet dataset. “±” indicates 95% conﬁdence intervals
over tasks.

ing and augmenting the training images in a way that max-
imizes the one-shot classiﬁcation accuracy.

6.3. Results on miniImageNet

Setup and Competitors. We use a ResNet-18 architecture
as the embedding sub-network. We randomly sample 30
images per base category to construct the gallery G. Other
settings are the same as those on the ImageNet 1k Challenge
dataset. As summarized in Table 4, we mainly focus on
three groups of competitors: (1) meta-learning algorithms,
such as MAML [5] and Meta-SGD [13]; (2) metric learning
algorithms, including matching networks [28], prototypical
networks [22], relation networks [23], SNAIL [14], delta-
encoder [21], and Cosine Classiﬁer & Att. Weight Gen (Cos
& Att.) [8].
Results. We report the results in Table 4.
Impressively,
our IDeMe-Net consistently outperforms all these state-of-
the-art competitors. This further validates the general effec-
tiveness of our proposed approach in addressing one-shot
learning tasks.

7. Conclusion

In this paper, we propose a conceptually simple yet
powerful approach to address one-shot learning that uses a
trained image deformation network to generate additional
examples. Our deformation network leverages unsuper-
vised gallery images to synthesize deformed images, which
was trained end-to-end by meta-learning. The extensive ex-
periments demonstrate that our approach achieves state-of-
the-art performance on multiple one-shot learning bench-
marks, surpassing the competing methods by large margins.
Acknowledgment: This work is supported in part by the
grants from NSFC (#61702108), STCSM (#16JC1420400),
Eastern Scholar (TP2017006), and The Thousand Talents
Plan of China (for young professionals, D1410009).

8687

References

[1] A. Boccolini, A. Fedrizzi, and D. Faccio. Ghost imag-
ing with the human eye. 2018. 1

[2] Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei. Memory
matching networks for one-shot image recognition. In
CVPR, 2018. 2

[3] K. Chatﬁeld, K. Simonyan, A. Vedaldi, and A. Zisser-
man. Return of the devil in the details: Delving deep
into convolutional nets. In BMVC, 2014. 2

[4] L. Fei-Fei, R. Fergus, and P. Perona. One-shot learn-
ing of object categories. IEEE TPAMI, 2006. 1

[5] C. Finn, P. Abbeel, and S. Levine. Model-agnostic
meta-learning for fast adaptation of deep networks. In
ICML, 2017. 1, 2, 3, 6.3, 6.3

[6] H. Gao, Z. Shou, A. Zareian, H. Zhang, and S.-F.
Chang. Low-shot learning via covariance-preserving
adversarial augmentation networks. In NeurIPS, 2018.
2, 6.1, 6

[7] V. Garcia and J. Bruna. Few-shot learning with graph
neural networks. In ICLR, 2018. 2

[8] S. Gidaris and N. Komodakis. Dynamic few-shot vi-
sual learning without forgetting. In CVPR, 2018. 2,
6.1, 6, 6.3, 6.3

[9] B. Hariharan and R. Girshick. Low-shot visual recog-
nition by shrinking and hallucinating features.
In
ICCV, 2017. 2, 6, 6.1, 6, 2

[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual
learning for image recognition. In CVPR, 2015. 4.1,
4.2

[11] G. Koch, R. Zemel, and R. Salakhutdinov. Siamese
neural networks for one-shot image recognition.
In
ICML – Deep Learning Workshok, 2015. 2

[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Im-
agenet classiﬁcation with deep convolutional neural
networks. In NeurIPS, 2012. 2

[13] Z. Li, F. Zhou, F. Chen, and H. Li. Meta-SGD:
Learning to learn quickly for few shot
learning.
arxiv:1707.09835, 2017. 1, 2, 6.3, 6.3

[14] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel.
A simple neural attentive meta-learner. In ICLR, 2018.
2, 6.3, 6.3

[15] T. Munkhdalai and H. Yu. Meta networks. In ICML,
2017. 2

[18] M. Ren, E. Triantaﬁllou, S. Ravi, J. Snell, K. Swer-
sky, J. B. Tenenbaum, H. Larochelle, and R. S. Zemel.
Meta-learning for semi-supervised few-shot classiﬁ-
cation. In ICLR, 2018. 2

[19] A. Santoro, S. Bartunov, M. Botvinick, D. Wier-
stra, and T. Lillicrap. Meta-learning with memory-
augmented neural networks. In ICML, 2016. 2

[20] J. Schmidhuber.
Evolutionary principles in self-
referential learning. On learning how to learn: The
meta-meta-... hook.) Diploma thesis, Institut f. Infor-
matik, Tech. Univ. Munich, 1987. 1, 2

[21] E. Schwartz, L. Karlinsky, J. Shtok, S. Harary,
M. Marder, R. Feris, A. Kumar, R. Giryes, and A. M.
Bronstein. Delta-encoder: An effective sample syn-
thesis method for few-shot object recognition.
In
NeurIPS, 2018. 2, 6.3, 6.3

[22] J. Snell, K. Swersky, and R. S. Zemeln. Prototypical
networks for few-shot learning. In NeurIPS, 2017. 1,
2, 3, 4.3, 6, 6.1, 6, 6.3, 6.3

[23] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and
T. M. Hospedales. Learning to compare: Relation net-
work for few-shot learning. In CVPR, 2018. 2, 6.3,
6.3

[24] S. Thrun. Learning to learn: Introduction. Kluwer
Academic Publishers, 1996. 2

[25] S. Thrun. Lifelong learning algorithms. Learning to
learn, 8:181–209, 1998. 1, 2

[26] L. van der Maaten and G. Hinton. Visualizing high-
dimensional data using t-SNE. Journal of Machine
Learning Research, 2008. 6.2

[27] J. Vermaak, S. Maskell, and M. Briers. Online sensor
registration. In IEEE Aerospace Conference, 2005. 1

[28] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu,
and D. Wierstra. Matching networks for one shot
learning.
In NeurIPS, 2016. 1, 2, 3, 6, 6.1, 6, 6.3,
6.3

[29] P. Wang, L. Liu, C. Shen, Z. Huang, A. Hengel, and
H. Tao Shen. Multi-attention network for one shot
learning. In CVPR, pages 6212–6220, 07 2017. 2

[30] Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariha-
ran. Low-shot learning from imaginary data. In CVPR,
2018. 1, 2, 3, 6.1, 6, 6

[31] Y.-X. Wang and M. Hebert. Learning from small
sample sets by combining unsupervised meta-training
with CNNs. In NeurIPS, 2016. 1, 2

[16] A. Rasmus, H. Valpola, M. Honkala, M. Berglund,
and T. Raiko. Semi-supervised learning with ladder
networks. In NeurIPS, 2015. 2

[32] Y.-X. Wang and M. Hebert. Learning to learn: Model
regression networks for easy small sample learning. In
ECCV, 2016. 1, 2

[17] S. Ravi and H. Larochelle. Optimization as a model
for few-shot learning. In ICLR, 2017. 1, 2, 3, 6

[33] Y.-X. Wang, D. Ramanan, and M. Hebert. Learning to
model the tail. In NeurIPS, 2017. 1, 2

8688

[34] Z. Xu, L. Zhu, and Y. Yang. Few-shot object recog-
nition from machine-labeled web images. In CVPR,
2017. 3

[35] M. D. Zeiler and R. Fergus. Visualizing and under-
standing convolutional networks. In ECCV, 2014. 2

[36] H. Zhang, M. Ciss ´e, Y. N. Dauphin, and D. Lopez-
Paz. Mixup: Beyond empirical risk minimization. In
ICLR, 2018. 2, 6.1, 6

[37] F. Zhou, B. Wu,
and Z. Li.
Deep meta-
learning: Learning to learn in the concept space.
arxiv:1802.03596, 2018. 2

8689

Incremental Object Learning from Contiguous Views

Stefan Stojanov1 , Samarth Mishra∗1 , Ngoc Anh Thai∗1 , Nikhil Dhanda1 , Ahmad Humayun1 ,
Chen Yu2 , Linda B. Smith2 , James M. Rehg1
Georgia Institute of Technology1
Indiana University Bloomington2
{sstojanov, smishra, athai6, nn3, ahmadh, rehg}@gatech.edu
{chenyu, smith4}@indiana.edu

Abstract

Pick up

Put down

Object Naming Event

In this work, we present CRIB (Continual Recognition
Inspired by Babies), a synthetic incremental object learn-
ing environment that can produce data that models visual
imagery produced by object exploration in early infancy.
CRIB is coupled with a new 3D object dataset, Toys-200,
that contains 200 unique toy-like object instances, and is
also compatible with existing 3D datasets. Through ex-
tensive empirical evaluation of state-of-the-art incremental
learning algorithms, we ﬁnd the novel empirical result that
repetition can signiﬁcantly ameliorate the effects of catas-
trophic forgetting. Furthermore, we ﬁnd that in certain
cases repetition allows for performance approaching that
of batch learning algorithms. Finally, we propose an unsu-
pervised incremental learning task with intriguing baseline
results.

1. Introduction

Children are amazing learning machines.1
Infants ac-
quire extensive object knowledge through self-directed play
with minimal supervision, a fact which is remarkable in
contrast to the quantity of labeled data required by current
deep learning methods. During play, infants pick up, exam-
ine, and put down toys of their own volition. The moments
in which a supervisory signal is available, for example when
an adult names an object, are extremely rare in compari-
son to the huge volume of unlabeled perceptual inputs. See
Fig. 1 for a schematic of this play process.
Research in child development [8, 49, 22] has identiﬁed
ﬁve key properties of infants’ play experiences. First, while
infants become experts at object categorization, the bulk of
their early visual experience involves object instances, in

∗ Equal contribution.
1 In the domain of word learning, for example, children acquire an aver-
age of 8 to 10 new words per day and reach a vocabulary of 60,000 words
by adulthood [34].

Time

Object 7

Object 4

Object 7

Figure 1: Schematic of incremental object learning based
on infant play. Objects occur sequentially as exposures con-
sisting of sets of frames with contiguous viewpoints. A
sparse and noisy supervisory signal for category learning
(naming events), accompanies the wealth of visual data.

the form of toys and everyday objects. Second, their ex-
posure to object instances is highly repetitive, with many
objects (e.g. a favorite sippy cup) recurring over and over
again [8]. Third, when infants hold and manipulate objects,
they generate extended, contiguous views that may help in
revealing 3D object shape [49, 21, 37]. Fourth, infant learn-
ing is fundamentally incremental, as objects are held and
examined in sequence, and once an object has been put
down, its imagery is no longer available for learning. Fifth,
infants must provide their own supervision when learning
about instances, and leverage a sparse, noisy, and unsyn-
chronized supervisory signal when learning object names.2
These properties of the infant learning environment stand
in stark contrast to current methods for object learning
in computer vision, which are based on processing mini-
batches of randomly-sampled, labelled frames that cover
a signiﬁcant subset of the label space. This approach en-
sures that gradient updates do not favor one class over
another in moving collectively towards higher accuracy.
However, when data is processed incrementally in standard
deep learning architectures, the result is catastrophic for-
getting, in which object representations developed early in
training are forgotten at the expense of more recent exam-
ples [14, 17]. Recent works on incremental learning have

2While there is a debate in developmental science about the extent to
which children’s knowledge is innate versus learned, in this paper we focus
on the task of learning from visual experience.

18777

Figure 2: A rendering of approximately one third of the 3D models in Toys-200.

developed methods using distillation loss [29] and exem-
plars [38, 6] to address the catastrophic forgetting problem,
and they represent a valuable point of contact with infant
learning. Crucially, however, these prior works have not
incorporated repetition, which we will demonstrate to be
critical for effective incremental learning (see Sec. 4.3).

This paper introduces a developmentally-motivated en-
vironment for object learning known as CRIB (Continual
Recognition Inspired by Babies), which supports incremen-
tal learning of object instances (and categories) from con-
tiguous views with repetition, in both supervised and un-
supervised settings. CRIB is an ideal testbed for research
in incremental learning, as it provides convenient access to
unlimited data with the ability to precisely manipulate key
dimensions of the learning task and ensure reproducibility.
CRIB comes with a novel dataset, Toys-200, consisting of
3D models of 200 diverse and developmentally-appropriate
object instances. Our experiments with CRIB have uncov-
ered some intriguing empirical properties of incremental
learning tasks which have not been observed in prior work.
Speciﬁcally, we show that in incremental learning with rep-
etition it is possible to ameliorate the effects of catastrophic
forgetting, with the performance of pre-trained models ap-
proaching that of batch-learning. These ﬁndings hold for
both instance and category learning across a diversity of
datasets (Toys-200, ShapeNet [7], and CIFAR[26]).

CRIB is implemented as an API that can easily be incor-
porated into data loaders for standard deep learning frame-
works like PyTorch and TensorFlow, and will be made
freely-available to the research community.
It supports
the paradigm illustrated in Figure 1, in which the learner
receives a sequence of object learning exposures, each
one consisting of a set of frames corresponding to a con-
tiguous sequence of views of a particular object instance.
CRIB supports three different incremental learning tasks,
and we provide baseline results and extensive experimen-
tal results for each in Sec. 4. We hope that CRIB will en-

able new lines of attack on both incremental learning and
developmentally-motivated object learning problems.
In
summary, this work makes the following contributions:

• The CRIB environment for developmentally-inspired
object learning along with the Toys-200 dataset of
developmentally-plausible 3D object instances
• A freely-available data generator which integrates into
standard deep learning platforms, supports existing 3D
datasets, and is capable of generating unlimited data
for incremental instance and category learning
• The identiﬁcation of incremental learning with repeti-
tion as a key learning task which makes it possible to
ameliorate the effects of catastrophic forgetting
• An extensive evaluation of the effects of distillation
loss, explicit exemplar memory and repetitions on
both supervised and unsupervised incremental learn-
ing tasks3

2. Related Work

This paper is most closely related to prior work on incre-
mental learning using deep models, and our experiments
leverage existing algorithms for learning without forget-
ting [29], iCARL [38], and E2EIL [6].
In comparison
to these works, we provide a novel learning environment
(CRIB with Toys-200) and several novel tasks, as well as
extensive experiments on multiple datasets that illuminate
important aspects of incremental learning approaches, such
as the role of repeated exposures, distillation loss, and the
impact of exemplar set size, on incremental learning per-
formance. In contrast, prior works [29, 24, 28, 32, 38, 6]
did not address instance learning or the use of 3D models
to learn from contiguous viewpoints. They addressed only

3All resources needed to reproduce the experimental results in this pa-
per and any subsequent releases of software and data will be available at
https://iolfcv.github.io/

8778

the single exposure paradigm for category learning using
existing image datasets of a ﬁxed size.

Another related body of work is open world recognition
(of which representative citations are [2, 3, 10]). It is rel-
evant due to its emphasis on self-supervision. Our exper-
iments on weakly-supervised learning from sequential ob-
ject exposures in Sec. 4.4 are a point of contact with this
literature, although our speciﬁc paradigm and methods dif-
fer from this prior work.

Our development of CRIB is part of an on-going effort
to explore the use of computer graphics rendering and sim-
ulation environments to investigate machine learning top-
ics in controlled settings and address the large scale data
requirements of deep learning. Examples include purpose-
built autonomous driving simulators such as TORCS [47]
and CARLA [13], and efforts to leverage commercial video
games [25, 40, 39]. Multiple synthetic optical ﬂow datsets
[33, 5, 46] have led to performance improvements, as have
generated 3D car assets from [35]. Although the Active Vi-
sion Dataset [1] is not synthetic, it is a dense collection of
RGB-D images of real scenes that can simulate the visual
information perceived by a robot moving through an envi-
ronment. We are not aware of any prior work on simulation
environments which speciﬁcally target the learning tasks or
synthetic data generation goals addressed by CRIB.

Our work on Toys-200 is related to other efforts in curat-
ing datasets of objects for recognition tasks. Prior work on
collecting real image datasets of 3D objects, such as NORB
[27], COIL [36], and more recently, CORe50 [31], are less
relevant to this work. More closely-related are works that
created synthetic 3D object datasets, such as ShapeNet [7]
and Sculptures [45], which have led to signiﬁcant progress
in the domain [41]. In comparison, Toys-200 contains fewer
instances (307 for Sculptures and 51K for ShapeNet). How-
ever, it occupies a sweet-spot in terms of size and diversity,
as the Toys-200 objects are highly diverse in comparison to
both Sculptures and ShapeNet and were designed to reﬂect
the types of toys and everyday objects that infants would
be likely to encounter. In conjunction with CRIB, we can
support a much wider range of data generation approaches
than any prior works, as summarized in Table ?? of the Ap-
pendix.

This paper is also connected to a long line of research on
developmentally-inspired approaches to robotics and learn-
ing (e.g. venues such as [9]). Works such as Gepperth
et. al. [15] and Kanan et. al. [23] connect to our interest
in biologically-inspired incremental learning. Recent work
by Haber et. al. [18] shares our interest in play behavior.
Other works have developed speciﬁc computational models
for children’s cognitive processes (see [30] for a recent ex-
ample). None of these works address the speciﬁc tasks or
settings which characterize our paper.

3. Approach

In order to achieve our goal of exploring the behavior of
incremental object recognition algorithms in a developmen-
tally plausible setting, we require a visual learning environ-
ment with the following characteristics:
Unlimited Data: The ability to efﬁciently generate unlim-
ited visual data for each of our objects is critical because
it allows us to vary the amount of repetition and generate
arbitrarily long experimental runs while ensuring that the
learning algorithm continues to receive novel inputs.
Developmental relevance: Our goal is to generate visual
data which simulates the object exploration behaviors in
early infancy. This requires the use of developmentally-
plausible object sets and the ability to generate sequences
of contiguous object views.
Integration: To facilitate rapid experimentation, it must
be easy to integrate our learning environment with existing
data loading mechanisms in modern deep learning frame-
works.
We develop CRIB (Continual Recognition Inspired by
Babies)—a synthetic visual learning environment that ful-
ﬁlls these requirements. CRIB can generate unlimited
learning exposures in the form of contiguous views of ob-
ject instances. Since CRIB is implemented as a Python
API it is directly compatible with all popular deep learning
frameworks. CRIB is built using the free and cross platform
3D graphics software Blender and uses the Cycles ray trac-
ing engine for rendering. The following section describes
how CRIB provides a novel environment for incremental
learning experiments.

3.1. CRIB Learning Environment

In this section we describe the process by which we cre-
ated the Toys-200 dataset and the details of the object ren-
dering approach in CRIB.

3.1.1

3D Object Models for Toys-200

A highly diverse set of toy-like objects is central to gen-
erating developmentally plausible object instance data for
visual recognition. We collected the Toys-200 dataset of
200 unique toy object models from Blendswap [4], select-
ing models that were freely-available under a CC license.
We began by targeting a core set of 30 speciﬁc object cate-
gories [49] that are frequently used in research with infants,
identifying the best 3D model instance for each one.
In
order to build a challenging and visually-diverse (See Fig-
ure 2) dataset, we supplemented this initial set with addi-
tional toy-like objects. The criterion of ”toy-like” was im-
plemented by selecting objects which were similar to the
core objects in terms of their level of detail in shape and
appearance, and their plausibility for being a child’s toy. A
speciﬁc material shader was developed to give Toys-200 by

8779

Figure 3: Steps in generating visual exposures using CRIB (top to bottom): 1. Foreground object rendering, 2. background
scene selection, 3. foreground and background compositing. A visual exposure consists of multiple clips corresponding to
arcs on view sphere.

combining basic Blender material shaders, set up to give the
objects surface texture and reﬂectance properties of plastic,
toy-like objects. For our experiments involving category
learning, we used the well-known ShapeNet [7] dataset,
with appropriate modiﬁcations to incorporate it into CRIB.
Refer to the Appendix for more details.

3.1.2 Generating Learning Exposures

A learning exposure is a sequence of images obtained by
rotating an object of interest relative to a ﬁxed camera, de-
signed to simulate the kind of object views that children are
known to generate during object play [21]. An exposure
consists of a sequence of short video clips, where each clip
is generated by rendering a sequence of images of the object
as its pose is linearly interpolated between two ﬁxed poses.
See Figure 3 for an illustration of one sequence. The ﬁnal
object pose for one sequence is the starting pose for the next,
and the images from each sequence are concatenated into a
single contiguous sequence to form the learning exposure.
CRIB generates learning exposures from 3D object mod-
els using a set of user-speciﬁed API parameters. Below we
discuss key aspects of the rendering process, and a detailed
technical description of all the steps to generate a learning
exposure can be found in the Appendix.
Lighting: In the API, the user speciﬁes a lighting setting
of either four point or three rod light sources placed above
the object. Further characteristics are deﬁned by parameters
for pose, temperature and intensity.
Object rotation: Object rotation in CRIB is generated
by linearly interpolating between object poses (azimuth, ro-
tation, elevation, scale) over a number of frames. To spec-
ify the qualitative characteristics of object rotation in the
learning exposure, the user speciﬁes parameters for the to-
tal number of frames in the learning exposure and the total
number of different object poses for interpolation.
Preprocessing: Once the API parameters are speciﬁed,

the following fully-automated process proceeds: The target
object is imported into Blender, its center of mass is esti-
mated and it is positioned in the center of the camera frame.
The object is then appropriately scaled so that it remains
remains inside the camera ﬁeld of view during the rotating
motion around its center of mass and the change in scale.
Foreground rendering: The speciﬁed light sources are
instantiated and the sequence of frames is rendered with-
out a background. At this step, instance segmentations and
bounding boxes are collected for the foreground object.
Background rendering: Backgrounds are image se-
quences of objects from Toys-200, which are distributed
over the ﬂoor to create a cluttered background. The cam-
era above the objects moves slightly over time to emulate
head motion (such background frames are illustrated in Fig-
ure 3). This results in a dynamic, cluttered background en-
vironment which makes the recognition task more challeng-
ing and simulates real-life play scenarios in which a child
interacts with a set of toys (e.g. dumped from a toybox).
We ensure that the foreground object is not also present in
the background. Once the background sequence has been
rendered, the ﬁnal step is to composite the foreground and
background layers in each frame, and add a small amount
of pixel-wise noise.
Testing image generation: For evaluation purposes,
CRIB can also generate single images of a target object at a
random rotation, elevation and scale, with random lighting
conditions and backgrounds.

3.2. Learning Tasks in CRIB

CRIB supports three different incremental learning sce-
narios, two of which are novel. In each case, CRIB provides
learning exposures which are combined with stored past im-
ages in forming minibatches which are used for training.
The details vary with the task and the architecture, and are
detailed in the following sections.
Supervised Single Exposure: This is the standard in-

8780

cremental learning task, in which classes or instances are
presented sequentially to the learner. The key property is
that the learner sees each object exactly once. This leads to
catastrophic forgetting in all of the cases that we evaluated.
Supervised Repeated Exposure:
In this novel task,
classes or instances are presented to the learner sequentially
with repetition. At random, the learner is given new learn-
ing exposures for previously-seen objects. Our experiments
demonstrate that allowing a limited amount of repetition
(e.g. 10 exposures each for 200 object instances) allows
existing algorithms to approach the batch performance.4
Unsupervised Repeated Exposure: In this task, learn-
ers receive repeated exposures to a sequence of objects, but
no labels are provided. This is similar to discriminative in-
cremental clustering [16]. This very challenging task re-
quires the learner to identify learning instances correspond-
ing to novel objects, and re-identify previously-seen ob-
jects. It mirrors the challenge infants face during play, as
most of their learning exposures will not be accompanied
by an object name.

4. Experiments

In this section, we introduce the baseline algorithms in
our study, and present novel experimental results for the
three incremental learning tasks from Sec. 3.2. Performance
is measured using incremental accuracy as in [38]: Follow-
ing training on each learning exposure, the classiﬁcation
accuracy is computed on unseen test samples from all in-
stances or categories the learner has seen up to that point.
All learning exposures generated with CRIB are 100
frames long and interpolate between three randomly-chosen
points on the view sphere, with scale smoothly varying
from 0.3 to 1.1. Light source position is jittered at random
and light intensity is randomly-sampled from 4000-6000K
(indoor lighting temperature range). 100 random testing
frames are generated for each object. Refer to the appendix
for additional details.

4.1. Incremental Learning Methods

We produced our own implementations of three recent
CNN-based incremental learning algorithms [29, 38, 6].
Differences in our implementation from the original are
described below and in the appendix. All methods use
ResNet-34 [19] as the backbone architecture.
LwF [29] addresses catastrophic forgetting by modify-
ing the loss function used to train a standard CNN incre-
mentally. Each time a new class is introduced, the fully
connected layer of the CNN is expanded by adding an out-
put sigmoid unit for the new class. Distillation loss [20] is

4Note that in experiments with 3D rendered data, such as Toys-200, it
is still the case that each rendered image is used only once, as each new
learning instance will correspond to a new trajectory on the view sphere.

applied to the outputs for the other classes in attempt to pre-
serve the information they encode and prevent signiﬁcant
changes due to backprop on the current class. Our LwF dif-
fers from [29] in using sigmoid units rather than a softmax
layer for classiﬁcation, and in performing additional data
augmentation.
iCaRL [38] builds on LwF by including explicit mem-
ory in the form of an exemplar set managed by the learn-
ing algorithm. The exemplars are used to perform nearest
exemplar mean classiﬁcation in feature space. The infer-
ence procedure consists of computing normalized exemplar
mean features per class using the CNN, and then classifying
by determining the nearest exemplar mean from the normal-
ized features of each testing sample. Training follows LwF
when distillation loss is used, otherwise it is standard CNN
training. Our iCaRL [38] implementation uses additional
data augmentation.
E2EIL [6] builds on the previous two methods. During
training, the loss takes into account ground truth labels of
the samples from the other classes as well as the current
class. Unlike iCaRL, training is end-to-end since the net-
work outputs are used for classiﬁcation. E2EIL adds bal-
anced ﬁne-tuning which targets the case when the number
of samples from the other classes is signiﬁcantly lower than
the number of samples for the current class. Exemplar set
construction follows iCaRL, but is done twice: after training
and after balanced ﬁne-tuning. Our implementation adopts
a temperature-squared weighting [20] for distillation loss,
computes distillation loss over all seen classes, and uses a
different data augmentation scheme.
Our experiments include both training from scratch and
initializing weights from a pre-trained ILSVRC-2014 [11]
architecture. Based on prior transfer learning results [44,
48, 12], we would expect that starting from a pretrained ar-
chitecture should yield better performance, and our results
conﬁrm this. We also train with and without distillation
loss, in order to quantify its beneﬁt. Note that when dis-
tillation loss is not used, we apply the classiﬁcation loss to
all output nodes and use the exemplar labels.
Our naming convention: in iCaRL-PT-ND, PT indicates
starting with a pre-trained backbone architecture, and ND
means that distillation loss is not used, whereas iCaRL-S-D
refers to training from scratch (i.e. random weight initial-
ization) and using distillation loss.

4.2. Single Exposure Yields Catastrophic Forgetting

In this section we demonstrate that the single exposure
task leads to catastrophic forgetting for all of our baseline
methods in two datasets: Toys-200 and CIFAR-100.
In
comparison to prior work [29, 38, 6], our Toys-200 experi-
ments are the ﬁrst demonstration of catastrophic forgetting
in instance learning, and our CIFAR-100 experiments differ
in that we present classes one-at-a-time rather than two or

8781

(a)

(b)

Figure 4: (a) Performance of iCaRL, E2EIL and LwF when presented with a single exposure for each object instance from
CRIB-Toys-200. (b) shows performance of the same methods with repeated exposure.

more. For the Toys-200 experiment, we use 200 exposures,
one for each object instance. E2EIL and iCaRL use an ex-
emplar set size of 600 images, or 3% of the total data (as
compared to 4% in [38, 6]). We computed standard error
bars by repeating each experiment 3 times.
As evident in Figure 4a, all results for Toys-200 have
a general downward trend, which is similar to the results
in [38, 29, 6] and is attributed to catastrophic forgetting of
the instances which were seen early in the sequence. The re-
sults for iCaRL-PT-(D/ND) show that training with distilla-
tion is not favorable in this task, while the results for E2EIL-
PT-(D/ND) show that distillation loss does not make a dif-
ference. We ﬁnd that test accuracy can be easily improved
by using a pre-trained model, aligning with [44, 48, 12].
In addition, we tested on CIFAR-100 [26] with iCaRL-S-
ND and iCaRL-PT-ND (see Figure 5) with single classes
presented sequentially. Further experiments using random
initialization are included in the Appendix.

Figure 5: Performance of iCaRL-S-ND and iCaRL-PT-ND
on CIFAR-100 conﬁrm catastrophic forgetting. Both algo-
rithms have an exemplar set size of 2000.

4.3. Repetition Reduces Catastrophic Forgetting

using 2000 learning exposures (each object appearing ten
times), with an explicit memory of 600 exemplars. For ev-
ery experimental run, we generate a random sequence of
object instances such that all methods experience the same
number of objects by each time step, but not the same in-
stances in the same order. Figure 4b shows the results.
E2EIL and iCaRL-PT-ND achieve an accuracy close to the
batch learning algorithm, whereas iCaRL-PT-D does not
show an improvement. Note that the performance gap be-
tween iCaRL-PT-D and iCaRL-PT-ND in this task is larger
in comparison to the single exposure task. This poten-
tially indicates that distillation loss is hindering the ability
to leverage repeated exposures for iCaRL, and highlights
the advantage of simply using the exemplar labels. Results
for algorithms trained from random initialization in the Ap-
pendix further conﬁrm this ﬁnding.
We perform further experiments using three datasets:
CRIB-Toys, CRIB-ShapeNet [7] and CIFAR [26] to (1)
evaluate whether incremental learning with repeated expo-
sures can allow incremental algorithms to get close to the
performance of batch algorithms beyond an instance learn-
ing task (2) evaluate the importance of the number of ex-
emplars on the accuracy gains from repeated exposure. We
perform the following experiments:

1. CRIB-Toys-50: 50 objects over 500 learning expo-
sures (each object is shown 10 times).
2. CRIB-ShapeNet-20: 20 categories over 500 learn-
ing exposures. (25 instances from each category are
shown)
3. CIFAR-20: 20 categories over 1000 learning expo-
sures (each category is shown 50 times).

In this section, we demonstrate that introducing repe-
titions during incremental learning ameliorates the effects
of catastrophic forgetting, resulting in improvements in ac-
curacy and enabling the majority of tested algorithms to
eventually approach the performance of a pre-trained batch
learning method. We also examine the effect of the number
of exemplars.
Our ﬁrst experiments with repetition are with Toys-200,

Figure 6 contains the results for this experiment. It is
evident that the same trend applies to all three datasets: the
performance of the algorithms declines at ﬁrst before in-
creasing as they get more repeated exposures of previously
seen objects, and towards the end gets close to the perfor-
mance of a batch learning algorithm. We believe these are
the ﬁrst ﬁndings for learning with repetitions in incremental
learning.

8782

(a)

(c)

(b)

(d)

Figure 6: Top : Performance of (a) iCaRL-PT-ND and (b) E2EIL-PT-ND with different number of exemplars on 50 objects
of CRIB-Toys. Bottom : (c) Performance of iCaRL-PT-ND (400 exemplars) on 20 categories of CIFAR (d) Performance of
iCaRL-PT-ND (1500 exemplars) on 20 categories of CRIB-ShapeNet. (Best viewed with zoom)

For CRIB-Toys-50, as evident from Figures 6a and 6b,
both iCaRL-PT-ND and E2EIL-PT-ND maintain the up-
ward trend ﬁrst observed in Figure 4b. Additionally, this
experiment demonstrates that for an instance task, regard-
less of the total number of exemplars (18%, 12%, 3% or
1% of the total data), given sufﬁcient repetition, the accu-
racy of iCaRL-PT-ND is close to pre-trained batch perfor-
mance. While E2EIL-PT-ND maintains an increasing trend,
the variants trained with small numbers of exemplars are not
as close to the pre-trained batch model after 500 exposures.

CRIB-ShapeNet-20 is a categorization task where re-
peated exposures are different instances from the same cat-
egory. 25 instances for training and 15 for testing are cho-
sen randomly from 20 categories of the ShapeNet Core55
dataset [7]. Learning exposures generated with CRIB for
each instance are provided over 500 exposures and testing is
done on 100 frames of random object views, scale and light-
ing for each instance in the test set for a seen category. The
performance (Figure 6d), shows that repeated exposures to
new instances in the same category leads to improvements
for categorization. This result extends our initial ﬁnding of
improvement towards batch performance via repeated ex-
posures to a different task and dataset.

For CIFAR-20, we sample with replacement 100 images
from a total of 500 images per category for each learning
exposure. This exposes the algorithm to images repeatedly
during different exposures. The performance on iCaRL-PT-
ND decreases at ﬁrst and starts to go up after all 20 objects
have been seen (Figure 6c). Coverage for each category is

the portion of unique images seen within a category, with
the mean over all categories shown on the plot. The accu-
racy improvement rate decreases after 100% of the data is
shown to iCaRL-PT-ND. The ﬁnal incremental accuracy is
on par with a batch algorithm, showing that improvements
due to repetition of concepts are not unique to CRIB.

4.4. Incremental Learning Without Supervision

In this task a learning algorithm needs to do novelty
detection—to determine whether or not an exposure comes
from a novel instance, and recognition—to determine which
previously-seen instance it belongs to, prior to updating pa-
rameters.
Prior work on open set and open world recognition
[42, 43] tackles the subproblem of novelty detection by
thresholding on known class scores to detect whether a new
data point belongs to a class that has not been encountered.
Drawing from these works, we use the following algorithm
for our straightforward baseline based on iCaRL-PT-ND. At
any given exposure, in unit normalized feature space, the
algorithm ﬁrst ﬁnds the distance of the images from a learn-
ing exposure to all the exemplar means. This is followed by
ﬁnding the mean distance of the images in the exposure, and
using it as a score to determine whether the current object
has been previously seen. If the minimum distance-score is
more than a given threshold, the exposure labeled as com-
ing from a new object instance, otherwise it gets classiﬁed
as the previously seen instance with the minimum distance-
score. The threshold used was found as the optimal oper-

8783

0
0
1
=
h

t

g
n
e

l

e

r

u

s

o
p
x
e
g
n

i

n

r

a
e

L

0
1
=
h

t

g
n
e

l

e

r

u

s
e
s
s
a

l

c

n
e
e
s

r

e
v
o
y
c
a

r

u
c
c

A

%

d
e

r

e
v
o
c
s

i

d

/

n
e
e
s

s

t

c
e

j

b
o

e
g
a

t

n
e
c

r

e

P

o
p
x
e
g
n

i

s

n

r

a
e

L

Number of Learning Exposures

Figure 7: Performance of iCaRL-PT-ND (600 exemplars) on unsupervised repeated exposures of CRIB-Toys-50, 100, 150,
200 (left to right) with learning exposure lengths of 10 and 100. The 10 length learning exposures are the ﬁrst 10 frames of
the 100 length learning exposure. (Best viewed with zoom)

ating point from a precision-recall analysis over the binary
classiﬁcation problem of novelty detection.
We evaluate this baseline algorithm on different learn-
ing exposure lengths and number of repeated exposure tasks
with CRIB. After each learning exposure, testing is done on
random views of ground truth seen objects. Since the labels
given by a learning algorithm in this task need not have any
correspondence with the ground truth labels, a one-to-one
correspondence is ﬁrst established between these sets of la-
bels based on a maximum accuracy matching, and then the
learning algorithm’s test accuracy is computed.
Figure 7 contains the results of this study. For 100 learn-
ing exposure length, the algorithm’s accuracy is constant
or decreases with a greater number of objects and four re-
peated exposures. Further, for the same learning exposure
length, the proportion of objects discovered compared to the
ground truth number of unique objects seen decreases with
greater object set sizes. Across all experiments with differ-
ent total numbers of objects, there is a consistent trend that
a smaller learning exposure length results in a lower ﬁnal
accuracy. Furthermore, a lower learning exposure length re-
sults in higher variability in performance over multiple runs
with different order of objects encountered.

categories and instances, based on rendering learning expo-
sures from 3D object models. CRIB models the kinds of
object views generated by infants during play. We intro-
duce a novel instance learning dataset called Toys-200. We
use CRIB to study three incremental learning scenarios and
demonstrate that allowing repeated exposures dramatically
improves the performance of state-of-the-art methods, al-
lowing them to converge to the batch learning accuracy in
many cases. Finally, we show intriguing results on the chal-
lenging new task of incremental learning without supervi-
sion. Our CRIB enviornment and data is freely-available,
and we hope that this work will enable and motivate the de-
velopment of new incremental learning methodology.

6. Acknowledgement

We would like to thank all the reviewers and Qian Shao
for his early contributions to the incremental learning ex-
periments. This work was supported by NSF Awards BCS-
1524565 and BCS-1523982. In addition, this work was par-
tially supported by the Indiana University Areas of Emer-
gent Research initiative in Learning: Brains, Machines,
Children.

5. Conclusion

References

We introduce CRIB, a novel environment for generating
unlimited training data for incremental learning of object

[1] P. Ammirato, A. C. Berg, and J. Kosecka. Active vision
dataset benchmark. In Proceedings of the IEEE Conference

8784

on Computer Vision and Pattern Recognition Workshops,
pages 2046–2049, 2018. 3
[2] A. Bendale and T. Boult. Towards open world recognition.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 1893–1902, 2015. 3
[3] A. Bendale and T. E. Boult. Towards open set deep networks.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 1563–1572, 2016. 3
[4] blendswap.com. https://blendswap.com. 3
[5] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A
naturalistic open source movie for optical ﬂow evaluation. In
European Conference on Computer Vision, pages 611–625.
Springer, 2012. 3
[6] F. M. Castro, M. J. Marin-Jimenez, N. Guil, C. Schmid, and
K. Alahari. End-to-end incremental learning. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 233–248, 2018. 2, 5, 6
[7] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
et al. Shapenet: An information-rich 3d model repository.
arXiv preprint arXiv:1512.03012, 2015. 2, 3, 4, 6, 7
[8] E. M. Clerkin, E. Hart, J. M. Rehg, C. Yu, and L. B. Smith.
Real-world visual statistics and infants’ ﬁrst-learned object
names. Phil. Trans. R. Soc. B, 372(1711):20160055, 2017. 1
[9] Conference.
ICDL-EPIROB.
http://www.
icdl-epirob.org/. 3
[10] R. De Rosa, T. Mensink, and B. Caputo. Online open world
recognition. arXiv preprint arXiv:1604.02275, 2016. 3
[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 248–255, 2009. 5
[12] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-
vation feature for generic visual recognition. In International
Conference on Machine Learning, pages 647–655, 2014. 5,
6
[13] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and
V. Koltun. CARLA: An open urban driving simulator.
In
Proceedings of the 1st Annual Conference on Robot Learn-
ing, pages 1–16, 2017. 3
[14] R. M. French. Catastrophic Forgetting in Connectionist Net-
works. Trends in Cognitive Sciences, 3(4):128–135, 1999.
1
[15] A. Gepperth and C. Karaoguz. A Bio-inspired Incremen-
tal Learning Architecture for Applied Perceptual Problems.
Cognitive Computation, 8(5):924–934, 2016. 3
[16] R. Gomes, M. Welling, and P. Perona. Incremental learning
of nonparametric bayesian mixture models. In Computer Vi-
sion and Pattern Recognition, 2008. CVPR 2008. IEEE Con-
ference on, pages 1–8. IEEE, 2008. 5
[17] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and
Y. Bengio. An empiritcal investigation of catastrophic for-
getting in gradient-based neural networks. In International
Conference on Learning Representations (ICLR), 2014. 1
[18] N. Haber, D. Mrowca, S. Wang, L. F. Fei-Fei, and D. L.
Yamins. Learning to play with intrinsically-motivated, self-

aware agents. In Advances in Neural Information Processing
Systems, pages 8398–8409, 2018. 3
[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning
for Image Recognition. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), pages 770–778, 2016.
5
[20] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531, 2015.
5
[21] K. H. James, S. S. Jones, L. B. Smith, and S. N.
Swain. Young children’s self-generated object views and
object recognition. Journal of Cognition and Development,
15(3):393–401, 2014. 1, 4
[22] P. J. Kellman and M. E. Arterberry. The cradle of knowledge:
Development of perception in infancy. MIT press, 2000. 1
[23] R. Kemker and C. Kanan. FearNet: Brain-Inspired Model
for Incremental Learning.
In International Conference on
Learning Representations (ICLR), Apr 2018. 3
[24] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des-
jardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho,
A. Grabska-Barwinska, et al. Overcoming Catastrophic For-
getting in Neural Networks. Proceedings of the National
Academy of Sciences, page 201611835, 2017. 2
[25] P. Kr ¨ahenb ¨uhl. Free supervision from video games.
In
Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR 18), pages
2955–2964, 2018. 3
[26] A. Krizhevsky and G. Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
2, 6
[27] Y. LeCun, F. J. Huang, and L. Bottou. Learning Methods
for Generic Object Recognition with Invariance to Pose and
Lighting.
In IEEE Computer Society Conference on Com-
puter Vision and Pattern Recognition (CVPR), volume 2,
pages II–104, 2004. 3
[28] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang.
Overcoming Catastrophic Forgetting by Incremental Mo-
ment Matching. In Advances in Neural Information Process-
ing Systems 30 (NIPS), pages 4652–4662. 2017. 2
[29] Z. Li and D. Hoiem. Learning without Forgetting. In Euro-
pean Conference on Computer Vision (ECCV), pages 614–
629, 2016. 2, 5, 6
[30] S. Liu, T. D. Ullman, J. B. Tenenbaum, and E. S. Spelke.
Ten-month-old infants infer the value of goals from the costs
of actions. Science, 358(6366):1038–1041, 2017. 3
[31] V. Lomonaco and D. Maltoni. CORe50: a New Dataset and
Benchmark for Continuous Object Recognition. In Proceed-
ings of the 1st Annual Conference on Robot Learning, 2017.
3
[32] D. Lopez-Paz and M. A. Ranzato. Gradient Episodic Mem-
ory for Continual Learning. In Advances in Neural Informa-
tion Processing Systems 30 (NIPS), pages 6467–6476. 2017.
2
[33] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers,
A. Dosovitskiy, and T. Brox. A large dataset to train con-
volutional networks for disparity, optical ﬂow, and scene
ﬂow estimation.
In Proceedings of the IEEE Conference

8785

on Computer Vision and Pattern Recognition, pages 4040–
4048, 2016. 3
[34] B. McMurray. Defusing the Childhood Vocabulary Explo-
sion. Science, 317(5838):631–631, 2007. 1
[35] Y. Movshovitz-Attias, T. Kanade, and Y. Sheikh. How Use-
ful is Photo-realistic Rendering for Visual Learning? In Eu-
ropean Conference on Computer Vision (ECCV), pages 202–
217, 2016. 3
[36] S. Nayar, S. Nene, and H. Murase. Columbia Object Im-
age Library (COIL 100). Department of Comp. Science,
Columbia University, Tech. Rep. CUCS-006-96, 1996. 3
[37] A. F. Pereira, K. H. James, S. S. Jones, and L. B. Smith. Early
biases and developmental changes in self-generated object
views. Journal of vision, 10(11):22–22, 2010. 1
[38] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert.
iCaRL: Incremental Classiﬁer and Representation Learning.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5533–5542, July 2017. 2, 5, 6
[39] S. R. Richter, Z. Hayder, and V. Koltun. Playing for bench-
marks. In Proceedings of the IEEE International Conference
on Computer Vision, pages 2213–2222, 2017. 3
[40] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
for data: Ground truth from computer games. In European
Conference on Computer Vision, pages 102–118, 2016. 3
[41] M. Savva, F. Yu, H. Su, A. Kanezaki, T. Furuya, R. Ohbuchi,
Z. Zhou, R. Yu, S. Bai, X. Bai, et al. Shrec17 track large-
scale 3d shape retrieval from shapenet core55. 3
[42] W. J. Scheirer, A. de Rezende Rocha, A. Sapkota, and T. E.
Boult. Toward open set recognition. IEEE transactions on
pattern analysis and machine intelligence, 35(7):1757–1772,
2013. 7
[43] W. J. Scheirer, L. P. Jain, and T. E. Boult. Probability mod-
els for open set recognition. IEEE transactions on pattern
analysis and machine intelligence, 36(11):2317–2324, 2014.
7
[44] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carls-
son. Cnn features off-the-shelf: an astounding baseline for
recognition. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition workshops, pages 806–
813, 2014. 5, 6
[45] O. Wiles and A. Zisserman. SilNet : Single- and Multi-View
Reconstruction by Learning from Silhouettes.
In BMVC.
BMVA Press, 2017. 3
[46] J. Wulff, D. J. Butler, G. B. Stanley, and M. J. Black. Lessons
and insights from creating a synthetic optical ﬂow bench-
mark. In European Conference on Computer Vision, pages
168–177. Springer, 2012. 3
[47] B. Wymann. TORCS - The Open Racing Car Simulator. 3
[48] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-
ferable are features in deep neural networks? In Advances
in neural information processing systems, pages 3320–3328,
2014. 5, 6
[49] D. Yurovsky, L. B. Smith, and C. Yu. Statistical Word Learn-
ing at Scale: The Baby’s View is Better. Developmental Sci-
ence, 16(6):959–966, 2013. 1, 3

8786

Learning the Depths of Moving People by Watching Frozen People

Zhengqi Li
Tali Dekel
Noah Snavely

Forrester Cole
Richard Tucker
Ce Liu
William T. Freeman

Google Research

Figure 1. Our model predicts dense depth when both an ordinary camera and people in the scene are freely moving (right). We train our
model on our new MannequinChallenge dataset—a collection of Internet videos of people imitating mannequins, i.e., freezing in diverse,
natural poses, while a camera tours the scene (left). Because people are stationary, geometric constraints hold; this allows us to use
multi-view stereo to estimate depth which serves as supervision during training.2

Abstract

We present a method for predicting dense depth in scenar-
ios where both a monocular camera and people in the scene
are freely moving. Existing methods for recovering depth for
dynamic, non-rigid objects from monocular video impose
strong assumptions on the objects’ motion and may only
recover sparse depth. In this paper, we take a data-driven
approach and learn human depth priors from a new source
of data: thousands of Internet videos of people imitating
mannequins, i.e., freezing in diverse, natural poses, while
a hand-held camera tours the scene. Because people are
stationary, training data can be generated using multi-view
stereo reconstruction. At inference time, our method uses
motion parallax cues from the static areas of the scenes to
guide the depth prediction. We demonstrate our method on
real-world sequences of complex human actions captured by
a moving hand-held camera, show improvement over state-
of-the-art monocular depth prediction methods, and show
various 3D effects produced using our predicted depth.

1. Introduction

A hand-held camera viewing a dynamic scene is a com-
mon scenario in modern photography. Recovering dense
geometry in this case is a challenging task: moving objects
violate the epipolar constraint used in 3D vision, and are
often treated as noise or outliers in existing Structure-from-
Motion (SfM) and Multi-view Stereo (MVS) methods. Our
human depth perception, however, is not easily fooled by

object motion–rather, we maintain a feasible interpretation
of the objects’ geometry and depth ordering even if both
objects and the observer are moving, and even when the
scene is observed with just one eye [11]. In this work, we
take a step towards achieving this ability computationally.
We focus on the task of predicting accurate, dense depth
from ordinary videos where both the camera and people
in the scene are naturally moving. We focus on humans
for two reasons: i) in many applications (e.g., augmented
reality), humans constitute the salient objects in the scene,
and ii) human motion is articulated and difﬁcult to model. By
taking a data-driven approach, we avoid the need to explicitly
impose assumptions on the shape or deformation of people,
but rather learn these priors from data.
Where do we get data to train such a method? Generating
high-quality synthetic data in which both the camera and the
people in the scene are naturally moving is very challenging.
Depth sensors (e.g., Kinect) can provide useful data, however
such data is typically limited to indoor environments and
requires signiﬁcant manual work in capture and process.
Furthermore, it is difﬁcult to gather people of different ages
and genders with diverse poses at scale. Instead, we derive
data from a surprising source: YouTube videos in which
people imitate mannequins, i.e., freeze in elaborate, natural
poses, while a hand-held camera tours the scene (Fig. 2).
These videos comprise our new MannequinChallenge (MC)
dataset, which we plan to release for the research community.

2 In all ﬁgures, we use inverse depth maps for visualization purposes,
and refer to them as depth maps.

4521

Since the entire scene, including the people, is stationary, we
estimate the camera poses and depth using SfM and MVS,
which serves as supervision for training.
Using this data, we design and train a deep neural network
that takes an input RGB image, a mask of human regions,
and an initial depth of the environment (i.e., non-human
regions), and outputs a dense depth map over the entire
image, both the environment and the people (see Fig. 1).
Note that the initial depth of the environment is computed
using motion parallax between two frames of the video,
providing the network with information not available from
a single frame. Once trained, our model can handle natural
videos with arbitrary camera and human motion.
We demonstrate the applicability of our method on a va-
riety of real-world Internet videos, shot with a hand-held
camera, depicting complex human actions such as walking,
running, and dancing. Our model predicts depth to higher
accuracy than state-of-the-art monocular depth prediction
and motion stereo methods. We further show how our depth
maps can be used to produce various 3D effects such as syn-
thetic depth-of-ﬁeld, depth-aware inpainting, and inserting
virtual objects into the 3D scene with correct occlusion.
In summary, our contributions are: i) a new source of data
for depth prediction consisting of a large number of Internet
videos in which the camera moves around people “frozen”
in natural poses, along with a methodology for generating
accurate depth maps and camera poses; ii) a deep-network-
based model designed and trained to predict dense depth
maps in the challenging case of simultaneous camera motion
and complex human motion.

2. Related Work

Learning-based depth prediction. Numerous algorithms,
based on both supervised and unsupervised learning, have
recently been proposed for predicting dense depth from a
single RGB image [46, 17, 7, 6, 3, 19, 33, 8, 52, 49, 21,
41]. Some recent learning based methods also consider
multiple images, either assuming known camera poses [12,
47] or simultaneously predicting camera poses along with
depth [39, 51]. However, none of them is designed to predict
the depth of dynamic objects, which is the focus of our work.

Depth estimation for dynamic scenes. RGBD data has
been widely used for 3D modeling of dynamic scenes
[25, 55, 48, 5, 14], but only a few methods attempt to es-
timate depth from a monocular camera. Several methods
have been proposed to reconstruct sparse geometry of a
dynamic scene [27, 50, 36, 40]. Russell et al. [31] and Ran-
ftl et al. [29] suggest motion/object segmentation based al-
gorithms to decompose a dynamic scene into piecewise rigid
parts. However, these methods impose strong assumptions
of the object’s motion that are violated by articulated human
motion. Konstantinos et al. [30] predict depth of moving
soccer players using synthetic training data from FIFA video
games. However, their method is limited to soccer players,
and cannot handle general people in the wild.

RGBD data for learning depth. There are a number of
RGBD datasets of indoor scenes, captured using depth sen-
sors [35, 2, 4, 45] or synthetically rendered [37]. How-
ever, none of these datasets provide depth supervision for
moving people in natural environments. Several action
recognition methods use depth sensors to capture human
actions [54, 34, 22, 26], however most of them are captured
by a static camera and provide only a limited number of
indoor scenes. REFRESH [20] is a recent semi-synthetic
scene ﬂow dataset created by overlaying animated people
on NYUv2 images. Here too, the dataset is limited to in-
door scenes and consists of synthetic humans placed in an
unrealistic conﬁguration with their surrounding.

Human shape and pose prediction. Recovery of a posed
3D human mesh from a single RGB image has attracted
signiﬁcant attention [18, 9, 16, 1, 28, 23]. Recent methods
achieve impressive results on natural images spanning a
variety of poses. However, such methods only model the
human body, disregarding hair, clothing, and the non-human
parts of the scenes. Finally, many of these methods rely on
correctly detecting human keypoints, requiring most of the
body to be within the frame.

3. MannequinChallenge Dataset

The Mannequin Challenge [42] is a popular video trend
in which people freeze in place—often in an interesting
pose—while the camera operator moves around the scene
ﬁlming them (e.g., Fig. 2). Thousands of such videos have
been created and uploaded to YouTube since late 2016. To
the extent that people succeed in staying still during the
videos, we can assume the scenes are static and obtain ac-
curate camera poses and depth information by processing
them with SfM and MVS algorithms. We found around
2,000 candidate videos for which this processing is possi-
ble. These videos comprise our new MannequinChallenge
Dataset, which spans a wide range of scenes with people of
different ages, naturally posing in different group conﬁgura-
tions. We next describe in detail how we process the videos
and derive our training data.

Estimating camera poses. Following a similar approach
to Zhou et al. [53], we use ORB-SLAM2 [24] to identify
trackable sequences in each video and to estimate an initial
camera pose for each frame. At this stage, we process a
lower-resolution version of the video for efﬁciency, and set
the ﬁeld of view to 60 degrees (typical value for modern
cell-phone cameras). We then reprocess each sequence at
a higher resolution using a visual SfM system [32], which
reﬁnes the initial camera poses and intrinsic parameters. This
method extracts and matches features across frames, then
performs a global bundle adjustment optimization. Finally,
sequences with non-smooth camera motion are removed
using the technique of Zhou et al. [53].

Computing dense depth with MVS. With the estimated
camera poses for each clip in hand, we turn to reconstruct-

4522

Figure 2. Sample images from Mannequin Challenge videos. Each image is a frame from a video sequence in which the camera is
moving but humans are all static. The videos span a variety of natural scenes, poses, and conﬁguration of people.

ing the scene’s dense geometry. Speciﬁcally, we recover
per-frame dense depth maps using the state-of-the-art MVS
system COLMAP [33].
Because our data consists of challenging Internet videos
(i.e., often involve camera motion blur, shadows and reﬂec-
tions), the raw depth maps estimated by MVS are often
too noisy for training purposes. We address this issue by a
careful depth ﬁltering mechanism. We ﬁrst reﬁne and ﬁlter
depth outliers using the depth reﬁnement method of [19].
Additionally, we remove additional erroneous depth values
by considering the consistency of the MVS depth and the
depth obtained from motion parallax between two frames.
Speciﬁcally, for each frame, we compute a normalized error
∆(p) for every valid pixel p:

∆(p) = |DMVS (p) − Dpp (p)|
DMVS (p) + Dpp (p)

(1)

where DMVS is the depth map obtained by MVS and Dpp
is the depth map computed from two-frame motion paral-
lax (see Sec. 4.1). Depth values for which ∆(p) > δ are
removed, where we empirically set δ = 0.2.
Fig. 3 shows sample frames from our processed sequences
with corresponding estimated MVS depths after ﬁltering.
See Supplemental Material (SM) for examples showing the
effect of the proposed cleaning approach.

Filtering clips. Several factors can make a video clip unsuit-
able for training. For example, people may “unfreeze” (start
moving) at some point in the video, or the video may contain
synthetic graphical elements in the background. Dynamic
objects and synthetic backgrounds do not obey multi-view
geometric constraints and hence are treated as outliers and
ﬁltered out by MVS, potentially leaving few valid pixels.
Therefore, we remove frames where < 20% of pixels have
valid MVS depth after our two-pass cleaning stage.
Further, we remove frames where the estimated radial
distortion coefﬁcient |k1 | > 0.1 (indicative of a ﬁsheye
camera) or where the estimated focal length is ≤ 0.6 or
≥ 1.2 (camera parameters are likely inaccurate). We keep
sequences that are at least 30 frames long, have an aspect
ratio of 16:9, and have a width of ≥ 1600 pixels. Finally,
we manually inspect the trajectories and point clouds of
the remaining sequences and remove obviously incorrect

reconstructions. Examples of removed images are shown in
SM.

After processing, we obtain 4,690 sequences with a total
of more then 170K valid image-depth pairs. We split our
MC dataset into training, validation and testing sets with a
80:3:17 split over clips.

4. Depth Prediction Model

We train the depth prediction model on the Mannequin-
Challenge dataset in a supervised manner, i.e., by regressing
to the depth generated by the MVS pipeline. A key ques-
tion is how to structure the input to the network to allow
training on frozen people but inference on freely moving
people. One option is to regress from a single RGB image
to depth, but this approach disregards geometric informa-
tion about the static regions of the scene that is available by
considering more than a single view. To beneﬁt from such
information, we input to the network a depth map for the
static, non-human regions, estimated from motion parallax
w.r.t. another view of the scene.

The full input to our network, illustrated in Fig. 3, in-
cludes a reference image I r , a binary mask of human re-
gions M , a depth map estimated from motion parallax (with
human regions removed) Dpp , conﬁdence map C , and an op-
tional human keypoint map K . We assume known, accurate
camera poses from SfM during both training and inference
stages. In an online inference setting, camera poses can be
obtained by visual-inertial odometry.

Given these inputs, the network predicts a full depth map
for the entire scene. To match the MVS depth values, the
network must inpaint the depth in human regions, reﬁne the
depth in non-human regions from the estimated Dpp , and
ﬁnally make the depth of entire scene consistent.

Our network architecture is a variant of the hourglass
network of [3], with the nearest-neighbor upsampling layers
replaced by bilinear upsampling layers.

The following sections describe each of the inputs to our
model and our training losses in detail. Please refer to the
SM for additional implementation details and full derivation.

4523

(a) Reference image I r
(b) Human mask M
(c) Input depth D
(d) Input conﬁdence C
(e) MVS depth DMVS
Figure 3. System inputs and training data. The input to our network consists of: (a) RGB image, (b) human mask, (c) masked depth
computed from motion parallax w.r.t. a selected source image, and (d) masked conﬁdence map. Low conﬁdence regions (dark circles) in the
ﬁrst two rows indicate the vicinity of the camera epipole, where depth from parallax is unreliable and is removed. The network is trained to
regress to MVS depth (e).

pp

4.1. Depth from motion parallax

Motion parallax between two frames in a video provides
our initial depth estimate for the static regions of the scene
(assuming humans are dynamic while the rest of the scene
is static). Given a reference image I r and source image I s
pair, we estimate an optical ﬂow ﬁeld from I r to I s using
FlowNet2.0 [13]. Using the relative camera poses between
the two views, we compute an initial depth map Dpp from
the estimated ﬂow ﬁeld, using the Plane-Plus-Parallax (P+P)
representation [15, 43].
In some cases, such as forward/backward relative camera
motion between the frames, the estimated depth may be
ill-deﬁned in some image regions (i.e., the epipole may be
located within the image). We detect and ﬁlter out such
depth values as described in Sec. 4.2.

Key-frame selection. Depth from motion parallax may
not be meaningful if the 2D displacement between the two
views is small or if it can be well-approximated by a homog-
raphy (e.g., in the case of pure camera rotation). To avoid
such cases, we apply a baseline criterion when selecting a
reference frame I r and a corresponding source key-frame
I s . We want the two views to have signiﬁcant overlap, while
having a large enough baseline. Formally, for each I r , we
ﬁnd the index s of I s as

s = arg max

drj orj

j

(2)

where drj is the L2 distance between the camera centers of
I r and its neighbor frame I j . The term orj is the fraction
of co-visible SfM features in I r and I j : orj = 2|V r T V j |
where V j is the set of features visible in I j . We discard
pairs of frames for which orj < τo , i.e., the fraction of
co-visible features should be larger than a threshold τo (we
set τo = 0.6), and limit the maximum frame interval to 10.

|V r |+|V j | ,

We found these view selection criteria work well in all our
experiments.

4.2. Conﬁdence

Our data consists of challenging Internet video clips with
camera motion blur, shadows, low lighting, and reﬂections.
In such cases, optical ﬂow is often noisy [44], compounding
uncertainty in the input depth map, Dpp . We thus estimate,
and input to the network, a conﬁdence map, C . This al-
lows the network to rely more on the input depth in high-
conﬁdence regions, and potentially use it to improve its
prediction in low-conﬁdence regions. The conﬁdence value
at each pixel p in the non-human regions is deﬁned as:

C (p) = Clr (p)Cep (p)Cpa (p).

(3)

The term Clr measures “left-right” consistency between
the forward and backward ﬂow ﬁelds. That is, Clr (p) =
max (cid:0)0, 1 − r(p)2 (cid:1), where r(p) is the forward-backward
warping error. For perfectly consistent forward and back-
ward ﬂows Clr = 1, while Clr = 0 when the error is greater
than 1px.
The term Cep measures how well the ﬂow ﬁeld complies
with the epipolar constraint between the views [10]. Specif-

ically, Cep (p) = max (cid:0)0, 1 − (γ (p)/¯γ )2 (cid:1), where γ (p) is

the distance between the warped pixel position of p based
on its optical ﬂow and its corresponding epipolar line; ¯γ
controls the epipolar distance tolerance (we set ¯γ = 2px in
our experiments).
Finally, Cpa assigns low conﬁdence to pixels for which
the parallax between the views is small [33]. This is mea-
sured by the angle β (p) between the camera rays meeting at
the pixel p. That is, Cpa (p) = 1−(cid:16) min( ¯β ,β (p))− ¯β
, where
¯β is the angle tolerance (we use ¯β = 1° in our experiments).

¯β

(cid:17)2

4524

Figure 4. Qualitative results on MC test set. From top to bottom: reference images and their corresponding MVS depth (pseudo ground
truth); our depth predictions using: our single view model (third row) and our two-frame model (forth row). The additional network inputs
give improved performance in both human and non-human regions.

Fig. 3(d) shows examples of computed conﬁdence maps.
Note that human regions as well as regions for which the
conﬁdence C (p) < 0.25 are masked out.

4.3. Losses

We train our network to regress to depth maps computed
by our data pipeline. Because the computed depth values
have arbitrary scale, we use a scale-invariant depth regression
loss. That is, our loss is computed on log-space depth values
and consists of three terms:

Lsi = LMSE + α1Lgrad + α2Lsm .

(4)

Scale-invariant MSE. LMSE denotes the scale-invariant
mean square error (MSE) [6]. This term computes the
squared, log-space difference in depth between two pixels in
the prediction and the same two pixels in the ground-truth,
averaged over all pairs of valid pixels. Intuitively, we look at
all pairs of points, and penalize the difference in their ratio
of depth values w.r.t. ground truth.

Multi-scale gradient term. We use a multi-scale gradient
term, Lgrad , which is the L1 difference between the predicted
log depth derivatives (in x and y directions) and the ground
truth log depth derivatives, at multiple scales [19]. This term
allows the network to recover sharp depth discontinuities
and smooth gradient changes in the predicted depth images.

Multi-scale, edge-aware smoothness terms. To encour-
age smooth interpolation of depth in texture-less regions
where MVS fails to recover depth, we use a simple smooth-
ness term, Lsm , which penalizes L1 norm of log depth deriva-
tives based on the ﬁrst- and second-order derivatives of im-

Net inputs

si-full si-env si-hum si-intra si-inter

I

I.
II.

I F CM
III. IDppM

0.333 0.338 0.317
0.330 0.349 0.312
0.255 0.229 0.264
IV. IDppCM 0.232 0.188 0.237
V.
IDppCM K 0.227 0.189 0.230

0.264
0.260
0.243
0.221
0.212

0.384
0.381
0.285
0.268
0.263

Table 1. Quantitative comparisons on MC test set. Different
input conﬁgurations of our model: (I.) single image; (II.) optical
ﬂow masked in the human region (F ), conﬁdence and human mask;
(III.) masked input depth, human mask, and additional conﬁdence
for IV.; in V, we also input human keypoints. Lower is better for
all metrics.

ages and is applied at multiple scales [41]. This term encour-
ages piecewise smoothness in depth regions where there is
no image intensity change.

5. Results

We tested our method quantitatively and qualitatively
and compare it with several state-of-the-art single-view and
motion-based depth prediction algorithms. We show ad-
ditional qualitative results on challenging Internet videos
with complex human motion and natural camera motion, and
demonstrate how our predicted depth maps can be used for
several visual effects.
RMSE (si-RMSE), equivalent to √LMSE , described in
Error metrics. We measure error using the scale-invariant
Sec. 4.3. We evaluate si-RMSE on 5 different regions: si-
full measures the error between all pairs of pixels, giving the
overall accuracy across the entire image; si-env measures
pairs of pixels in non-human regions E , providing depth ac-
curacy of the environment; and si-hum measures pairs where

4525

(a) I r

(b) I s

(c) GT

(d) DORN [7]

(e) DeMoN [39]

(f) Ours (RGB)

(g) Ours (full)

Figure 5. Qualitative comparisons on TUM RGBD dataset. (a) Reference images, (b) source images (used to compute our initial depth
input), (c) ground truth sensor depth, (d) single view depth prediction method DORN [7], (e) two-frame motion stereo DeMoN [39], (f-g)
depth predictions from our single view and two-frame models, respectively.

at least one pixel lies in the human region H, providing depth
accuracy for people. si-hum can further be divided into two
error measures: si-intra measures si-RMSE within H, or
human accuracy independent of the environment; si-inter
measures si-RMSE between pixels in H and in E , or human
accuracy w.r.t. the environment. We include derivations in
SM.

5.1. Evaluation on MC test set

We evaluated our method on our MC test set, which
consists of more than 29K images taken from 756 video
clips. Processed MVS depth values DMVS obtained by our
pipeline (see Sec. 3) are considered as ground truth.
To quantify the importance of our designed model’s input,
we compare the performance of several models, each trained
on our MC dataset with a different input conﬁguration. The
two main conﬁgurations are: (i) a single-view model (input
is RGB image) and (ii) our full two-frame model, where the
input includes a reference image, an initial masked depth
map Dpp , a conﬁdence map C , and a human mask M . We
also perform ablation studies by replacing the input depth
with optical ﬂow F , removing C from the input, and adding
a human keypoint map K .
Quantitative evaluations are shown in Table 1. By com-
paring rows (I.), (III.) and (IV.), it is clear that adding the
initial depth of environment as well as a conﬁdence map
signiﬁcantly improves the performance for both human and
non-human regions. Adding human keypoint locations to

the network input further improves performance. Note that
if we input an optical ﬂow ﬁeld to the network instead of
depth (II.), the performance is only on a par with the single
view method. The mapping from 2D optical ﬂow to depth
depends on the relative camera poses, which are not given to
the network. This result indicates that the network is not able
to implicitly learn the relative poses and extract the depth
information.
Fig. 4 shows qualitative comparisons between our single-
view model (I ) and our full model (IDppCM K ). Our full
model results are more accurate in both human regions (e.g.,
ﬁrst column) and non-human regions (e.g., second column).
In addition, the depth relations between people and their
surroundings are improved in all examples.

5.2. Evaluation on TUM RGBD dataset

We used a subset of the TUM RGBD dataset [38], which
contains indoor scenes of people performing complex ac-
tions, captured from different camera poses. Sample images
from this dataset are shown in Fig. 5(a-b).
To run our model, we ﬁrst estimate camera poses using
ORB-SLAM2 3 . In some cases, due to severe low image
quality, motion blur and rolling shutter effects, the estimated
camera poses may be incorrect. We manually ﬁlter such
failures by inspecting the camera trajectory and point cloud.
In total, we obtain 11 valid image sequences with 1815

3We found estimates from ORB-SLAM2 to be better synchronized with
the RGB images than the ground truth poses provided by the TUM dataset.

4526

Methods

Dataset

two-view?

si-full

si-env

si-hum si-intra

si-inter RMSE

Rel

Russell et al. [31]
DeMoN [39]
Chen et al. [3]
Laina et al. [17]
Xu et al. [46]
Fu et al. [7]

-
Yes
RGBD+MVS Yes
NYU+DIW
No
NYU
No
NYU
No
NYU
No

I

I F CM
IDppM

MC
MC
MC
IDppCM (w/o d. cleaning) MC
MC
MC

IDppCM
IDppCM K

No
Yes
Yes
Yes
Yes
Yes

2.146
0.338
0.441
0.358
0.427
0.351

0.318
0.316
0.246
0.272
0.232
0.221

2.021
0.302
0.398
0.356
0.419
0.357

0.334
0.330
0.225
0.238
0.203
0.195

2.207
0.360
0.458
0.349
0.411
0.334

0.294
0.302
0.260
0.293
0.252
0.238

2.206
0.293
0.408
0.270
0.302
0.257

0.227
0.228
0.233
0.258
0.224
0.215

2.093
0.384
0.470
0.377
0.451
0.360

0.319
0.323
0.273
0.282
0.262
0.247

2.520
0.866
1.004
0.947
1.085
0.925

0.840
0.843
0.635
0.688
0.570
0.541

0.772
0.220
0.262
0.223
0.274
0.194

0.204
0.206
0.136
0.147
0.129
0.125

Table 2. Results on TUM RGBD datasets. Different si-RMSE metrics as well as standard RMSE and relative error (Rel) are reported. We
evaluate our models (light gray background) under different input conﬁgurations, as described in Table 1. w/o d. cleaning indicates the
model is trained using raw MVS depth predictions as supervision, without our depth cleaning method. Dataset ‘-’ indicates the method is not
learning based. Lower is better for all error metrics.

(a) I r
(b) I s
(c) DORN [7]
(d) Chen et al. [3]
(e) DeMoN [39]
(f) Ours (full)
Figure 6. Comparisons on Internet video clips with moving cameras and people. From left to right: (a) reference image, (b) source
image, (c) DORN [7], (d) Chen et al. [3], (e) DeMoN [39], (f) our full method.

images in total for evaluations.
We compare our depth predictions (using our MC trained
models) with several state-of-the-art monocular depth predic-
tion methods trained on indoor NYUv2 [17, 46, 7] and Depth
in the Wild (DIW) datasets [3], and the recent two-frame
stereo model DeMoN [39], which assumes a static scene.
We also compare with Video-Popup [31], which deals with
dynamic scenes. We use the same image pairs for computing
Dpp as inputs to DeMoN and Video-Popup.
Quantitative comparisons are show in Table 2, where we
report 5 different scale-invariance error measures as well
as standard RMSE and relative error; the last two are com-
puted by applying a single scaling factor that aligns the
predicted and ground-truth depth in the least-squares sense.

Our single-view model already outperforms the other single-
view models,demonstrating the beneﬁt of the MC dataset
for training. Note that VideoPopup [31] failed to produce
meaningful results due to the challenging camera and ob-
ject motion. Our full model, by making use of the initial
(masked) depth map, signiﬁcantly improves performance
for all the error measures. Consistent with our MC test set
results, when we use optical ﬂow as input (instead of initial
depth map) the performance is only slightly better than the
single-view network. Finally, we show the importance of our
proposed “depth cleaning” method, applied to the training
data (see Eq. 1). Compared to the same model, only trained
using the raw MVS depth predictions as supervision (“w/o
d. cleaning”), we see a drop of about 15% in performance.

4527

(a) Input

(b) Defocus

(c) Object insertion

(d) People removal

(e) Input
(f) People removal
Figure 7. Depth-based visual effects. We use our predicted depth
maps to apply depth-aware visual effects on (a, e) input images; we
show (b) defocus, (c) object insertion, and (d, f) people removal
with inpainting results.

Fig. 5 shows qualitative comparison between the differ-
ent methods. Our models’ depth predictions (Fig. 5(f-g))
strongly resemble the ground truth and show high level of
details and sharp depth discontinuities. This result is a no-
table improvement over competing methods, which often
produce signiﬁcant errors in both human regions (e.g., legs
in the second row of Fig. 5), and non-human regions (e.g.,
table and ceiling in the last two rows).

5.3. Internet videos of dynamic scenes

We tested our method on challenging Internet videos
(downloaded from YouTube and Shutterstock), involving
simultaneous natural camera motion and human motion. Our
SLAM/SfM pipeline was used to generate sequences ranging
from 5 seconds to 15 seconds with smooth and accurate
camera trajectories, after which we apply our method to
obtain the required network input buffers.
We qualitatively compare our full model (IDppCM K )
with several recent learning based depth prediction models:
DORN [7], Chen et al. [3], and DeMoN [39]. For fair com-
parisons, we use DORN with a model trained on NYUv2
for indoor videos and a model trained on KITTI for outdoor
videos; For [3], we use the models trained on both NYUv2
and DIW. For all of our predictions, we use a single model
trained from scratch on our MC dataset.
As illustrated in Fig. 6, our depth predictions are sig-
niﬁcantly better than the baseline methods. In particular,
DORN [7] has very limited generalization to Internet videos,
and Chen et al. [3], which is mainly trained on Internet pho-
tos, is not able to capture accurate depth. DeMoN often

Figure 8. Failure cases. Moving, non-human objects such as cars
and shadows can cause bad estimates (left and middle, boxed);
ﬁne structures such as limbs may be blurred for distant people in
challenging poses (right, boxed).

produces incorrect depth, especially in human regions, as it
designed for static scenes. Our predicted depth maps depict
accurate depth ordering both between people and other ob-
jects in the scene (e.g., between people and buildings, fourth
row of Fig. 6), and within human regions (such as the arms
and legs of people in the ﬁrst three rows of Fig. 6).

Depth-Based Visual Effects. Our depth can be used to
apply a range of depth-based visual effects. Fig. 7 shows
depth-based defocus, insertion of synthetic 3D graphics, and
removal of nearby humans with inpainting. See SM for more
examples including mono-to-stereo conversion.
The depth estimates are sufﬁciently stable over time to
allow inpainting from frames elsewhere in the video. To use
a frame for inpainting, we construct a triangle heightﬁeld
from the depth map, texture the heightﬁeld with the video
frame, and render the heightﬁeld from the target frame using
the relative camera transformation. Fig. 7 (d, f) show the
results of inpainting two street scenes. Humans near the
camera are removed using the human mask M , and holes are
ﬁlled with colors from up to 200 frames later in the video.
Some artifacts are visible in areas the human mask misses,
such as shadows on the ground.

6. Discussion and Conclusion

We demonstrated the power of a learning-based approach
for predicting dense depth of dynamic scenes where a monoc-
ular camera and people are freely moving. We make a new
source of data available for training: a large corpus of Man-
nequin Challenge videos from YouTube, in which the cam-
era moves around and people “frozen” in natural poses. We
showed how to obtain reliable depth supervision from such
noisy data, and demonstrated that our models signiﬁcantly
improve over state-of-the-art methods.
Our approach still has limitations. We assume known
camera poses, which may difﬁcult to infer if moving objects
cover most of the scene. In addition, the predicted depth
may be inaccurate for non-human, moving regions such as
cars and shadows (Fig. 8). Our approach also only uses
two views, sometimes leading to temporally inconsistent
depth estimates. However, we hope this work can guide and
trigger further progress in monocular dense reconstruction
of dynamic scenes.

4528

References

[1] F. Bogo, A. Kanazawa, C. Lassner, P. V. Gehler, J. Romero,
and M. J. Black. Keep it SMPL: Automatic Estimation of
3D Human Pose and Shape from a Single Image. In Proc.
European Conf. on Computer Vision (ECCV), 2016. ii
[2] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner,
M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3D:
Learning from rgb-d data in indoor environments. Int. Conf.
on 3D Vision (3DV), 2017. ii
[3] W. Chen, Z. Fu, D. Yang, and J. Deng. Single-image depth
perception in the wild. In Neural Information Processing
Systems, pages 730–738, 2016. ii, iii, vii, viii
[4] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser,
and M. Niessner. ScanNet: Richly-annotated 3D reconstruc-
tions of indoor scenes. In CVPR, volume 2, page 10, 2017.
ii
[5] M. Dou, S. Khamis, Y. Degtyarev, P. L. Davidson, S. R.
Fanello, A. Kowdle, S. Orts, C. Rhemann, D. Kim, J. Tay-
lor, P. Kohli, V. Tankovich, and S. Izadi. Fusion4D: real-
time performance capture of challenging scenes. ACM Trans.
Graphics, 35:114:1–114:13, 2016. ii
[6] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network. In
Neural Information Processing Systems, pages 2366–2374,
2014. ii, v
[7] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao.
Deep ordinal regression network for monocular depth esti-
mation. In Proc. Computer Vision and Pattern Recognition
(CVPR), 2018. ii, vi, vii, viii
[8] C. Godard, O. M. Aodha, and G. J. Brostow. Unsupervised
monocular depth estimation with left-right consistency. Proc.
Computer Vision and Pattern Recognition (CVPR), pages
6602–6611, 2017. ii
[9] R. A. G ¨uler, N. Neverova, and I. Kokkinos. DensePose:
Dense Human Pose Estimation In The Wild. Proc. Computer
Vision and Pattern Recognition (CVPR), 2018. ii
[10] R. Hartley and A. Zisserman. Multiple view geometry in
computer vision. Cambridge university press, 2003. iv
[11] I. P. Howard. Seeing in depth, Vol. 1: Basic mechanisms.
University of Toronto Press, 2002. i
[12] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang.
DeepMVS: Learning multi-view stereopsis. In Proc. Com-
puter Vision and Pattern Recognition (CVPR), pages 2821–
2830, 2018. ii
[13] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: Evolution of Optical Flow Estimation
With Deep Networks. In Proc. Computer Vision and Pattern
Recognition (CVPR), pages 2462–2470, 2017. iv
[14] M. Innmann, M. Zollh ¨ofer, M. Niessner, C. Theobalt, and
M. Stamminger. VolumeDeform: Real-time volumetric non-
rigid reconstruction. In Proc. European Conf. on Computer
Vision (ECCV), 2016. ii
[15] M. Irani and P. Anandan. Parallax geometry of pairs of points
for 3d scene analysis. In Proc. European Conf. on Computer
Vision (ECCV), pages 17–30. Springer, 1996. iv
[16] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
to-end recovery of human shape and pose. In Proc. Computer
Vision and Pattern Recognition (CVPR), 2018. ii

[17] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and
N. Navab. Deeper depth prediction with fully convolutional
residual networks. In Int. Conf. on 3D Vision (3DV), pages
239–248. IEEE, 2016. ii, vii
[18] C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J. Black, and
P. V. Gehler. Unite the people: Closing the loop between 3D
and 2D human representations. In Proc. Computer Vision and
Pattern Recognition (CVPR), 2017. ii
[19] Z. Li and N. Snavely. MegaDepth: Learning Single-View
Depth Prediction from Internet Photos. In Proc. Computer
Vision and Pattern Recognition (CVPR), 2018. ii, iii, v
[20] Z. Lv, K. Kim, A. Troccoli, D. Sun, J. M. Rehg, and J. Kautz.
Learning rigidity in dynamic scenes with a moving camera
for 3d motion ﬁeld estimation. Proc. European Conf. on
Computer Vision (ECCV), 2018. ii
[21] R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised
Learning of Depth and Ego-Motion from Monocular Video
Using 3D Geometric Constraints. In Proc. Computer Vision
and Pattern Recognition (CVPR), 2018. ii
[22] O. Mees, A. Eitel, and W. Burgard. Choosing Smartly: Adap-
tive Multimodal Fusion for Object Detection in Changing
Environments. In Int. Conf. on Intelligent Robots and Systems
(IROS), 2016. ii
[23] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shaﬁei,
H.-P. Seidel, W. Xu, D. Casas, and C. Theobalt. VNect: Real-
time 3D Human Pose Estimation with a Single RGB Camera.
ACM Trans. Graphics, 36:44:1–44:14, 2017. ii
[24] R. Mur-Artal and J. D. Tard ´os. Orb-slam2: An open-source
slam system for monocular, stereo, and rgb-d cameras. IEEE
Transactions on Robotics, 33(5):1255–1262, 2017. ii
[25] R. A. Newcombe, D. Fox, and S. M. Seitz. DynamicFusion:
Reconstruction and tracking of non-rigid scenes in real-time.
In Proc. Computer Vision and Pattern Recognition (CVPR),
2015. ii
[26] B. Ni, G. Wang, and P. Moulin. RGBD-HuDaAct: A color-
depth video database for human daily activity recognition. In
Proc. ICCV Workshops, 2011. ii
[27] H. S. Park, T. Shiratori, I. A. Matthews, and Y. Sheikh. 3D
Reconstruction of a Moving Point from a Series of 2D Projec-
tions. In Proc. European Conf. on Computer Vision (ECCV),
2010. ii
[28] G. Pavlakos, X. Zhou, K. G. Derpanis, and K. Daniilidis.
Coarse-to-ﬁne volumetric prediction for single-image 3D hu-
man pose. Proc. Computer Vision and Pattern Recognition
(CVPR), pages 1263–1272, 2017. ii
[29] R. Ranftl, V. Vineet, Q. Chen, and V. Koltun. Dense monoc-
ular depth estimation in complex dynamic scenes. In Proc.
Computer Vision and Pattern Recognition (CVPR), 2016. ii
[30] K. Rematas, I. Kemelmacher-Shlizerman, B. Curless, and
S. Seitz. Soccer on your tabletop. In Proc. Computer Vision
and Pattern Recognition (CVPR), June 2018. ii
[31] C. Russell, R. Yu, and L. Agapito. Video pop-up: Monocular
3d reconstruction of dynamic scenes. In Proc. European Conf.
on Computer Vision (ECCV), pages 583–598. Springer, 2014.
ii, vii
[32] J. L. Schonberger and J.-M. Frahm. Structure-from-motion
revisited. In Proc. Computer Vision and Pattern Recognition
(CVPR), 2016. ii
[33] J. L. Sch ¨onberger, E. Zheng, J.-M. Frahm, and M. Pollefeys.
Pixelwise view selection for unstructured multi-view stereo.

4529

[49] Z. Yin and J. Shi. GeoNet: Unsupervised Learning of Dense
Depth, Optical Flow and Camera Pose. In Proc. Computer
Vision and Pattern Recognition (CVPR), 2018. ii
[50] E. Zheng, D. Ji, E. Dunn, and J.-M. Frahm. Sparse Dynamic
3D Reconstruction from Unsynchronized Videos. Proc. Int.
Conf. on Computer Vision (ICCV), pages 4435–4443, 2015. ii
[51] H. Zhou, B. Ummenhofer, and T. Brox. DeepTAM: Deep
Tracking and Mapping. In Proc. European Conf. on Computer
Vision (ECCV), 2018. ii
[52] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsuper-
vised learning of depth and ego-motion from video. Proc.
Computer Vision and Pattern Recognition (CVPR), 2017. ii
[53] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely. Stereo
Magniﬁcation: Learning view synthesis using multiplane
images. ACM Trans. Graphics (SIGGRAPH), 2018. ii
[54] Y. Zhu, W. Chen, and G. Guo. Evaluating spatiotemporal in-
terest point features for depth-based action recognition. Image
and Vision Computing, 32(8):453–464, 2014. ii
[55] M. Zollh ¨ofer, M. Niessner, S. Izadi, C. Rehmann, C. Zach,
M. Fisher, C. Wu, A. Fitzgibbon, C. Loop, C. Theobalt, et al.
Real-time non-rigid reconstruction using an rgb-d camera.
ACM Trans. Graphics, 33(4):156, 2014. ii

In Proc. European Conf. on Computer Vision (ECCV), pages
501–518, 2016. ii, iii, iv
[34] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang, and
R. Webb. Learning from Simulated and Unsupervised Images
through Adversarial Training. In Proc. Computer Vision and
Pattern Recognition (CVPR), 2017. ii
[35] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor
segmentation and support inference from rgbd images. In
Proc. European Conf. on Computer Vision (ECCV), 2012. ii
[36] T. Simon, J. Valmadre, I. A. Matthews, and Y. Sheikh.
Kronecker-Markov Prior for Dynamic 3D Reconstruction.
Trans. Pattern Analysis and Machine Intelligence, 39:2201–
2214, 2017. ii
[37] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and
T. Funkhouser. Semantic scene completion from a single
depth image. Proc. Computer Vision and Pattern Recognition
(CVPR), 2017. ii
[38] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of RGB-D SLAM
systems. In IEEE/RSJ International Conference on Intelli-
gent Robots and Systems (IROS), pages 573–580. IEEE, 2012.
vi
[39] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Doso-
vitskiy, and T. Brox. DeMoN: Depth and motion network
for learning monocular stereo. In Proc. Computer Vision and
Pattern Recognition (CVPR), volume 5, page 6, 2017. ii, vi,
vii, viii
[40] M. Vo, S. G. Narasimhan, and Y. Sheikh. Spatiotemporal
Bundle Adjustment for Dynamic 3D Reconstruction. Proc.
Computer Vision and Pattern Recognition (CVPR), pages
1710–1718, 2016. ii
[41] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey.
Learning depth from monocular videos using direct methods.
In Proc. Computer Vision and Pattern Recognition (CVPR),
June 2018. ii, v
[42] Wikipedia.
Mannequin Challenge.
https://en.
wikipedia.org/wiki/Mannequin_Challenge,
2018. ii
[43] J. Wulff, L. Sevilla-Lara, and M. J. Black. Optical Flow in
Mostly Rigid Scenes. In Proc. Computer Vision and Pattern
Recognition (CVPR), pages 6911–6920. IEEE, 2017. iv
[44] K. Xian, C. Shen, Z. Cao, H. Lu, Y. Xiao, R. Li, and Z. Luo.
Monocular Relative Depth Perception with Web Stereo Data
Supervision. In Proc. Computer Vision and Pattern Recogni-
tion (CVPR), 2018. iv
[45] J. Xiao, A. Owens, and A. Torralba. Sun3D: A database of big
spaces reconstructed using sfm and object labels. In Proc. Int.
Conf. on Computer Vision (ICCV), pages 1625–1632, 2013. ii
[46] D. Xu, E. Ricci, W. Ouyang, X. Wang, and N. Sebe. Monoc-
ular Depth Estimation using Multi-Scale Continuous CRFs
as Sequential Deep Networks=. Trans. Pattern Analysis and
Machine Intelligence, 2018. ii, vii
[47] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan. MVSNet:
Depth Inference for Unstructured Multi-view Stereo. Proc.
European Conf. on Computer Vision (ECCV), 2018. ii
[48] M. Ye and R. Yang. Real-time simultaneous pose and shape
estimation for articulated objects using a single depth camera.
In Proc. Computer Vision and Pattern Recognition (CVPR),
2014. ii

4530

Learning to Compose Dynamic Tree Structures for Visual Contexts

Kaihua Tang1 , Hanwang Zhang1 , Baoyuan Wu2 , Wenhan Luo2 , Wei Liu2
1 Nanyang Technological University
2 Tencent AI Lab
kaihua001@e.ntu.edu.sg, hanwangzhang@ntu.edu.sg,
{wubaoyuan1987, whluo.china}@gmail.com, wl2223@columbia.edu

Abstract

We propose to compose dynamic tree structures that
place the objects in an image into a visual context, help-
ing visual reasoning tasks such as scene graph generation
and visual Q&A. Our visual context tree model, dubbed
VCTR E E, has two key advantages over existing structured
object representations including chains and fully-connected
graphs: 1) The efﬁcient and expressive binary tree encodes
the inherent parallel/hierarchical relationships among ob-
jects, e.g., “clothes” and “pants” are usually co-occur and
belong to “person”; 2) the dynamic structure varies from
image to image and task to task, allowing more content-
/task-speciﬁc message passing among objects. To construct
a VCTR E E, we design a score function that calculates the
task-dependent validity between each object pair, and the
tree is the binary version of the maximum spanning tree
from the score matrix. Then, visual contexts are encoded by
bidirectional TreeLSTM and decoded by task-speciﬁc mod-
els. We develop a hybrid learning procedure which inte-
grates end-task supervised learning and the tree structure
reinforcement learning, where the former’s evaluation re-
sult serves as a self-critic for the latter’s structure explo-
ration. Experimental results on two benchmarks, which re-
quire reasoning over contexts: Visual Genome for scene
graph generation and VQA2.0 for visual Q&A, show that
VCTR E E outperforms state-of-the-art results while discov-
ering interpretable visual context structures.

1. Introduction

Objects are not alone. They are placed in the visual con-
text: a coherent object conﬁguration attributed to the fact
that they co-vary with each other. Extensive studies in cog-
nitive science show that our brains inherently exploit visual
contexts to understand cluttered visual scenes comprehen-
sively [4, 6, 37]. For example, even the girl’s leg and the
horse are not fully observed in Figure 1, we can still infer
“girl riding horse”. Inspired by this, modeling visual con-

Is the girl sitting on 
the horse correctly?

based semantic segmentation [8, 9, 56].

Modeling visual contexts explicitly on the object-level
has also been shown effective in “high-level” vision tasks
such as image captioning [54] and visual Q&A [46].
In
fact, the visual context serves as a powerful inductive bias
that connects objects in a particular layout for high-level
reasoning [26, 30, 46, 54, 36, 28]. For example, the spa-
tial layout of “person” on “horse” is useful for determining
the relationship “ride”, which is in turn informative to lo-
calize the “person” if we want to answer “who is riding on
the horse?”. However, those works assume that the con-
text is a scene graph, whose detection per se is a high-
level task and not yet reliable. Without high-quality scene
graphs, we have to use a prior layout structure. As shown in
Figure 1, two popular structures are chains [57] and fully-
connected graphs [7, 10, 15, 25, 50, 55, 49], where the con-
text is encoded by sequential models such as bidirectional
LSTM [19] for chains and CRF-RNN [61] for graphs.

However,
these two prior structures are sub-optimal.
First, chains are oversimpliﬁed and may only capture sim-
ple spatial information or co-occurrence bias; though fully-
connected graphs are complete, they lack the discrimina-
tion between hierarchical relations, e.g., “helmet afﬁliated
to head”, and parallel relations, e.g., “girl on horse”; in addi-
tion, dense connections could also lead to message passing
saturation in the subsequent context encoding [50]. Second,
visual contexts are inherently content-/task-driven, e.g., the
object layouts should vary from content to content, question
to question. Therefore, ﬁxed chains and graphs are incom-
patible with the dynamic nature of visual contexts [47].

In this paper, we propose a model dubbed VCTR E E, pi-
oneering to compose dynamic tree structures for encoding
object-level visual context for high-level visual reasoning
tasks, such as scene graph generation (SGG) and visual
Q&A (VQA). Given a set of object proposals in an im-
age (e.g., obtained from Faster-RCNN [40]), we maintain
a trainable task-speciﬁc score matrix of the objects, where
each entry indicates the contextual validity of the pairwise
objects. Then, a maximum spanning tree can be trimmed
from the score matrix, e.g., the multi-branch trees shown in
Figure 1. This dynamic structure represents a “hard” hierar-
chical layout bias of what objects should gain more contex-
tual information from others, e.g., objects on the person’s
head are most informative given the question “what on the
little girl’s head?”; while the whole person’s body is more
important given the question “Is the girl sitting on the horse
correctly?”. To avoid the saturation issue caused by the
densely connected arbitrary number of children, we further
morph the multi-branch trees to the equivalent left-child
right-sibling binary trees [14], where the left branches (red)
indicate the hierarchical relations and right branches (blue)
indicate the parallel relations, then use TreeLSTM [44] to
encode the context.

As the above VCTR E E construction is in a discrete
and non-differentiable nature, we develop a hybrid learn-
ing strategy using REINFORCE [20, 41, 48] for tree struc-
ture exploration and supervised learning for context encod-
ing and its subsequent tasks. In particular, the evaluation
result (Recall for SGG and Accuracy for VQA) from super-
vised task can be exploited as a critic function that guide
the “action” of tree construction. We evaluate VCTR E E
on two benchmarks: Visual Genome [24] for SGG and
VQA2.0 [17] for VQA. For SGG, we achieve a new state-
of-the-art on all three standard tasks, i.e., Scene Graph Gen-
eration, Scene Graph Classiﬁcation, and Predicate Classiﬁ-
cation; for VQA, we achieve competitive results on single
model performances.
In particular, VCTR E E helps high-
level vision models ﬁght against the dataset bias. For ex-
ample, we achieve 4.1% absolute gain in proposed Mean
Recall@100 metric of Predicate Classiﬁcation than MO-
TIFS [57], and observe higher improvement in VQA2.0 bal-
anced pair subset [45] than normal validation set. Qualita-
tive results also show that VCTR E E composes interpretable
structures.

2. Related Work

Visual Context Structures. Despite the consensus on the
value of visual contexts, existing context models are diver-
siﬁed into a variety of implicit or explicit approaches. Im-
plicit models directly encode surrounding pixels into multi-
scale feature maps, e.g., dilated convolution [56] presents a
efﬁcient way to increase receptive ﬁeld, applicable in var-
ious dense prediction tasks [8, 9]; feature pyramid struc-
ture [27] combines low-resolution contextual features with
high-resolution detailed features, facilitating object detec-
tion with rich semantics. Explicit models incorporate con-
textual cues through object connections. However, such
methods [25, 50, 57] group objects into ﬁxed layouts, i.e.,
chains or graphs.
Learning to Compose Structures. Learning to compose
structures is becoming popular in NLP for sentence rep-
resentation, e.g., Cho et al. [11] applied a gated recur-
sive convolutional neural network (grConv) to control the
bottom-up feature ﬂow for a dynamic structure; Choi et
al. [12] combines TreeLSTM with Gumbel-Softmax, al-
lowing task-speciﬁc tree structures automatically learned
from plain text. Yet, only few works compose visual struc-
tures for images. Conventional approaches construct a sta-
tistical dependency graph/tree for the entire dataset based
on object categories [13] or exemplars [32]. Those sta-
tistical methods cannot put per-image objects in a context
as a whole to reason over content-/task-speciﬁc fashion.
Socher et al. [43] constructed a bottom-up tree structure to
parse images; however, their tree structure learning is super-
vised while ours is reinforced, which does not require tree
ground-truth.

6620

(a) Feature Extraction

(cid:144)(cid:2869)(cid:144)(cid:2870)(cid:144)(cid:2871)

RoI

Spatial

(b) Tree Construction

(cid:144)(cid:2869) (cid:144)(cid:2870) (cid:144)(cid:2871)

(cid:144)(cid:2869)(cid:144)(cid:2870)(cid:144)(cid:2871)

(cid:144)(cid:2869)

(cid:144)(cid:2871)

(cid:144)(cid:2870)

REINFORCE

(c) Context Encoding

Visual

Context

(d1) Scene Graph Generation
Man

R

e

l

a

t
i

o
n

M

o
d
e

l

Ride

Wear

Motorcycle

Helmet

Scene Graph

(d2) Visual Question Answering
Context
Attention

Q: What is on the 
man’s head?

Visual

Context

Visual
Attention

A: Helmet

Supervised Learning

Figure 2. The framework of the proposed VCTR E E model. We extract visual features from proposals and construct a dynamic VCTR E E
using the learnable score matrix. The tree structure is used to encode the object-level visual context, which will be decoded for each speciﬁc
end-task. Parameters in stages (c)&(d) are trained by supervised learning, while those in stage (b) are using REINFORCE with a self-critic
baseline.

Visual Reasoning Tasks. Scene Graph Generation (SGG)
task [50, 52] is derived from Visual Relationship Detection
(VRD) [31, 53]. Early work on VRD [31] treats objects as
isolated individuals, while SGG considers each image as a
whole. Along with the widely used message passing mech-
anism [50], a variety of context models [25, 26, 34, 51]
have been exploited in SGG to ﬁne-tune local predictions
through rich global contexts, making it the best competition
ﬁeld for different contextual models. Visual Question An-
swering (VQA) as a high-level task bridges the gap between
computer vision and natural language processing. State-of-
the-art VQA models [1, 3, 45] rely on bag-of-object visual
attentions which can be considered as a trivial context struc-
ture. However, we propose to learn a tree context structure
that is dynamic to visual content and questions.

3. Approach

TreeLSTM) to encode the contextual cues using the con-
structed VCTR E E.
(d) The encoded contexts will be de-
coded for each speciﬁc end-task detailed in Section 3.3 and
Section 3.4.

3.1. VCTR E E Construction

VCTR E E construction aims to learn a score matrix S ,
which approximates the task-dependent validity between
each object pair. Two principles guide the formulation of
this matrix: 1) inherent object correlations should be main-
tained, e.g., “man wears helmet” in Figure 2; (2) task re-
lated object pair has higher score than irrelevant ones, e.g.,
given question “what is on the man’s head?”, “man-helmet”
pair should be more important than “man-motorcycle” and
“helmet-motorcycle” pairs. Therefore, we deﬁne each ele-
ment of S as the product of the object correlation f (xi , xj )
and the pairwise task-dependency g(xi , xj , q):

As illustrated in Figure 2, our VCTR E E model can be
summarized into the following four steps.
(a) We adopt
Faster-RCNN to detect object proposals [40]. The visual
feature of each proposal i is presented as xi , concatenat-
ing a RoIAlign feature [18] vi ∈ R2048 and spatial feature
bi ∈ R8 , where 8 elements indicate the bounding box co-
ordinates (x1 , y1 , x2 , y2 ), center ( x1+x2
), and size
(x2 − x1 , y2 − y1 ), respectively. Note that the visual feature
xi is not limited to bounding box; segment feature from in-
stance segmentations [18] or panoptic segmentations [23]
could also be alternatives. (b) In Section 3.1, a learnable
matrix will be introduced to construct VCTR E E. Moreover,
since the VCTR E E construction is discrete in nature and the
score matrix is non-differentiable from the loss of end-task,
we develop a hybrid learning strategy in Section 3.5. (c)
In Section 3.2, we employ Bidirectional Tree LSTM (Bi-

, y1+y2

2

2

Sij = f (xi , xj ) · g(xi , xj , q),
f (xi , xj ) = σ (MLP(xi , xj )) ,
g(xi , xj , q) = σ(h(xi , q)) · σ(h(xj , q)),

(1)

⎧⎨
⎩

where σ(·) is the sigmoid function; q is the task feature,
e.g., the question feature encoded by GRU in VQA; MLP is
a multi-layer perceptron; h(xi , q) is the object-task correla-
tion in VQA, which will be introduced later in Section 3.4.
In SGG, the entire g(xi , xj , q) is set to 1, as we assume
that each object pair contributes equally without the ques-
tion prior. We pretrain f (xi , xj ) on Visual Genome [24]
for a reasonable binary prior if two objects are related. Yet,
such a pretrained model is not perfect due to the lack of co-
herent graph-level constraint or question prior, so it will be
further ﬁne-tuned in Section 3.5.
Considering S as a symmetric adjacency matrix, we
can obtain a maximum spanning tree using the Prim’s

6621

(cid:2196)(cid:2778)

(cid:2196)(cid:2779)

(cid:23)(cid:1870)(cid:1857)(cid:1857)
(cid:19)(cid:1867)(cid:1867)(cid:1864)(cid:2196)(cid:2781) (cid:2196)(cid:2782) (cid:2196)(cid:2783)

(cid:2196)(cid:2780)

(cid:2196)(cid:2778) (cid:2196)(cid:2779) (cid:2196)(cid:2780)

(cid:2196)(cid:2781)
(cid:2196)(cid:2783)(cid:2196)(cid:2782)

(cid:23)(cid:1870)(cid:1857)(cid:1857)
(cid:19)(cid:1867)(cid:1867)(cid:1864)

(cid:2196)(cid:2778)
(cid:2196)(cid:2779) (cid:2196)(cid:2780)
(cid:2196)(cid:2782)
(cid:2196)(cid:2781) (cid:2196)(cid:2782) (cid:2196)(cid:2783)

Figure 3. The maximum spanning tree from S . In each step, a
node in the remaining pool is connected to the current tree, if it
has the highest validity score.

algorithm [39], with a root (source node) i satisfying
arg maxi (cid:5)j (cid:2)= i Sij .
In a nutshell, as illustrated in Fig-
ure 3, we construct the tree recursively by connecting the
node from the pool to the tree node if it has the most valid-
ity. Note that during the tree structure exploration in Sec-
tion 3.5, each of the i-th step t(i) in the above tree construc-
tion is sampled from all possible choices in a multinomial
distribution with the probability p(t(i) |t(1) , ..., t(i−1) , S ) in
proportion to the validity. The resultant tree is multi-branch
and is merely a sparse graph with only one kind of connec-
tion, which is still unable to discriminate the hierarchical
and parallel relations in the subsequent context encoding.
To this end, we convert the multi-branch tree into an equiv-
alent binary tree, i.e., VCTR E E by changing non-leftmost
edges into right branches as in Figure 1.
In this fashion,
the right branches (blue) indicate parallel contexts, and left
ones (red) indicate hierarchical contexts. Such a binary tree
structure achieves signiﬁcant improvements in our SGG and
VQA experiments compared to its multi-branch alternative.

3.2. TreeLSTM Context Encoding

Given the above constructed VCTR E E, we adopt Bi-
TreeLSTM as our context encoder:

D = BiTreeLSTM({zi }i=1,2,...,n ),

(2)

where zi is the input node feature, which will be speciﬁed
in each task, and D = [d1 , d2 , ..., dn ] is the encoded object-
level visual context. Each di = [
hi ;
hi ] is the concatenated
hidden states from both TreeLSTM [44] directions:

(cid:2)

(cid:3)

(cid:2)

(cid:2)

hi = TreeLSTM(zi ,

hp ),

(cid:3)

(cid:3)

(cid:3)

hi = TreeLSTM(zi , [

hl ;

hr ]),

(3)

(4)

where (cid:2) and(cid:3) denote the top-down and bottom-up direc-
tions, respectively; we slightly abuse the subscripts p, l, r to
denote the parent, left child, and right child of node i. The
order of the concatenation [
hl ;
hr ] in Eq. (4) indicates the
explicit discrimination between the left and right branches
in context encoding. We use zero vectors to pad all the miss-
ing branches.

(cid:3)

(cid:3)

Relation (cid:1777)(cid:1815)(cid:1814)(cid:1820)(cid:1805)(cid:1824)(cid:1820)

Object (cid:1777)(cid:1815)(cid:1814)(cid:1820)(cid:1805)(cid:1824)(cid:1820)

Relationship Decoding

D
O

e
c
b
o
d
e
c

j

i

n
g

t

Relation Context

Union Box RoI Feature

Bounding Box Feature

O

b

j

e
c

t

P

r

e
d

i

c

t
i

o
n

R

e

l

a

t
i

o
n

P

r

e
d

i

c

t
i

o
n

Figure 4. The overview of our SGG Model. The object context
feature will be used to decode object categories, and the pair-
wise relationship decoding jointly fuses the relation context fea-
ture, RoIAlign feature of union box, and bounding box feature,
before prediction.

3.3. Scene Graph Generation Model

2 , ..., dr

2 , ..., do

i ∈ R512 .

Now we detail the implementation of Eq. (2) and how to
decode them for the SGG task as illustrated in Figure 4.
Object Context Encoding. We employ BiTreeLSTM from
Eq. (2) to encode object context representation into Do =
[do
1 , do
n ], do
i ∈ R512 . We set inputs zi of Eq. (2) to
[xi ; W1 ˆci ], i.e., concatenation of object visual features and
embedded N-way original Faster-RCNN class probabilities,
where W1 is the embedding matrix that maps each original
label distribution ˆci into R200 .
Relation Context Encoding. We apply an additional Bi-
TreeLSTM using the above do
i as input zi to further encode
the relation context Dr = [dr
1 , dr
n ], dr
Context Decoding. The goal of SGG is to detect objects
and then predict their relationship. Similar to [57], we adopt
a dynamic object prediction which can be viewed as a de-
coding process in a top-down direction using Eq. (3), that is,
the object class of a child is dependent on its parent. Speciﬁ-
cally, we set the input zi of Eq. (3) to be [do
i ; W2cp ], where
cp is the predicted label distribution of the i’s parent, and
W2 embeds it into R200 , then the output hidden is passed to
a softmax classiﬁer to achieve object label distribution ci .
The relationship prediction is in a pairwise fashion. First,
we collect three pairwise features for each object pair: (1)
dij = MLP([dr
i ; dr
j ]) as the context feature, (2) bij =
MLP([bi ; bj ; bi∪j ; bi∩j ]) as the bounding box pair feature,
with i ∪ j, i ∩ j being union box and intersection box, (3)
vij as the RoIAlign feature [18] from the union bounding
box of the object pair. All dij , vij , bij are under the same
dimension R2048 . Then, we fuse them into a ﬁnal pairwise
feature: gij = dij · vij · bij , before feed it into the softmax
predicate classiﬁer, where · is element-wise product.

3.4. Visual Question Answering Model

Now we detail the implementation of Eq. (2) for VQA,
and illustrate our VQA model in Figure 5.

6622

Visual Attention Model

(cid:963)

attention

Visual Feature

Question

BiTreeLSTM

Q

u
e
s

t
i

o
n

G

u

i

d
e
d

P

r

e
d

i

c

t
i

o
n

G

a

t

e

C

a
n
d

i

d
a

t

e

A

n

s

w

e

r
s

Context Attention Model

(cid:963)

attention

Context Feature

Question

Figure 5. The overview of our VQA framework. It contains two
multimodal attention models for visual feature and context feature.
Outputs from both models will be concatenated and passed to a
question-guided gate before answer prediction.

2 , ..., dq

Context Encoding. The context feature in VQA: Dq =
[dq
1 , dq
n ], dq
i ∈ R1024 is directly encoded from the
bounding box visual feature xi by Eq. (2).
Multimodal Attention Feature. We adopt a popular at-
tention model from previous work [1, 45] to calculate the
multimodal joint feature m ∈ R1024 for each question and
image pair:

m = fd ( ˆz , q),

(5)

where q ∈ R1024 is the question feature from a one-layer
GRU encoding the sentence; ˆz = (cid:5)N
i=1 αizi is the atten-
tive image feature calculated from the input feature set {zi },
αi = exp (ui )/ (cid:5)k exp (uk ) is the attention weight from
object-task correlation ui = h(zi , q) = MLP(cid:6)fd (zi , q)(cid:7),
with the output of MLP being a scalar; fd can be any
multi-modal feature fusion function, in particular, we adopt
fd (x, y) = ReLU(W3x + W4y) − (W3x − W4y)2 as
in [59], with W3 and W4 projecting x, y into the same di-
mension. Therefore, we can use Eq. (5) to obtain both the
multimodal visual attention feature mx by setting input zi
to xi and multimodal contextual attention feature md by
setting zi to dq
i .
Question Guided Gate Decoding. However, the impor-
tance of mx and md varies from question to question, e.g.,
“is there a dog?” only requires visual features for detection,
while “is the man dressed formally?” is highly context de-
pendent. Inspired by [42], we adopt a question guided gate
to select the most related channels from [mx ; md ]. The
gate vector g ∈ R2048 is deﬁned as:

g = σ(cid:6)MLP([q ; W5 lq ])(cid:7),

(6)

where lq ∈ R65 is a one-hot question type vector deﬁned by
preﬁxed words of questions, which is embedded into R256
by matrix W5 , and σ(·) denotes the sigmoid function.
Finally, we fuse g · [mx ; md ] as the ﬁnal VQA feature
and feed it into the softmax classiﬁer.

3.5. Hybrid Learning

Due to the discrete nature of VCTR E E construction, the
score matrix S is not fully differentiable from the loss back-
propagated from the end-task loss.
Inspired by [20], we
use a hybrid learning strategy that combines reinforcement
learning, i.e., policy gradient [48] for the parameters θ of
S in the tree construction and supervised learning for the
rest parameters. Suppose a layout l, i.e., a constructed VC -
TR E E, is sampled from π(l|I , q ; θ), i.e., the construction
procedure in Section 3.1, where I is the given image, q is
the task, e.g., questions in VQA. To avoid clutter, we drop
I and q . Then, we deﬁne the reinforcement learning loss

Lr (θ) as:

Lr (θ) = −El∼π(l|θ) [r(l)],

(7)

where Lr (θ) aims to minimize the negative expected re-
ward r(l), which can be the end-task evaluation met-
rics such as Recall@100 for SGG and Accuracy for
VQA. Then,
the above gradient will be ∇θ Lr (θ) =
−El∼π(l|θ) [r(l)∇θ logπ(l|θ)]. Since it is impractical to esti-
mate all possible layouts, we use the Monte-Carlo sampling
to estimate the gradient:

∇θ Lr (θ) ≈ −

1
M

M

(cid:8)

m=1

(cid:9)r(lm )∇θ logπ(lm |θ)(cid:10),

(8)

where we set M to 1 in our implementation.
To reduce the gradient variance, we apply a self-critic
baseline [41] b = r(ˆl), where ˆl is the greedy constructed
tree without sampling. So the original reward r(lm ) can be
replaced by r(lm ) − b in Eq. (8). We observe faster conver-
gence than using a traditional moving baseline [33].
The overall hybrid learning will be alternatively con-
ducted between supervised learning and reinforcement
learning, where we ﬁrst train the supervised end-task on
pretrained π(l|θ), then ﬁx the end-task as reward function
to learn our reinforcement policy network, after that, we
update the supervised end-task by new π(l|θ). The latter
two stages are running alternatively 2 times in our model.

4. Experiments on Scene Graph Generation

4.1. Settings

Dataset. Visual Genome (VG) [24] is a popular benchmark
for SGG. It contains 108,077 images with tens of thousands
of unique object and predicate relation categories, yet most
of categories have very limited instances. Therefore, pre-
vious works [26, 50, 58] proposed various VG splits that
remove rare categories. We adopted the most popular one
from [50], which selects top-150 object categories and top-
50 predicate categories by frequency. The entire dataset is
divided into the training set and test set by 70%, 30%, re-
spectively. We further picked 5,000 images from training
set as the validation set for hyper-parameter tuning.

6623

Scene Graph Generation
Scene Graph Classiﬁcation
Predicate Classiﬁcation
Model
R@20 R@50 R@100 R@20 R@50 R@100 R@20 R@50 R@100
VRD [31]
-
0.3
0.5
-
11.8
14.1
-
27.9
35.0
AsscEmbed [34]
6.5
8.1
8.2
18.2
21.8
22.6
47.9
54.1
55.4
IMP⋄ [50]
14.6
20.7
24.5
31.7
34.6
35.4
52.7
59.3
61.3
TFR [21]
3.4
4.8
6.0
19.6
24.3
26.6
40.1
51.9
58.3
FREQ⋄ [57]
20.1
26.2
30.1
29.3
32.3
32.9
53.6
60.6
62.2
MOTIFS⋄ [57]
21.4
27.2
30.3
32.9
35.8
36.5
58.5
65.2
67.1
Graph-RCNN [51]
-
11.4
13.7
-
29.6
31.6
-
54.2
59.1
Chain
21.2
27.1
30.3
33.3
36.1
36.8
59.4
66.0
67.7
Overlap
21.4
27.3
30.4
33.7
36.5
37.1
59.5
66.0
67.8
Multi-Branch
21.5
27.3
30.6
34.3
37.1
37.8
59.5
66.1
67.8
VCTR E E-SL
21.7
27.7
31.1
35.0
37.9
38.6
59.8
66.2
67.9
22.0
27.9
31.3
35.2
38.1
38.8
60.1
66.4
68.1
VCTR E E-HL
Table 1. SGG performances (%) of various methods. ⋄ denotes the methods using the same Faster-RCNN detector as ours. IMP⋄ is reported
from the re-implemented version [57].

3%

Model
MOTIFS⋄ [57]
FREQ⋄ [57]
VCTR E E-HL

SGGen
SGCls
PredCls
mR@100 mR@100 mR@100
6.6
8.2
15.3
7.1
8.5
16.0
8.0
10.8
19.4

Table 2. Mean recall (%) of various methods across all the 50 pred-
icate categories.

Protocols. We followed three conventional protocols to
evaluate our SGG model: (1) Scene Graph Generation
(SGGen): given an image, detect object bounding boxes
and their categories, and predict their relationships; (2)
Scene Graph Classiﬁcation (SGCls): given ground-truth
object bounding boxes in an image, predict the object cate-
gories and their relationships; (3) Predicate Classiﬁcation
(PredCls): given the object categories and their bounding
boxes in the image, predict their relationships.
Metrics. Since the annotation in VG is incomplete and
biased, we followed the conventional Recall@K (R@K =
20,50,100) as the evaluation metrics [31, 50, 57]. How-
ever, it is well-known that SGG models trained on biased
datasets such as VG have low performances for less fre-
quent categories. To this end, we introduced a balanced
metric called: Mean Recall (mR@K). It calculates the re-
call on each predicate category independently, and then av-
erages the results. So, each category contributes equally.
Such a metric reduces the inﬂuence of some common yet
meaningless predicates, e.g., “on”, “of ”, and gives equal
attention to those infrequent predicates, e.g., “riding”, “car-
rying”, which are more valuable to high-level reasoning.

4.2. Implementation Details

We adopted Faster-RCNN [40] with VGG backbone
to detect object bounding boxes and extract RoI features.
Since the performance of SGG highly depends on the under-
lying detector, we used the same set of parameters as [57]
for fair comparison. Object correlations f (xi , xj ) in Eq. (1)
will be pretrained on ground-truth bounding boxes with
class-agnostic relationships (i.e., foreground/background

Model
Graph
Chain
Overlap
Multi-Branch
VCTR E E-SL
VCTR E E-HL

VQA2.0 Validation Accuracy
Yes/No Number Other All
81.8
44.9
56.6
64.5
81.8
44.5
56.9
64.6
81.8
44.8
57.0
64.7
82.1
44.3
56.9
64.7
82.3
45.0
57.0
64.9
82.6
45.1
57.1
65.1

Balanced Pairs
36.3
36.3
36.4
36.6
36.9
37.2

Table 3. Accuracies (%) of various context structures on the
VQA2.0 validation set.

VCTR E E, so there is no need to conduct hybrid learning ex-
periment on Multi-Branch. We observe that VCTR E E per-
forms better than other structures, and it is further improved
by hybrid learning for structure exploration.

4.4. Comparisons with State-of-the-Arts

Comparing Methods. We compared VCTR E E with state-
of-the-art methods in Table 1: (1) VRD [31], FREQ [57]
are methods without using visual contexts. (2) AssocEm-
bed [34] assembles implicit contextual features by stacked
hourglass backbone [35]. (3) IMP [50], TFR [21], MO-
TIFS [57], Graph-RCNN [51] are explicit context models
with a variety of structures.
Quantitative Analysis. From Table 1, compared with the
previous state-of-the-art MOTIFS [57], the proposed VC -
TR E E has the best performances. Interestingly, Overlap tree
and Multi-Branch tree are better than other non-tree context
models. From Table 2, the proposed VCTR E E-HL shows
larger absolute gains of PredCls under mR@100, which in-
dicates that our model learns non-trivial visual context, i.e.,
not merely class distribution bias as in FREQ and partially
in MOTIFS. Note that MOTIFS [57] is even worse than its
FREQ [57] baseline under mR@100.
Qualitative Analysis. To better understand what context is
learned by VCTR E E, we visualized a statistics of left-/right-
branch nodes for nodes classiﬁed as “street” in Figure 6.
From the left pie, the hierarchical relations, we can see the
node categories are long-tailed, i.e., top-10 categories cover
the 73% of the instances; while the right pie, the parallel re-
lations, are more uniformly distributed. This demonstrates
that VCTR E E captures the two types of context success-
fully. More qualitative examples of VCTR E Es and their
generated scene graph can be viewed in Figure 7. The com-
mon errors are generally synonymous labels, e.g., “jeans”
vs. “pants”, “man” vs. “person”, and over-interpretation,
e.g., the “tail” of bottom left “dog” is considered as “leg”,
as it appears at the place where “leg” should be.

5. Experiments on Visual Q&A

5.1. Settings

Datasets. We evaluated the proposed VQA model on
VQA2.0 [17]. Compared with VQA1.0 [2], VQA2.0
has more question-image pairs for
training (443,757)

Model
Teney [45]
MUTAN [5]
MLB [22]
DA-NTN [3]
Count [59]
Chain
Graph
VCTR E E-HL

VQA2.0 test-dev
Yes/No Number Other
81.82
44.21
56.05
82.88
44.54
56.50
83.58
44.92
56.34
84.29
47.14
57.92
83.14
51.62
58.97
82.74
47.31
58.93
83.53
47.09
58.6
84.28
47.78
59.11

All
65.32
66.01
66.27
67.56
68.09
67.42
67.56
68.19

Table 4. Single-model accuracies (%) on VQA2.0 test-dev, where
MUTAN and MLB are re-implemented versions from [3].

Model
Teney [45]
MUTAN [5]
MLB [22]
DA-NTN [3]
Count [59]
Chain
Graph
VCTR E E-HL

VQA2.0 test-standard
Yes/No Number Other
82.20
43.90
56.26
83.06
44.28
56.91
83.96
44.77
56.52
84.60
47.13
58.20
83.56
51.39
59.11
83.06
47.38
58.95
84.03
47.08
58.82
84.55
47.36
59.34

All
65.67
66.38
66.62
67.94
68.41
67.68
68.0
68.49

Table 5. Single-model accuracies (%) on VQA2.0 test-standard,
where MUTAN and MLB are re-implemented versions from [3].

3

and validation (214,354), and all
the question-answer
pairs are balanced by making sure the same question
can have different answers.
In VQA2.0,
the ground-
truth accuracy of a candidate answer is considered as
the average of min( #Humans votes
, 1) over all 10 select 9
sets. Question-answer pairs are organized in three answer
types: i.e. “Yes/No”, “Number”, “Other”. There are also
65 question types determined by preﬁxed words, which we
used to generate question-guided gates. We also tested
our models on a balanced subset of validation set, called
Balanced Pairs [45], which requires the same question on
different images with two different yet perfect (with 1.0
ground-truth score) answers. Since Balanced Pairs strictly
removes question-related bias, it reﬂects the ability of a con-
text model to distinguish subtle differences between images.

5.2. Implementation Details

We employed a simple text preprocessing for questions
and answers, which changes all characters into lower-case
and removes special characters. Questions were encoded
into a vocabulary of the size 13,758 without trimming. An-
swers used a 3,000 vocabulary selected by frequency. For
fair comparison, we used the same bottom-up feature [1] as
previous methods [1, 3, 45, 59], which contains 10 to 100
object proposals per image extracted by Faster-RCNN [40].
We used the same Faster-RCNN detector to pretrain the
f (xi , xj ). Since candidate answers were represented by
probabilities rather than one-hot vectors in VQA2.0, we al-
lowed the cross-entropy loss calculating soft categories, i.e.,
probabilities of ground-truth candidate answers. We used
Adam optimizer with learning rate lr = 0.0015 and batch
size b = 256, lr decayed at ratio of 0.5 every 20 epochs.

6625

References

[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and
visual question answering. In CVPR, 2018. 3, 5, 7, 8
[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
Vqa: Visual question answering. In ICCV, 2015. 7
[3] Yalong Bai, Jianlong Fu, Tiejun Zhao, and Tao Mei. Deep at-
tention neural tensor network for visual question answering.
In ECCV, 2018. 3, 7, 8
[4] Moshe Bar. Visual objects in context. Nature Reviews Neu-
roscience, 2004. 1
[5] Hedi Ben-Younes, R ´emi Cadene, Matthieu Cord, and Nico-
las Thome. Mutan: Multimodal tucker fusion for visual
question answering. In ICCV, 2017. 7, 8
[6] Irving Biederman, Robert J Mezzanotte, and Jan C Rabi-
nowitz. Scene perception: Detecting and judging objects un-
dergoing relational violations. Cognitive Psychology, 1982.
1
[7] Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shil-
iang Pu, and Shih-Fu Chang. Scene dynamics: Counterfac-
tual critic multi-agent training for scene graph generation.
arXiv preprint arXiv:1812.02347, 2018. 2
[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. TPAMI, 2018. 1, 2
[9] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for seman-
tic image segmentation. arXiv preprint arXiv:1706.05587,
2017. 2
[10] Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iter-
ative visual reasoning beyond convolutions. In CVPR, 2018.
2
[11] Kyunghyun Cho, Bart Van Merri ¨enboer, Dzmitry Bahdanau,
and Yoshua Bengio. On the properties of neural machine
translation: Encoder-decoder approaches. In SSST-8, 2014.
2
[12] Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Learning to
compose task-speciﬁc tree structures. In AAAI, 2018. 2
[13] Myung Jin Choi, Antonio Torralba, and Alan S Willsky. A
tree-based context model for object recognition. TPAMI,
2012. 2
[14] Thomas H. Cormen, Clifford Stein, Ronald L. Rivest, and
Charles E. Leiserson. Introduction to Algorithms. McGraw-
Hill Higher Education, 2001. 1, 2
[15] Bo Dai, Yuqi Zhang, and Dahua Lin. Detecting visual re-
lationships with deep relational networks. In CVPR, 2017.
2
[16] Santosh K Divvala, Derek Hoiem, James H Hays, Alexei A
Efros, and Martial Hebert. An empirical study of context in
object detection. In CVPR, 2009. 1
[17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answer-
ing. In CVPR, 2017. 2, 7

[18] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In ICCV, 2017. 1, 3, 4
[19] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term
memory. Neural computation, 1997. 2
[20] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor
Darrell, and Kate Saenko. Learning to reason: End-to-end
module networks for visual question answering.
In ICCV,
2017. 2, 5
[21] Seong Jae Hwang, Sathya N Ravi, Zirui Tao, Hyunwoo J
Kim, Maxwell D Collins, and Vikas Singh. Tensorize, fac-
torize and regularize: Robust visual relationship learning. In
CVPR, 2018. 6, 7
[22] Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee
Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard
product for low-rank bilinear pooling.
In ICLR, 2016. 7,
8
[23] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ´ar. Panoptic segmentation. arXiv
preprint arXiv:1801.00868, 2018. 3
[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. IJCV, 2017. 2, 3, 5
[25] Yikang Li, Wanli Ouyang, Bolei Zhou, Jianping Shi, Chao
Zhang, and Xiaogang Wang. Factorizable net: An efﬁcient
subgraph-based framework for scene graph generation.
In
ECCV, 2018. 2, 3
[26] Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xi-
aogang Wang. Scene graph generation from objects, phrases
and caption regions. In ICCV, 2017. 2, 3, 5
[27] Tsung-Yi Lin, Piotr Doll ´ar, Ross B Girshick, Kaiming He,
Bharath Hariharan, and Serge J Belongie. Feature pyramid
networks for object detection. In CVPR, 2017. 1, 2
[28] Daqing Liu, Zheng-Jun Zha, Hanwang Zhang, Yongdong
Zhang, and Feng Wu. Context-aware visual policy network
for sequence-level image captioning. ACM on Multimedia
Conference, 2018. 2
[29] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. Ssd: Single shot multibox detector. In ECCV, 2016. 1
[30] Yong Liu, Ruiping Wang, Shiguang Shan, and Xilin Chen.
Structure inference net: Object detection using scene-level
context and instance-level relationships. In CVPR, 2018. 2
[31] Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-
Fei. Visual relationship detection with language priors. In
ECCV, 2016. 3, 6, 7
[32] Tomasz Malisiewicz and Alyosha Efros. Beyond categories:
The visual memex model for reasoning about object relation-
ships. In NIPS, 2009. 2
[33] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recur-
rent models of visual attention. In NIPS, 2014. 5
[34] Alejandro Newell and Jia Deng. Pixels to graphs by associa-
tive embedding. In NIPS, 2017. 3, 6, 7
[35] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV, 2016.
7

6627

[54] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring
visual relationship for image captioning. In ECCV, 2018. 2
[55] Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang
Wang, Jing Shao, and Chen Change Loy. Zoom-net: Mining
deep feature interactions for visual relationship recognition.
In ECCV, 2018. 2
[56] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-
tion by dilated convolutions. In ICLR, 2016. 2
[57] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin
Choi. Neural motifs: Scene graph parsing with global con-
text. In CVPR, 2018. 1, 2, 4, 6, 7
[58] Ji Zhang, Mohamed Elhoseiny, Scott Cohen, Walter Chang,
and Ahmed M Elgammal. Relationship proposal networks.
In CVPR, 2017. 5
[59] Yan Zhang, Jonathon Hare, and Adam Pr ¨ugel-Bennett.
Learning to count objects in natural images for visual ques-
tion answering. In ICLR, 2018. 5, 7, 8
[60] Rui Zhao, Wanli Ouyang, Hongsheng Li, and Xiaogang
Wang. Saliency detection by multi-context deep learning.
In CVPR, 2015. 1
[61] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-
Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang
Huang, and Philip HS Torr. Conditional random ﬁelds as
recurrent neural networks. In ICCV, 2015. 2

[36] Yulei Niu, Hanwang Zhang, Manli Zhang, Jianhong Zhang,
Zhiwu Lu, and Ji-Rong Wen. Recursive visual attention in
visual dialog. CVPR, 2019. 2

[37] Aude Oliva and Antonio Torralba. The role of context in
object recognition. Trends in Cognitive Sciences, 2007. 1

[38] Pedro O Pinheiro, Tsung-Yi Lin, Ronan Collobert, and Piotr
Doll ´ar. Learning to reﬁne object segments. In ECCV, 2016.
1

[39] Robert Clay Prim. Shortest connection networks and some
generalizations. Bell System Technical Journal, 1957. 4

[40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In NIPS, 2015. 1, 2, 3, 6, 7

[41] Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret
Ross, and Vaibhava Goel. Self-critical sequence training for
image captioning. In CVPR, 2017. 2, 5

[42] Yang Shi, Tommaso Furlanello, Sheng Zha, and Animashree
Anandkumar. Question type guided attention in visual ques-
tion answering. In ECCV, 2018. 5

[43] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y
Ng. Parsing natural scenes and natural language with recur-
sive neural networks. In ICML, 2011. 2

[44] Kai Sheng Tai, Richard Socher, and Christopher D Manning.
Improved semantic representations from tree-structured long
short-term memory networks. In ACL, 2015. 1, 2, 4, 6

[45] Damien Teney, Peter Anderson, Xiaodong He, and Anton
van den Hengel. Tips and tricks for visual question answer-
ing: Learnings from the 2017 challenge. In CVPR, 2018. 2,
3, 5, 7, 8

[46] Damien Teney, Lingqiao Liu, and Anton van den Hengel.
Graph-structured representations for visual question answer-
ing. In CVPR, 2017. 2

[47] Takeo Watanabe, Alexander M Harner, Satoru Miyauchi,
Yuka Sasaki, Matthew Nielsen, Daniel Palomo, and Ikuko
Mukai. Task-dependent inﬂuences of attention on the acti-
vation of human primary visual cortex. Proceedings of the
National Academy of Sciences, 1998. 2

[48] Ronald J Williams. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Machine
learning, 1992. 2, 5

[49] Yingjie Xia, Luming Zhang, Zhenguang Liu, Liqiang Nie,
and Xuelong Li. Weakly supervised multimodal kernel for
categorizing aerial photographs. IEEE Transactions on Im-
age Processing, 2017. 2

[50] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.
Scene graph generation by iterative message passing.
In
CVPR, 2017. 1, 2, 3, 5, 6, 7, 8

[51] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi
Parikh. Graph r-cnn for scene graph generation. In ECCV,
2018. 3, 6, 7

[52] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai.
Auto-encoding graphical inductive bias for descriptive image
captioning. arXiv preprint arXiv:1812.02378, 2018. 3

[53] Xu Yang, Hanwang Zhang, and Jianfei Cai. Shufﬂe-then-
assemble:
learning object-agnostic visual relationship fea-
tures. In ECCV, 2018. 3

6628

Locating Objects Without Bounding Boxes

Javier Ribera, David G ¨uera, Yuhao Chen, Edward J. Delp
Video and Image Processing Laboratory (VIPER), Purdue University

Abstract

Recent advances in convolutional neural networks
(CNN) have achieved remarkable results in locating objects
in images. In these networks, the training procedure usually
requires providing bounding boxes or the maximum number
of expected objects. In this paper, we address the task of es-
timating object locations without annotated bounding boxes
which are typically hand-drawn and time consuming to la-
bel. We propose a loss function that can be used in any fully
convolutional network (FCN) to estimate object locations.
This loss function is a modiﬁcation of the average Haus-
dorff distance between two unordered sets of points. The
proposed method has no notion of bounding boxes, region
proposals, or sliding windows. We evaluate our method
with three datasets designed to locate people’s heads, pupil
centers and plant centers. We outperform state-of-the-art
generic object detectors and methods ﬁne-tuned for pupil
tracking.

1. Introduction

Locating objects in images is an important task in com-
puter vision. A common approach in object detection is to
obtain bounding boxes around the objects of interest. In this
paper, we are not interested in obtaining bounding boxes.
Instead, we deﬁne the object localization task as obtaining
a single 2D coordinate corresponding to the location of each
object. The location of an object can be any key point we are
interested in, such as its center. Figure 1 shows an example
of localized objects in images. Differently from other key-
point detection problems, we do not know in advance the
number of keypoints in the image. To also make the method
as generic as possible we do not assume any physical con-
straint between the points, unlike in cases such as pose esti-
mation. This deﬁnition of object localization is more appro-
priate for applications where objects are very small, or sub-
stantially overlap (see the overlapping plants in Figure 1).
In these cases, bounding boxes may not be provided by the
dataset or they may be infeasible to groundtruth.
Bounding-box annotation is tedious,
time-consuming
and expensive [37]. For example, annotating ImageNet [43]

Figure 1. Object localization with human heads, eye pupils and
plant centers. (Bottom) Heat map and estimations as crosses.

required 42 seconds per bounding box when crowdsourcing
on Amazon’s Mechanical Turk using a technique speciﬁ-
cally developed for efﬁcient bounding box annotation [50].
In [6], Bell et al. introduce a new dataset for material recog-
nition and segmentation. By collecting click location labels
in this dataset instead of a full per-pixel segmentation, they
reduce the annotation costs an order of magnitude.

In this paper, we propose a modiﬁcation of the average
Hausdorff distance as a loss function of a CNN to estimate
the location of objects. Our method does not require the use
of bounding boxes in the training stage, and does not require
to know the maximum number of objects when designing
the network architecture. For simplicity, we describe our
method only for a single class of objects, although it can
trivially be extended to multiple object classes. Our method
is object-agnostic, thus the discussion in this paper does
not include any information about the object characteristics.
Our approach maps input images to a set of coordinates, and
we validate it with diverse types of objects. We evaluate our
method with three datasets. One dataset contains images ac-
quired from a surveillance camera in a shopping mall, and
we locate the heads of people. The second dataset contains
images of human eyes, and we locate the center of the pupil.
The third dataset contains aerial images of a crop ﬁeld taken

6479

from an Unmanned Aerial Vehicle (UAV), and we locate the
centers of highly occluded plants.
Our approach to object localization via keypoint detec-
tion is not a universal drop-in replacement for bounding box
detection, specially for those tasks that inherently require
bounding boxes, such as automated cropping. Also, a limi-
tation of this approach is that bounding box labeling incor-
porates some sense of scale, while keypoints do not.
The contributions of our work are:

• We propose a loss function for object localization,
which we name weighted Hausdorff distance (WHD),
that overcomes the limitations of pixelwise losses such
as L2 and the Hausdorff distances.

• We develop a method to estimate the location and
number of objects in an image, without any notion of
bounding boxes or region proposals.

• We formulate the object localization problem as the
minimization of distances between points, indepen-
dently of the model used in the estimation. This al-
lows to use any fully convolutional network architec-
tural design.

• We outperform state-of-the-art generic object detectors
and achieve comparable results with crowd counting
methods without any domain-speciﬁc knowledge, data
augmentation, or transfer learning.

2. Related Work

Generic object detectors. Recent advances in deep
learning [16, 27] have increased the accuracy of localiza-
tion tasks such as object or keypoint detection. By generic
object detectors, we mean methods that can be trained to
detect any object type or types, such as Faster-RCNN [15],
Single Shot MultiBox Detector (SSD) [31], or YOLO [40].
In Fast R-CNN, candidate regions or proposals are gener-
ated by classical methods such as selective search [59]. Al-
though activations of the network are shared between region
proposals, the system cannot be trained end-to-end. Re-
gion Proposal Networks (RPNs) in object detectors such
as Faster R-CNN [15, 41] allow for end-to-end training
of models. Mask R-CNN [18] extends Faster R-CNN by
adding a branch for predicting an object mask but it runs in
parallel with the existing branch for bounding box recog-
nition. Mask R-CNN can estimate human pose keypoints
by generating a segmentation mask with a single class in-
dicating the presence of the keypoint. The loss function
in Mask R-CNN is used location by location, making the
keypoint detection highly sensitive to alignment of the seg-
mentation mask. SDD provides ﬁxed-sized bounding boxes
and scores indicating the presence of an object in the boxes.
The described methods either require groundtruthed bound-
ing boxes to train the CNNs or require to set the maximum

number of objects in the image being analyzed. In [19], it
is observed that generic object detectors such as Faster R-
CNN and SSD perform very poorly for small objects.

Counting and locating objects. Counting the number
of objects in an image is not a trivial task. In [28], Lem-
pitsky et al. estimate a density function whose integral cor-
responds to the object count. In [47], Shao et al. proposed
two methods for locating objects. One method ﬁrst counts
and then locates, and the other ﬁrst locates and then counts.

Locating and counting people is necessary for many ap-
plications such as crowd monitoring in surveillance sys-
tems, surveys for new businesses, and emergency manage-
ment [28, 60]. There are multiple studies in the litera-
ture, where people in videos of crowds are detected and
tracked [2, 7]. These detection methods often use bound-
ing boxes around each human as ground truth. Acquiring
bounding boxes for each person in a crowd can be labor in-
tensive and imprecise under conditions where lots of people
overlap, such as sports events or rush-hour agglomerations
in public transport stations. More modern approaches avoid
the need of bounding boxes by estimating a density map
whose integral yields the total crowd count. In approaches
that involve a density map, the label of the density map is
constructed from the labels of the people’s heads. This is
typically done by centering Gaussian kernels at the location
of each head. Zhang et al. [62] estimate the density im-
age using a multi-column CNN that learns features at dif-
ferent scales. In [44], Sam et al. use multiple independent
CNNs to predict the density map at different crowd densi-
ties. An additional CNN classiﬁes the density of the crowd
scene and relays the input image to the appropriate CNN.
Huang et al. [20] propose to incorporate information about
the body part structure to the conventional density map to
reformulate the crowd counting as a multi-task problem.
Other works such as Zhang et al. [61] use additional in-
formation such as the groundtruthed perspective map.

Methods for pupil tracking and precision agriculture are
usually domain-speciﬁc. In pupil tracking, the center of the
pupil must be resolved in images obtained in real-world il-
lumination conditions [13]. A wide range of applications,
from commercial applications such as video games [52],
driving [48, 17] or microsurgery [14] rely on accurate pupil
tracking.
In remote precision agriculture, it is critical to
locate the center of plants in a crop ﬁeld. Agronomists
use plant traits such as plant spacing to predict future crop
yield [56, 51, 57, 12, 8], and plant scientists to breed new
plant varieties [3, 35].
In [1], Aich et al. count wheat
plants by ﬁrst segmenting plant regions and then counting
the number of plants in each segmented patch.

Hausdorff distance. The Hausdorff distance can be
used to measure the distance between two sets of points [5].
Modiﬁcations of the Hausdorff distance [10] have been
used for various multiple tasks, including character recog-

6480

nition [33], face recognition [23] and scene matching [23].
Schutze et al. [46] use the average Hausdorff distance
to evaluate solutions in multi-objective optimization prob-
lems. In [24], Elkhiyari et al. compare features extracted by
a CNN according to multiple variants of the Hausdorff dis-
tance for the task of face recognition. In [11], Fan et al. use
the Chamfer and Earth Mover’s distance, along with a new
neural network architecture, for 3D object reconstruction
by estimating the location of a ﬁxed number of points. The
Hausdorff distance is also a common metric to evaluate the
quality of segmentation boundaries in the medical imaging
community [54, 63, 30, 55].

3. The Average Hausdorff Distance

Our work is based on the Hausdorff distance which we
brieﬂy review in this section. Consider two unordered non-
empty sets of points X and Y and a distance metric d(x, y)
between two points x ∈ X and y ∈ Y . The function d(·, ·)
could be any metric. In our case we use the Euclidean dis-
tance. The sets X and Y may have different number of
points. Let Ω ⊂ R2 be the space of all possible points. In
its general form, the Hausdorff distance between X ⊂ Ω
and Y ⊂ Ω is deﬁned as

dH (X, Y ) = max (cid:26) sup

x∈X

inf

y∈Y

d(x, y), sup

y∈Y

inf

x∈X

d(x, y)(cid:27) .

(1)
When considering a discretized and bounded Ω, such as
all the possible pixel coordinates in an image, the suprema
and inﬁma are achievable and become maxima and minima,
respectively. This bounds the Hausdorff distance as

d(X, Y ) ≤ dmax = max

x∈Ω,y∈Ω

d(x, y),

(2)

which corresponds to the diagonal of the image when using
the Euclidean distance. As shown in [5], the Hausdorff dis-
tance is a metric. Thus ∀X, Y , Z ⊂ Ω we have the following
properties:

dH (X, Y ) ≥ 0
dH (X, Y ) = 0 ⇐⇒ X = Y
dH (X, Y ) = dH (Y , X )
dH (X, Y ) ≤ dH (X, Z ) + dH (Z, Y )

(3a)

(3b)

(3c)

(3d)

Equation (3b) follows from X and Y being closed, be-
cause in our task the pixel coordinate space Ω is discretized.
These properties are very desirable when designing a func-
tion to measure how similar X and Y are [4].
A shortcoming of the Hausdorff function is its high sen-
sitivity to outliers [46, 54]. Figure 2 shows an example for
two ﬁnite sets of points with one outlier. To avoid this, the

Figure 2. Illustration of two different conﬁgurations of point sets
X = {x1 , ..., x5 } (solid dots) and Y = {y1 , ..., y4 } (dashed dots).
Despite the clear difference in the distances between points, their
Hausdorff distance are equal because the worst outlier is the same.

average Hausdorff distance is more commonly used:

|Y | Xy∈Y
1

|X | Xx∈X
1

dAH (X, Y ) =

min

y∈Y

d(x, y) +

min

x∈X

d(x, y),

(4)
where |X | and |Y | are the number of points in X and Y , re-
spectively. Note that properties (3a), (3b) and (3c) are still
true, but (3d) is not. Also, the average Hausdorff distance is
differentiable with respect to any point in X or Y .
Let Y contain the ground truth pixel coordinates, and X
be our estimation. Ideally, we would like to use dAH (X, Y )
as the loss function during the training of our convolutional
neural network (CNN). We ﬁnd two limitations when incor-
porating the average Hausdorff distance as a loss function.
First, CNNs with linear layers implicitly determine the esti-
mated number of points |X | as the size of the last layer. This
is a drawback because the actual number of points depends
on the content of the image itself. Second, FCNs such as U-
Net [42] can indicate the presence of an object center with a
higher activation in the output layer, but they do not return
the pixel coordinates. In order to learn with backpropaga-
tion, the loss function must be differentiable with respect to
the network output.

4. The Weighted Hausdorff Distance

To overcome these two limitations, we modify the aver-
age Hausdorff distance as follows:

px min

d(x, y)+

y∈Y

S + ǫ Xx∈Ω
1
1
|Y | Xy∈Y
Mα

x∈Ω

[ pxd(x, y) + (1 − px )dmax ] ,

(5)

(6)

px ,

S = Xx∈Ω

dWH (p, Y ) =

where

6481

Mα

a∈A

[f (a)] = 1
|A| Xa∈A

1
α

,

f α (a)!

(7)

is the generalized mean, and ǫ is set to 10−6 . We
call dWH (p, Y ) the weighted Hausdorff distance (WHD).
px ∈ [0, 1] is the single-valued output of the network at
pixel coordinate x. The last activation of the network can
be bounded between zero and one by using a sigmoid non-
linearity. Note that p does not need to be normalized, i.e.,
Px∈Ω px = 1 is not necessary. Note that the generalized
mean Mα [·] corresponds to the minimum function when
α = −∞. We justify the modiﬁcations applied to Equa-
tion (4) to obtain Equation (5) as follows:

1. The ǫ in the denominator of the ﬁrst term provides nu-
merical stability when px ≈ 0 ∀x ∈ Ω.

2. When px = {0, 1}, α = −∞, and ǫ = 0, the weighted
Hausdorff distance becomes the average Hausdorff
distance. We can interpret this as the network indi-
cating with complete certainty where the object cen-
ters are. As dWH (p, Y ) ≥ 0, the global minimum
(dWH (p, Y ) = 0) corresponds to px = 1 if x ∈ Y
and 0 otherwise.

3. In the ﬁrst term, we multiply by px to penalize high ac-
tivations in areas of the image where there is no ground
truth point y nearby. In other words, the loss function
penalizes estimated points that should not be there.

4. In the second term, by using the expression

f (·) := pxd(x, y) + (1 − px )dmax we enforce that

(a) If px0 ≈ 1, then f (·) ≈ d(x0 , y). This means the
point x0 will contribute to the loss as in the AHD
(Equation (4)).

(b) If px0 ≈ 0, x0 6= y , then f (·) ≈ dmax . Then,
if α = −∞, the point x0 will not contribute to
the loss because the “minimum” Mx∈Ω [ · ] will
ignore x0 . If another point x1 closer to y with
px1 > 0 exists, x1 will be “selected” instead by
M [ · ]. Otherwise Mx∈Ω [ · ] will be high. This
means that low activations around ground truth
points will be penalized.

Note that f (·) is not the only expression that would
enforce these two constraints (f |px=1 = d(x, y) and
f |px=0 = dmax ). We chose a linear function because
of its simplicity and numerical stability.

Both terms in the WHD are necessary. If the ﬁrst term
is removed, then the trivial solution is px = 1 ∀x ∈ Ω.
If the second term is removed, then the trivial solution is
px = 0 ∀x ∈ Ω. These two cases hold for any value of

α and the proof can be found in the suplemental material.
Ideally, the parameter α → −∞ so that Mα (·) = || · ||−∞
becomes the minimum operator [26]. However, this would
make the second term ﬂat with respect to the output of the
network. For a given y , changes in px0 in a point x0 that is
far from y would be ignored by M−∞ (·), if there is another
point x1 with high activation and closer to y . In practice,
this makes training difﬁcult because the minimum is not a
smooth function with respect to its inputs. Thus, we ap-
proximate the minimum with the generalized mean Mα (·),
with α < 0. The more negative α is, the more similar to the
AHD the WHD becomes, at the expense of becoming less
smooth. In our experiments, α = −1. There is no need to
use Mα (·) in the ﬁrst term because px is not inside the min-
imum, thus the term is already differentiable with respect to

p.

If the input image needs to be resized to be fed into the
network, we can normalize the WHD to account for this dis-
tortion. Denote the original image size as (S (1)
o ) and
the resized image size as (S (1)
r ). In Equation (5), we
compute distances in the original pixel space by replacing

o , S (2)

, S (2)

r

d(x, y) with d(Sx, Sy), where x, y ∈ Ω and
r ! .
4.1. Advantage Over Pixelwise Losses

S = S (1)

S (2)
o /S (2)

o /S (1)

0

r

0

(8)

A naive alternative is to use a one-hot map as label, de-
ﬁned as lx = 1 for x ∈ Y and lx = 0 otherwise, and then
use a pixelwise loss such as the Mean Squared Error (MSE)
or the L2 norm, where L2 (l, p) = P∀x∈Ω |px − lx |2 ∝
MSE(l, x). The issue with pixelwise losses is that they are
not informative of how close two points x ∈ Ω and y ∈ Y
are unless x = y . In other words, it is ﬂat for the vast major-
ity of the pixels, making training unfeasible. This issue is
locally mitigated in [58] by using the MSE loss with Gaus-
sians centered at each x ∈ Y . By contrast, the WHD in
Equation (5) will decrease the closer x is to y , making the
loss function informative outside of the global minimum.

5. CNN Architecture And Location Estimation

In this section, we describe the architecture of the fully
convolutional network (FCN) we use, and how we esti-
mate the ﬁnal object locations. We want to emphasize that
the network design is not a meaningful contribution of this
work, thus we have not made any attempt to optimize it. Our
main contribution is the use of the weighted Hausdorff dis-
tance as the loss function. We adopt the U-Net architecture
[42] and modify it minimally for this task. Networks similar
to U-Net have been proven to be capable of accurately map-
ping the input image into an output image, when trained in
a conditional adversarial network setting [22] or when us-
ing a carefully tuned loss function [42]. Figure 3 shows the

6482

Figure 3. The FCN architecture used for object localization, min-
imally adapted from the U-Net [42] architecture. We add a small
fully-connected layer that combines the deepest features and the
estimated probability map to regress the number of points.

hourglass design of U-Net. The residuals connections be-
tween each layer in the encoder and its symmetric layer in
the decoder are not shown for simplicity.
This FCN has two well differentiated blocks. The ﬁrst
block follows the typical architecture of a CNN. It consists
of the repeated application of two 3 × 3 convolutions (with
padding 1), each followed by a batch normalization opera-
tion and a Rectiﬁed Linear Unit (ReLU). After the ReLU,
we apply a 2 × 2 max pooling operation with stride 2 for
downsampling. At each downsampling step we double the
number of feature channels, starting with 64 channels and
using 512 channels for the last 5 layers.
The second block consists of repeated applications of the
following elements: a bilinear upsampling, a concatenation
with the feature map from the downsampling block, and two
3 × 3 convolutions, each followed by a batch normalization
and a ReLU. The ﬁnal layer is a convolution layer that maps
to the single-channel output of the network, p.
To estimate the number of objects in the image, we add
a branch that combines the information from the deepest
level features and also from the estimated probability map.
This branch combines both features (the 1 × 1 × 512 feature
vector and the 256 × 256 probability map) into a hidden
layer, and uses the 128-dimensional feature vector to output
a single number. We then apply a ReLU to ensure the output
is positive, and round it to the closest integer to obtain our
ﬁnal estimate of the number of objects, ˆC .
Although we use this particular network architecture,
any other architecture could be used. The only requirement
is that the output images of the network must be of the same
size as the input image. The choice of a FCN arises from the
natural interpretation of its output as the weights (px ) in the
WHD (Equation (5)). In previous works [24, 11], variants
of the average Haussdorf distance were successfully used
with non-FCN networks that estimate the point set directly.
However, in those cases the size of the estimated set is ﬁxed
by the size of the last layer. To locate an unknown number

Figure 4. First row: Input image. Second row: Output of the net-
work (p in the text) overlaid onto the input image. This can be
considered a saliency map of object locations. Third row: The
estimated object locations are marked with a red cross.

of objects, the network must be able to estimate a variable
number of object locations. Thus, we could envision the
WHD also being used in non-FCN networks as long as the
output of the network is used as p in Equation (5).
The training loss we use to train the network is a combi-
nation of Equation (5) and a smooth L1 loss for the regres-
sion of the object count. The ﬁnal training loss is

L(p, Y ) = dW H (p, Y ) + Lreg (C − ˆC (p)),

(9)

where Y is the set containing the ground truth coordi-
nates of the objects in the image, p is the output of the
network, C = |Y |, and ˆC (p) is the estimated number of
objects. Lreg (·) is the regression term, for which we use the
smooth L1 or Huber loss [21], deﬁned as

Lreg (x) = (0.5x2 ,
|x| − 0.5,

for|x| < 1
for|x| ≥ 1

(10)

This loss is robust to outliers when the regression error is
high, and at the same time is differentiable at the origin.
The network outputs a saliency map p indicating with
px ∈ [0, 1] the conﬁdence that there is an object at pixel
x. Figure 4 shows p in the second row. During evaluation,
our ultimate goal is to obtain ˆY , i. e., the estimate of all
object locations. In order to convert p to ˆY , we threshold
p to obtain the pixels T = {x ∈ Ω | px > τ }. We can use
three different methods to decide which τ to use:

6483

1. Use a constant τ for all images.

2. Use Otsu thresholding [36] to ﬁnd an adaptive τ dif-
ferent for every image.

3. Use a Beta mixture model-based thresholding (BMM).
This method ﬁts a mixture of two Beta distributions to
the values of p using the algorithm described in [45],
and then takes the mean value of the distribution with
highest mean as τ .

Figure 4 shows in the third row an example of the result
of thresholding the saliency map p. Then, we ﬁt a Gaussian
mixture model to the points T . This is done using the expec-
tation maximization (EM) [34] algorithm and the estimated
number of plants ˆC .
The means of the ﬁtted Gaussians are considered the ﬁ-
nal estimate ˆY . The third row of Figure 4 shows the esti-
mated object locations with red crosses. Note that even if
the map produced by the FCN is of good quality, i.e., there
is a cluster on each object location, EM may not yield the
correct object locations if | ˆC − C | > 0.5. An example can
be observed in the ﬁrst column of Figure 4, where a single
head is erroneously estimated as two heads.

6. Experimental Results

We evaluate our method with three datasets.
The ﬁrst dataset
consists of 2,000 images
ac-
quired from a surveillance camera in a shopping
mall.
It contains annotated locations of
the heads
of the crowd.
This dataset
is publicly available at

http://personal.ie.cuhk.edu.hk/˜ccloy/
downloads_mall_dataset.html [32]. 80%, 10%

and 10% of the images were randomly assinged to the
training, validation, and testing datasets, respectively.
The
second dataset
is presented in [13] with
the
roman
letter V and
publicly
available
at

http://www.ti.uni-tuebingen.de/
Pupil-detection.1827.0.html.

It
contains
2,135 images with a single eye, and the goal is to detect the
center of the pupil. It was also randomly split into training,
validation and testing datasets as 80/10/10 %, respectively.
The third dataset consists of aerial images of a crop ﬁeld
taken from a UAV ﬂying at an altitude of 40 m. The im-
ages were stitched together to generate a 6, 000 × 12, 000
orthoimage of 0.75 cm/pixel resolution shown in Figure 5.
The location of the center of all plants in this image was
groundtruthed, resulting in a total of 15,208 unique plant
centers. This mosaic image was split, and the left 80% area
was used for training, the middle 10% for validation, and
the right 10% for testing. Within each region, random im-
age crops were generated. These random crops have a uni-
formly distributed height and width between 100 and 600
pixels. We extracted 50,000 random image crops in the

Figure 5. An orthorectiﬁed image of a crop ﬁeld with 15,208
plants. The red region was used for training, the region in green
for validation, and the region in blue for testing.

training region, 5, 000 in the validation region, and 5, 000
in the testing region. Note that some of these crops may
highly overlap. We are making the third dataset publicly

available at https://engineering.purdue.edu/
˜sorghum/dataset-plant-centers-2016. We

believe this dataset will be valuable for the community, as it
poses a challenge due to the high occlusion between plants.
All the images were resized to 256 × 256 because
that is the minimum size our architecture allows. The
groundtruthed object locations were also scaled accord-
ingly. As for data augmentation, we only use random hori-
zontal ﬂip. For the plant dataset, we also ﬂipped the images
vertically. We set α = −1 in Equation (7). We have also
experimented with α = −2 with no apparent improvement,
but we did not attempt to ﬁnd an optimal value. We retrain
the network for every dataset, i.e., we do not use pretrained
weights. For the mall and plant dataset, we used a batch
size of 32 and Adam optimizer [25, 39] with a learning rate
of 10−4 and momentum of 0.9. For the pupil dataset, we
reduced the size of the network by removing the ﬁve central
layers, we used a batch size of 64, and stochastic gradient
descent with a learning rate of 10−3 and momentum of 0.9.
At the end of each epoch, we evaluate the average Hauss-
dorf distance (AHD) in Equation (4) over the validation set,
and select the epoch with lowest AHD on validation.
As metrics, we report Precision, Recall, F-score, AHD,
Mean Absolute Error (MAE), Root Mean Squared Error
(RMSE), and Mean Absolute Percent Error (MAPE):

MAE =

1
N

N

Xi=1

|ei |, RMSE = vuut
1
MAPE = 100
N

N

Xi=1

Ci 6=0

1
N

(cid:12)(cid:12)ei (cid:12)(cid:12)Ci

2

N

Xi=1 (cid:12)(cid:12)ei (cid:12)(cid:12)

(11)

(12)

where ei = ˆCi −Ci , N is the number of images, Ci is the
true object count in the i-th image, and ˆCi is our estimate.
A true positive is counted if an estimated location is at
most at distance r from a ground truth point. A false pos-
itive is counted if an estimated location does not have any

6484

ground truth point at a distance at most r . A false negative
is counted if a true location does have any estimated loca-
tion at a distance at most r . Precision is the proportion of
our estimated points that are close enough to a true point.
Recall is the proportion of the true points that we are able
to detect. The F-score is the harmonic mean of precision
and recall. Note that one can achieve a precision and recall
of 100% even if we estimate more than one object location
per ground truth point. This would not be an ideal local-
ization. To take this into account, we also report metrics
(MAE, RMSE and MAPE) that indicate if the number of
objects is incorrect. The AHD can be interpreted as the av-
erage location error in pixels.
Figure 8 shows the F-score as a function of r . Note that
r is only an evaluation parameter. It is not needed during
training or testing. MAE, RMSE, and MAPE are shown in
Table 1. Note that we are using the same architecture for all
tasks, except for the pupil dataset, where we removed inter-
mediate layers. Also, in the case of the pupil detection, we
know that there is always one object in the image. Thus, re-
gression is not necessary and we can remove the regression
term in Equation (9) and ﬁx ˆCi = Ci = 1 ∀i.
A naive alternative approach to object localization would
be to use generic object detectors such as Faster R-CNN
[41]. One can train these detectors by constructing bound-
ing boxes with ﬁxed size centered at each labeled point.
Then the center of each bounding box can be taken as the es-
timated location. We used bounding boxes of size 20 × 20
(the approximate average head and pupil size) and anchor
sizes of 16 × 16 and 32 × 32. Note that these parameters
may be suboptimal even though they were selected to match
the type of object. The threshold we used for the softmax
scores was 0.5 and for the intersection over union it was
0.4, because they minimize the AHD over the validation set.
We used the VGG-16 architecture [49] and trained it using
stochastic gradient descent with learning rate of 10−3 and
momentum of 0.9. For the pupil dataset, we always selected
the bounding box with the highest score. We experimentally
observed that Faster R-CNN struggles with detecting very
small objects that are very close to each other. Tables 2-4
show the results of Faster R-CNN results on the mall, pupil,
and plant datasets. Note that the mall and plant datasets,
with many small and highly overlapping objects, are the
most challenging for Faster R-CNN. This behaviour is con-
sistent with the observations in [19], where, all generic ob-
ject detectors perform very poorly and Faster R-CNN yields
a mean Average Precision (mAP) of 5% in the best case.

We also experimented using mean shift [9] instead of
Gaussian mixtures (GM) to detect the local maxima. How-
ever, mean shift is prone to detect multiple local maxima,
and GMs are more robust against outliers. In our experi-
ments, we observed that precision and recall were substan-
tially worse than using GM. More importantly, using Mean

Figure 6. Effect on the F-score of the threshold τ .

Figure 7. Beta mixture model ﬁtted on the values of px , and the
thresholds τ used by the BMM method.

Shift slowed down validation an order of magnitude. The
average time for the Mean Shift algorithm to run on one of
our images was 12 seconds, while ﬁtting GM using expec-
tation maximization took around 0.5 seconds, when using
the scikit-learn implementations [38].
We also investigated the effect of the parameter τ , and
the three methods to select it presented in Section 5. One
may think that this parameter could be a trade-off between
some metrics, and that it should be cross-validated. In prac-
tice, we observed that τ does not balance precision and re-
call, thus a precision-recall curve is not meaningful.
In-
stead, we plot the F-score as a function of r in Figure 8.
Also, cross-validating τ would imply ﬁxing an “optimal”
value for all images. Figure 6 shows that we can do better
with adaptive thresholding methods (Otsu or BMM). Note
that BMM thresholding (dashed lines) always outperforms
Otsu (solid lines), and most of ﬁxed τ . To justify the appro-
priateness of the BMM method, note that in Figure 4 most
of the values in the estimated map are very high or very low.
This makes a Beta distribution a better ﬁt than a Normal dis-
tribution (as used in Otsu’s method) to model px . Figure 7
shows the ﬁtted BMM and a kernel density estimation of
the values of τ adaptively selected by the BMM method.

6485

99.5
95.7
88.6
80.0

60.0

40.0

20.0

%

F-score

Table 2. Head location results using the mall dataset, using r = 5.

Metric

Faster-RCNN Ours

Precision
Recall
F-score
AHD
MAE
RMSE
MAPE

81.1%
76.7%
78.8 %
7.6 px
4.7
5.6
14.8%

95.2 %
96.2 %
95.7 %
4.5 px
1.4
1.8
4.4 %

Pupil dataset
Mall dataset
Plant dataset

Table 3. Pupil detection results, using r = 5. Precision and recall
are equal because there is only one estimated and one true object.

0

2

4

6

8
r (in pixels)

10

12

14

Figure 8. F-score as a function of r , the maximum distance be-
tween a true and an estimated object location to consider it correct
or incorrect. A higher r makes correctly locating an object easier.

Table 1. Results of our method for object localization, using r = 5.
Metrics are deﬁned in Equations (4), (11)-(12). Regression metrics
for the pupil dataset are not shown because there is always a single
pupil ( ˆC = C = 1). Figure 8 shows the F-score for other r values.

Metric

Precision
Recall
F-score
AHD
MAE
RMSE
MAPE

Mall
dataset

Pupil
dataset

Plant
dataset

Average

95.2%
96.2%
95.7%
4.5 px
1.4
1.8
4.4%

99.5%
99.5%
99.5%
2.5 px
-
-
-

88.1%
89.2%
88.6%
7.1 px
1.9
2.7
4.2%

94.4%
95.0%
94.6%
4.7 px
1.7
2.3
4.3 %

Lastly, as our method locates and counts objects simul-
taneously, it could be used as a counting technique. We also
evaluated our technique in the task of crowd counting us-
ing the ShanghaiTech Part B dataset presented in [62], and
achieve a MAE of 19.9. Even though we do not outper-
form state of the art methods that are speciﬁcally ﬁne-tuned
for crowd counting [29], we can achieve comparable results
with our generic method. We expect future improvements
such as architectural changes or using transfer learning to
further increase the performance.
A PyTorch implementation of
the weighted Haus-
dorff distance
loss
and trained models
are
avail-
able
at

https://github.com/javiribera/
locating-objects-without-bboxes.

7. Conclusion

We have presented a loss function for the task of locating
objects in images that does not need bounding boxes. This
loss function is a modiﬁcation of the average Hausdorff dis-
tance (AHD), which measures the similarity between two

Method

Precision Recall AHD

Swirski [53]
77 %
ExCuSe [13]
77 %
Faster-RCNN 99.5 %
Ours
99.5 %

77 %
-
77 %
-
99.5 % 2.7 px
99.5 % 2.5 px

Table 4. Plant location results using the plant dataset, using r = 5.

Metric

Faster-RCNN Ours

Precision
Recall
F-score
AHD
MAE
RMSE
MAPE

86.6 %
78.3 %
82.2 %
9.0 px
9.4
13.4
17.7 %

88.1 %
89.2 %
88.6 %
7.1 px
1.9
2.7
4.2 %

unordered sets of points. To make the AHD differentiable
with respect to the network output, we have considered the
certainty of the network when estimating an object location.
The output of the network is a saliency map of object loca-
tions and the estimated number of objects. Our method is
not restricted to a maximum number of objects in the im-
age, does not require bounding boxes, and does not use re-
gion proposals or sliding windows. This approach can be
used in tasks where bounding boxes are not available, or the
small size of objects makes the labeling of bounding boxes
impractical. We have evaluated our approach with three dif-
ferent datasets, and outperform generic object detectors and
task-speciﬁc techniques. Future work will include develop-
ing a multi-class object location estimator in a single net-
work, and evaluating more modern CNN architectures.

Acknowledgements: This work was funded by the Advanced
Research Projects Agency-Energy (ARPA-E), U.S. Department of
Energy, under Award Number DE-AR0000593. The views and
opinions of the authors expressed herein do not necessarily reﬂect
those of the U.S. Government or any agency thereof. We thank
Professor Ayman Habib for the orthophotos used in this paper.
Contact information: Edward J. Delp, ace@ecn.purdue.edu

6486

References

[1] S. Aich,
I. Ahmed,
I. Obsyannikov,
I. Stavness,
A. Josuttes, K. Strueby, H. Duddu, C. Pozniak, and
S. Shirtliffe. Deepwheat: Estimating phenotypic traits
from crop images with deep learning. Proceedings of
the IEEE Winter Conference on Applications of Com-
puter Vision, March 2018. Stateline, NV.

[2] M. Andriluka, S. Roth, and B. Schiele.
People-
tracking-by-detection
and
people-detection-by-
tracking. Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, June 2008.
Anchorage, AK.

[3] J. L. Araus and J. E. Cairns. Field high-throughput
phenotyping: the new crop breeding frontier. Trends
in Plant Science, 19(1):52–61, January 2014.

[4] E. M. Arkin, L. P. Chew, D. P. Huttenlocher, K. Ke-
dem, and J. S. Mitchell. An efﬁciently computable
metric for comparing polygonal shapes. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
13(3), March 1991.

[5] H. Attouch, R. Lucchetti, and R. J. B. Wets. The topol-
ogy of the ρ-Hausdorff distance. Annali di Matem-
atica Pura ed Applicata, 160(1):303–320, December
1991.

[6] S. Bell, P. Upchurch, N. Snavely, and K. Bala. Mate-
rial recognition in the wild with the materials in con-
text database (supplemental material). Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, June 2015. Boston, MA.

[7] M. D. Breitenstein, F. Reichlin, B. Leibe, E. Koller-
Meier, and L. V. Gool. Online multiperson tracking-
by-detection from a single, uncalibrated camera. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 33(9):1820–1833, 2011.

[8] B. S. Chauhan and D. E. Johnson. Row spacing and
weed control timing affect yield of aerobic rice. Field
Crops Research, 121(2):226–231, March 2001.

[9] D. Comaniciu and P. Meer. Mean shift: A robust ap-
proach toward feature space analysis. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
24(5):603–619, 2002.

[10] M.-P. Dubuisson and A. K. Jain. A modiﬁed Haus-
dorff distance for object matching. Pattern Recogni-
tion, pages 566–568, October 1994.

[11] H. Fan, H. Su, and L. Guibas. A point set generation
network for 3D object reconstruction from a single im-
age. Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 2463–
2471, July 2017. Honolulu, HI.

[12] D. E. Farnham. Row spacing, plant density, and hybrid
effects on corn grain yield and moisture. Agronomy
Journal, 93:1049–1053, September 2001.

[13] W. Fuhl, T. K ¨ubler, K. Sippel, W. Rosenstiel, and
E. Kasneci. ExCuSe: Robust pupil detection in real-
world scenarios. Proceedings of the International
Conference on Computer Analysis of Images and Pat-
terns, pages 39–51, September 2015. Valletta, Malta.

[14] W. Fuhl, T. Santini, C. Reichert, D. Claus, A. Herkom-
mer, H. Bahmani, K. Rifai, S. Wahl, and E. Kasneci.
Non-intrusive practitioner pupil detection for unmod-
iﬁed microscope oculars. Computers in Biology and
Medicine, 79:36–44, December 2016.

[15] R. Girshick. Fast R-CNN. Proceedings of the IEEE
International Conference on Computer Vision, pages
1440–1448, December 2015.

[16] I. Goodfellow, Y. Bengio, and A. Courville. Deep
Learning. MIT Press, November 2016.

[17] J. Gu, X. Yang, S. De Mello, and J. Kautz. Dynamic
facial analysis: From bayesian ﬁltering to recurrent
neural network. Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
1548–1557, July 2017. Honolulu, HI.

[18] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick. Mask
R-CNN. arXiv:1703.06870, April 2017.

[19] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadar-
rama, and K. Murphy. Speed/accuracy trade-offs for
modern convolutional object detectors. Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, July 2017. Honolulu, HI.

[20] S. Huang, X. Li, Z. Zhang, F. Wu, S. Gao, R. Ji, and
J. Han. Body structure aware deep crowd counting.
IEEE Transactions on Image Processing, 27(3):1049–
1059, March 2018.

[21] P. J. Huber. Robust estimation of a location parameter.
The Annals of Mathematical Statistics, pages 73–101,
1964.

[22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-
to-image translation with conditional adversarial net-
works. Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, July 2017. Hon-
olulu, HI.

[23] K. L. K. Lin and W. Siu. Spatially eigen-weighted
Hausdorff distances for human face recognition. Pat-
tern Recognition, 36(8):1827–1834, August 2003.

[24] H. E. Khiyari and H. Wechsler.
Age invariant
face recognition using convolutional neural networks
and set distances. Journal of Information Security,
8(3):174–185, July 2017.

6487

[25] D. P. Kingma and J. Ba. Adam: A method for
stochastic optimization. Proceedings of the Inter-
national Conference for Learning Representations,
abs/1412.6980, April 2015. San Diego, CA.

[26] C. S. Kubrusly. Banach spaces Lp . In Essentials of
Measure Theory, page 83. Springer, Cham, 2005.

[27] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning.
Nature, 521:436–444, May 2015.

[28] V. Lempitsky and A. Zisserman. Learning to count
objects in images. Proceedings of the Advances in
Neural Information Processing Systems, pages 1324–
1332, December 2010. Vancouver, Canada.

[29] Y. Li, X. Zhang, and D. Chen. CSRNet: Dilated
convolutional neural networks for understanding the
highly congested scenes. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 1091–1100, June 2018.

[30] S. Liao, Y. Gao, A. Oto, and D. Shen. Representation
learning: A uniﬁed deep learning framework for au-
tomatic prostate mr segmentation. Proceedings of the
Medical Image Computing and Computer-Assisted In-
tervention, pages 254–261, September 2013. Nagoya,
Japan.

[31] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed,
C. Fu, and A. C. Berg. SSD: Single shot multibox
detector. Proceedings of the European Conference on
Computer Vision, pages 21–37, October 2016. Ams-
terdam, The Netherlands.

[32] C. C. Loy, K. Chen, S. Gong, and T. Xiang. Crowd
counting and proﬁling: Methodology and evalua-
tion. In Modeling, Simulation and Visual Analysis of
Crowds, pages 347–382. Springer, October 2013.

[33] Y. Lu, C. L. Tan, W. Huang, and L. Fan. An approach
to word image matching based on weighted Hausdorff
distance. Proceedings of International Conference on
Document Analysis and Recognition, pages 921–925,
September 2001.

[34] T. K. Moon. The expectation-maximization algo-
rithm. IEEE Signal Processing Magazine, 13(6):47–
60, November 1996.

[35] E. H. Neilson, A. M. Edwards, C. K. Blomstedt,
B. Berger, B. L. Mller, and R. M. Gleadow. Utilization
of a high-throughput shoot imaging system to examine
the dynamic phenotypic responses of a C4 cereal crop
plant to nitrogen and water deﬁciency over time. Jour-
nal of Experimental Botany, 66(7):1817–1832, 2015.

[37] D. P. Papadopoulos, J. R. R. Uijlings, F. Keller, and
V. Ferrari. We don’t need no bounding-boxes: Train-
ing object class detectors using only human veriﬁca-
tion. Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 854–863,
June 2016. Las Vegas, NV.

[38] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. Scikit-learn: Machine learning in Python. Jour-
nal of Machine Learning Research, 12:2825–2830,
2011.

[39] S. J. Reddi, S. Kale, and S. Kumar. On the conver-
gence of adam and beyond. Proceedings of the In-
ternational Conference on Learning Representations,
April 2018. Vancouver, Canada.

[40] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi.
You only look once: Uniﬁed, real-time object detec-
tion. Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages
779–788, June 2016. Las Vegas, NV.

[41] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN:
Towards real-time object detection with region pro-
posal networks. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 36(6):1137–1149, June
2017.

[42] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Con-
volutional networks for biomedical image segmenta-
tion. Proceedings of the International Conference on
Medical Image Computing and Computer-Assisted In-
tervention, pages 234–241, October 2015. Munich,
Germany.

[43] O. Russakovsky,
J. Deng, H. Su,
J. Krause,
S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet
large scale visual recognition challenge. International
Journal of Computer Vision, 11(3):211–252, Decem-
ber 2015.

[44] D. B. Sam, S. Surya, and R. V. Babu. Switching con-
volutional neural network for crowd counting. Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 4031–4039, July
2017.

[45] C. Schr ¨oder. A hybrid parameter estimation algo-
rithm for beta mixtures and applications to methyla-
tion state classiﬁcation. Algorithms for Molecular Bi-
ology, 12(21):62–66, August 2017.

[36] N. Otsu. A threshold selection method from gray-level
histograms. IEEE Transactions on Systems, Man, and
Cybernetics, 9(1):62–66, January 1979.

[46] O. Schutze, X. Esquivel, A. Lara, and C. A. C. Coello.
Using the averaged Hausdorff distance as a perfor-
mance measure in evolutionary multiobjective opti-

6488

[59] J. R. R. Uijlings, , K. E. A. Van De Sande, T. Gev-
ers, and A. W. M. Smeulders. Selective search for ob-
ject recognition.
International Journal of Computer
Vision, 104(2):154–171, September 2013.

[60] F. Xiong, X. Shi, and D. Yeung. Spatiotemporal mod-
eling for crowd counting in videos. Proceedings of the
IEEE International Conference on Computer Vision,
pages 5151–5159, October 2017. Venice, Italy.

[61] C. Zhang, H. Li, X. Wang, and X. Yang. Cross-
scene crowd counting via deep convolutional neural
networks. Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 833–
841, June 2015. Boston, MA.

[62] Y. Zhang, D. Zhou, S. Chen, S. Gao, and Y. Ma.
Single-image crowd counting via multi-column con-
volutional neural network. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 589–597, June 2016. Las Vegas, NV.

[63] S. K. Zhou, H. Greenspan, and D. Shen. Deep Learn-
ing for Medical Image Analysis. Academic Press,
London, United Kingdom, 2017.

mization. IEEE Transactions on Evolutionary Com-
putation, 16(4):504–522, August 2012.

[47] J. Shao, D. Wang, X. Xue, and Z. Zhang. Learning to
point and count. arXiv:1512.02326, December 2015.

[48] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind,
W. Wang, and R. Webb. Learning from simulated
and unsupervised images through adversarial training.
Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, pages 2107–2116, July
2017. Honolulu, HI.

[49] K. Simonyan and A. Zisserman. Very deep con-
volutional networks for large-scale image recogni-
tion. Proceedings of the International Conference on
Learning Representations, May 2015. San Diego, CA.

[50] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing anno-
tations for visual object detection. Proceedings of the
Association for the Advancement of Artiﬁcial Intelli-
gence Human Computation Workshop, WS-12-08:40–
46, July 2012. Toronto, Canada.

[51] R. Sui, B. E. Hartley, J. M. Gibson, C. Yang, J. A.
Thomasson, and S. W. Searcy. High-biomass sorghum
yield estimate with aerial imagery. Journal of Applied
Remote Sensing, 5(1):053523, January 2011.

[52] V. Sundstedt. Gazing at Games: An Introduction to
Eye Tracking Control, volume 5. Morgan & Claypool
Publishers, San Rafael, CA, 2012.
[53] L. ´Swirski, A. Bulling, and N. Dodgson. Robust real-
time pupil tracking in highly off-axis images. Pro-
ceedings of the Symposium on Eye Tracking Research
and Applications, pages 173–176, March 2012. Santa
Barbara, CA.

[54] A. A. Taha and A. Hanbury. Metrics for evaluating 3D
medical image segmentation: Analysis, selection, and
tool. BMC Medical Imaging, 15(1):29, August 2015.

[55] P. Teikari, M. Santos, C. Poon, and K. Hyny-
nen.
Deep learning convolutional networks for
multiphoton microscopy vasculature segmentation.
arXiv:1606.02382, June 2016.

[56] J. H. M. Thornley. Crop yield and planting density.
Annals of Botany, 52(2):257–259, August 1983.

[57] I. Tokatlidis and S. D. Koutroubas. A review of maize
hybrids’ dependence on high plant populations and its
implications for crop yield stability. Field Crops Re-
search, 88(2):103–114, August 2004.

[58] J. Tompson, R. Goroshin, A. Jain, Y. LeCun, and
C. Bregler. Efﬁcient object localization using convolu-
tional networks. Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
pages 648–656, June 2015. Boston, MA.

6489

Neural Illumination: Lighting Prediction for Indoor Environments

Shuran Song
Thomas Funkhouser
Google and Princeton University

Abstract

This paper addresses the task of estimating the light ar-
riving from all directions to a 3D point observed at a se-
lected pixel in an RGB image. This task is challenging be-
cause it requires predicting a mapping from a partial scene
observation by a camera to a complete illumination map for
a selected position, which depends on the 3D location of the
selection, the distribution of unobserved light sources, the
occlusions caused by scene geometry, etc. Previous meth-
ods attempt to learn this complex mapping directly using
a single black-box neural network, which often fails to esti-
mate high-frequency lighting details for scenes with compli-
cated 3D geometry. Instead, we propose “Neural Illumina-
tion,” a new approach that decomposes illumination predic-
tion into several simpler differentiable sub-tasks: 1) geom-
etry estimation, 2) scene completion, and 3) LDR-to-HDR
estimation. The advantage of this approach is that the sub-
tasks are relatively easy to learn and can be trained with
direct supervision, while the whole pipeline is fully differ-
entiable and can be ﬁne-tuned with end-to-end supervision.
Experiments show that our approach performs signiﬁcantly
better quantitatively and qualitatively than prior work.

1. Introduction

The goal of this paper is to estimate the illumination ar-
riving at a location in an indoor scene based on a selected
pixel in a single RGB image. As shown in Figure 1(a), the
input is a low dynamic range RGB image and a selected
2D pixel, and the output is a high dynamic range RGB illu-
mination map encoding the incident radiance arriving from
every direction at the 3D location (“locale”) associated with
the selected pixel (Figure 1(b)). This task is important for
a range of applications in mixed reality and scene under-
standing. For example, the output illumination map can be
used to light virtual objects placed at the locale so that they
blend seamlessly into the real world imagery (Figure 8) and
can assist estimating other scene properties, such as surface
materials.
This goal is challenging because it requires a compre-
hensive understanding of the lighting environment. First,

Figure 1. Given a single LDR image and a selected 2D pixel, the
goal of Neural Illumination is to infer a panoramic HDR illu-
mination map representing the light arriving from all directions at
the locale. The illumination map is encoded as a spherical image
parameterized horizontally by φ (0-360◦ ) and vertically by θ (0-
180◦ ), where each pixel (e.g. A,B,C,D) stores the RGB intensity
of light arriving at the “locale” from the direction (φ, θ).

it requires understanding the 3D geometry of the scene in
order to map between illumination observations at one 3D
location (the camera) and another (the selected 3D locale).
Second, it requires predicting the illumination coming from
everywhere in the scene, even though only part of the scene
is observed in the input image (e.g. the unobserved win-
dow in Figure 2). Third, it requires inferring HDR illumi-
nation from LDR observations so that virtual objects can be
lit realistically. While it is possible to train a single neu-
ral network that directly models the illumination function
end-to-end (from an input LDR image to an output HDR il-
lumination map) [7], in practice optimizing a model for this
complex function is challenging, and thus previous attempts
have not been able to model high-frequency lighting details
for scenes with complicated 3D geometry.

In this paper, we propose to address these challenges by
decomposing the problem into three sub-tasks. First, to es-
timate the 3D geometric relationship between pixels in the
input image and the output illumination map, we train a net-

16918

Figure 2. Neural Illumination. In contrast to prior work (a) [7] that directly trains a single network to learn the mapping from input
images to output illumination maps, our network (b) decomposes the problem into three sub-modules: ﬁrst the network takes a single LDR
RGB image as input and estimate the 3D geometry of the observed scene. This geometry is used to warp pixels from the input image
onto a spherical projection centered around an input locale. The warped image is then fed into LDR completion network to predict color
information for the pixels in the unobserved regions. Finally, the completed image is passed through the LDR2HDR network to infer the
HDR image. The entire network is differentiable and is trained with supervision end-to-end as well as for each intermediate sub-module.

work that estimates the 3D geometry from the observed im-
age – the estimated geometry is then used to warp pixels
from the input image to a spherical projection centered at
the target locale to produce a partial LDR illumination map.
Second, to estimate out-of-view and occluded lighting, we
train a generative network that takes in the resulting par-
tial illumination map and “completes” it – i.e., estimates the
LDR illumination for all unobserved regions of the illumi-
nation map. Finally, to recover high dynamic range infor-
mation, we train another network that maps estimated LDR
colors to HDR light intensities. All these sub-modules are
differentiable. They are ﬁrst pre-trained individually with
direct supervision and then ﬁne-tuned end-to-end with the
supervision of the ﬁnal illumination estimation.

Our key idea is that by decomposing the problem into
sub-tasks, it becomes practical to train an end-to-end neu-
ral network – each sub-module is able to focus on a rela-
tively easier task and can be trained with direct supervision.
The ﬁrst sub-task is of particular importance – by predict-
ing the 3D structure of the scene from the input image and
using it to geometrically warp the input image such that it
is spatially aligned with the output illumination map, we
are able to enforce pixel-to-pixel spatial correspondence be-
tween the input and output representations, which has pre-
viously been shown to be crucial for other dense prediction
tasks, such as image segmentation and edge detection.

To train and evaluate networks for this task, we have cu-
rated a benchmark dataset of paired input LDR images and
output HDR illumination maps for a diverse set of locales
in real-world scenes. In contrast to prior work, our dataset
leverages panoramas captured densely in real-world scenes
with HDR color and depth cameras [2]. We use the depth
channel to warp and resample those panoramas at arbitrary
locales to produce a set of 90,280 “ground truth” illumina-
tion maps observed in 129,600 images.

The primary contribution of our paper is introducing an
end-to-end neural network architecture for illumination es-
timation (Neural Illumination) that decomposes the illumi-
nation estimation task into three sub-tasks. Our problem
decomposition enables us 1) to provide both direct interme-
diate and end-to-end supervision, and 2) to convert the input
observation into an intermediate representation that shares
the pixel-wise spatial correspondence with the output rep-
resentation. We show that this combination of neural net-
work sub-modules leads to signiﬁcantly better quantitative
and qualitative results over prior work in experiments with
our new benchmark dataset.

2. Related Work

Illumination estimation has been a long-standing prob-
lem in both computer vision and graphics. In this section,
we brieﬂy review work most relevant to this paper.

Capture-based Methods A direct way of obtaining the
illumination of an environment is to capture the light inten-
sity at a target location using a physical probe. Debevec et
al. [3] ﬁrst showed that photographs of a mirrored sphere
with different exposures can be used to compute the illu-
mination at the sphere’s location. Subsequent works show
that beyond mirrored spheres, it is also possible to capture
illumination using hybrid spheres [4], known 3D objects
[24], object’s with know surface material [8], or even hu-
man faces [1] as proxies for light probes.
However,
the process of physically capturing high-
quality illumination maps can be expensive and difﬁcult to
scale, especially when the goal is to obtain training data for
a dense set of visible locations in a large variety of envi-
ronments. In this paper, we propose to use existing large-
scale datasets with RGB-D and HDR panoramas (Matter-
port3D [2]) combined with image-based rendering methods

6919

as an end-to-end neural network. However, since the in-
put and output representation of their network architecture
does not share any immediate notion of pixel-to-pixel spa-
tial correspondence, their model tends to generate illumi-
nation maps that reﬂect general color statistics of the train-
ing dataset, as opposed to important high-frequency light-
ing details. In contrast, our model predicts the 3D geomet-
ric structure of the scene and uses it to warp the observed
image into an intermediate representation that encodes the
input information in a way that is spatially aligned to the
output illumination map. This results in the ability to fully
utilize input information and preserve high-frequency light-
ing details. Moreover, Gardner et al.’s algorithm does not
generate illumination conditioned on a selected target pixel
(i.e., it produces only one solution for each input image). In
contrast, our algorithm is able to recover the spatially vary-
ing illumination for any selected pixel (Figure 3).
Apart from differences in network design, Gardner et al.
also suffers from the lack of accurate ground truth train-
ing data. Since their training data does not have depth,
they use a sphere to approximate the scene geometry to
warp a panorama to the target location. Moreover, since
most of their training data is LDR, they use a binary light
mask to approximate the bright HDR illumination during
pre-training. While reasonable in the absence of 3D geo-
metric and HDR information, these methods serve as weak
approximations of ground truth. We address both of these
data issues by directly training our model on a dataset that
has both accurate 3D geometry and illumination informa-
tion for a dense set of observations.

3. Problem formulation

We formulate illumination estimation as a pixel-wise re-
gression problem modeled by a function f : f (I |ℓ) = Hℓ
where I is an input LDR image of a scene, p is a selected
pixel in the image. ℓ is the 3D location of the pixel, and
Hℓ is the output HDR illumination around ℓ. Hℓ is repre-
sented as a spherical panoramic image with a 180◦ vertical
FoV and 360◦ horizontal FoV. Each pixel h(φ, θ) ∈ Hℓ of
the panorama encodes the RGB intensity of incoming light
to ℓ from the direction (φ, θ). We model f as a feedforward
convolutional neural network, the details of the network are
described in Sec. 5. We train f on a large dataset of {I ,ℓ}
and H ∗
ℓ pairs generated from Matterport3D (Sec. 4).

4. Generating a Dataset of Illumination Maps

Obtaining a large dataset of ground truth illumination
maps for training is challenging. On the one hand, using
physical probes to directly capture illumination at a target
locale [3, 20, 4] provides accurate data, but scaling this cap-
turing process across a diverse set of environments can be
both costly and time-consuming. On the other hand, exist-

6920

Figure 3. Spatially varying illumination. By using the 3D ge-
ometry, we can generate ground truth illumination for any target
locale. As a result, our model is also able to infer spatially varying
illumination conditioned on the target pixel location.

to generate a large training set of high-resolution illumina-
tion maps in diverse lighting environments.

Optimization-based Methods One standard approach to
estimating illumination is to jointly optimize the geome-
try, reﬂectance properties, and lighting models of the scene
in order to ﬁnd the set of values that best explain the ob-
served input image. However, directly optimizing all scene
parameters is often a highly under-constrained problem –
an error in one parameter estimation can easily propagate
into another. Therefore to ease the optimization process,
many prior methods either assume additional user-provided
ground truth information as input or make strong assump-
tions about the lighting models. For example, Karsch et al.
[12] uses user annotations for initial lighting and geometry
estimates. Zhang et al. [26] uses manually-annotated light-
source locations and assumes knowledge of depth informa-
tion. Lombardi and Nishino [19] propose approximating
illumination with a low-dimensional model, which subse-
quently has been shown to be sub-optimal for indoor scenes
due to object reﬂective and geometric properties [7].
There are also works that explore the idea that similar
images share similar illumination estimates. For example,
Karsch et al. [13] uses image matching to ﬁnd the most sim-
ilar image crop from a panoramic database [25] and then use
the lighting annotations on those panoramic images to pre-
dict out-of-view light sources. Khan et al. [14] directly ﬂips
observed HDR images to produce environment maps.
In
contrast, our system does not require additional user inputs
or manual annotations, and it does not make any explicit
assumptions about the scene content or its lighting models.
Instead, we enable our learning-based model to learn illu-
mination priors directly from data.

Learning-based Methods Deep learning has recently
shown promising results on a number of computer vision
tasks, including depth estimation [17, 15] and intrinsic im-
age decomposition [27, 16]. Recently Gardner et al. [7]
propose to formulate the illumination estimation function

Figure 4. Ground truth illumination map generation. We generate reconstructions of over 90 different building-scale indoor scenes
using HDR RGB-D images from the Matterport3D dataset [2]. From these reconstructions, we sample target locales (a) on supporting
surfaces (ﬂoors and ﬂat horizontal surfaces on furniture). For each locale, we use HDR and 3D geometric information from nearby RGB-D
images to generate ground truth panoramic illumination maps.

ing panoramic datasets (e.g. [25]) provide a simple way to
obtain illumination maps, but only at the camera locations
around which the panoramas were captured.

Instead, we propose to leverage the HDR RGB-D im-
ages from the Matterport3D dataset [2] in combination with
geometric warping to generate training data for arbitrary
locales. Matterport3D contains 194,400 registered HDR
RGB-D images arranged in 10,800 panoramas within 90
different building-scale indoor scenes. Since the panoramas
provide ground truth HDR illumination maps for their cen-
ter locations by direct observation, since they are acquired
densely throughout each scene (separated by 2.5m or so),
and since they have depth in addition to color, an RGB-
D image-based rendering algorithm can reconstruct the il-
lumination map at any point in the scene by warping and
compositing nearby panoramas.

The ﬁrst step of our dataset curation process is to sam-
ple a set of target locales. Ideally, the locales would cover
the range of locations at which virtual objects could be
placed in a scene. Accordingly, we densely sample loca-
tions 10cm above the surface of the input mesh and create
a new locale if a) it is supported by a horizontal surface
(nz >cos(π/8)), b) the support surface has semantic label
∈ {ﬂoor, furniture}, c) there is sufﬁcient volumetric clear-
ance to ﬁt an object with radius of 10cm, d) it is not within
50cm of any previously created locale. For each locale, we
backproject its location into every image I , check the depth
channel to discard occlusions, and form a image-locale pair,
{I ,ℓ}, for all others.

ℓ

For each locale ℓ, we construct an illumination map H ∗
using RGB-D image-based rendering. Though straightfor-
ward in principle, this process is complicated by missing
depths at bright regions of the image (light sources, win-
dows, strong specular highlights, etc.). A simple forward
projection algorithm based on observed depths would omit
these important elements of the illumination map. There-
fore, we implemented a two-step process. During the ﬁrst
step, we estimate the distance to the closest surface in every
direction d(φ, θ) by forward mapping the depth channel of
every input image I to ℓ, remembering the minimum dis-

tance in every direction, and ﬁlling holes with interpolation
where no samples were mapped. Then, we reconstruct the
illumination map for ℓ by resampling the HDR color chan-
nels of the input images via reverse mapping and blend-
ing the samples with weights proportional to 1/d4 , where
d is the distance between the camera and the locale. This
process produces illumination maps with smooth blends of
pixels from the nearest panoramas with holes ﬁlled by other
panoramas further away. Overall, we generate 90,280 lo-
cales and 360,432 {I ,ℓ} and H ∗
ℓ pairs using this process
Figure 4 (a) shows examples for one scene.
Though the illumination maps produced this way are
not always perfect (especially for highly specular surfaces),
they have several favorable properties for training on our
task. First, they are sampled from data collected by a large
number of photographers [2] (mostly for real estate appli-
cations), and thus they contain a diverse set of lighting en-
vironments that would be difﬁcult to gain access to other-
wise. Second, they provide a unique illumination map for
each 3D locale in a scene. Since multiple locales are usually
visible in every single image, the dataset supports learning
of spatial dependencies between pixel selections and illumi-
nation maps. For example, Figure 3 shows that our network
is able to infer different illumination maps for different pix-
els selections in the same input image. Third, the “ground
truth” illumination maps produced with our RGB-D warp-
ing procedure are more geometrically accurate than others
produced with spherical warping [7]. As shown in Figure 5,
our warping procedure is able to account for complex geo-
metric structures and occlusions in the scene.

5. Network Architecture

In this section, we describe the convolutional neural net-
work architecture used to model f , which consists of four
sequential modules: 1) a geometry (RGB-to-3D) estimation
module, 2) a differential warping module which warps the
input RGB observation to the target locale using the esti-
mated 3D information, 3) an out-of-view illumination esti-
mation module, and 4) an LDR-to-HDR estimation module.
Each module is pre-trained individually with its input and

6921

Figure 5. Comparison of warping methods. In our data genera-
tion process, we use 3D scene geometry to generate geometrically
accurate ground truth illumination, which accounts for complex
geometric structures and therefore more accurate than using spher-
ical warping from 2D panoramas as in [7].

output pairs derived from ground truth information. Then
all the modules are ﬁne-tuned together end-to-end. Figure
2, shows the network architecture. By decomposing the net-
work into sub-modules we allow each sub-module to focus
on a relatively easier task with direct supervision. We ﬁnd
that providing both intermediate and end-to-end supervision
is crucial for efﬁcient learning.

5.1. Geometry Estimation

The geometry estimation module takes a single RGB im-
age I as input and outputs a dense pixel-wise prediction of
the visible 3D geometry GI . Similar to Song et al. [22],
GI is represented with a “plane equation” for each pixel.
Speciﬁcally, we feed I through a two-stream fully convo-
lutional U-Net [21] to infer pixel-wise predictions of sur-
face normals and plane offsets (i.e. distance-to-origin). We
then pass both predicted outputs through a differentiable
PN-layer [22] to convert the estimated surface normals and
plane distances into a pixel-wise prediction of 3D locations.
Direct supervision is provided to the 1) surface normal pre-
dictions via a cosine loss, 2) plane offset predictions via
an ℓ1 loss, and 3) ﬁnal 3D point locations via an ℓ1 to en-
sure consistency between the surface normal and plane off-
set predictions. Training labels are automatically obtained
from the 3D data available in the Matterport3D dataset [2].
As shown in [22], this output representation provides strong
regularization for large planar surface and is therefore able
to produce higher quality predictions than directly predict-
ing raw depth values [5, 15]. At the same time, it also main-
tains the ﬂexibility of representing any surfaces – i.e., is not
limited to a ﬁxed number of planar surfaces, as in [18]).

5.2. Geometry(cid:173)aware Warping

The next module uses the estimated scene geometry GI
to map the pixels in the input image I to a panoramic im-

Figure 6. Examples of a) LDR image, b) log scaled HDR J , c)
HDR intensity H , d) diffuse convolution of HDR intensity D(H ).

age φℓ representing the unit sphere of rays arriving at ℓ. We
do this warping through a forward projection using the es-
timated scene geometry and camera pose. The unit sphere
projection that deﬁnes the panorama φℓ is oriented upright
along nℓ , which should be aligned with the gravity direc-
tion assuming that ℓ lays on a supporting surface (e.g. ﬂoors
and ﬂat horizontal surfaces on furniture). Image regions in
φℓ that do not have a projected pixel are set to -1. The re-
sulting warped input observation is a panorama image with
missing values that shares a pixel-wise spatial correspon-
dence to the output illumination map. Since this warping
module is entirely differentiable, we implement it as a sin-
gle network layer.

5.3. LDR Panorama Completion

The third module takes the mapped observed pixels of
φℓ as input and outputs a dense pixel-wise prediction of il-
lumination for the full panoramic image ψℓ including both
observed and unobserved pixels. ψℓ is represented as a 3-
channel LDR color panorama.
One of the biggest challenges for out-of-view illumina-
tion estimation comes from the multi-modal nature of the
problem – there can be multiple possible solutions of ψℓ
with illumination patterns that result in similar observa-
tions. Therefore, in addition to providing only pixel-wise
supervision, we train this module with adversarial loss us-
ing a discriminator network [9, 11]. This adversarial loss
provides a learnable high-level objective by learning a loss
that tries to classify if the output image is real or fake, while
simultaneously training the generative model to minimize
this loss. Our experiments show that this adversarial loss
enables the network to produce and more realistic illumina-
tion outputs with sharper and richer details.
This module is implemented as a fully convolutional
ResNet50 [10]. Since both the input and output of this mod-
ule are represented as spherical panoramic images, we uti-
lize distortion-aware convolutional ﬁlters that account for
the different spherical distortion distributions for different
regions of the image [23]. This distortion-aware convolu-
tion resamples the feature space according to the image dis-
tortion model in order to improve the translational invari-
ance of the learned ﬁlters in the network.

6922

5.4. LDR(cid:173)to(cid:173)HDR Estimation

The ﬁnal module takes the predicted LDR illumination
as input and outputs a dense pixel-wise prediction of HDR
illumination intensities representing incident radiance in ev-
ery direction at ℓ. This prediction is important because LDR
images may have intensity clipping and/or tone-mapping,
which would not be suitable for lighting virtual objects.
Like Eilertsen et al. [6], we formulate the LDR-to-HDR
estimation as a pixel-wise regression problem, but instead
of predicting the HDR value for only bright pixels and us-
ing a ﬁxed function to map the rest of the pixels, our LDR-
to-HDR module learns the mapping function for all pixels
from the LDR space to the HDR space. The module is
trained with supervision from: 1) a pixel-wise ℓ2 loss Lℓ2 ,
and 2) a diffuse convolutional loss Ld .
The pixel-wise ℓ2 loss measures the visual error when
re-lighting a perfectly specular surface at ℓ:

Lℓ2 =

1

N

N

Xi=1

(J (i) − J ∗ (i))

where the J is log-scaled image of the ﬁnal light intensity
H , deﬁned as:

H (i) = (J (i) ∗ 65536 ∗ 8e−8 ,

2.4e−4 ∗ 1.0002(J (i)∗65536−3000) , J (i) > 3000

J (i) ≤ 3000

The diffuse convolutional loss measures the visual error
when re-lighting a perfectly diffuse surface:

Ld =

1

N

N

Xi=1

(D(H (i)) − D(H ∗ (i)))

where D is the diffuse convolution function deﬁned as:

D(H, i) =

H (ω)s(ω)(ω · ~ni )

1

Ki Xω∈Ωi

and Ωi is the hemisphere centered at pixel i on the illumi-
nation map, ~ni the unit normal at pixel i, and Ki the sum
of solid angles on Ωi . ω is a unit vector of direction on
ωi and s(ω) the solid angle for the pixel in the direction
ω . This loss function is similar to the “cosine loss” func-
tion proposed by Gardner et al. [7], but rather than progres-
sively increasing the Phong exponent value during training,
we keep the Phong exponent value equal to 1. In our imple-
mentation, we reduce the memory usage by computing Ld
on a downsized illumination map with average pooling.
The ﬁnal loss is computed as L = λ1Lℓ2 + λ2Ld ,
where λ1 = 0.1 and λ2 = 0.05. By combining these
two losses during training, we encourage our model to re-
produce both low and high frequency illumination signals.
Figure 6 shows examples of HDR images and their diffuse
convolution.

6. Evaluation

We train and test our algorithm on the data generated
from Section 4, using the train/test split provided by the

Matterport3D dataset [2]. The following experiments in-
vestigate qualitative and quantitative comparisons to prior
work and results of ablation studies. More results and visu-
alizations can be found in the supplementary material.

Evaluation metrics. We use the following evaluation
metrics to quantitatively evaluate our predicted illumination
maps Hℓ :
· Pixel-wise ℓ2 distance error is the sum of all pixel-
wise ℓ2 distances between the predicted Hℓ and ground
truth H ∗
ℓ2(log) computes the
ℓ illumination maps.
ℓ2 distance in the log intensity. Intuitively, this error
measures the approximate visual differences observed
when the maps are used to render a perfectly specular
surface at the target locale.
· Pixel-wise diffuse convolution error is the sum of all
pixel-wise ℓ2 distances between D(Hℓ ) and D(H ∗
ℓ ).
This error measures the approximate visual differences
observed when the maps are used to render a perfectly
diffuse surface at the target locale.

Comparisons to state-of-the-art. Table 1 shows quanti-
tative comparisons of our approach to two alternative base-
lines: 1) Gardner et al. [7], and 2) a nearest neighbour re-
trieval method. Gardner et al. estimates the illumination
condition of a given input image by training a single con-
volutional neural network with end-to-end supervision. We
rotated each of the predicted panorama along the x-axis to
align with ground truth coordinate frame before evaluation.
Row 1 of Table 1 shows the performance of Gardner et
al.’s model trained on their original LRD+HDR panorama
dataset and tested on our test set. We also re-implement an
image-to-image prediction network that is similar to Gard-
ner et al.’s model and train it directly on our training data
(LDR and full HDR illumination pairs) to remove poten-
tial dataset biases. This model [Im2Im network] achieves
better performance than the original model but is still less
accurate than ours. With a qualitative comparison (Figure
7,8), we can observe that by estimating and utilizing the
3D scene geometry, our algorithm is able to produce output
illumination maps that contain much richer and more realis-
tic high frequency details. Moreover, Gardner et al.’s algo-
rithm does not allow users to input a speciﬁc target pixel –
i.e., they generate only one lighting solution for each input
image. In contrast, our algorithm is able to recover the spa-
tially varying lighting distribution for any selected locale in
the image, which can be quite different from one another
(Figure 3).

Modularization v.s. additional supervision. While we
show that our network is able to achieve better performance
than the single end-to-end model, it is still unclear whether
the performance gain comes from the additional supervision

6923

Figure 7. Qualitative Results (Row 1) show the input image and selected locale. (Row 2,3) show the warped observation using ground
truth an predicted geometry. (Row 4,5) show the completed LDR. (Row 6-10) show the ﬁnal HDR illumination visualized with gamma
correction (γ =3.3). We can observe that the illumination estimation from our approach is more accurate and also contain richer high
frequency details.

Method
Gardner et al. [7]
Im2Im network
Nearest Neighbour
Ours

ℓ2(log)

ℓ2

0.375
0.229
0.296
0.202

0.977
0.369
0.647
0.280

diffuse
1.706
0.927
1.679
0.772

Table 1. Comparing the quantitative performance of our method to
that of Gardner et al. [7] and a nearest neighbour retrieval method.

or the network modularization. To investigate this ques-
tion, we trained an end-to-end network that takes in a single
LDR image as input and directly outputs the completed 3D
geometry, LDR images, and HDR images at the ﬁnal lay-
ers. This network is trained with supervision for all three
predictions but does not have any network decomposition.
Table 2 shows the results. The performance gap between
this network and ours demonstrates that naively adding all
of the available supervision at the end of the network with-
out proper network modularization and intermediate super-
vision does not work as well as our approach and generates

signiﬁcantly lower-quality illumination maps.

without
with (ours)

ℓ2(log)

ℓ2

0.213
0.202

0.319
0.280

diffuse
0.856
0.772

Table 2. Effects of modularization.

Comparisons to variants with oracles. To study how er-
rors in intermediate predictions impacts our results, we ex-
ecute a series of experiments where some data is provided
by oracles rather than our predictions. In the ﬁrst experi-
ment, we trained a network that takes as input a LDR im-
age already warped by a depth oracle and omits the ﬁrst
two modules (LDR+D→HDR). In a second experiment,
we trained a version of that network that instead inputs a
warped HDR image and omits execution of the last mod-
ule. These networks utilize ground truth data, and thus are
not fair comparisons. Yet, they provide valuable informa-
tion about how well our network possibly could perform
and which modules contribute most to the error. Looking

6924

Figure 8. Object relighting example. Here we show qualitative comparisons of relighting results rendered by Mitsuba using the illumi-
nation maps from (a) ground truth, (b) our algorithm, and (c) Gardner et al. We show images rendered with two different surface materials
composited over the original observations (the ﬁrst and second rows) and the illumination maps (third row). Compared to (c), our algorithm
is able to produce output illumination maps that contain much richer and more realistic high frequency detail. Of course, it also makes
mistakes, for example by predicting extra light sources on the ceiling, which is incorrect but still plausible given the observation.

at Table 3, we see that providing ground truth depth im-
proves our algorithm marginally (e.g., ∆ℓ2 = 0.011), while
also providing ground truth HDR improves it more (e.g.,
∆ℓ2 = 0.043). We conjecture it is because errors are con-
centrated on bright light sources. Overall, the performance
of our algorithm is about halfway between the best oracled
version [HDR+D→HDR] and the baselines in Table 1.

Method
LDR→HDR
LDR+D→HDR
HDR+D→HDR

ℓ2(log)

ℓ2

0.202
0.188
0.131

0.280
0.269
0.212

diffuse
0.772
0.761
0.619

Table 3. Comparisons to variants with oracles.

Effects of different losses. To study the effects of dif-
ferent loss functions, we evaluate the performance of the
model “HDR+D→HDR” using different combinations of
loss functions. Figure 4 shows quantitative and qualitative
results. From the results we can observe that with only an
ℓ2 loss, the network tends to produce very blurry estima-
tions that are close to the mean intensity of the input im-
ages. By adding the adversarial loss, the network starts to
be able to infer more realistic high frequency signals and
spotlights, but also introduces additional noises and errors
in the prediction. By further adding a diffuse convolution
loss [l2+gan+df], the network is able to predict overall more
accurate illumination especially for the high intensity areas.

loss
l2
l2+gan
l2+gan+df

ℓ2(log)

ℓ2

0.116
0.224
0.131

0.235
0.275
0.212

diffuse
0.691
0.713
0.619

Table 4. Effects of different losses.

work module for: 1) inferring 3D scene geometry, 2) warp-
ing observations to illumination maps, 3) estimating unob-
served illumination, and 4) mapping LDR to HDR. Experi-
ments show that we can train a network with this decompo-
sition that predicts illumination maps with better details and
accuracy than alternative methods. While “Neural Illumina-
tion” is able to improve the accuracy of existing methods,
it is still far from perfect. In particular, it often produces
plausible illumination maps rather than accurate ones when
no lights are observed directly in the input. Possible direc-
tions for future work include explicit modeling of surface
material and reﬂective properties and exploring alternative
3D geometric representations that facilitate out-of-view il-
lumination estimation through whole scene understanding.

Conclusion and Future Work This paper presents “Neu-
ral Illumination,” an end-to-end framework for estimating
high dynamic range illumination maps for a selected pixel
in a low dynamic range image of an indoor scene. We pro-
pose to decompose the task into subtasks and train a net-

Acknowledgments We thank Paul Debevec, John Flynn,
Chloe LeGendre, and Wan-Chun Alex Ma for their valu-
able advice, Marc-Andr Gardner for producing results
for comparison, Matterport for their dataset, and NSF
1539014/1539099 for funding.

6925

residual networks. In 3D Vision (3DV), 2016 Fourth Interna-
tional Conference on, pages 239–248. IEEE, 2016.
[16] Z. Li and N. Snavely. Cgintrinsics: Better intrinsic image
decomposition through physically-based rendering. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 371–387, 2018.
[17] F. Liu, C. Shen, and G. Lin. Deep convolutional neural ﬁelds
for depth estimation from a single image.
In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5162–5170, 2015.
[18] M. Liu, X. He, and M. Salzmann. Geometry-aware deep net-
work for single-image novel view synthesis. arXiv preprint
arXiv:1804.06008, 2018.
[19] S. Lombardi and K. Nishino. Reﬂectance and illumination
recovery in the wild. IEEE transactions on pattern analysis
and machine intelligence, 38(1):129–141, 2016.
[20] E. Reinhard, W. Heidrich, P. Debevec, S. Pattanaik, G. Ward,
and K. Myszkowski. High dynamic range imaging: acquisi-
tion, display, and image-based lighting. Morgan Kaufmann,
2010.
[21] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation.
In
International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer,
2015.
[22] S. Song, A. Zeng, A. X. Chang, M. Savva, S. Savarese, and
T. Funkhouser. Im2pano3d: Extrapolating 360 structure and
semantics beyond the ﬁeld of view. Proceedings of 31th
IEEE Conference on Computer Vision and Pattern Recog-
nition, 2018.
[23] K. Tateno, N. Navab, and F. Tombari. Distortion-aware con-
volutional ﬁlters for dense prediction in panoramic images.
In Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 707–722, 2018.
[24] H. Weber, D. Pr ´evost, and J.-F. Lalonde. Learning to es-
timate indoor lighting from 3d objects.
In 2018 Inter-
national Conference on 3D Vision (3DV), pages 199–207.
IEEE, 2018.
[25] J. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba. Recogniz-
ing scene viewpoint using panoramic place representation.
In Computer Vision and Pattern Recognition (CVPR), 2012
IEEE Conference on, pages 2695–2702. IEEE, 2012.
[26] E. Zhang, M. F. Cohen, and B. Curless. Emptying, refur-
nishing, and relighting indoor spaces. ACM Transactions on
Graphics (TOG), 35(6):174, 2016.
[27] T. Zhou, P. Krahenbuhl, and A. A. Efros. Learning data-
driven reﬂectance priors for intrinsic image decomposition.
In Proceedings of the IEEE International Conference on
Computer Vision, pages 3469–3477, 2015.

References

[1] D. A. Calian,
J.-F. Lalonde, P. Gotardo, T. Simon,
I. Matthews, and K. Mitchell. From faces to outdoor light
probes.
In Computer Graphics Forum, volume 37, pages
51–61. Wiley Online Library, 2018.
[2] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nießner,
M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d:
Learning from rgb-d data in indoor environments. 3DV,
2017.
[3] P. Debevec. Rendering synthetic objects into real scenes:
Bridging traditional and image-based graphics with global
illumination and high dynamic range photography. In Pro-
ceedings of the 25th annual conference on Computer graph-
ics and interactive techniques, pages 189–198. ACM, 1998.
[4] P. Debevec, P. Graham, J. Busch, and M. Bolas. A single-
shot light probe. In ACM SIGGRAPH 2012 Talks, page 10.
ACM, 2012.
[5] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in neural information processing systems, pages
2366–2374, 2014.
[6] G. Eilertsen, J. Kronander, G. Denes, R. K. Mantiuk, and
J. Unger. Hdr image reconstruction from a single exposure
using deep cnns. ACM Transactions on Graphics (TOG),
36(6):178, 2017.
[7] M.-A. Gardner, K. Sunkavalli, E. Yumer, X. Shen, E. Gam-
baretto, C. Gagn ´e, and J.-F. Lalonde. Learning to predict
indoor illumination from a single image. ACM Transactions
on Graphics (TOG), 36(6):176, 2017.
[8] S. Georgoulis, K. Rematas, T. Ritschel, M. Fritz, T. Tuyte-
laars, and L. Van Gool. What is around the camera? In The
IEEE International Conference on Computer Vision (ICCV),
Oct 2017.
[9] I. Goodfellow,
J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in neural information
processing systems, pages 2672–2680, 2014.
[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.
[11] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
Image-to-
image translation with conditional adversarial networks. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 1125–1134, 2017.
[12] K. Karsch, V. Hedau, D. Forsyth, and D. Hoiem. Rendering
synthetic objects into legacy photographs. ACM Transac-
tions on Graphics (TOG), 30(6):157, 2011.
[13] K. Karsch, K. Sunkavalli, S. Hadap, N. Carr, H. Jin, R. Fonte,
M. Sittig, and D. Forsyth. Automatic scene inference for 3d
object compositing. ACM Transactions on Graphics (TOG),
33(3):32, 2014.
[14] E. A. Khan, E. Reinhard, R. W. Fleming, and H. H. B ¨ulthoff.
Image-based material editing.
In ACM Transactions on
Graphics (TOG), volume 25, pages 654–663. ACM, 2006.
[15] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and
N. Navab. Deeper depth prediction with fully convolutional

6926

Neural Rerendering in the Wild

Moustafa Meshry∗ 1 , Dan B Goldman2 , Sameh Khamis2 , Hugues Hoppe2 , Rohit Pandey2 ,
Noah Snavely2 , Ricardo Martin-Brualla2

1University of Maryland, 2Google Inc.

Abstract

We explore total scene capture — recording, modeling,
and rerendering a scene under varying appearance such
as season and time of day. Starting from internet photos
of a tourist landmark, we apply traditional 3D reconstruc-
tion to register the photos and approximate the scene as a
point cloud. For each photo, we render the scene points
into a deep framebuffer, and train a neural network to learn
the mapping of these initial renderings to the actual pho-
tos. This rerendering network also takes as input a latent
appearance vector and a semantic mask indicating the lo-
cation of transient objects like pedestrians. The model is
evaluated on several datasets of publicly available images
spanning a broad range of illumination conditions. We cre-
ate short videos demonstrating realistic manipulation of the
image viewpoint, appearance, and semantic labeling. We
also compare results with prior work on scene reconstruc-
tion from internet photos.

1. Introduction

Imagine spending a day sightseeing in Rome in a fully
realistic interactive experience without ever stepping on a
plane. One could visit the Pantheon in the morning, enjoy
the sunset overlooking the Colosseum, and ﬁght through the
crowds to admire the Trevi Fountain at night time. Realiz-
ing this goal involves capturing the complete appearance
space of a scene, i.e., recording a scene under all possible
lighting conditions and transient states in which the scene
might be observed—be it crowded, rainy, snowy, sunrise,
spotlit, etc.—and then being able to summon up any view-
point of the scene under any such condition. We call this
ambitious vision total scene capture. It is extremely chal-
lenging due to the sheer diversity of appearance—scenes
can look dramatically different under night illumination,
during special events, or in extreme weather.

∗Work performed during an internship at Google.

(a) Input deep buffer

(b) Output renderings

Figure 1: Our neural rerendering technique uses a large-scale in-
ternet photo collection to reconstruct a proxy 3D model and trains
a neural rerendering network that takes as input a deferred-shading
deep buffer (consisting of depth, color and semantic labeling) gen-
erated from the proxy 3D model (left), and outputs realistic ren-
derings of the scene under multiple appearances (right).

In this paper, we focus on capturing tourist landmarks
around the world using publicly available community pho-
tos as the sole input, i.e., photos in the wild. Recent ad-
vances in 3D reconstruction can generate impressive 3D
models from such photo collections [1, 39, 41], but the ren-
derings produced from the resulting point clouds or meshes
lack the realism and diversity of real-world images. Alter-
natively, one could use webcam footage to record a scene
at regular intervals but without viewpoint diversity, or use
specialized acquisition (e.g., Google Street View, aerial, or
satellite images) to snapshot the environment over a short
time window but without appearance diversity. In contrast,
community photos offer an abundant (but challenging) sam-
pling of appearances of a scene over many years.

Our approach to total scene capture has two main com-
ponents: (1) creating a factored representation of the input
images, which separates viewpoint, appearance conditions,
and transient objects such as pedestrians, and (2) rendering
realistic images from this factored representation. Unlike
recent approaches that extract implicit disentangled repre-
sentations of viewpoint and content [31, 34, 43], we employ
state-of-the-art reconstruction methods to create an explicit
intermediate 3D representation, in the form of a dense but
noisy point cloud, and use this 3D representation as a “scaf-
folding” to predict images.

16878

An explicit 3D representation lets us cast the rendering
problem as a multimodal image translation [15, 25, 53]. The
input is a deferred-shading framebuffer [35] in which each
rendered pixel stores albedo, depth, and other attributes, and
the outputs are realistic views under different appearances.
We train the model by generating paired datasets, using the
recovered viewpoint parameters of each input image to ren-
der a deep buffer of the scene from the same view, i.e., with
pixelwise alignment. Our model effectively learns to take an
approximate initial scene rendering and rerender a realistic
image. This is similar to recent neural rerendering frame-
works [20, 28, 44] but using uncontrolled internet images
rather than carefully captured footage.
We explore a novel strategy to train the multimodal im-
age translation model. Rather than jointly estimating an em-
bedding space for the appearance together with the render-
ing network [15, 25, 53], our system performs staged train-
ing of both. First, an appearance encoding network is pre-
trained using a proxy style-based loss [9], an efﬁcient way
to capture the style of an image. Then, the rerendering net-
work is trained with ﬁxed appearance embeddings from the
pretrained encoder. Finally, both the appearance encoding
and rerendering networks are jointly ﬁnetuned. This sim-
ple yet effective strategy lets us train simpler networks on
large datasets. We demonstrate experimentally how a model
trained in this fashion better captures scene appearance.
Our system is a ﬁrst step towards addressing total scene
capture and focuses primarily on the static parts of scenes.
Transient objects (e.g., pedestrians and cars) are handled by
conditioning the rerendering network on the expected se-
mantic labeling of the output image, so that the network can
learn to ignore these objects rather than trying to halluci-
nate their locations. This semantic labeling is also effective
at discarding small or thin scene features (e.g., lampposts)
whose geometry cannot be robustly reconstructed, yet are
easily identiﬁed using image segmentation methods. Con-
ditioning our network on a semantic mask also enables the
rendering of scenes free of people if desired. Code will be

available at https://bit.ly/2UzYlWj.

In summary, our contributions include:
• A ﬁrst step towards total scene capture, i.e., recording
and rerendering a scene under any appearance from in-
the-wild photo collections.
• A factorization of input images into viewpoint, appear-
ance, and semantic labeling, conditioned on an approx-
imate 3D scene proxy, from which we can rerender re-
alistic views under varying appearance.
• A more effective method to learn the appearance latent
space by pretraining the appearance embedding net-
work using a proxy loss.
• Compelling results including view and appearance in-
terpolation on ﬁve large datasets, and direct compar-
isons to previous methods [39].

2. Related work

Scene reconstruction Traditional methods for scene re-
construction ﬁrst generate a sparse reconstruction using
large-scale structure-from-motion [1], then perform Multi-
View Stereo (MVS) [7, 38] or variational optimization [13]
to reconstruct dense scene models. However, most such
techniques assume a single appearance, or else simply re-
cover an average appearance of the scene. We build upon
these techniques, using dense point clouds recovered from
MVS as proxy geometry for neural rerendering.
In image-based rendering [4, 10], input images are used
to generate new viewpoints by warping input pixels into the
outputs using proxy geometry. Recently, Hedman et al. [12]
introduce a neural network to compute blending weights for
view-dependent texture mapping that reduces artifacts in
poorly reconstructed regions. However, image-based ren-
dering generally assumes the captured scene has static ap-
pearance, so it is not well-suited to our problem setup in
which the appearance varies across images.
Neural scene rendering [6] applies deep neural networks
to learn a latent scene representation that allows generation
of novel views, but is limited to simple synthetic geometry.

Appearance modeling A given scene can have dramati-
cally different appearances at different times of day, in dif-
ferent weather conditions, and can also change over the
years. Garg et al. [8] observe that for a given viewpoint,
the dimensionality of scene appearance as captured by in-
ternet photos is relatively low, with the exception of out-
liers like transient objects. One can recover illumination
models for a photo collection by estimating albedo using
cloudy images [39], retrieving the sun’s location through
timestamps and geolocation [11], estimating coherent albe-
dos across the collection [22], or assuming a ﬁxed view-
point [42]. However, these methods assume simple light-
ing models that do not apply to nighttime scene appearance.
Radenovic et al. [33] recover independent day and night re-
constructions, but do not enable smooth appearance inter-
polations between the two.
Laffont et al. [23] assign transient attributes like “fall” or
“sunny” to each image, and learn a database of patches that
allows for editing such attributes. Other works require di-
rect supervision from lighting models estimated using 360-
degree images [14], or ground truth object geometry [48].
In contrast, we use a data-driven implicit representation of
appearance that is learned from the input image distribution
and does not require direct supervision.

Deep image synthesis The seminal work of pix2pix [16]
trains a deep neural network to translate an image from one
domain, such as a semantic labeling, into another domain,
such as a realistic image, using paired training data. Image-
to-image (I2I) translation has since been applied to many
tasks [5, 24, 32, 49, 47, 50]. Several works propose im-

6879

provements to stabilize training and allow for high-quality
image synthesis [18, 46, 47]. Others extend the I2I frame-
work to unpaired settings where images from two domains
are not in correspondence [21, 26, 52], multimodal outputs
where an input image can map to multiple images [46, 53],
or unpaired datasets with multimodal outputs where an im-
age in one domain is converted to another domain while
preserving the content [2, 15, 25].
Image translation techniques can be used to rerender
scenes in a more realistic domain, to enable facial expres-
sion synthesis [20], to ﬁx artifacts in captured 3D perfor-
mances [28], or to add viewpoint-dependent effects [44].
In our paper, we demonstrate an approach for training a
neural rerendering framework in the wild, i.e., with uncon-
trolled data instead of captures under constant lighting con-
ditions. We cast this as a multimodal image synthesis prob-
lem, where a given viewpoint can be rendered under multi-
ple appearances using a latent appearance vector, and with
editable semantics by conditioning the output on the desired
semantic labeling of the output.

3. Total scene capture

We deﬁne the problem of total scene capture as creat-
ing a generative model for all images of a given scene. We
would like such a model to:
– encode the 3D structure of the scene, enabling render-
ing from an arbitrary viewpoint,
– capture all possible appearances of the scene, e.g., all
lighting and weather conditions, and allow rendering
the scene under any of them, and
– understand the location and appearance of transient ob-
jects in the scene, e.g., pedestrians and cars, and allow
for reproducing or omitting them.
Although these goals are ambitious, we show that one can
create such a generative model given sufﬁcient images of a
scene, such as those obtained for popular tourist landmarks.
We ﬁrst describe a neural rerendering framework that we
adapt from previous work in controlled capture settings [28]
to the more challenging setting of unstructured photo col-
lections (Section 3.1). We extend this model to enable ap-
pearance capture and multimodal generation of renderings
under different appearances (Section 3.2). We further ex-
tend the model to handle transient objects in the training
data by conditioning its inputs on a semantic labeling of the
ground truth images (Section 3.3).

3.1. Neural rerendering framework

We adapt recent neural rerendering frameworks [20, 28]
to work with unstructured photo collections. Given a large
internet photo collection {Ii } of a scene, we ﬁrst generate
a proxy 3D reconstruction using COLMAP [36, 37, 38],
which applies Structure-from-Motion (SfM) and Multi-
View Stereo (MVS) to create a dense colored point cloud.

Figure 2: Output frames of a standard image translation net-
work [16] trained for neural rerendering in a small dataset of 250
photos of San Marco. The network overﬁts the dataset and learns
to hallucinate lampposts close to their approximate location in the
scene (green), and virtual tourists (yellow), as well as memorizing
a per-viewpoint appearance matching the speciﬁc input photos.

An alternative to a point cloud is to generate a textured
mesh [19, 45]. Although meshes generate more complete
renderings, they tend to also contain pieces of misregistered
ﬂoating geometry which can occlude large regions of the
scene [39]. As we show later, our neural rerendering frame-
work can produce highly realistic images given only point-
based renderings as input.
Given the proxy 3D reconstruction, we generate an
aligned dataset of rendered images and real images by ren-
dering the 3D point cloud from the viewpoint vi of each in-
put image Ii , where vi consists of camera intrinsics and ex-
trinsics recovered via SfM. We generate a deferred-shading
deep buffer Bi for each image [35], which may contain per-
pixel albedo, normal, depth and any other derivative infor-
mation. In our case, we only use albedo and depth and ren-
der the point cloud by using point splatting with a z-buffer
with a radius of 1 pixel.
However, the image-to-image translation paradigm used
in [20, 28] is not appropriate for our use case, as it assumes
a one-to-one mapping between inputs and outputs. A scene
observed from a particular viewpoint can look very different
depending on weather, lighting conditions, color balance,
post processing ﬁlters, etc. In addition, a one-to-one map-
ping fails to explain transient objects in the scene, such as
pedestrians or cars, whose location and individual appear-
ance is impossible to predict from the static scene geometry
alone. Interestingly, if one trains a sufﬁciently large neural
network on this simple task on a dataset, the network learns
to (1) associate viewpoint with appearance via memoriza-
tion and (2) hallucinate the location of transient objects, as
shown in Figure 2.

3.2. Appearance modeling

To capture the one-to-many relationship between input
viewpoints (represented by their deep buffers Bi ) and out-
put images Ii under different appearances, we cast the
rerendering task as multimodal image translation [53]. In
such a formulation, the goal is to learn a latent appearance
vector z a
that captures variations in the output domain Ii
that cannot be inferred from the input domain Bi . We com-
pute the latent appearance vector as z a
i = E a (Ii , Bi ) where

i

6880

is computed through a cross-cycle with a second image
(I2 , B2 ), i.e. ˆz a
1 = E a (R(B2 , z a
1 )), B2 ). We also apply
a GAN loss on the intermediate appearance transfer output
R(B2 , z a
1 ) as in [15, 25].

Staged appearance training The key to our staged train-
ing approach is the appearance pretraining stage, where
we pretrain the appearance encoder E a independently on
a proxy task. We then train the rendering network R while
ﬁxing the weights of E a , allowing R to ﬁnd the correlations
between output images and the embedding produced by the
proxy task. Finally, we ﬁne-tune both E a and R jointly.
This staged approach simpliﬁes and stabilizes the train-
ing of R, enabling training of a simpler network with fewer
regularization terms. In particular, we remove the cycle and
cross-cycle consistency losses, the latent vector reconstruc-
tion loss, and the KL-divergence loss, leaving only a direct
reconstruction loss and a GAN loss. We show experimen-
tally in Section 4 that this approach results in better appear-
ance capture and rerenderings than the baseline model.

Appearance pretraining To pretrain the appearance en-
coder E a , we choose a proxy task that optimizes an embed-
ding of the input images into the appearance latent space
using a suitable distance metric between input images. This
training encourages embeddings such that if two images
are close under the distance metric, then their appearance
embeddings should also be close in the appearance latent
space. Ideally the distance metric we choose should ignore
the content or viewpoint of Ii and Bi , as our goal is to en-
code a latent space that is independent of viewpoint. Ex-
perimentally we ﬁnd that the style loss employed in neural
style-transfer work [9] has such a property; it largely ig-
nores content and focuses on more abstract properties.
To train the embedding, we use a triplet loss, where for
each image Ii , we ﬁnd the set of k closest and furthest
neighbor images given by the style loss, from which we
can sample a positive sample Ip and negative sample In ,
respectively. The loss is then:
L(Ii , Ip , In ) = X
i − g j
p k2 − kg j
i − g j

n k2 + α, 0(cid:17)

max (cid:16)kg j

j

where g j
i is the Gram matrix of activations at the j th layer of
a VGG network of image Ii , and α is a separation margin.

3.3. Semantic conditioning

To account for transient objects in the scene, we condi-
tion the rerendering network on a semantic labeling Si of
image Ii that depicts the location of transient objects such
as pedestrians. Speciﬁcally, we concatenate the semantic
labeling Si to the deep buffer Bi wherever the deep buffer
was previously used. This discourages the network from
encoding variations caused by the location of transient ob-
jects in the appearance vector, or associating such transient
objects with speciﬁc viewpoints, as shown in Figure 2.

6881

Figure 3: An aligned dataset is created using Structure from Mo-
tion (SfM) and Multi-View Stereo (MVS). Our staged approach
pre-trains the appearance encoder E a using a triplet loss (left).
Then the rerenderer R is trained using standard reconstruction and
GAN losses (right), and ﬁnally ﬁne-tuned together with E a . Photo

Credits Rafael Jimenez (Creative Commons).

E a is an appearance encoder that takes as input both the out-
put image Ii and the deep buffer Bi . We argue that having
the appearance encoder E a observe the input Bi allows it to
learn more complex appearance models by correlating the
lighting in Ii with scene geometry in Bi . Finally, a reren-
dering network R generates a scene rendering conditioned
on both viewpoint Bi and the latent appearance vector z a .
Figure 3 shows an overview of the overall process.
To train the appearance encoder E a and rendering net-
work R, we ﬁrst adopted elements from recent methods in
multimodal synthesis [15, 25, 53] to ﬁnd a combination that
is most effective in our scenario. However, this combination
still has shortcomings as it is unable to model infrequent
appearances well. For instance, it does not reliably capture
night appearances for scenes in our datasets. We hypoth-
esize that the appearance encoder (which is jointly trained
with the rendering network) is not expressive enough to cap-
ture the large variability in the data.
To improve the model expressiveness, our approach is
to stabilize the joint training of R and E a by pretraining
the appearance network E a independently on a proxy task.
We then employ a staged training approach in which the
rendering network R is ﬁrst trained using ﬁxed appearance
embeddings, and ﬁnally we jointly ﬁne-tune both networks.
This staged training regime allows for a simpler model that
captures more complex appearances.
We present our baseline approach, which adapts state-
of-the-art multimodal synthesis techniques, and then our
staged training strategy, which pretrains the appearance en-
coder on a proxy task.

Baseline Our baseline uses BicycleGAN [53] with two
main adaptations. First, our appearance encoder also takes
as input the buffer Bi , as described above. Second, we
add a cross-cycle consistency loss similar to [15, 25] to en-
courage appearance transfer across viewpoints. Let z a
E a (I1 , B1 ) be the captured appearance of an input im-
age I1 . We apply a reconstruction loss between image I1
and cross-cycle reconstruction ˆI1 = R(B1 , ˆz a
1 ), where ˆz a

1 =

1

Input

Segmentation

I2I

+Sem

+Sem+BaseApp +Sem+StagedApp Ground Truth

Figure 4: Example visual results of our ablative study in Table 1. From left to right, input color render, segmentation mask from the
corresponding ground truth images, result using an image-to-image baseline (I2I), with semantic conditioning (+Sem), and with semantic
conditioning and a baseline appearance modeling based on [53] (+Sem+BaseApp), with semantic conditioning and staged appearance

training (+Sem+StagedApp). Photo Credits: Flickr users Gary Campbell-Hall, Steve Collis, and Tahbepet (Creative Commons).

A separate beneﬁt of semantic labeling is that it allows
the rerendering network to reason about static objects in the
scene not captured in the 3D reconstruction, such as lamp-
posts in San Marco Square. This prevents the network from
haphazardly introducing such objects, and instead lets them
appear where they are detected in the semantic labeling,
which is a signiﬁcantly simpler task. In addition, by adding
the segmentation labeling to the deep buffer, we allow the
appearance encoder to reason about semantic categories like
sky or ground when computing the appearance latent vector.

We compute “ground truth” semantic segmentations
on the input
images Ii using DeepLab [3] trained on
ADE20K [51]. ADE20K contains 150 classes, which we
map to a 3-channel color image. We ﬁnd that the quality of
the semantic labeling is poor on the landmarks themselves,
as they contain unique buildings and features, but is reason-
able on transient objects.

Using semantic conditioning, the rerendering network
takes as input a semantic labeling of the scene.
In order
to rerender virtual camera paths, we need to synthesize se-
mantic labelings for each frame in the virtual camera path.
To do so, we train a separate semantic labeling network that
takes as input the deep buffer Bi , instead of the output im-
age Ii , and estimates a “plausible” semantic labeling ˆSi for
that viewpoint given the rendered deep buffer Bi . For sim-
plicity, we train a network with the same architecture as the
rendering network (minus the injected appearance vector)
on samples (Bi , Si ) from the aligned dataset, and we mod-
ify the semantic labelings of the ground truth images Si and
mask out the loss on pixels labeled as transient as deﬁned
by a curated list of transient object categories in ADE20K.

4. Evaluation

Here we provide an extensive evaluation of our system.
Please also refer to the supplementary video to best appreci-
ate the quality of the results, available in the project website:

https://bit.ly/2UzYlWj.

Implementation details Our rerendering network is a
symmetric encoder-decoder with skip connections, where
the generator is adopted from [18] without using progres-
sive growing. We use a multiscale-patchGAN discrimina-
tor [46] with 3 scales and employ a LSGAN [27] loss. As
a reconstruction loss, we use the perceptual loss [17] evalu-
ated at convi,2 for i ∈ [1, 5] of VGG [40]. The appearance
encoder architecture is adopted from [25], and we use a la-
tent appearance vector z a ∈ R8 . We train on 8 GPUs for
∼ 40 epochs using 256 × 256 crops of input images, but we
show compelling results on up to 600× 900 at test time. The
generator runtime for the staged training network is 330 ms
for a 512x512 frame on a TitanV without fp16 optimiza-
tions. Architecture and training details can be found in the
supplementary material.

Datasets We evaluate our method on ﬁve datasets recon-
structed with COLMAP [36] from public images, summa-
rized in Table 1. A separate model is trained for each
dataset. We create aligned datasets by rendering the recon-
structed point clouds with a minimum dimension of 600
pixels, and throw away sparse renderings (>85% empty
pixels), and small images (<450 pixels across). We ran-
domly select a validation set of 100 images per dataset.

Ablative study We perform an ablative study of our sys-
tem and compare the proposed methods in Figure 4. The

6882

Dataset

#Images #Points VGG

I2I

L1

+Sem

PSNR VGG

L1

+Sem+BaseApp
+Sem+StagedApp
PSNR VGG
PSNR VGG
PSNR

L1

L1

Sacre Coeur
Trevi
Pantheon
Dubrovnik
San Marco

1165
3006
4972
5891
7711

33M 70.78 39.98
35M 86.52 42.95
9M
68.28 39.77
33M 78.42 40.60
7M
80.18 44.04

14.36
14.14
14.50
14.21
13.97

66.17 34.78
81.82 36.46
67.47 36.27
78.58 39.88
78.36 39.34

15.62
15.57
15.13
14.51
14.58

60.06 21.58
79.10 28.12
64.06 28.85
76.61 34.57
70.35 26.24

18.98
17.37
16.76
15.38
17.87

61.23 25.22
75.55 25.00
60.66 23.77
71.65 27.48
68.96 23.11

17.81
18.19
17.95
17.01
18.32

Table 1: Dataset statistics (number of registered images and size of reconstructed point cloud) and average error on the validation set using
VGG/perceptual loss (lower is better), L1 loss (lower is better), and PSNR (higher is better), for four methods: an image-to-image baseline
(I2I), with semantic conditioning (+Sem), with semantic conditioning and a baseline appearance modeling based on [53] (+Sem+BaseApp),
and with semantic conditioning and staged appearance training (+Sem+StagedApp).

e

.

n

i
l

e
s
a

B

d

e

g

a

t

S

e

n

i
l

e
s
a

B

d

e

g

a

t

S

Figure 5: Examples of appearance interpolation for a ﬁxed viewpoint. The left- and rightmost appearances are captured from real images,
and the intermediate frames are generated by linearly interpolating the appearances in the latent space. Notice how the baseline method is
unable to capture complex scenes, like the sunset and night scene, and its interpolations are rather linear, as can be appreciated in the street
lamps (top). The staged training method performs better, but generates twilight artifacts in the sky when interpolating between day and
night appearances (bottom).

results of the image-to-image translation baseline method
contain additional blurry artifacts near the ground because
it hallucinates the locations of pedestrians. Using semantic
conditioning, the results improve slightly in those regions.
Finally, encoding the appearance of the input photo allows
the network to match the appearance. The staged training
recovers a closer appearance in San Marco and Pantheon
datasets (two bottom rows). However, in Sacre Coeur (top
row), the smallest dataset, the baseline appearance model
is able to better capture the general appearance of the im-
age, although the staged training model reproduces the di-
rectionality of the lighting with more ﬁdelity.

Reconstruction metrics We report image reconstruction
errors in the validation set using several metrics: perceptual
loss [17], L1 loss, and PSNR. We use the ground truth se-
mantic mask from the source image, and we extract the ap-
pearance latent vector using the appearance encoder. Staged
training of the appearance fares better than the baseline for
all but the smallest dataset (Sacre Coeur), where the staged
training overﬁts to the training data and is unable to gener-
alize. The baseline method assumes a prior distribution of

the latent space and is less prone to overﬁtting at the cost of
poorer modeling of appearance.

Appearance interpolation The rerendering network al-
lows for interpolating the appearance of two images by in-
terpolating their latent appearance vectors. Figure 5 depicts
two examples, showing that the staged training approach
is able to generate more complex appearance changes, al-
though its generated interpolations lack realism when tran-
sitioning between day and night. In the following, we only
show results for the staged training model.

Appearance transfer Figure 6 demonstrates how our full
model can transfer the appearance of a given photo to oth-
ers. It shows realistic renderings of the Trevi fountain from
ﬁve different viewpoints under four different appearances
obtained from other photos. Note the sunny highlights
and the spotlit night illumination appearance of the stat-
ues. However, these details can ﬂicker when synthesizing
a smooth camera path or smoothly interpolating the appear-
ance in the latent space, as seen in the supplementary video.

6883

Figure 6: We capture the appearance of the original images in the left column, and rerender several viewpoints under them. The last column
is a detail of the previous one. The top row shows the renderings part of the input to the rerenderer, that exhibit artifacts like incomplete
features in the statue, and an inconsistent mix of day and night appearances. Note the hallucinated twilight scene in the sky using the last

appearance. Image credits: Flickr users William Warby, Neil Rickards, Rafael Jimenez, acme401 (Creative Commons).

Photo

Frame 0

Frame 20

Frame 40

Frame 60

Frame 80

Frame 100

Photo

Figure 7: Frames from a synthesized camera path that smoothly transitions from the photo on the left to the photo on the right by smoothly
interpolating both viewpoint and the latent appearance vectors. Please see the supplementary video. Photo Credits: Allie Caulﬁeld, Tahbepet,

Till Westermayer, Elliott Brown (Creative Commons).

Image interpolation Figure 7 shows sets of two images
and frames of smooth image interpolations between them,
where both viewpoint and appearance transition smoothly
between them. Note how the illumination of the scene can
transition smoothly from night to day. The quality of the
results is best appreciated in the supplementary video.

Semantic consistency Figure 8 shows the output of the
staged training model with ground truth and predicted seg-
mentation masks. Using the predicted masks, the network
produces similar results on the building and renders a scene
free of people. Note however how the network depicts
pedestrians as black, ghostly ﬁgures when they appear in
the segmentation mask.

(a) w/ GT segmentation

(b) w/ predicted segmentations

Figure 8: Example semantic labelings and output renders when
using the “ground truth” segmentation mask computed from the
corresponding real image (from the validation set) and the pre-
dicted one from the associated deep buffer. Note the artifacts on
the bottom right where the ground is misclassiﬁed as building.

6884

(a) Neural artifacts

(b) Sparse reconstructions

(c) Segmentations artifacts

Figure 10: Limitations of the current system.

Segmentation Our model relies heavily on the segmenta-
tion mask to synthesize parts of the image not modeled in
the proxy geometry, like the ground or sky regions. Thus
our results are very sensitive to errors in the segmentation
network, like in the sky region in Figure 10c or an appear-
ing “ghost pole” artifact in San Marco (frame 40 of bottom
row in Figure 7, best seen in video). Jointly training the
neural rerenderer together with the segmentation network
could reduce such artifacts.

Neural artifacts Neural networks are known to produce
screendoor patterns [30] and other intriguing artifacts [29].
We observe such artifacts in repeated structures, like the
patterns on the ﬂoor of San Marco, which in our renderings
are misaligned as if hand-painted. Similarly, the inscription
above the Trevi fountain is reproduced with a distorted font
(see Figure 10a).

Incomplete reconstructions Sometimes an image con-
tains partially reconstructed parts of the 3D model, creat-
ing large holes in the rendered Bi . This forces the network
to hallucinate the incomplete regions, generally leading to
blurry outputs (see Figure 10b).

Temporal artifacts When smoothly varying the view-
point, sometimes the appearance of the scene can ﬂicker
considerably, especially under complex appearance, such
as when the sun hits the Trevi Fountain, creating complex
highlights and cast shadows. Please see the supplementary
video for an example.

In summary, we present a ﬁrst attempt at solving the total
scene capture problem. Using unstructured internet photos,
we can train a neural rerendering network that is able to
produce highly realistic scenes under different illumination
conditions. We propose a novel staged training approach
that better captures the appearance of the scene as seen in
internet photos. Finally, we evaluate our system on ﬁve
challenging datasets and against state-of-the-art 3D recon-
struction methods.

6885

(a) [39]

(b) ours

(c) original images

Figure 9: Comparison of [39] and our approach. Rows 1 & 3:
original photos. Rows 2 & 4: detailed crops. Image credits: Graeme

Churchard, Sarah-Rose (Creative Commons).

Comparison to 3D reconstruction methods We evalu-
ated our technique against the one of Shan et al. [39] on the
Colosseum, which contains 3K images, 10M color vertices
and 48M triangles and was generated from Flickr, Google
Street View, and aerial images. Their 3D representation
is a dense vertex-colored mesh, where the albedo and ver-
tex normals are jointly recovered together with a simple 8-
dimensional lighting model (diffuse, plus directional light-
ing) for each image in the photo collection.

Figure 9 compares both methods and the original ground
truth image. Their method suffers from ﬂoating white ge-
ometry on the top edge of the Colosseum, and has less de-
tail, although it recovers the lighting better than our method,
thanks to its explicit lighting reasoning. Note that both mod-
els are accessing the test image to compute lighting coefﬁ-
cients and appearance latent vectors, with dimension 8 in
both cases, and that we use the predicted segmentation la-
belings from Bi .
We ran a randomized user study on 20 random sets of
output images that do not contain close-ups of people or
cars, and were not in our training set. For each viewpoint,
200 participants chose “which image looks most real?” be-
tween an output of their system and ours (without seeing
the original). Respondents preferred images generated by
our system a 69.9% of the time, with our technique being
preferred on all but one of the images. We show the 20 ran-
dom sets of the user study in the supplementary material.

5. Discussion

Our system’s limitations are signiﬁcantly different from
those of traditional 3D reconstruction pipelines:

References

[1] S. Agarwal, N. Snavely,
I. Simon, S. M. Seitz, and
R. Szeliski. Building Rome in a day.
In ICCV, 2009. 1,
2

[2] A. Almahairi, S. Rajeshwar, A. Sordoni, P. Bachman, and
A. Courville. Augmented CycleGAN: Learning many-to-
many mappings from unpaired data. In ICML, 2018. 3

[3] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. DeepLab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE Trans. PAMI, 2018. 5

[4] P. E. Debevec, C. J. Taylor, and J. Malik. Modeling and ren-
dering architecture from photographs: A hybrid geometry-
and image-based approach. In Proc. SIGGRAPH, 1996. 2

[5] H. Dong, S. Yu, C. Wu, and Y. Guo. Semantic image synthe-
sis via adversarial learning. In ICCV, 2017. 2

[6] S. A. Eslami, D. J. Rezende, F. Besse, F. Viola, A. S. Mor-
cos, M. Garnelo, A. Ruderman, A. A. Rusu, I. Danihelka,
K. Gregor, et al. Neural scene representation and rendering.
Science, 2018. 2

[7] Y. Furukawa and J. Ponce. Accurate, dense, and robust mul-
tiview stereopsis. IEEE Trans. PAMI, 2010. 2

[8] R. Garg, H. Du, S. M. Seitz, and N. Snavely. The dimension-
ality of scene appearance. In ICCV, 2009. 2

[9] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer
using convolutional neural networks. In CVPR, 2016. 2, 4

[10] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen.
The lumigraph. In Proc. SIGGRAPH, 1996. 2

[11] D. Hauagge, S. Wehrwein, P. Upchurch, K. Bala, and
N. Snavely. Reasoning about photo collections using models
of outdoor illumination. In BMVC, 2014. 2

[12] P. Hedman, J. Philip, T. Price, J.-M. Frahm, G. Drettakis, and
G. Brostow. Deep blending for free-viewpoint image-based
rendering. In Proc. SIGGRAPH, 2018. 2

[20] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Niessner,
P. P ´erez, C. Richardt, M. Zollh ¨ofer, and C. Theobalt. Deep
video portraits. In Proc. SIGGRAPH, 2018. 2, 3

[21] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to
discover cross-domain relations with generative adversarial
networks. In ICML, 2017. 3

[22] P.-Y. Laffont, A. Bousseau, S. Paris, F. Durand, and G. Dret-
takis. Coherent intrinsic images from photo collections. In
Proc. SIGGRAPH Asia, 2012. 2

[23] P.-Y. Laffont, Z. Ren, X. Tao, C. Qian, and J. Hays. Transient
attributes for high-level understanding and editing of outdoor
scenes. In Proc. SIGGRAPH, 2014. 2

[24] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham,
A. Acosta, A. P. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In CVPR, 2017. 2

[25] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. K. Singh, and M.-H.
Yang. Diverse image-to-image translation via disentangled
representations. In ECCV, 2018. 2, 3, 4, 5

[26] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In NeurIPS, 2017. 3

[27] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In ICCV,
2017. 5

[28] R. Martin-Brualla, R. Pandey, S. Yang, P. Pidlypenskyi,
J. Taylor, J. Valentin, S. Khamis, P. Davidson, A. Tkach,
P. Lincoln, A. Kowdle, C. Rhemann, D. B. Goldman, C. Ke-
skin, S. Seitz, S. Izadi, and S. Fanello. LookinGood: Enhanc-
ing performance capture with real-time neural re-rendering.
In Proc. SIGGRAPH Asia, 2018. 2, 3

[29] A. Mordvintsev, C. Olah, and M. Tyka. Inceptionism: Go-
ing deeper into neural networks. Google Research Blog. Re-
trieved June, 2015. 8

[30] A. Odena, V. Dumoulin, and C. Olah. Deconvolution and
checkerboard artifacts. Distill, 2016. 8

[13] V. H. Hiep, R. Keriven, P. Labatut, and J.-P. Pons. Towards
high-resolution large-scale multi-view stereo.
In CVPR,
2009. 2

[31] E. Park, J. Yang, E. Yumer, D. Ceylan, and A. C.
Berg. Transformation-grounded image generation network
for novel 3D view synthesis. In CVPR, 2017. 1

[14] Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto,
and J.-F. Lalonde. Deep outdoor illumination estimation. In
CVPR, 2017. 2

[32] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A.
Efros. Context encoders: Feature learning by inpainting. In
CVPR, 2016. 2

[15] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
unsupervised image-to-image translation. In ECCV, 2018. 2,
3, 4

[16] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
2017. 2, 3

[17] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for
real-time style transfer and super-resolution. In ECCV, 2016.
5, 6

[18] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of GANs for improved quality, stability, and varia-
tion. In ICLR, 2018. 3, 5

[33] F. Radenovi ´c, J. L. Sch ¨onberger, D. Ji, J.-M. Frahm,
O. Chum, and J. Matas. From dusk till dawn: Modeling
in the dark. In CVPR, 2016. 2

[34] H. Rhodin, M. Salzmann, and P. Fua.
Unsupervised
geometry-aware representation learning for 3D human pose
estimation. In ECCV, 2018. 1

[35] T. Saito and T. Takahashi. Comprehensible rendering of 3-D
shapes. In Proc. SIGGRAPH, 1990. 2, 3

[36] J. L. Sch ¨onberger. Colmap. http://colmap.github.
io, 2016. 3, 5

[37] J. L. Sch ¨onberger and J.-M. Frahm. Structure-from-motion
revisited. In CVPR, 2016. 3

[19] M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface
reconstruction. In Proc. Eurographics Symposium on Geom-
etry Processing, 2006. 3

[38] J. L. Sch ¨onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm.
Pixelwise view selection for unstructured multi-view stereo.
In ECCV, 2016. 2, 3

6886

[39] Q. Shan, R. Adams, B. Curless, Y. Furukawa, and S. M.
Seitz. The Visual Turing Test for scene reconstruction. In
Proc. 3DV, 2013. 1, 2, 3, 8
[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. CoRR, 2014. 5
[41] N. Snavely, S. M. Seitz, and R. Szeliski. Photo Tourism:
Exploring photo collections in 3D.
In Proc. SIGGRAPH,
2006. 1
[42] K. Sunkavalli, W. Matusik, H. Pﬁster, and S. Rusinkiewicz.
Factored time-lapse video. In Proc. SIGGRAPH, 2007. 2
[43] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3D
models from single images with a convolutional network. In
ECCV, 2016. 1
[44] J. Thies, M. Zollh ¨ofer, C. Theobalt, M. Stamminger, and
M. Nießner. IGNOR: Image-guided neural object rendering.
arXiv 2018, 2018. 2, 3
[45] M. Waechter, N. Moehrle, and M. Goesele. Let there be
color! Large-scale texturing of 3D reconstructions.
In
ECCV, 2014. 3
[46] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional gans. In CVPR, 2018. 3, 5
[47] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, N. Yakovenko, A. Tao,
J. Kautz, and B. Catanzaro. Video-to-video synthesis.
In
NeurIPS, 2018. 2, 3
[48] T. Y. Wang, T. Ritschel, and N. J. Mitra. Joint material and
illumination estimation from photo sets in the wild. In Proc.
3DV, 2018. 2
[49] X. Wang and A. Gupta. Generative image modeling using
style and structure adversarial networks. In ECCV, 2016. 2
[50] Z. Zhang, Y. Song, and H. Qi. Age progression/regression
by conditional adversarial autoencoder. In CVPR, 2017. 2
[51] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Tor-
ralba. Scene parsing through ADE20K dataset. In CVPR,
2017. 5
[52] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In ICCV, 2017. 3
[53] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros,
O. Wang, and E. Shechtman. Toward multimodal image-
to-image translation. In NeurIPS, 2017. 2, 3, 4, 5, 6

6887

Neural RGB→D Sensing: Depth and Uncertainty from a Video Camera

Chao Liu1

2 ∗

,

3 ∗

,

Jinwei Gu1
1NVIDIA

Kihwan Kim1
Srinivasa G. Narasimhan2
2Carnegie Mellon University
3SenseTime

Jan Kautz1

Abstract

Depth sensing is crucial for 3D reconstruction and scene
understanding. Active depth sensors provide dense metric
measurements, but often suffer from limitations such as re-
stricted operating ranges, low spatial resolution, sensor in-
terference, and high power consumption. In this paper, we
propose a deep learning (DL) method to estimate per-pixel
depth and its uncertainty continuously from a monocular
video stream, with the goal of effectively turning an RGB
camera into an RGB-D camera. Unlike prior DL-based
methods, we estimate a depth probability distribution for
each pixel rather than a single depth value, leading to an
estimate of a 3D depth probability volume for each input
frame. These depth probability volumes are accumulated
over time under a Bayesian ﬁltering framework as more in-
coming frames are processed sequentially, which effectively
reduces depth uncertainty and improves accuracy, robust-
ness, and temporal stability. Compared to prior work, the
proposed approach achieves more accurate and stable re-
sults, and generalizes better to new datasets. Experimental
results also show the output of our approach can be directly
fed into classical RGB-D based 3D scanning methods for
3D scene reconstruction.

1. Introduction

Depth sensing is crucial for 3D reconstruction [31, 32,
52] and scene understanding [43, 18, 34]. Active depth
sensors (e.g., time of ﬂight cameras [19, 35], LiDAR [7])
measure dense metric depth, but often have limited operat-
ing range (e.g., indoor) and spatial resolution [5], consume
more power, and suffer from multi-path reﬂection and inter-
ference between sensors [29]. In contrast, estimating depth
directly from image(s) solves these issues, but faces other
long-standing challenges such as scale ambiguity and drift
for monocular methods [37], as well as the correspondence
problem and high computational cost for stereo [47] and
multi-view methods [41].
Inspired by recent success of deep learning in 3D vi-
sion [13, 56, 50, 17, 20, 51, 53, 6, 55, 4, 46], in this paper,
we propose a DL-based method to estimate depth and its

∗ The authors contributed on this work when they were at NVIDIA.

Figure 1. We proposed a DL-based method to estimate depth and
its uncertainty (or, conﬁdence) continuously for a monocular video
stream, with the goal of turning an RGB camera into an RGB-D
camera. Its output can be directly fed into classical RGB-D based
3D scanning methods [31, 32] for 3D reconstruction.

uncertainty continuously from a monocular video stream,
with the goal of effectively turning an RGB camera into an
RGB-D camera. We have two key ideas:
1. Unlike prior work, for each pixel, we estimate a
depth probability distribution rather than a single
depth value, leading to an estimate of a Depth Prob-
ability Volume (DPV) for each input frame. As
shown in Fig. 1, the DPV provides both a Maximum-
Likelihood-Estimate (MLE) of the depth map, as well
as the corresponding per-pixel uncertainty measure.

2. These DPVs across different frames are accumulated
over time, as more incoming frames are processed se-
quentially. The accumulation step, originated from the
Bayesian ﬁltering theory and implemented as a learn-
able deep network, effectively reduces depth uncer-
tainty and improves accuracy, robustness, and tempo-
ral stability over time, as shown later in Sec. 4.
We argue that all DL-based depth estimation methods
should predict not depth values but depth distributions,
and should integrate such statistical distributions over time
(e.g., via Bayesian ﬁltering). This is because dense depth
estimation from image(s) – especially for single-view meth-
ods – inherently has a lot of uncertainty, due to factors such

10986

as lack of texture, specular/transparent material, occlusion,
and scale drift. While some recent work started focusing on
uncertainty estimation [15, 23, 24, 21] for certain computer
vision tasks, to our knowledge, we are the ﬁrst to predict a
depth probability volume from images and integrate it over
time in a statistical framework.
We evaluate our method extensively on multiple datasets
and compare with recent state-of-the-art, DL-based, depth
estimation methods [13, 17, 50]. We also perform the so-
called “cross-dataset” evaluation task, which tests models
trained on a different dataset without ﬁne-tuning. We be-
lieve such cross-dataset tasks are essential to evaluate the
robustness and generalization ability [1]. Experimental re-
sults show that, with reasonably good camera pose estima-
tion, our method outperforms these prior methods on depth
estimation with better accuracy, robustness, and temporal
stability. Moreover, as shown in Fig. 1, the output of the
proposed method can be directly fed into RGB-D based 3D
scanning methods [31, 32] for 3D scene reconstruction.

2. Related Work

Depth sensing from active sensors Active depth sen-
sors, such as depth cameras [19, 35] or LiDAR sen-
sors [7] provide dense metric depth measurements as well
as sensor-speciﬁc conﬁdence measure [36]. Despite of their
wide usage [31, 52, 18, 34], they have several inherent
drawbacks[33, 49, 29, 5], such as limited operating range,
low spatial resolution, sensor interference, and high power
consumption. Our goal in this paper is to mimic a RGB-D
sensor with a monocular RGB camera, which continuously
predicts depth (and its uncertainty) from a video stream.

Depth estimation from images Depth estimation directly
from images has been a core problem in computer vi-
sion [38, 41]. Classical single view methods [9, 37] often
make strong assumptions on scene structures. Stereo and
multi-view methods [41] rely on triangulation and suffer
from ﬁnding correspondences for textureless regions, trans-
parent/specular materials, and occlusion. Moreover, due to
global bundle adjustment, these methods are often compu-
tationally expensive for real-time applications. For depth
estimation from a monocular video, there is also scale ambi-
guity and drifting [30]. Because of these challenges, many
computer vision systems [39, 30] use RGB images mainly
for camera pose estimation but rarely for dense 3D recon-
struction [40]. Nevertheless, depth sensing from images has
great potentials, since it addresses all the above drawbacks
of active depth sensors. In this paper, we take a step in this
direction using a learning-based method.

Learning-based depth estimation Recently researchers
have shown encouraging results for depth sensing directly
from images(s), including single-view methods [56, 13, 17],
video-based methods [28, 54, 51], depth and motion from

two views [50, 6], and multi-view stereo [55, 20, 53]. A
few work also incorporated these DL-based depth sensing
methods into visual SLAM systems [4, 46]. Despite of
the promising performance, however, these DL-based meth-
ods are still far from real-world applications, since their ro-
bustness and generalization ability is yet to be thoroughly
tested [1].
In fact, as shown in Sec. 4, we found many
state-of-the-art methods degrade signiﬁcantly even for sim-
ple cross-dataset tasks. This gives rise to an increasing de-
mand for a systematic study of uncertainty and Bayesian
deep learning for depth sensing, as performed in our paper.

Uncertainty and Bayesian deep learning Uncertainty
and Bayesian modeling have been long studied in last few
decades, with various deﬁnitions ranging from the vari-
ance of posterior distributions for low-level vision [45] and
motion analysis [25] to variability of sensor input mod-
els [22]. Recently, uncertainty [15, 23] for Bayesian deep
learning were introduced for a variety of computer vision
tasks [24, 21, 8]. In our work, the uncertainty is deﬁned as
the posterior probability of depth, i.e., the DPV estimated
from a local window of several consecutive frames. Thus,
our network estimates the “measurement uncertainty” [23]
rather than the “model uncertainty”. We also learn an ad-
ditional network module to integrate this depth probability
distribution over time in a Bayesian ﬁltering manner, in or-
der to improve the accuracy and robustness for depth esti-
mation from a video stream.

3. Our Approach

Fig. 2 shows an overview of our proposed method for
depth sensing from an input video stream, which consists of
three parts. The ﬁrst part (Sec. 3.1) is the D-Net, which es-
timates the Depth Probability Volume (DPV) for each input
frame. The second part (Sec. 3.2) is the K-Net, which helps
to integrate the DPVs over time. The third part (Sec. 3.3) is
the reﬁnement R-Net, which improves the spatial resolution
of DPVs with the guidance from input images.

Speciﬁcally, we denote the depth probability volume
(DPV) as p(d; u, v), which represents the probability
of pixel (u, v) having a depth value d, where d ∈
[dmin , dmax ]. Due to perspective projection, the DPV is
deﬁned on the 3D view frustum attached to the camera, as
shown in Fig. 3(a). dmin and dmax are the near and far
planes of the 3D frustum, which is discretized into N = 64
planes uniformly over the inverse of depth (i.e., disparity).
The DPV contains the complete statistical distribution of
depth for a given scene.
In this paper, we directly use
the non-parametric volume to represent DPV. Parametric
models, such as Gaussian Mixture Model [3], can be also
be used. Given the DPV, we can compute the Maximum-

10987

Estimate Depth Probability (Sec. 3.1)

Integrate Depth Probability Over Time (Sec. 3.2)

Refine Depth Probability(Sec. 3.3)

!"

…

D-Net

Depth

Confidence

Predicted DPV

#(%" |') :"+) )

Shared

K-Net

R-Net

Depth Confidence

Depth

Confidence

Softmax

-

+

Measured DPV

#(%" |'" )

Warp

Residual

Residual Gain

Updated DPV

#(%" |') :" )

Refined DPV

…

Skip connection from image features to R-Net

Figure 2. Overview of the proposed network for depth estimation with uncertainty from a video. Our method takes the frames in a local
time window in the video as input and outputs a Depth Probability Volume (DPV) that is updated over time. The update procedure is in
a Bayesian ﬁlter fashion: we ﬁrst take the difference between the local DPV estimated using the D-Net (Sec. 3.1) and the predicted DPV
from previous frames to get the residual; then the residual is modiﬁed by the K-Net (Sec. 3.2) and added back to the predicted DPV; at last
the DPV is reﬁned and upsampled by the R-Net (Sec. 3.3), which can be used to compute the depth map and its conﬁdence measure.

Figure 3. Representation and update for DPV. (a) The DPV is de-
ﬁned over a 3D frustrum deﬁned by the pinhole camera model .
(b) The DPV gets updated over time as the camera moves.

Likelihood Estimates (MLE) for depth and its conﬁdence:

Depth :

ˆd(u, v) =

d=dmax

X

d=dmin

p(d; (u, v)) · d,

(1)

Conﬁdence :

ˆC (u, v) = p( ˆd, (u, v)).

(2)

To make notations more concise, we will omit (u, v) and
use p(d) for DPVs in the rest of the paper.
When processing a video stream, the DPV can be treated
as a hidden state of the system. As the camera moves,
as shown in Fig. 3(b), the DPV p(d) is being updated as
new observations arrive, especially for the overlapping vol-
umes. Meanwhile, if camera motion is known, we can eas-
ily predict the next state p(d) from the current state. This
predict-update iteration naturally implies a Bayesian ﬁlter-
ing scheme to update the DPV over time for better accuracy.

3.1. D(cid:173)Net: Estimating DPV

For each frame It , we use a CNN, named D-Net, to esti-
mate the conditional DPV, p(dt |It ), using It and its tempo-
rally neighboring frames. In this paper, we consider a local
time window of ﬁve frames Nt = [t − 2∆t, t − ∆t, t, t +
∆t, t + 2∆t], and we set ∆t = 5 for all our testing videos
(25fps/30fps). For a given depth candidate d, we can com-
pute a cost map by warping all the neighboring frames into

the current frame It and computing their differences. Thus,
for all depth candidates, we can compute a cost volume,
which produces the DPV after a softmax layer:

L(dt |It ) = X

||f (It ) − warp(f (Ik ); dt , δTkt )||,

k∈Nt ,k 6=t

p(dt |It ) = softmax(L(dt |It )),

(3)

where f (·) is a feature extractor, δTkt is the relative cam-
era pose from frame Ik to frame It , warp(·) is an operator
that warps the image features from frame Ik to the refer-
ence frame It , which is implemented as 2D grid sampling.
In this paper, without loss of generality, we use the feature
extractor f (·) from PSM-Net [6], which outputs a feature
map of 1/4 size of the input image. Later in Sec. 3.3, we
learn a reﬁnement R-Net to upsample the DPV back to the
original size of the input image.
Figure 4 shows an example of a depth map ˆd(u, v) and
its conﬁdence map ˆC (u, v) (blue means low conﬁdence) de-
rived from a Depth Probability Volume (DPV) from the in-
put image. The bottom plot shows the depth probability
distributions p(d; u, v) for the three selected points, respec-
tively. The red and green points have sharp peaks, which
indicates high conﬁdence in their depth values. The blue
point is in the highlight region, and thus it has a ﬂat depth
probability distribution and a low conﬁdence for its depth.

3.2. K(cid:173)Net: Integrating DPV over Time

When processing a video stream, our goal is to integrate
the local estimation of DPVs over time to reduce uncer-
tainty. As mentioned earlier, this integration can be natu-
rally implemented as Bayesian ﬁltering. Let us deﬁne dt
as the hidden state, which is the depth (in camera coordi-
nates) at frame It . The “belief ” volume p(dt |I1:t ) is the
conditional distribution of the state giving all the previous
frames. A simple Bayesian ﬁltering can be implemented in

10988

Figure 4. An example of a depth map ˆd(u, v) and its conﬁdence
map ˆC (u, v) (blue means low conﬁdence) derived from a Depth
Probability Volume (DPV). The bottom plot shows the depth prob-
ability distributions p(d; u, v) for the three selected points, respec-
tively. The red and green points have sharp peaks, which indicates
high conﬁdence in their depth values. The blue point is in the
highlight region, which results in a ﬂat depth probability distribu-
tion and a low conﬁdence for its depth value.

two iterative steps:

Predict : p(dt |I1:t ) → p(dt+1 |I1:t ),
Update : p(dt+1 |I1:t ) → p(dt+1 |I1:t+1 ),

(4)

where the prediction step is to warp the current DPV from
the camera coordinate at t to the camera coordinate at t + 1:

p(dt+1 |I1:t ) = warp(p(dt |I1:t ), δTt,t+1 ),

(5)

where δTt,t+1 is the relative camera pose from time t to time
t + 1, and warp(·) here is a warping operator implemented
as 3D grid sampling. At time t + 1, we can compute the
local DPV p(dt+1 |It+1 ) from the new measurement It+1
using the D-Net. This local estimate is thus used to update
the hidden state, i.e., the “belief ” volume,

p(dt+1 |I1:t+1 ) = p(dt+1 |I1:t ) · p(dt+1 |It+1 ).

(6)

dmin

Note that we always normalize the DPV in the above equa-
tions and ensure R dmax
p(d) = 1. Fig. 5 shows an example.
As shown in the second row, with the above Bayesian ﬁlter-
ing (labeled as ”no damping”), the estimated depth map is
less noisy, especially in the regions of the wall and the ﬂoor.
However, one problem with directly applying Bayesian
ﬁltering is it integrates both correct and incorrect informa-
tion in the prediction step. For example, when there are
occlusions or disocclusions, the depth values near the oc-
clusion boundaries change abruptly. Applying Bayesian
ﬁltering directly will propagate wrong information to the
next frames for those regions, as highlighted in the red box
in Fig. 5. One straightforward solution is to reduce the
weight of the prediction in order to prevent incorrect infor-
mation being integrated over time. Speciﬁcally, by deﬁning
E (d) = − log p(d), Eq. 6 can be re-written as

Figure 5. Comparison between different methods for integrating
DPV over time. Part of the wall is occluded by the chair at frame
t and disoccluded in frame t + 1. No ﬁltering: not integrating
the DPV over time. No damping: integrating DPV directly with
Bayesian ﬁltering. Global damping: down-weighting the pre-
dicted DPV for all voxels using Eq. 7 with λ = 0.8. Adaptive
damping: down-weighting the predicted DPV adaptively with the
K-Net (Sec. 3.2). Using the K-net, we get the best depth estima-
tion for regions with/without disocclusion.

where the ﬁrst term is the prediction and the second term is
the measurement. To reduce the weight of the prediction,
we multiply a weight λ ∈ [0, 1] with the ﬁrst term,

E (dt+1 |I1:t+1 ) = λ · E (dt+1 |I1:t ) + E (dt+1 |It+1 ).

(7)

We call this scheme “global damping”. As shown in Fig. 5,
global damping helps to reduce the error in the disoc-
cluded regions. However, global damping may also prevent
some correct depth information to be integrated to the next
frames, since it reduces the weights equally for all voxels
in the DPV. Therefore, we propose an “adaptive damping”
scheme to update the DPV:

E (dt+1 |I1:t+1 ) = E (dt+1 |I1:t ) + g(∆Et+1 , It+1 ),

(8)

where ∆Et+1 is the difference between the measurement
and the prediction,

E (dt+1 |I1:t+1 ) = E (dt+1 |I1:t ) + E (dt+1 |It+1 ),

∆Et+1 = E (dt+1 |It+1 ) − E (dt+1 |I1:t ),

(9)

10989

and g(·) is a CNN, named K-Net, which learns to trans-
form ∆Et+1 into a correction term to the prediction. Intu-
itively, for regions with correct depth probability estimates,
the values in the overlapping volume of DPVs are consis-
tent. Thus the residual in Eq. 9 is small and the DPV will
not be updated in Eq. 8. On the other hand, for regions with
incorrect depth probability, the residual would be large and
the DPV will be corrected by g(∆E , It+1 ). This way, the
weight for prediction will be changed adaptively for differ-
ent DPV voxels. As shown in Fig. 5, the adaptive damping,
i.e., K-Net, signiﬁcantly improve the accuracy for depth es-
timation. In fact, K-Net is closely related to the derivation
of Kalman ﬁlter, where “K” stands for Kalman gain. Please
refer to the supplementary for details.

3.3. R(cid:173)Net and Training Details

Finally, since the DPV p(dt |I1:t ) is estimated with 1/4
spatial resolution (on both width and height) of the input
image, we employ a CNN, named R-Net, to upsample and
reﬁne the DPV back to the original image resolution. The
R-Net, h(·), is essentially an U-Net with skip connections,
which takes input the low-res DPV from the K-Net g(·) and
the image features extracted from the feature extractor f (·),
and outputs a high-resolution DPV.
In summary, as shown in Fig. 2, the entire network
has three modules, i.e., the D-Net, f (·; Θ1 ), the K-Net,
g(·; Θ2 ), and the R-Net, h(·; Θ3 ). Detailed network archi-
tectures are provided in the supplementary material. The
full network is trained end-to-end, with simply the Neg-
ative Log-Likelihood (NLL) loss over the depth, Loss =
NLL(p(d), dGT ). We also tried to add image warping as
an additional loss term (i.e., minimizing the difference be-
tween It and the warped neighboring frames), but we found
that it does not improve the quality of depth prediction.
During training, we use ground truth camera poses. For
all our experiments, we use the ADAM optimizer [26] with
a learning rate of 10−5 , β1 = .9 and β2 = .999. The whole
framework, including D-Net, K-Net and R-Net, is trained
together in an end-to-end fashion for 20 epochs.

3.4. Camera Poses during Inference

During inference, given an input video stream, our
method requires relative camera poses δT between consec-
utive frames — at least for all the ﬁrst ﬁve frames — to
bootstrap the computation of the DPV. In this paper, we
evaluated several options to solve this problem.
In many
applications, such as autonomous driving and AR, initial
camera poses may be provided by additional sensors such
as GPS, odometer, or IMU. Alternatively, we can also run
state-of-the-art monocular visual odometry methods, such
as DSO [12], to obtain the initial camera poses. Since our
method outputs continuous dense depth maps and their un-
certainty maps, we can in fact further optimize the initial

Local Time Window !"

Ref. Frame

Local Time Window !"#$

Warp depth map

Ref. Frame

Time

Figure 6. Camera pose optimization in a sliding local time window
during inference. Given the relative camera pose from the refer-
ence frame in Nt to the reference frame in Nt+1 , we can predict
the depth map for the reference frame in Nt+1 . Then, we opti-
mize the relative camera poses between every source frame and
the reference frame in Nt+1 using Eq.10.

camera poses within a local time window, similar to local
bundle adjustment [48].
Speciﬁcally, as shown in Fig. 6, given p(dt |I1:t ), the
DPV of the reference frame It in the local time window
Nt , we can warp p(dt |I1:t ) to the reference camera view in
Nt+1 to predict the DPV p(dt+1 |I1:t ) using Eq. 5. Then
we get the depth map ˆd and conﬁdence map ˆC for the new
reference frame using Eq. 2. The camera poses within the
local time window Nt+1 are optimized as:

min.

δTk,t+1

k∈Nt+1 ,k 6=t+1

X

k

ˆC |It+1 − warp(Ik ; ˆd; δTk,t+1 )|1 ,

(10)

where δTk,t+1 is the relative camera pose of frame k to
frame t + 1; Ik is the source image at frame k ; warp(·) is a
warping operator from the source to the reference view.

4. Experimental Results

We evaluate our method on multiple indoor and outdoor
datasets [42, 44, 14, 16], with an emphasis on accuracy and
robustness. For accuracy evaluation, we argue the widely-
used statistical metrics [11, 50] are insufﬁcient because they
can only provide an overall estimate over the entire depth
map. Rather, we feed the estimated depth maps directly
into classical RGB-D based 3D scanning systems [31, 32]
for 3D reconstruction — this will show the metric accuracy,
the consistency, and the usefulness of the estimation. For
robustness evaluation, we performed the aforementioned
cross-dataset evaluation tasks, i.e., testing on new datasets
without ﬁne-tuning. The performance degradation over new
datasets will show the generalization ability and robustness
for a given algorithm.
As no prior work operates in the exact setting as ours, it
is difﬁcult to choose methods to compare with. We care-
fully select a few recent DL-based depth estimation meth-
ods and try our best for a fair comparison. For single-
view methods, we select DORN [13] which is the current
state-of-the-art [1]. For two-view methods, we compare

10990

Figure 7. Exemplar results of our approach on ScanNet [10]. In addition to high quality depth output, we also obtain reasonable conﬁdence
maps (as shown in the marked regions for occlusion and specularity) which correlates with the depth error. Moreover, the conﬁdence maps
accumulate correctly over time with more input frames.

Table 1. Comparison of depth estimation over the 7-Scenes
dataset [42] with the metrics deﬁned in [11].

σ < 1.25

abs. rel

rmse

scale inv.

DeMoN [50]
MVSNet [53]
DORN [13]
Ours

31.88
54.87
60.05
69.26

0.3888
0.3481
0.2000
0.1758

0.8549
0.8305
0.4591
0.4408

0.4473
0.3743
0.2207
0.1899

with DeMoN [50], which shows high quality depth pre-
diction from a pair of images. For multi-view methods,
we compared with MVSNet [53]. We also compare with
MonoDepth [17], which is a semi-supervised learning ap-
proach from stereo images. To improve the temporal con-
sistency for these per-frame estimations, we trained a post-
processing network [27], but we observed it does not im-
prove the performance. Since there is always scale ambigu-
ity for depth from a monocular camera, for fair comparison,
we normalize the scale for the outputs from all the above
methods before we compute statistical metrics [11].
The inference time for processing one frame in our
method is ∼ 0.7 second per frame without pose optimiza-
tion and ∼ 1.5 second with pose estimation on a worksta-
tion with GTX 1080 GPU and 64 GB RAM memory, with
the framework implemented in Python. The pose estimation
part can be implemented with C++ to improve efﬁciency.

Results for Indoor Scenarios We ﬁrst evaluated our
method for indoor scenarios, for which RGB-D sensors
were used to capture dense metric depth for ground truth.
We trained our network on ScanNet [10]. Figure 7 shows
two exemplar results. As shown, in addition to depth maps,
our method also outputs reasonable conﬁdence maps (e.g.,
low conﬁdence in the occluded or specular regions) which
correlates with the depth errors. Moreover, with more in-
put frames, the conﬁdence maps accumulate correctly over
time: the conﬁdence of the books (top row) increases and
the depth error decreases; the conﬁdence of the glass region
(bottom row) decreases and the depth error increases.
For comparison, since the models provided by DORN,
DeMoN and MVSNet were trained on different datasets,
we compare with these two methods on a separate indoor
dataset 7Scenes [42]. For our method, we assume that

the relative camera rotation δR within a local time win-
dow is provided (e.g. measured by IMU). We also com-
pared with DeMoN with given camera poses, by ﬁxing the
camera poses inputs for their warping module to be the GT
poses. But we observed that this does not improve the ﬁnal
depth estimation. For MVSNet, we noticed that the depth
map estimation result degrades severely when the video in-
cludes camera motion patterns that are not well included in
the training data, such as camera rotation and z-axis transla-
tion. As shown in Table 1, our method signiﬁcantly outper-
forms DeMoN, DORN and MVSNet on this dataset based
on the commonly used statistical metrics [11]. We include
the complete metrics in the supplementary material.
For qualitative comparison, as shown in Fig. 8,
the
depth maps from our method are less noisy, more
sharper, and temporally more consistent (see supplemental
videos). More importantly, using an RGB-D 3D scanning
method [32], we can reconstruct a much higher quality 3D
mesh with our estimated depths compared to other meth-
ods. Even when compared with 3D reconstruction using a
real RGB-D sensor, our result has better coverage and accu-
racy in some regions (e.g., monitors / glossy surfaces) where
active depth sensors cannot capture.

Results for Outdoor Scenarios We evaluated our method
on outdoor datasets — KITTI [16] and virtual KITTI [14].
The virtual KITTI dataset is used because it has dense, ac-
curate metric depth as ground truth, while KITTI only has
sparse depth values from LiDAR as ground truth. For our
method, we use the camera poses measured by the IMU and
GPS. Table 2 lists the comparison results with DORN [13],
Eigen [11], and MonoDepth [17] which are also trained
on KITTI [16]. Our method has similar performance with
DORN [13], and is better than the other two methods. We
also tested our method with camera poses from DSO [12]
and obtain slightly worse performance (see supplementary).
Figure 9 shows qualitative comparison for depth maps
in KITTI dataset. As shown, our method generate sharper
and less noisier depth maps. In addition, our method out-
puts depth conﬁdence maps (e.g., lower conﬁdence on the
car window). Our depth estimation is temporally consis-
tent, which leads to the possibility of fusing multiple depth

10991

Input frame

GT depth

DORN depth 

DeMoN depth

Our depth

Our conﬁdence

GT view 1

GT view 2

DORN view 1

DeMoN view 1

Our view 1

Our view 2

Input frame

GT depth

DORN depth 

DeMoN depth

Our depth

Our conﬁdence

GT view 1

GT view 2

DORN view 1

DeMoN view 1

Our view 1

Our view 2

Input frame

GT depth

DORN depth 

DeMoN depth

Our depth

Our conﬁdence

GT view 1

GT view 2

DORN view 1

DeMoN view 1

Our view 1

Our view 2

Figure 8. Depth and 3D reconstruction results on indoor datasets (best viewed when zoomed in). We compare our method with DORN [13]
and DeMoN [50], in terms of both depth maps and 3D reconstruction using Voxel Hashing [32] that accumulates the estimated depth maps
for multiple frames. To show the temporal consistency of the depths, we use different numbers of depth maps for Voxel Hashing: 2 depth
maps for the ﬁrst sample and 30 depth maps for the other samples. The depth maps from DORN contain block artifacts as marked in red
boxes. This is manifested as the rippled shapes in the 3D reconstruction. DeMoN generates sharp depth boundaries but fails to recover the
depth faithfully in the regions marked in the green box. Also, the depths from DeMoN is not temporally consistent. This leads to the severe
misalignment artifacts in the 3D reconstructions. In comparison, our method generates correct and temporally consistent depths maps,
especially at regions with high conﬁdence, such as the monitor where even the Kinect sensor fails to get the depth due to low reﬂectance.

maps with voxel hashing [32] in the outdoors for a large-
scale dense 3D reconstruction, as shown in Fig. 9.
In Table 3, we performed the cross-dataset task. The upper
shows the results with training from KITTI [16] and test-
ing on virtual KITTI [14]. The lower shows the results with
training from indoor datasets (NYUv2 for DORN and Scan-
Net for ours) and testing on KITTI. As shown, our method
achieves better robustness and generalization ability.

Ablation Study The performance of our method relies on
accurate estimate of camera poses, so we test our method
with different camera pose estimation schemes as shown in
Table 4: (1) Relative camera rotation δR is read from an
IMU sensor (denoted as “GT R”). (2) δR of all frames are

Table 2. Comparison of depth estimation on KITTI [16].

σ < 1.25

abs. rel

rmse

scale inv.

Eigen [11]
Mono [17]
DORN [13]
Ours

67.80
86.43
92.62
93.15

0.1904
0.1238
0.0874
0.0998

5.114
2.8684
3.1375
2.8294

0.2628
0.1635
0.1233
0.1070

initialized with DSO [12] (denoted as “VO pose”) (3) δR
of the ﬁrst ﬁve frames are initialized with DSO [12] (de-
noted as “1st win”). We observe that when only the camera
poses in the ﬁrst time window are initialized using DSO,
the performance in terms of depth estimation is better than
that using the DSO pose initialization for all frames. This

10992

Figure 9. Depth map and 3D reconstruction for KITTI, compared with DORN [13], MonoDepth [50] (best viewed when zoomed in). First
row: Our depth map is sharper and contains less noise. For specular region (marked in the pink box), the conﬁdence is lower. Second
row, from left to right: reconstructions using depth maps of the same 100 frames estimated from MonoDepth, DORN and our method. All
meshes are viewed from above. Within the 100 frames, the vehicle was travelling in a straight line without turning.

Table 3. Cross-dataset tests for depth estimation in the outdoors.

KITTI (train) → virtual KITTI (test)

σ < 1.25

abs. rel

rmse

scale inv.

DORN [13]
Ours

69.61
73.38

0.2256
0.2537

9.618
6.452

0.3986
0.2548

Indoor (train) → KITTI (test)

σ < 1.25

abs. rel

rmse

scale inv.

DORN [13]
Ours

25.44
72.96

0.6352
0.2798

8.603
5.437

0.4448
0.2139

Table 4. Performance on 7Scenes with different initial poses

σ < 1.25

abs. rel

rmse

scale inv.

VO pose
1st win.
GT R
GT pose

60.63
62.08
69.26
70.54

0.1999
0.1923
0.1758
0.1619

0.4816
0.4591
0.4408
0.3932

0.2158
0.2001
0.1899
0.1586

may seem counter-intuitive, but it is because monocular VO
methods sometimes have large errors for textureless regions
while optimization with depths may overcome this problem.

Usefulness of the Conﬁdence Map The estimated conﬁ-
dence maps can also be used to further improve the depth
maps. As shown in Fig. 10(a), given the depth map and the
corresponding conﬁdence, we can correct the regions with
lower conﬁdence due to specular reﬂection. Also, for 3D
reconstruction algorithm, given the depth conﬁdence, we
can mask out the regions with lower conﬁdence for better
reconstruction, as shown in Fig. 10(b).

5. Conclusions and Limitations

In this paper, we present a DL-based method for contin-
uous depth sensing from a monocular video camera. Our
method estimates a depth probability distribution volume
from a local time window, and integrates it over time under
a Bayesian ﬁltering framework. Experimental results show

Figure 10. Using the conﬁdence map. (a) Correct depth map using
Fast Bilateral Solver [2]. (b) Mask out pixels with low conﬁdence
before applying Voxel Hashing [32].

our approach achieves high accuracy, temporal consistency,
and robustness for depth sensing, especially for the cross-
dataset tasks. The estimated depth maps from our method
can be fed directly into RGB-D scanning systems for 3D
reconstruction and achieve on-par or sometimes more com-
plete 3D meshes than using a real RGB-D sensor.
There are several limitations that we plan to address in
the future. First, camera poses from a monocular video of-
ten suffer from scale drifting, which may affect the accuracy
of our depth estimation. Second, in this work we focus on
depth sensing from a local time window, rather than solving
it in a global context using all the frames.

Acknowledgements Chao Liu was supported in by NSF
grant CNS-1446601.

10993

References

http://www.

[1] Robust Vision Challenge Workshop.
robustvision.net, 2018. 2, 5
[2] J. T. Barron and B. Poole. The fast bilateral solver. In Euro-
pean Conference on Computer Vision (ECCV), 2016. 8
[3] C. M. Bishop. Mixture density networks. 1994. 2
[4] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and
A. Davison. CodeSLAM - Learning a compact, optimisable
representation for dense visual SLAM. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
1, 2
[5] D. Chan, H. Buisman, C. Theobalt, and S. Thrun. A noise-
aware ﬁlter for real-time depth upsampling.
In Workshop
on Multi-camera and Multi-modal Sensor Fusion Algorithms
and Applications - M2SFA2 2008, Marseille, France, 2008.
Andrea Cavallaro and Hamid Aghajan. 1, 2
[6] J.-R. Chang and Y.-S. Chen. Pyramid stereo matching net-
work. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 5410–5418, 2018. 1, 2, 3
[7] J. A. Christian and S. Cryan. A survey of LiDAR technology
and its use in spacecraft relative navigation. In AIAA Guid-
ance, Navigation, and Control (GNC) Conference, 2013. 1,
2
[8] R. Clark, S. Wang, A. Markham, N. Trigoni, and H. Wen.
VidLoc: a deep spatial-temporal model for 6-DoF video-clip
relocalization. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017. 2
[9] A. Criminisi, I. Reid, and A. Zisserman. Single view metrol-
ogy. International Journal of Computer Vision (IJCV), 2000.
2
[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser,
and M. Nießner. ScanNet: Richly-annotated 3D reconstruc-
tions of indoor scenes.
In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2017. 6
[11] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in Neural Information Processing Systems (NIPS),
2014. 5, 6, 7
[12] J. Engel, V. Koltun, and D. Cremers. Direct sparse odom-
etry.
IEEE Transactions on Pattern Analysis and Machine
Intelligence (TPAMI), 40:611–625, 2018. 5, 6, 7
[13] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao.
Deep ordinal regression network for monocular depth esti-
mation. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 1, 2, 5, 6, 7, 8
[14] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as
proxy for multi-object tracking analysis. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
5, 6, 7
[15] Y. Gal and Z. Ghahramani. Dropout as a Bayesian approx-
imation: Representing model uncertainty in deep learning.
In International Conference on Machine Learning (ICML),
2016. 2
[16] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au-
tonomous driving? The KITTI vision benchmark suite. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3354–3361, 2012. 5, 6, 7

[17] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised
monocular depth estimation with left-right consistency.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2, 6, 7
[18] S. Gupta, R. Girshick, P. Arbelaez, and J. Malik. Learning
rich features from RGB-D images for object detection and
segmentation. In European Conference on Computer Vision
(ECCV), 2014. 1, 2
[19] R. Horaud, M. Hansard, G. Evangelidis, and C. M ´enier. An
overview of depth cameras and range scanners based on
time-of-ﬂight technologies. Machine Vision and Applica-
tions Journal, 27(7):1005–1020, 2016. 1, 2
[20] P.-H. Huang, K. Matzen, J. Kopf, N. Ahuja, and J.-B. Huang.
DeepMVS: Learning multi-view stereopsis. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018. 1, 2
[21] E. Ilg, ¨O. C¸ ic¸ ek, S. Galesso, A. Klein, O. Makansi, F. Hutter,
and T. Brox. Uncertainty Estimates and Multi-Hypotheses
Networks for Optical Flow.
In European Conference on
Computer Vision (ECCV), 2018. 2
[22] G. Kamberova and R. Bajcsy. Sensor errors and the uncer-
tainties in stereo reconstruction.
In Empirical Evaluation
Techniques in Computer Vision, pages 96–116. IEEE Com-
puter Society Press, 1998. 2
[23] A. Kendall and Y. Gal. What uncertainties do we need in
bayesian deep learning for computer vision? In Advances in
Neural Information Processing Systems (NIPS), 2017. 2
[24] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using
uncertainty to weigh losses for scene geometry and seman-
tics. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 2
[25] K. Kim, D. Lee, and I. Essa. Gaussian process regression
ﬂow for analysis of motion trajectories.
In International
Conference on Computer Vision (ICCV), 2011. 2
[26] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In International Conference on Learning Rep-
resentations (ICLR), 2015. 5
[27] W.-S. Lai, J.-B. Huang, O. Wang, E. Shechtman, E. Yumer,
and M.-H. Yang. Learning blind video temporal consistency.
In European Conference on Computer Vision (ECCV), 2018.
6
[28] R. Mahjourian, M. Wicke, and A. Angelova. Unsupervised
learning of depth and ego-motion from monocular video us-
ing 3D geometric constraints. In IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. 2
[29] A. Maimone and H. Fuchs. Reducing interference between
multiple structured light depth sensors using motion.
In
IEEE Virtual Reality Workshops (VRW), pages 51–54, 2012.
1, 2
[30] R. Mur-Artal and J. D. Tard ´os. ORB-SLAM2: an open-
source SLAM system for monocular, stereo and RGB-D
cameras. IEEE Transactions on Robotics, 33(5):1255–1262,
2017. 2
[31] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges, and
A. Fitzgibbon. KinectFusion: Real-time dense surface map-
ping and tracking. In IEEE and ACM International Sympo-

10994

[46] K. Tateno, F. Tombari, I. Laina, and N. Navab. CNN-SLAM:
Real-time dense monocular SLAM with learned depth pre-
diction. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 1, 2
[47] B. Tippetts, D. J. Lee, K. Lillywhite, and J. Archibald. Re-
view of stereo vision algorithms and their suitability for
resource-limited systems. Journal of Real-Time Image Pro-
cessing, 11(1):5–25, 2016. 1
[48] B. Triggs, P. McLauchlan, R. Hartley, and A. Fitzgibbon.
Bundle adjustment a modern synthesis.
In International
Conference on Computer Vision (ICCV), 1999. 5
[49] J. Tuley, N. Vandapel, and M. Hebert. Analysis and removal
of artifacts in 3-d LIDAR data. In International Conference
on Robotics and Automation (ICRA), 2005. 2
[50] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg,
A. Dosovitskiy, and T. Brox. DeMoN: Depth and motion
network for learning monocular stereo. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2017.
1, 2, 5, 6, 7, 8
[51] C. Wang, J. M. Buenaposada, R. Zhu, and S. Lucey. Learn-
ing depth from monocular videos using direct methods. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018. 1, 2
[52] T. Whelan, S. Leutenegger, R. S. Moreno, B. Glocker, and
A. Davison. ElasticFusion: dense SLAM without a pose
graph.
In Robotics: Science and Systems (RSS), 2015. 1,
2
[53] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan. MVSNet: Depth
inference for unstructured multi-view stereo.
In European
Conference on Computer Vision (ECCV), 2018. 1, 2, 6
[54] Z. Yin and J. Shi. GeoNet: Unsupervised learning of dense
depth, optical ﬂow and camera pose. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 2
[55] H. Zhou, B. Ummenhofer, and T. Brox. DeepTAM: Deep
tracking and mapping.
In European Conference on Com-
puter Vision (ECCV), 2018. 1, 2
[56] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu-
pervised learning of depth and ego-motion from video.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2

sium on Mixed and Augmented Reality (ISMAR), pages 127–
136, 2011. 1, 2, 5

[32] M. Nießner, M. Zollh ¨ofer, S. Izadi, and M. Stamminger.
Real-time 3D reconstruction at scale using voxel hashing.
ACM Transactions on Graphics (TOG), 2013. 1, 2, 5, 6,
7, 8

[33] F. Pomerleau, A. Breitenmoser, M. Liu, F. Colas, and
R. Siegwart. Noise characterization of depth sensors for
surface inspections.
In International Conference on Ap-
plied Robotics for the Power Industry (CARPI), pages 16–21,
2012. 2

[34] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frus-
tum PointNets for 3D object detection from RGB-D data. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2

[35] F. Remondino and D. Stoppa. TOF Range-Imaging Cameras.
Springer Publishing Company, Incorporated, 2013. 1, 2

[36] M. Reynolds, J. Dobo, L. Peel, T. Weyrich, and G. J. Bros-
tow. Capturing time-of-ﬂight data with conﬁdence. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2011. 2

[37] A. Saxena, S. H. Chung, and A. Y. Ng. 3D depth recon-
struction from a single still image. International Journal of
Computer Vision (IJCV), 76(1):53–69, Jan. 2008. 1, 2

[38] A. Saxena, J. Schulte, and A. Y. Ng. Depth estimation us-
ing monocular and stereo cues. In Proceedings of the 20th
International Joint Conference on Artiﬁcial Intelligence, IJ-
CAI’07, pages 2197–2203, 2007. 2

[39] J. L. Sch ¨onberger and J.-M. Frahm. Structure-from-motion
revisited. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), 2016. 2

[40] J. L. Sch ¨onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm.
Pixelwise view selection for unstructured multi-view stereo.
In European Conference on Computer Vision (ECCV), 2016.
2

[41] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and
R. Szeliski. A comparison and evaluation of multi-view
stereo reconstruction algorithms.
In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2006. 1,
2

[42] J. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and
A. Fitzgibbon. Scene coordinate regression forests for cam-
era relocalization in RGB-D images. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2013. 5,
6

[43] S. Song, S. P. Lichtenberg, and J. Xiao. SUN RGB-D:
A RGB-D scene understanding benchmark suite.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2015. 1

[44] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of RGB-D SLAM sys-
tems. In IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2012. 5

[45] R. Szeliski. Bayesian modeling of uncertainty in low-level
vision. International Journal of Computer Vision, 5(3):271–
301, Dec 1990. 2

10995

Occupancy Networks: Learning 3D Reconstruction in Function Space

Lars Mescheder1 Michael Oechsle1,2 Michael Niemeyer1
Sebastian Nowozin3 † Andreas Geiger1
1Autonomous Vision Group, MPI for Intelligent Systems and University of T ¨ubingen
2ETAS GmbH, Stuttgart
3Google AI Berlin

{firstname.lastname}@tue.mpg.de

nowozin@gmail.com

Abstract

With the advent of deep neural networks, learning-based
approaches for 3D reconstruction have gained popularity.
However, unlike for images, in 3D there is no canonical rep-
resentation which is both computationally and memory ef-
ﬁcient yet allows for representing high-resolution geometry
of arbitrary topology. Many of the state-of-the-art learning-
based 3D reconstruction approaches can hence only repre-
sent very coarse 3D geometry or are limited to a restricted
domain. In this paper, we propose Occupancy Networks,
a new representation for learning-based 3D reconstruction
methods. Occupancy networks implicitly represent the 3D
surface as the continuous decision boundary of a deep neu-
ral network classiﬁer. In contrast to existing approaches,
our representation encodes a description of the 3D output
at inﬁnite resolution without excessive memory footprint.
We validate that our representation can efﬁciently encode
3D structure and can be inferred from various kinds of in-
put. Our experiments demonstrate competitive results, both
qualitatively and quantitatively, for the challenging tasks of
3D reconstruction from single images, noisy point clouds
and coarse discrete voxel grids. We believe that occupancy
networks will become a useful tool in a wide variety of
learning-based 3D tasks.

1. Introduction

Recently, learning-based approaches for 3D reconstruc-
tion have gained popularity [4, 9, 23, 58, 75, 77]. In contrast
to traditional multi-view stereo algorithms, learned models
are able to encode rich prior information about the space of
3D shapes which helps to resolve ambiguities in the input.
While generative models have recently achieved remark-
able successes in generating realistic high resolution im-
ages [36, 47, 72], this success has not yet been replicated
in the 3D domain. In contrast to the 2D domain, the com-

† Part of this work was done while at MSR Cambridge.

(a) Voxel

(b) Point

(c) Mesh

(d) Ours

Figure 1: Overview: Existing 3D representations discretize
the output space differently: (a) spatially in voxel represen-
tations, (b) in terms of predicted points, and (c) in terms of
vertices for mesh representations. In contrast, (d) we pro-
pose to consider the continuous decision boundary of a clas-
siﬁer fθ (e.g., a deep neural network) as a 3D surface which
allows to extract 3D meshes at any resolution.

munity has not yet agreed on a 3D output representation
that is both memory efﬁcient and can be efﬁciently inferred
from data. Existing representations can be broadly cate-
gorized into three categories: voxel-based representations
[4, 19, 43, 58, 64, 69, 75] , point-based representations [1, 17]
and mesh representations [34, 57, 70], see Fig. 1.

Voxel representations are a straightforward generaliza-
tion of pixels to the 3D case. Unfortunately, however, the
memory footprint of voxel representations grows cubically
with resolution, hence limiting na¨ıve implementations to
323 or 643 voxels. While it is possible to reduce the memory
footprint by using data adaptive representations such as oc-
trees [61, 67], this approach leads to complex implementa-
tions and existing data-adaptive algorithms are still limited
to relatively small 2563 voxel grids. Point clouds [1, 17] and
meshes [34, 57, 70] have been introduced as alternative rep-
resentations for deep learning, using appropriate loss func-
tions. However, point clouds lack the connectivity structure
of the underlying mesh and hence require additional post-
processing steps to extract 3D geometry from the model.

4460

Existing mesh representations are typically based on de-
forming a template mesh and hence do not allow arbitrary
topologies. Moreover, both approaches are limited in the
number of points/vertices which can be reliably predicted
using a standard feed-forward network.
In this paper1 , we propose a novel approach to 3D-
reconstruction based on directly learning the continuous
3D occupancy function (Fig. 1d). Instead of predicting a
voxelized representation at a ﬁxed resolution, we predict
the complete occupancy function with a neural network fθ
which can be evaluated at arbitrary resolution. This dras-
tically reduces the memory footprint during training. At
inference time, we extract the mesh from the learned model
using a simple multi-resolution isosurface extraction algo-
rithm which trivially parallelizes over 3D locations.
In summary, our contributions are as follows:
• We introduce a new representation for 3D geometry
based on learning a continuous 3D mapping.
• We show how this representation can be used for re-
constructing 3D geometry from various input types.
• We experimentally validate that our approach is able
to generate high-quality meshes and demonstrate that
it compares favorably to the state-of-the-art.

2. Related Work

Existing work on learning-based 3D reconstruction can
be broadly categorized by the output representation they
produce as either voxel-based, point-based or mesh-based.

Voxel Representations: Due to their simplicity, voxels are
the most commonly used representation for discriminative
[45, 55, 63] and generative [9, 23, 58, 64, 75, 77] 3D tasks.
Early works have considered the problem of recon-
structing 3D geometry from a single image using 3D con-
volutional neural networks which operate on voxel grids
[9, 68, 77]. Due to memory requirements, however, these
approaches were limited to relatively small 323 voxel grids.
While recent works [74, 76, 79] have applied 3D convolu-
tional neural networks to resolutions up to 1283 , this is only
possible with shallow architectures and small batch sizes,
which leads to slow training.
The problem of reconstructing 3D geometry from mul-
tiple input views has been considered in [31, 35, 52].
Ji
et al. [31] and Kar et al. [35] encode the camera parame-
ters together with the input images in a 3D voxel represen-
tation and apply 3D convolutions to reconstruct 3D scenes
from multiple views. Paschalidou et al. [52] introduced an
architecture that predicts voxel occupancies from multiple
images, exploiting multi-view geometry constraints [69].
Other works applied voxel representations to learn gen-
erative models of 3D shapes. Most of these methods are

1Also see [8, 48, 51] for concurrent work that proposes similar ideas.

either based on variational auto-encoders [39, 59] or gener-
ative adversarial networks [25]. These two approaches were
pursued in [4, 58] and [75], respectively.

Due to the high memory requirements of voxel repre-
sentations, recent works have proposed to reconstruct 3D
objects in a multi-resolution fashion [28, 67]. However, the
resulting methods are often complicated to implement and
require multiple passes over the input to generate the ﬁnal
3D model. Furthermore, they are still limited to comparably
small 2563 voxel grids. For achieving sub-voxel precision,
several works [12, 42, 60] have proposed to predict truncated
signed distance ﬁelds (TSDF) [11] where each point in a
3D grid stores the truncated signed distance to the closest
3D surface point. However, this representation is usually
much harder to learn compared to occupancy representa-
tions as the network must reason about distance functions
in 3D space instead of merely classifying a voxel as occu-
pied or not. Moreover, this representation is still limited by
the resolution of the underlying 3D grid.

Point Representations: An interesting alternative repre-
sentation of 3D geometry is given by 3D point clouds which
are widely used both in the robotics and in the computer
graphics communities. Qi et al. [54, 56] pioneered point
clouds as a representation for discriminative deep learning
tasks. They achieved permutation invariance by applying a
fully connected neural network to each point independently
followed by a global pooling operation. Fan et al. [17] intro-
duced point clouds as an output representation for 3D recon-
struction. However, unlike other representations, this ap-
proach requires additional non-trivial post-processing steps
[3, 6, 37, 38] to generate the ﬁnal 3D mesh.

Mesh Representations: Meshes have ﬁrst been consid-
ered for discriminative 3D classiﬁcation or segmentation
tasks by applying convolutions on the graph spanned by the
mesh’s vertices and edges [5, 27, 71].

More recently, meshes have also been considered as out-
put representation for 3D reconstruction [26,33,41,70]. Un-
fortunately, most of these approaches are prone to generat-
ing self-intersecting meshes. Moreover, they are only able
to generate meshes with simple topology [70], require a
reference template from the same object class [33, 41, 57]
or cannot guarantee closed surfaces [26]. Liao et al. [43]
proposed an end-to-end learnable version of the marching
cubes algorithm [44]. However, their approach is still lim-
ited by the memory requirements of the underlying 3D grid
and hence also restricted to 323 voxel resolution.
In contrast to the aforementioned approaches, our ap-
proach leads to high resolution closed surfaces without self-
intersections and does not require template meshes from the
same object class as input. This idea is related to classical
level set [10, 14, 50] approaches to multi-view 3D recon-
struction [18, 24, 32, 40, 53, 78]. However, instead of solving

4461

a differential equation, our approach uses deep learning to
obtain a more expressive representation which can be natu-
rally integrated into an end-to-end learning pipeline.

3. Method

In this section, we ﬁrst introduce Occupancy Networks as
a representation of 3D geometry. We then describe how we
can learn a model that infers this representation from vari-
ous forms of input such as point clouds, single images and
low-resolution voxel representations. Lastly, we describe a
technique for extracting high-quality 3D meshes from our
model at test time.

3.1. Occupancy Networks

Ideally, we would like to reason about the occupancy not
only at ﬁxed discrete 3D locations (as in voxel respresenta-
tions) but at every possible 3D point p ∈ R3 . We call the
resulting function

o : R3 → {0, 1}

(1)

the occupancy function of the 3D object. Our key insight
is that we can approximate this 3D function with a neu-
ral network that assigns to every location p ∈ R3 an occu-
pancy probability between 0 and 1. Note that this network is
equivalent to a neural network for binary classiﬁcation, ex-
cept that we are interested in the decision boundary which
implicitly represents the object’s surface.
When using such a network for 3D reconstruction of an
object based on observations of that object (e.g., image,
point cloud, etc.), we must condition it on the input. Fortu-
nately, we can make use of the following simple functional
equivalence: a function that takes an observation x ∈ X
as input and has a function from p ∈ R3 to R as output
can be equivalently described by a function that takes a pair
(p, x) ∈ R3 × X as input and outputs a real number. The
latter representation can be simply parameterized by a neu-
ral network fθ that takes a pair (p, x) as input and outputs a
real number which represents the probability of occupancy:

fθ : R3 × X → [0, 1]

(2)

We call this network the Occupancy Network.

3.2. Training

To learn the parameters θ of the neural network fθ (p, x),
we randomly sample points in the 3D bounding volume of
the object under consideration: for the i-th sample in a train-
ing batch we sample K points pij ∈ R3 , j = 1, . . . , K . We
then evaluate the mini-batch loss LB at those locations:

LB (θ) =

1
|B |

|B|

K

X

i=1

X

j=1

L(fθ (pij , xi ), oij )

(3)

Here, xi is the i’th observation of batch B , oij ≡ o(pij ) de-
notes the true occupancy at point pij , and L(·, ·) is a cross-
entropy classiﬁcation loss.
The performance of our method depends on the sam-
pling scheme that we employ for drawing the locations
pij that are used for training.
In Section 4.6 we per-
form a detailed ablation study comparing different sampling
schemes. In practice, we found that sampling uniformly in-
side the bounding box of the object with an additional small
padding yields the best results.
Our 3D representation can also be used for learn-
ing probabilistic latent variable models.
Towards this
goal, we introduce an encoder network gψ (·) that takes
locations pij and occupancies oij as input and pre-
dicts mean µψ and standard deviation σψ of a Gaus-
sian distribution qψ (z |(pij , oij )j=1:K ) on latent z ∈ RL
as output. We optimize a lower bound [21, 39, 59]
to the negative log-likelihood of the generative model

p((oij )j=1:K |(pij )j=1:K ):

Lgen

B (θ , ψ) =

|B|

K

i=1

h

X

j=1

X

L(fθ (pij , zi ), oij )

1
|B |
+ KL (qψ (z |(pij , oij )j=1:K ) k p0 (z ))i

(4)

where KL denotes the KL-divergence, p0 (z ) is a prior dis-
tribution on the latent variable zi (typically Gaussian) and
zi is sampled according to qψ (zi |(pij , oij )j=1:K ).

3.3. Inference

For extracting the isosurface corresponding to a new ob-
servation given a trained occupancy network, we introduce
Multiresolution IsoSurface Extraction (MISE), a hierarchi-
cal isosurface extraction algorithm (Fig. 2). By incremen-
tally building an octree [30, 46, 66, 73], MISE enables us to
extract high resolution meshes from the occupancy network
without densely evaluating all points of a high-dimensional
occupancy grid.
We ﬁrst discretize the volumetric space at an initial reso-
lution and evaluate the occupancy network fθ (p, x) for all p
in this grid. We mark all grid points p as occupied for which
fθ (p, x) is bigger or equal to some threshold2 τ . Next, we
mark all voxels as active for which at least two adjacent
grid points have differing occupancy predictions. These are
the voxels which would intersect the mesh if we applied
the marching cubes algorithm at the current resolution. We
subdivide all active voxels into 8 subvoxels and evaluate all
new grid points which are introduced to the occupancy grid
through this subdivision. We repeat these steps until the
desired ﬁnal resolution is reached. At this ﬁnal resolution,

2 The threshold τ is the only hyperparameter of our occupancy network.
It determines the “thickness” of the extracted 3D surface. In our experi-
ments we cross-validate this threshold on a validation set.

4462

in (6) uses second order gradient information and can be ef-
ﬁciently implemented using Double-Backpropagation [15].
Note that this last step removes the discretization arti-
facts of the Marching Cubes approximation and would not
be possible if we had directly predicted a voxel-based rep-
resentation. In addition, our approach also allows to efﬁ-
ciently extract normals for all vertices of our output mesh
by simply backpropagating through the occupancy network.
In total, our inference algorithm requires 3s per mesh.

3.4. Implementation Details

We implemented our occupancy network using a fully-
connected neural network with 5 ResNet blocks [29] and
condition it on the input using conditional batch normal-
ization [13, 16]. We exploit different encoder architectures
depending on the type of input. For single view 3D recon-
struction, we use a ResNet18 architecture [29]. For point
clouds we use the PointNet encoder [54]. For voxelized in-
puts, we use a 3D convolutional neural network [45]. For
unconditional mesh generation, we use a PointNet [54] for
the encoder network gψ . More details are provided in the
supplementary material.

4. Experiments

We conduct three types of experiments to validate the
proposed occupancy networks. First, we analyze the repre-
sentation power of occupancy networks by examining how
well the network can reconstruct complex 3D shapes from a
learned latent embedding. This gives us an upper bound on
the results we can achieve when conditioning our represen-
tation on additional input. Second, we condition our occu-
pancy networks on images, noisy point clouds and low reso-
lution voxel representations, and compare the performance
of our method to several state-of-the-art baselines. Finally,
we examine the generative capabilities of occupancy net-
works by adding an encoder to our model and generating
unconditional samples from this model.4

Baselines: For the single image 3D reconstruction task, we
compare our approach against several state-of-the-art base-
lines which leverage various 3D representations: we eval-
uate against 3D-R2N2 [9] as a voxel-based method, Point
Set Generating Networks (PSGN) [17] as a point-based
technique and Pixel2Mesh [70] as well as AtlasNet [26] as
mesh-based approaches. For point cloud inputs, we adapted
3D-R2N2 and PSGN by changing the encoder. As mesh-
based baseline, we use Deep Marching Cubes (DMC) [43]
which has recently reported state-of-the-art results on this
task. For the voxel super-resolution task we assess the im-
provements wrt. the input.

4 The code to reproduce our experiments is available under https://
github.com/LMescheder/Occupancy-Networks.

4463

Figure 2: Multiresolution IsoSurface Extraction: We ﬁrst
mark all points at a given resolution which have already
been evaluated as either occupied (red circles) or unoccu-
pied (cyan diamonds). We then determine all voxels that
have both occupied and unoccupied corners and mark them
as active (light red) and subdivide them into 8 subvoxels
each. Next, we evaluate all new grid points (empty circles)
that have been introduced by the subdivision. The previous
two steps are repeated until the desired output resolution is
reached. Finally we extract the mesh using the marching
cubes algorithm [44], simplify and reﬁne the output mesh
using ﬁrst and second order gradient information.

we apply the Marching Cubes algorithm [44] to extract an
approximate isosurface

{p ∈ R3 | fθ (p, x) = τ }.

(5)

Our algorithm converges to the correct mesh if the occu-
pancy grid at the initial resolution contains points from ev-
ery connected component of both the interior and the ex-
terior of the mesh. It is hence important to take an initial
resolution which is high enough to satisfy this condition.
In practice, we found that an initial resolution of 323 was
sufﬁcient in almost all cases.
The initial mesh extracted by the Marching Cubes algo-
rithm can be further reﬁned.
In a ﬁrst step, we simplify
the mesh using the Fast-Quadric-Mesh-Simpliﬁcation algo-
rithm3 [20]. Finally, we reﬁne the output mesh using ﬁrst
and second order (i.e., gradient) information. Towards this
goal, we sample random points pk from each face of the
output mesh and minimize the loss

K

X

k=1

(fθ (pk , x) − τ )2 + λ (cid:13)(cid:13)(cid:13)(cid:13)

∇p fθ (pk , x)
k∇p fθ (pk , x)k

2

− n(pk )(cid:13)(cid:13)(cid:13)(cid:13)

(6)

where n(pk ) denotes the normal vector of the mesh at pk . In
practice, we set λ = 0.01. Minimization of the second term

3 https://github.com/sp4cerat/Fast-Quadric-Mesh- Simpliﬁcation

163

323

643

1283

ours

Figure 3: Discrete vs. Continuous. Qualitative comparison
of our continuous representation (right) to voxelizations at
various resolutions (left). Note how our representation en-
codes details which are lost in voxel-based representations.

Dataset: For all of our experiments we use the ShapeNet
[7] subset of Choy et al. [9]. We also use the same vox-
elization, image renderings and train/test split as Choy et al.
Moreover, we subdivide the training set into a training and
a validation set on which we track the loss of our method
and the baselines to determine when to stop training.
In order to generate watertight meshes and to determine
if a point lies in the interior of a mesh (e.g., for measuring
IoU) we use the code provided by Stutz et al. [64]. For a fair
comparison, we sample points from the surface of the wa-
tertight mesh instead of the original model as ground truth
for PSGN [17], Pixel2Mesh [70] and DMC [43]. All of our
evaluations are conducted wrt. these watertight meshes.

Metrics: For evaluation we use the volumetric IoU, the
Chamfer-L1 distance and a normal consistency score.
Volumetric IoU is deﬁned as the quotient of the volume
of the two meshes’ union and the volume of their intersec-
tion. We obtain unbiased estimates of the volume of the in-
tersection and the union by randomly sampling 100k points
from the bounding volume and determining if the points lie
inside our outside the ground truth / predicted mesh.
The Chamfer-L1 distance is deﬁned as the mean of an
accuracy and and a completeness metric. The accuracy met-
ric is deﬁned as the mean distance of points on the output
mesh to their nearest neighbors on the ground truth mesh.
The completeness metric is deﬁned similarly, but in oppo-
site direction. We estimate both distances efﬁciently by ran-
domly sampling 100k points from both meshes and using
a KD-tree to estimate the corresponding distances. Like
Fan et al. [17] we use 1/10 times the maximal edge length
of the current object’s bounding box as unit 1.
Finally, to measure how well the methods can capture
higher order information, we deﬁne a normal consistency
score as the mean absolute dot product of the normals in one
mesh and the normals at the corresponding nearest neigh-
bors in the other mesh.

4.1. Representation Power

In our ﬁrst experiment, we investigate how well occu-
pancy networks represent 3D geometry, independent of the
inaccuracies of the input encoding. The question we try
to answer in this experiment is whether our network can

Figure 4: IoU vs. Resolution. This plot shows the IoU
of a voxelization to the ground truth mesh (solid blue line)
in comparison to our continuous representation (solid or-
ange line) as well as the number of parameters per model
needed for the two representations (dashed lines). Note how
our representation leads to larger IoU wrt. the ground truth
mesh compared to a low-resolution voxel representation. At
the same time, the number of parameters of a voxel repre-
sentation grows cubically with the resolution, whereas the
number of parameters of occupancy networks is indepen-
dent of the resolution.

learn a memory efﬁcient representation of 3D shapes while
at the same time preserving as many details as possible.
This gives us an estimate of the representational capacity
of our model and an upper bound on the performance we
may expect when conditioning our model on additional in-
put. Similarly to [67], we embed each training sample in a
512 dimensional latent space and train our neural network
to reconstruct the 3D shape from this embedding.
We apply our method to the training split of the “chair”
category of the ShapeNet dataset. This subset is challeng-
ing to represent as it is highly varied and many models con-
tain high-frequency details. Since we are only interested
in reconstructing the training data, we do not use separate
validation and test sets for this experiment.
For evaluation, we measure the volumetric IoU to the
ground truth mesh. Quantitative results and a comparison
to voxel representations at various resolutions are shown
in Fig. 4. We see that the Occupancy Network (ONet) is
able to faithfully represent the entire dataset with a high
mean IoU of 0.89 while a low-resolution voxel represen-
tation is not able to represent the meshes accurately. At the
same time, the occupancy network is able to encode all 4746
training samples with as little as 6M parameters, indepen-
dently of the resolution. In contrast, the memory require-
ments of a voxel representation grow cubically with reso-
lution. Qualitative results are shown in Fig. 3. We observe
that the occupancy network enables us to represent details
of the 3D geometry which are lost in a low-resolution vox-
elization.

4464

Input

3D-R2N2

PSGN Pix2Mesh AtlasNet

Ours

Figure 5: Single Image 3D Reconstruction. The input im-
age is shown in the ﬁrst column, the other columns show
the results for our method compared to various baselines.

4.2. Single Image 3D Reconstruction

In our second experiment, we condition the occupancy
network on an additional view of the object from a random
camera location. The goal of this experiment is to eval-
uate how well occupancy functions can be inferred from
complex input. While we train and test our method on the
ShapeNet dataset, we also present qualitative results for the
KITTI [22] and the Online Products dataset [49].

ShapeNet:
In this experiment, we use a ResNet-18 image
encoder, which was pretrained on the ImageNet dataset. For
a fair comparison, we use the same image encoder for both
3D-R2N2 and PSGN5 . For PSGN we use a fully connected
decoder with 4 layers and 512 hidden units in each layer.
The last layer projects the hidden representation to a 3072
dimensional vector which we reshape into 1024 3D points.
As we use only a single input view, we remove the recur-
rent network in 3D-R2N2. We reimplemented the method
of [70] in PyTorch, closely following the Tensorﬂow imple-
mentation provided by the authors. For the method of [26],
we use the code and pretrained model from the authors6 .

For all methods, we track the loss and other metrics on
the validation set and stop training as soon as the target met-
ric reaches its optimum. For 3D-R2N2 and our method
we use the IoU to the ground truth mesh as target metric,
for PSGN and Pixel2Mesh we use the Chamfer distance to
the ground truth mesh as target metric. To extract the ﬁnal
mesh, we use a threshold of 0.4 for 3D-R2N2 as suggested
in the original publication [9]. To choose the threshold pa-
rameter τ for our method, we perform grid search on the
validation set (see supplementary) and found that τ = 0.2
yields a good trade-off between accuracy and completeness.
Qualitative results from our model and the baselines are
shown in Fig. 5. We observe that all methods are able to
capture the 3D geometry of the input image. However,
3D-R2N2 produces a very coarse representation and hence
lacks details.
In contrast, PSGN produces a high-ﬁdelity
output, but lacks connectivity. As a result, PSGN requires
additional lossy post-processing steps to produce a ﬁnal
mesh7 . Pixel2Mesh is able to create compelling meshes,
but often misses holes in the presence of more complicated
topologies. Such topologies are frequent, for example, for
the “chairs“ category in the ShapeNet dataset. Similarly,
AtlasNet captures the geometry well, but produces artifacts
in form of self-intersections and overlapping patches.
In contrast, our method is able to capture complex
topologies, produces closed meshes and preserves most of
the details. Please see the supplementary material for addi-
tional high resolution results and failure cases.
Quantitative results are shown in Table 1. We observe
that our method achieves the highest IoU and normal con-
sistency to the ground truth mesh. Surprisingly, while not
trained wrt. Chamfer distance as PSGN, Pixel2Mesh or At-
lasNet, our method also achieves good results for this met-
ric. Note that it is not possible to evaluate the IoU for PSGN
or AtlasNet, as they do not yield watertight meshes.

Real Data: To test how well our model generalizes to real
data, we apply our network to the KITTI [22] and Online
Products datasets [49]. To capture the variety in viewpoints
of KITTI and Online Products, we rerendered all ShapeNet
objects with random camera locations and retrained our net-
work for this task.
For the KITTI dataset, we additionally use the instance
masks provided in [2] to mask and crop car regions. We
then feed these images into our neural network to predict
the occupancy function. Some selected qualitative results
are shown in Fig. 6a. Despite only trained on synthetic data,
we observe that our method is also able to generate realistic
reconstructions in this challenging setting.
For the Online Products dataset, we apply the same pre-
trained model. Several qualitative results are shown in
Fig. 6b. Again, we observe that our method generalizes rea-

5 See supplementary for a comparison to the original architectures.
6 https://github.com/ThibaultGROUEIX/AtlasNet

7 See supplementary material for meshing results.

4465

3D-R2N2

IoU
PSGN Pix2Mesh AtlasNet ONet

3D-R2N2

Chamfer-L1
PSGN Pix2Mesh AtlasNet ONet

3D-R2N2

Normal Consistency
PSGN Pix2Mesh AtlasNet ONet

category

airplane
bench
cabinet
car
chair
display
lamp
loudspeaker
riﬂe
sofa
table
telephone
vessel

mean

0.426
0.373
0.667
0.661
0.439
0.440
0.281
0.611
0.375
0.626
0.420
0.611
0.482

0.493

-
-
-
-
-
-
-
-
-
-
-
-
-

-

0.420
0.323
0.664
0.552
0.396
0.490
0.323
0.599
0.402
0.613
0.395
0.661
0.397

0.480

-
-
-
-
-
-
-
-
-
-
-
-
-

-

0.571
0.485
0.733
0.737
0.501
0.471
0.371
0.647
0.474
0.680
0.506
0.720
0.530

0.571

0.227
0.194
0.217
0.213
0.270
0.314
0.778
0.318
0.183
0.229
0.239
0.195
0.238

0.278

0.137
0.181
0.215
0.169
0.247
0.284
0.314
0.316
0.134
0.224
0.222
0.161
0.188

0.215

0.187
0.201
0.196
0.180
0.265
0.239
0.308
0.285
0.164
0.212
0.218
0.149
0.212

0.216

0.104
0.138
0.175
0.141
0.209
0.198
0.305
0.245
0.115
0.177
0.190
0.128
0.151

0.147
0.155
0.167
0.159
0.228
0.278
0.479
0.300
0.141
0.194
0.189
0.140
0.218

0.175

0.215

0.629
0.678
0.782
0.714
0.663
0.720
0.560
0.711
0.670
0.731
0.732
0.817
0.629

0.695

-
-
-
-
-
-
-
-
-
-
-
-
-

-

0.759
0.732
0.834
0.756
0.746
0.830
0.666
0.782
0.718
0.820
0.784
0.907
0.699

0.772

0.836
0.779
0.850
0.836
0.791
0.858
0.694
0.825
0.725
0.840
0.832
0.923
0.756

0.840
0.813
0.879
0.852
0.823
0.854
0.731
0.832
0.766
0.863
0.858
0.935
0.794

0.811

0.834

Table 1: Single Image 3D Reconstruction. This table shows a numerical comparison of our approach and the baselines for
single image 3D reconstruction on the ShapeNet dataset. We measure the IoU, Chamfer-L1 distance and Normal Consistency
for various methods wrt. the ground truth mesh. Note that in contrast to prior work, we compute the IoU wrt. the high-
resolution mesh and not a coarse voxel representation. All methods apart from AtlasNet [26] are evaluated on the test split by
Choy et al. [9]. Since AtlasNet uses a pretrained model, we evaluate it on the intersection of the test splits from [9] and [26].

Input Reconstruction

Input Reconstruction

(a) KITTI

(b) Online Products

Figure 6: Qualitative results for real data. We applied our
trained model to the KITTI and Online Products datasets.
Despite only trained on synthetic data, our model general-
izes reasonably well to real data.

sonably well to real images despite being trained solely on
synthetic data. An additional quantitative evaluation on the
Pix3D dataset [65] can be found in the supplementary.

4.3. Point Cloud Completion

As a second conditional task, we apply our method to
the problem of reconstructing the mesh from noisy point
clouds. Towards this goal, we subsample 300 points from
the surface of each of the (watertight) ShapeNet models and
apply noise using a Gaussian distribution with zero mean
and standard deviation 0.05 to the point clouds.
Again, we measure both the IoU and Chamfer-L1 dis-
tance wrt. the ground truth mesh. The results are shown in
Table 2. We observe that our method achieves the highest

IoU

Chamfer-L1 Normal Consistency

3D-R2N2
PSGN
DMC
ONet

0.565
-
0.674
0.778

0.169
0.202
0.117
0.079

0.719
-
0.848
0.895

Table 2: 3D Reconstruction from Point Clouds. This ta-
ble shows a numerical comparison of our approach wrt. the
baselines for 3D reconstruction from point clouds on the
ShapeNet dataset. We measure IoU, Chamfer-L1 distance
and Normal Consistency wrt. the ground truth mesh.

IoU and normal consistency as well as the lowest Chamfer-
L1 distance. Note that all numbers are signiﬁcantly better
than for the single image 3D reconstruction task. This can
be explained by the fact that this task is much easier for the
recognition model, as there is less ambiguity and the model
only has to ﬁll in the gaps.

4.4. Voxel Super(cid:173)Resolution

As a ﬁnal conditional task, we apply occupancy net-
works to 3D super-resolution [62]. Here, the task is to re-
construct a high-resolution mesh from a coarse 323 vox-
elization of this mesh.
The results are shown in Table 3. We observe that our
model considerably improves IoU, Chamfer-L1 distance
and normal consistency compared to the coarse input mesh.
Please see the supplementary for qualitative results.

4.5. Unconditional Mesh Generation

Finally, we apply our occupancy network to uncondi-
tional mesh generation, training it separately on four cat-
egories of the ShapeNet dataset in an unsupervised fashion.
Our goal is to explore how well our model can represent

4466

IoU

Chamfer-L1 Normal Consistency

IoU

Chamfer-L1 Normal Consistency

Input
ONet

0.631
0.703

0.136
0.109

0.810
0.879

Table 3: Voxel Super-Resolution. This table shows a nu-
merical comparison of the output of our approach in com-
parison to the input on the ShapeNet dataset.

Uniform
Uniform (64)
Equal
Surface

0.571
0.554
0.475
0.536

0.215
0.256
0.291
0.254

0.834
0.829
0.835
0.822

(a) Inﬂuence of Sampling Strategy

IoU

Chamfer-L1 Normal Consistency

Full model
No ResNet
No CBN

0.571
0.559
0.522

0.215
0.243
0.301

0.834
0.831
0.806

(b) Inﬂuence of Occupancy Network Architecture

Table 4: Ablation Study. When we vary the sampling strat-
egy, we observe that uniform sampling in the bounding vol-
ume performs best. Similarly, when we vary the architec-
ture, we ﬁnd that our ResNet architecture with conditional
batch normalization yields the best results.

Figure 7: Unconditional 3D Samples. Random samples
of our unsupervised models trained on the categories “car“,
“airplane“, “sofa“ and “chair“ of the ShapeNet dataset. We
see that our models are able to capture the distribution of
3D objects and produce compelling new samples.

serve thickening artifacts in the model’s output. Moreover,
we ﬁnd that reducing the number of sampling points from
2048 to 64 still leads to good performance, although the
model does not perform as well as a model trained with
2048 sampling points.

the latent space of 3D models. Some samples are shown
in Figure 7. Indeed, we ﬁnd that our model can generate
compelling new models. In the supplementary material we
show interpolations in latent space for our model.

4.6. Ablation Study

In this section, we test how the various components of
our model affect its performance on the single-image 3D-
reconstruction task.

Effect of sampling strategy First, we examine how the
sampling strategy affects the performance of our ﬁnal
model. We try three different sampling strategies: (i) sam-
pling 2048 points uniformly in the bounding volume of the
ground truth mesh (uniform sampling), (ii) sampling 1024
points inside and 1024 points outside mesh (equal sam-
pling) and (iii) sampling 1024 points uniformly and 1024
points on the surface of the mesh plus some Gaussian noise
with standard deviation 0.1 (surface sampling). We also ex-
amine the effect of the number of sampling points by de-
creasing this number from 2048 to 64.
The results are shown in Table 4a. To our surprise, we
ﬁnd that uniform, the simplest sampling strategy, works
best. We explain this by the fact that other sampling strate-
gies introduce bias to the model: for example, when sam-
pling an equal number of points inside and outside the mesh,
we implicitly tell the model that every object has a volume
of 0.5. Indeed, when using this sampling strategy, we ob-

Effect of architecture To test the effect of the various
components of our architecture, we test two variations: (i)
we remove the conditional batch normalization and replace
it with a linear layer in the beginning of the network that
projects the encoding of the input to the required hidden
dimension and (ii) we remove all ResNet blocks in the de-
coder and replace them with linear blocks. The results are
presented in Table 4b. We ﬁnd that both components are
helpful to achieve good performance.

5. Conclusion

In this paper, we introduced occupancy networks, a new
representation for 3D geometry. In contrast to existing rep-
resentations, occupancy networks are not constrained by the
discretization of the 3D space and can hence be used to rep-
resent realistic high-resolution meshes.
Our experiments demonstrate that occupancy networks
are very expressive and can be used effectively both for su-
pervised and unsupervised learning. We hence believe that
occupancy networks are a useful tool which can be applied
to a wide variety of 3D tasks.

Acknowledgements

This work was supported by the Intel Network on Intel-
ligent Systems and by Microsoft Research through its PhD
Scholarship Programme.

4467

References

[1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. J. Guibas.
Learning representations and generative models for 3D point
clouds. In Proc. of the International Conf. on Machine learn-
ing (ICML), 2018. 1

[2] H. A. Alhaija, S. K. Mustikovela, L. Mescheder, A. Geiger,
and C. Rother. Augmented reality meets deep learning for
car instance segmentation in urban scenes. In Proc. of the
British Machine Vision Conf. (BMVC), 2017. 6

[3] F. Bernardini, J. Mittleman, H. Rushmeier, C. Silva, and
G. Taubin.
The ball-pivoting algorithm for surface re-
construction.
IEEE Trans. on Visualization and Computer
Graphics (VCG), 5(4):349–359, 1999. 2

[4] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Generative
and discriminative voxel modeling with convolutional neural
networks. arXiv.org, 1608.04236, 2016. 1, 2

[5] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Van-
dergheynst. Geometric deep learning: Going beyond eu-
clidean data.
Signal Processing Magazine, 34(4):18–42,
2017. 2

[6] F. Calakli and G. Taubin.
SSD: smooth signed dis-
tance surface reconstruction. Computer Graphics Forum,
30(7):1993–2002, 2011. 2

[7] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. ShapeNet: An information-rich 3D
model repository. arXiv.org, 1512.03012, 2015. 5

[8] Z. Chen and H. Zhang. Learning implicit ﬁelds for generative
shape modeling. Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), 2019. 2

[9] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3D-
R2N2: A uniﬁed approach for single and multi-view 3D ob-
ject reconstruction. In Proc. of the European Conf. on Com-
puter Vision (ECCV), 2016. 1, 2, 4, 5, 6, 7

[10] D. Cremers, M. Rousson, and R. Deriche. A review of statis-
tical approaches to level set segmentation: integrating color,
texture, motion and shape. International journal of computer
vision, 72(2):195–215, 2007. 2

[11] B. Curless and M. Levoy. A volumetric method for build-
ing complex models from range images. In ACM Trans. on
Graphics (SIGGRAPH), 1996. 2

[12] A. Dai, C. R. Qi, and M. Nießner. Shape completion using
3D-encoder-predictor CNNs and shape synthesis. In Proc.
IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2017. 2

[13] H. de Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin, and
A. C. Courville. Modulating early visual processing by lan-
guage. In Advances in Neural Information Processing Sys-
tems (NIPS), 2017. 4

[14] A. Dervieux and F. Thomasset. A ﬁnite element method for
the simulation of a rayleigh-taylor instability.
In Approxi-
mation methods for Navier-Stokes problems, pages 145–158.
Springer, 1980. 2

[15] H. Drucker and Y. Le Cun. Improving generalization perfor-
mance using double backpropagation. IEEE Trans. on Neu-
ral Networks, 3(6):991–997, 1992. 4

[16] V. Dumoulin,
I. Belghazi, B. Poole, O. Mastropietro,
A. Lamb, M. Arjovsky, and A. Courville. Adversarially
learned inference.
In Proc. of the International Conf. on
Learning Representations (ICLR), 2017. 4
[17] H. Fan, H. Su, and L. J. Guibas. A point set generation net-
work for 3D object reconstruction from a single image. In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2, 4, 5
[18] O. Faugeras and R. Keriven. Level set methods and the stereo
problem. In International Conference on Scale-Space Theo-
ries in Computer Vision, 1997. 2
[19] M. Gadelha, S. Maji, and R. Wang. 3D shape induction from
2d views of multiple objects. In Proc. of the International
Conf. on 3D Vision (3DV), 2017. 1
[20] M. Garland and P. S. Heckbert. Simplifying surfaces with
color and texture using quadric error metrics. In Visualiza-
tion’98. Proceedings, pages 263–269. IEEE, 1998. 4
[21] M. Garnelo, J. Schwarz, D. Rosenbaum, F. Viola, D. J.
Rezende, S. M. A. Eslami, and Y. W. Teh. Neural processes.
arXiv.org, 1807.01622, 2018. 3
[22] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision
meets robotics: The KITTI dataset.
International Journal
of Robotics Research (IJRR), 32(11):1231–1237, 2013. 6
[23] R. Girdhar, D. F. Fouhey, M. Rodriguez, and A. Gupta.
Learning a predictable and generative vector representation
for objects.
In Proc. of the European Conf. on Computer
Vision (ECCV), 2016. 1, 2
[24] B. Goldluecke and M. Magnor. Space-time isosurface evo-
lution for temporally coherent 3d reconstruction.
In Proc.
IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2004. 2
[25] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Bengio.
Generative adversarial nets. In Advances in Neural Informa-
tion Processing Systems (NIPS), 2014. 2
[26] T. Groueix, M. Fisher, V. G. Kim, B. Russell, and M. Aubry.
AtlasNet: A papier-m ˆach ´e approach to learning 3d surface
generation.
In Proc. IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), 2018. 2, 4, 6, 7
[27] K. Guo, D. Zou, and X. Chen. 3D mesh labeling via deep
convolutional neural networks. In ACM Trans. on Graphics
(SIGGRAPH), 2015. 2
[28] C. H ¨ane, S. Tulsiani, and J. Malik. Hierarchical surface pre-
diction for 3D object reconstruction. In Proc. of the Interna-
tional Conf. on 3D Vision (3DV), 2017. 2
[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition.
In Proc. IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), 2016. 4
[30] C. L. Jackins and S. L. Tanimoto. Oct-trees and their use in
representing three-dimensional objects. Computer Graphics
and Image Processing, 14(3):249–270, 1980. 3
[31] M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang. SurfaceNet:
an end-to-end 3D neural network for multiview stereopsis.
In Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2017. 2
[32] H. Jin, D. Cremers, A. J. Yezzi, and S. Soatto. Shedding
light on stereoscopic segmentation. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), 2004. 2

4468

[33] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
to-end recovery of human shape and pose.
In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2018. 2
[34] A. Kanazawa, S. Tulsiani, A. A. Efros, and J. Malik. Learn-
ing category-speciﬁc mesh reconstruction from image col-
lections. In Proc. of the European Conf. on Computer Vision
(ECCV), 2018. 1
[35] A. Kar, C. H ¨ane, and J. Malik. Learning a multi-view stereo
machine. In Advances in Neural Information Processing Sys-
tems (NIPS), 2017. 2
[36] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive
growing of GANs for improved quality, stability, and varia-
tion. In Proc. of the International Conf. on Learning Repre-
sentations (ICLR), 2018. 1
[37] M. M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface
reconstruction.
In Eurographics Symposium on Geometry
Processing (SGP), 2006. 2
[38] M. M. Kazhdan and H. Hoppe. Screened poisson surface
reconstruction. ACM Trans. on Graphics (SIGGRAPH),
32(3):29, 2013. 2
[39] D. P. Kingma and M. Welling. Auto-encoding variational
bayes. In Proc. of the International Conf. on Learning Rep-
resentations (ICLR), 2014. 2, 3
[40] K. Kolev, T. Brox, and D. Cremers. Robust variational seg-
mentation of 3d objects from multiple views. In Joint Pattern
Recognition Symposium, 2006. 2
[41] C. Kong, C.-H. Lin, and S. Lucey. Using locally correspond-
ing CAD models for dense 3D reconstructions from a single
image. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2017. 2
[42] L. Ladicky, O. Saurer, S. Jeong, F. Maninchedda, and
M. Pollefeys. From point clouds to mesh using regression.
In Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2017. 2
[43] Y. Liao, S. Donne, and A. Geiger. Deep marching cubes:
Learning explicit surface representations.
In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2018. 1, 2, 4, 5
[44] W. E. Lorensen and H. E. Cline. Marching cubes: A high
resolution 3D surface construction algorithm. In ACM Trans.
on Graphics (SIGGRAPH), 1987. 2, 4
[45] D. Maturana and S. Scherer. Voxnet: A 3D convolutional
neural network for real-time object recognition.
In Proc.
IEEE International Conf. on Intelligent Robots and Systems
(IROS), 2015. 2, 4
[46] D. Meagher. Geometric modeling using octree encoding.
Computer graphics and image processing, 19(2):129–147,
1982. 3
[47] L. Mescheder, A. Geiger, and S. Nowozin. Which training
methods for GANs do actually converge?
In Proc. of the
International Conf. on Machine learning (ICML), 2018. 1
[48] M. Michalkiewicz, J. K. Pontes, D. Jack, M. Baktashmot-
lagh, and A. Eriksson. Deep level sets: Implicit surface rep-
resentations for 3d shape inference. arXiv.org, 2019. 2
[49] H. Oh Song, Y. Xiang, S. Jegelka, and S. Savarese. Deep
metric learning via lifted structured feature embedding. In

Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2016. 6
[50] S. Osher and J. A. Sethian.
Fronts propagating with
curvature-dependent speed: algorithms based on hamilton-
jacobi formulations.
Journal of computational physics,
79(1):12–49, 1988. 2
[51] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Love-
grove. DeepSDF: Learning continuous signed distance func-
tions for shape representation. arXiv.org, 2019. 2
[52] D. Paschalidou, A. O. Ulusoy, C. Schmitt, L. van Gool, and
A. Geiger. Raynet: Learning volumetric 3D reconstruction
with ray potentials. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2018. 2
[53] J.-P. Pons, R. Keriven, and O. Faugeras. Modelling dynamic
scenes by registering multi-view image sequences. In Proc.
IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), 2005. 2
[54] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In Proc. IEEE Conf. on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 2, 4
[55] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. Guibas.
Volumetric and multi-view CNNs for object classiﬁcation on
3D data. In Proc. IEEE Conf. on Computer Vision and Pat-
tern Recognition (CVPR), 2016. 2
[56] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. PointNet++:
Deep hierarchical feature learning on point sets in a metric
space. In Advances in Neural Information Processing Sys-
tems (NIPS), 2017. 2
[57] A. Ranjan, T. Bolkart, S. Sanyal, and M. J. Black. Generating
3D faces using convolutional mesh autoencoders. In Proc. of
the European Conf. on Computer Vision (ECCV), 2018. 1, 2
[58] D. J. Rezende, S. M. A. Eslami, S. Mohamed, P. Battaglia,
M. Jaderberg, and N. Heess. Unsupervised learning of 3D
structure from images. In Advances in Neural Information
Processing Systems (NIPS), 2016. 1, 2
[59] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic
backpropagation and approximate inference in deep genera-
tive models. In Proc. of the International Conf. on Machine
learning (ICML), 2014. 2, 3
[60] G. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger. Oct-
NetFusion: Learning depth fusion from data. In Proc. of the
International Conf. on 3D Vision (3DV), 2017. 2
[61] G. Riegler, A. O. Ulusoy, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2017. 1
[62] E. Smith, S. Fujimoto, and D. Meger. Multi-view silhouette
and depth decomposition for high resolution 3d object rep-
resentation. In Advances in Neural Information Processing
Systems (NIPS). 2018. 7
[63] S. Song and J. Xiao. Deep sliding shapes for amodal 3D
object detection in RGB-D images.
In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), June
2016. 2
[64] D. Stutz and A. Geiger. Learning 3D shape completion from
laser scan data with weak supervision. In Proc. IEEE Conf.

4469

on Computer Vision and Pattern Recognition (CVPR), 2018.
1, 2, 5
[65] X. Sun, J. Wu, X. Zhang, Z. Zhang, C. Zhang, T. Xue, J. B.
Tenenbaum, and W. T. Freeman. Pix3d: Dataset and methods
for single-image 3d shape modeling. In Proc. IEEE Conf. on
Computer Vision and Pattern Recognition (CVPR), 2018. 7
[66] R. Szeliski. Rapid octree construction from image se-
quences. CVGIP: Image understanding, 58(1):23–32, 1993.
3
[67] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3D outputs.
In Proc. of the IEEE Interna-
tional Conf. on Computer Vision (ICCV), 2017. 1, 2, 5
[68] S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik. Multi-view
supervision for single-view reconstruction via differentiable
ray consistency.
In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2017. 2
[69] A. O. Ulusoy, A. Geiger, and M. J. Black. Towards prob-
abilistic volumetric reconstruction using ray potentials.
In
Proc. of the International Conf. on 3D Vision (3DV), 2015.
1, 2
[70] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang.
Pixel2Mesh: Generating 3D mesh models from single RGB
images. In Proc. of the European Conf. on Computer Vision
(ECCV), 2018. 1, 2, 4, 5, 6
[71] P. Wang, Y. Gan, Y. Zhang, and P. Shui. 3D shape segmen-
tation via shape fully convolutional networks. Computers &
Graphics, 1702.08675, 2017. 2
[72] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional GANs. In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2018.
1
[73] K.-Y. Wong and R. Cipolla. Structure and motion from sil-
houettes. In Proc. of the IEEE International Conf. on Com-
puter Vision (ICCV), 2001. 3
[74] J. Wu, Y. Wang, T. Xue, X. Sun, B. Freeman, and J. Tenen-
baum. MarrNet: 3D shape reconstruction via 2.5D sketches.
In Advances in Neural Information Processing Systems
(NIPS), 2017. 2
[75] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum.
Learning a probabilistic latent space of object shapes via 3D
generative-adversarial modeling. In Advances in Neural In-
formation Processing Systems (NIPS), 2016. 1, 2
[76] J. Wu, C. Zhang, X. Zhang, Z. Zhang, W. T. Freeman, and
J. B. Tenenbaum. Learning shape priors for single-view 3D
completion and reconstruction.
In Proc. of the European
Conf. on Computer Vision (ECCV), 2018. 2
[77] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumetric
shapes. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2015. 1, 2
[78] A. Yezzi and S. Soatto. Stereoscopic segmentation. Interna-
tional Journal of Computer Vision, 53(1):31–43, 2003. 2
[79] X. Zhang, Z. Zhang, C. Zhang, J. B. Tenenbaum, W. T. Free-
man, and J. Wu. Learning to reconstruct shapes from unseen
classes. In Advances in Neural Information Processing Sys-
tems (NIPS), 2018. 2

4470

Path-Invariant Map Networks

Zaiwei Zhang
UT Austin

Zhenxiao Liang
UT Austin

Lemeng Wu
UT Austin

Xiaowei Zhou
Zhejiang University∗

Qixing Huang
UT Austin†

Abstract

Optimizing a network of maps among a collection of ob-
jects/domains (or map synchronization) is a central prob-
lem across computer vision and many other relevant ﬁelds.
Compared to optimizing pairwise maps in isolation, the
beneﬁt of map synchronization is that there are natural con-
straints among a map network that can improve the qual-
ity of individual maps. While such self-supervision con-
straints are well-understood for undirected map networks
(e.g.,
the cycle-consistency constraint),
they are under-
explored for directed map networks, which naturally arise
when maps are given by parametric maps (e.g., a feed-
forward neural network). In this paper, we study a natural
self-supervision constraint for directed map networks called
path-invariance, which enforces that composite maps along
different paths between a ﬁxed pair of source and target
domains are identical. We introduce path-invariance bases
for efﬁcient encoding of the path-invariance constraint and
present an algorithm that outputs a path-variance basis
with polynomial time and space complexities. We demon-
strate the effectiveness of our approach on optimizing object
correspondences, estimating dense image maps via neural
networks, and semantic segmentation of 3D scenes via map
networks of diverse 3D representations. In particular, for
3D semantic segmentation our approach only requires 8%
labeled data from ScanNet to achieve the same performance
as training a single 3D segmentation network with 30% to
100% labeled data.

1. Introduction

Optimizing a network of maps among a collection of ob-
jects/domains (or map synchronization) is a central problem
across computer vision and many other relevant ﬁelds. Im-
portant applications include establishing consistent feature
correspondences for multi-view structure-from-motion [1,
11, 44, 5], computing consistent relative camera poses for
3D reconstruction [20, 17], dense image ﬂows [57, 56], im-
age translation [59, 52], and optimizing consistent dense

∗Xiaowei Zhou is afﬁliated with the StateKey Lab of CAD&CG and
the ZJU-SenseTime Joint Lab of 3D Vision.
† huangqx@cs.utexas.edu

Input Model

PCI

PCII

PCIII

Output Seg.

VOLI

VOLII

Input

Input

PCI

VOLI

PCI

VOLII

Output

Input

Output

Input

PCI

PCIII

PCI

PCII

Output

Input

Output

Input

PCI

PCI

PCI

PCII

PCII

PCIII

PCIII

Figure 1: (Left) A network of 3D representations for the task
of semantic segmentation of 3D scenes. (Right) Computed path-
invariance basis for regularizing individual neural networks.

correspondences for co-segmentation [47, 16, 48] and ob-
ject discovery [40, 8], just to name a few. The beneﬁt
of optimizing a map network versus optimizing maps be-
tween pairs of objects in isolation comes from the cycle-
consistency constraint [31, 18, 15, 47]. For example, this
constraint allows us to replace an incorrect map between a
pair of dissimilar objects by composing maps along a path
of similar objects [18]. Computationally, state-of-the-art
map synchronization techniques [3, 7, 15, 19, 43, 16, 18,
25, 57, 58, 29] employ matrix representations of maps [25,
18, 15, 48, 16]. This allows us to utilize a low-rank formu-
lation of the cycle-consistency constraint (c.f. [15]), leading
to efﬁcient and robust solutions [16, 58, 43, 19].
In this paper, we focus on a map synchronization set-
ting, where matrix-based map encodings become too costly
or even infeasible. Such instances include optimizing dense
ﬂows across many high-resolution images [30, 24, 41] or
optimizing a network of neural networks, each of which
maps one domain to another domain (e.g., 3D semantic seg-
mentation [12] maps the space of 3D scenes to the space
of 3D segmentations).
In this setting, maps are usually
encoded as broadly deﬁned parametric maps (e.g., feed-
forward neural networks), and map optimization reduces
to optimizing hyper-parameters and/or network parameters.
Synchronizing parametric maps introduces many technical
challenges. For example, unlike correspondences between
objects, which are undirected, a parametric map may not
have a meaningful inverse map. This raises the challenge

111084

of formulating an equivalent regularization constraint of
cycle-consistency for directed map networks. In addition,
as matrix-based map encodings are infeasible for paramet-
ric maps, another key challenge is how to efﬁciently enforce
the regularization constraint for map synchronization.
We introduce a computational framework for optimiz-
ing directed map networks that addresses the challenges de-
scribed above. Speciﬁcally, we propose the so-called path-
invariance constraint, which ensures that whenever there ex-
ists a map from a source domain to a target domain (through
map composition along a path), the map is unique. This
path-invariance constraint not only warrants that a map net-
work is well-deﬁned, but more importantly it provides a nat-
ural regularization constraint for optimizing directed map
networks. To effectively enforce this path-invariance con-
straint, we introduce the notion of a path-invariance basis,
which collects a subset of path pairs that can induce the
path-invariance property of the entire map network. We also
present an algorithm for computing a path-invariance basis
from an arbitrary directed map network. The algorithm pos-
sesses polynomial time and space complexities.
We demonstrate the effectiveness of our approach on
three settings of map synchronization. The ﬁrst setting con-
siders undirected map networks that can be optimized using
low-rank formulations [16, 58]. Experimental results show
that our new formulation leads to competitive and some-
times better results than state-of-the-art low-rank formula-
tions. The second setting studies consistent dense image
maps, where each pairwise map is given by a neural net-
work. Experimental results show that our approach signiﬁ-
cantly outperforms state-of-the-art approaches for comput-
ing dense image correspondences. The third setting consid-
ers a map network that consists of 6 different 3D representa-
tions (e.g., point cloud and volumetric representations) for
the task of semantic 3D semantic segmentation (See Fig-
ure 1). By enforcing the path-invariance of neural networks
on unlabeled data, our approach only requires 8% labeled
data from ScanNet [12] to achieve the same performance as
training a single semantic segmentation network with 30%
to 100% labeled data.

2. Related Works

Map synchronization. Most map synchronization tech-
niques [20, 17, 54, 31, 15, 57, 16, 7, 49, 5, 58, 2, 53, 19, 34,
43, 14, 56, 59, 52] have focused on undirected map graphs,
where the self-regularization constraint is given by cycle-
consistency. Depending on how the cycle-consistency con-
straint is applied, existing approaches fall into three cate-
gories. The ﬁrst category of methods [20, 17] utilizes the
fact that a collection of cycle-consistent maps can be gen-
erated from maps associated with a spanning tree. How-
ever, it is hard to apply them for optimizing cycle-consistent
neural networks, where the neural networks change during
the course of the optimization. The second category of ap-
proaches [54, 31, 57] applies constrained optimization to

select cycle-consistent maps. These approaches are typi-
cally formulated so that the objective functions encode the
score of selected maps, and the constraints enforce the con-
sistency of selected maps along cycles. Our approach is rel-
evant to this category of methods but addresses a different
problem of optimizing maps along directed map networks.

The third category of approaches apply modern numer-
ical optimization techniques to optimize cycle-consistent
maps. Along this line, people have introduced convex op-
timization [15, 16, 7, 49], non-convex optimization [5, 58,
2, 53, 19], and spectral techniques [34, 43]. To apply these
techniques for parametric maps, we have to hand-craft an
additional latent domain, as well as parametric maps be-
tween each input domain and this latent domain, which may
suffer from the issue of sub-optimal network design. In con-
trast, we focus on directed map networks among diverse do-
mains and explicitly enforce the path-invariance constraint
via path-invariance bases.
Joint learning of neural networks. Several recent works
have studied the problem of enforcing cycle-consistency
among a cycle of neural networks for improving the qual-
ity of individual networks along the cycle. Zhou et al. [56]
studied how to train dense image correspondences between
real image objects through two real-2-synthetic networks
and ground-truth correspondences between synthetic im-
ages.
[59, 52] enforce the bi-directional consistency of
transformation networks between two image domains to im-
prove the image translation results. People have applied
such techniques for multilingual machine translation [21].
However, in these works the cycles are explicitly given.
In contrast, we study how to extend the cycle-consistency
constraint on undirected graphs to the path-invariance con-
straint on directed graphs. In particular, we focus on how
to compute a path-invariance basis for enforcing the path-
invariance constraint efﬁciently. A recent work [55] studies
how to build a network of representations for boosting in-
dividual tasks. However, self-supervision constraints such
as cycle-consistency and path-invariance are not employed.
Another distinction is that our approach seeks to leverage
unlabeled data, while [55] focuses on transferring labeled
data under different representations/tasks. Our approach is
also related to model/data distillation (See [38] and refer-
ences therein), which can be considered as a special graph
with many edges between two domains. In this paper, we
focus on deﬁning self-supervision for general graphs.
Cycle-bases. Path-invariance bases are related to cycle-
bases on undirected graphs [22], in which any cycle of
a graph is given by a linear combination of the cycles
in a cycle-basis. However, besides fundamental cycle-
bases [22] that can generalize to deﬁne cycle-consistency
bases, it is an open problem whether other types of cycle-
bases generalize or not. Moreover, there are fundamental
differences between undirected and directed map networks.
This calls for new tools for deﬁning and computing path-
invariance bases.

11085

3. Path-Invariance of Directed Map Networks

In this section, we focus on the theoretical contribution
of this paper, which introduces an algorithm for comput-
ing a path-invariance basis that enforces the path-invariance
constraint of a directed map network. Note that the proofs
of theorems and propositions in this section are deferred to
the supplementary material.

3.1. Path(cid:173)Invariance Constraint

We ﬁrst deﬁne the notion of a directed map network:

Deﬁnition 1. A directed map network F is an attributed
directed graph G = (V , E ) where V = {v1 , . . . , v|V | }.
Each vertex vi ∈ V is associated with a domain Di . Each
edge e ∈ E with e = (i, j ) is associated with a map
fij : Di → Dj . In the following, we always assume E con-
tains the self-loop at each vertex, and the map associated
with each self-loop is the identity map.

For simplicity, whenever it can be inferred from the con-
text we simplify the terminology of a directed map network
as a map network. The following deﬁnition considers in-
duced maps along paths of a map network.

Deﬁnition 2. Consider a path p = (i0 , · · · , ik ) along G .
We deﬁne the composite map along p induced from a map
network F on G as

fp = fik−1 ik ◦ · · · ◦ fi0 i1 .

(1)

We also deﬁne f∅ := I where ∅ can refer to any self-loop.

In the remaining text, for two successive paths p and q ,
we use p ∼ q to denote their composition.
Now we state the path-invariance constraint.

Deﬁnition 3. Let Gpath (u, v) collect all paths in G that con-
nect u to v . We deﬁne the set of all possible path pairs of G
as

{(p, q)|p, q ∈ Gpath (u, v)}.

Gpair = [
u,v∈V

We say F is path-invariant if

fp = fq ,

∀(p, q) ∈ Gpair .

(2)

Remark 1. It is easy to check that path-invariance induces
cycle-consistency (c.f.[15]), but cycle-consistency does not
necessarily induce path-invariance. For example, a map
network with three vertices {a, b, c} and three directed
maps fab , fbc , and fac has no-cycle, but one path pair
(fbc ◦ fab , fac ).

3.2. Path(cid:173)Invariance Basis

pairs. This raises the question of how to compute a path-
invariance basis B ⊂ Gpair , which is a small set of path pairs
that are sufﬁcient for enforcing the path-invariance property
of any map network F . To rigorously deﬁne path-invariance
basis, we introduce three primitive operations on path pairs
merge, stitch and cut(See Figure 2):

Deﬁnition 4. Consider a directed graph G . We say two path
pairs (p, q) and (p′ , q ′ ) are compatible if one path in {p, q}
is a sub-path of one path in {p′ , q ′ } or vice-versa. Without
losing generality, suppose p is a sub-path of p′ and we write
p′ = r ∼ p ∼ r ′ , which stitches three sub-paths r ,p,and r ′
in order. We deﬁne the merge operation so that it takes two
compatible path pairs (p, q) and (r ∼ p ∼ r ′ , q ′ ) as input
and outputs a new path pair (r ∼ q ∼ r ′ , q ′ ).

We proceed to deﬁne the stitch operation:

Deﬁnition 5. We deﬁne the stitch operation so that it
takes as input two path pairs (p, q), p, q ∈ Gpath (u, v) and
(p′ , q ′ ), p′ , q ′ ∈ Gpath (v , w) and outputs (p ∼ p′ , q ∼ q ′ ).

Finally we deﬁne the cut operation on two cycles, which
will be useful for strongly connected graphs:

Deﬁnition 6. Operation cut takes as input two path pairs
(C1 , ∅) and (C2 , ∅) where C1 and C2 are two distinct cycles
that have two common vertices u, v and share a common
path from v to u. Speciﬁcally, we assume these two cycles
are u
−→ u and u
−→ u where p, p′ ∈ Gpath (u, v)
and q ∈ Gpath (v , u). We deﬁne the output of the cut opera-
tion as a new path pair (p, p′ ).

−→ v

p′

q

p

−→ v

q

Deﬁnition 6 is necessary because fp ◦ fq = fp′ ◦ fq = I
implies fp = fp′ . As we will see later, this operation is
useful for deriving new path-invariance basis.
Now we deﬁne path-invariance basis, which is the criti-
cal concept of this paper:

Deﬁnition 7. We say a collection of path pairs B = {(p, q)}
is a path-invariance basis on G if every path-pair (p, q) ∈
Gpair \ B can be induced from a subset of B through a series
of merge, stitch and/or cut operations.

The following proposition shows the importance of path-
invariance basis:

p

′

q q

r

′

r

p

q

q

′

q

′

pp

′

qq

′ p q

p

′ p

p

′

p

′

q

A challenge of enforcing the path-invariant constraint is
that there are many possible paths between each pair of do-
mains in a graph, leading to an intractable number of path

merge

stitch

cut

Figure 2: Illustrations of Operations

11086

Proposition 1. Consider a path-invariance basis B of a
graph G . Then for any map network F on G , if

fp = fq ,

(p, q) ∈ B ,

then F is path-invariant.

3.3. Path(cid:173)Invariance Basis Computation

in |V |,

We ﬁrst discuss the criteria for path-invariance basis
computation. Since we will formulate a loss term for each
path pair in a path-invariance basis, we place the following
three objectives. First, we require the length of the paths in
each path pair to be small. Intuitively, enforcing the con-
sistency between long paths weakens the regularization on
each involved map. Second, we want the size of the re-
sulting path-invariance basis to be small to increase the ef-
ﬁciency of gradient-descent based optimization strategies.
Finally, we would like the resulting path-invariance basis
to be nicely distributed to improve the convergence prop-
erty of the induced optimization problem. Unfortunately,
achieving these goals exactly appears to be intractable. For
example, we conjecture that computing a path-invariance
basis of a given graph with minimum size is NP-hard1 .
In light of this, our approach seeks to compute a path-
invariance basis whose size is polynomial
i.e.,
O(|V ||E |) in the worst case. Our approach builds upon the
classical result that a directed graph G can be factored into a
directed acyclic graph (or DAG) whose vertices are strongly
connected components of G (c.f. [4]). More precisely, we
ﬁrst show how to compute a path-invariance basis for a
DAG. We then discuss the case of strongly connected com-
ponents. Finally, we show how to extend the result of the
ﬁrst two settings to arbitrary directed graphs. Note that our
approach implicitly takes two other criteria into account.
Speciﬁcally, we argue that small path-invariance basis fa-
vors short path-pairs, as it is less likely to combine long
path-pairs to produce new path-pairs through merge, stitch
and cut operations. In addition, this construction takes the
global structure of the input graph G into account, leading
to nicely distributed path-pairs.
Directed acyclic graph (or DAG). Our algorithm utilizes
an important property that every DAG admits a topological
order of vertices that are consistent with the edge orienta-
tions (c.f. [4]). Speciﬁcally, consider a DAG G = (V , E ).
A topological order is a bijection σ : {1, · · · , |V |} → V
so that we have σ−1 (u) < σ−1 (v) whenever (u, v) ∈ E .
A topological order of a DAG can be computed by Tarjan’s
algorithm (c.f. [46]) in linear time.
Our algorithm starts with a current graph Gcur = (V , ∅)
to which we add all edges in E in some order later. Speciﬁ-
cally, the edges in E will be visited with respect to a (partial)
edge order ≺ where ∀(u, v), (u′ , v ′ ) ∈ E , (u, v) ≺ (u′ , v ′ )
if and only if σ−1 (v) < σ−1 (v ′ ). Note that two edges
(u, v), (u′ , v) with the same head can be in arbitrary order.

1Unlike cycle bases that have a known minimum size (c.f. [22]), the
sizes of minimum path-invariance bases vary

1:

Algorithm 1 The high level algorithm ﬂow to ﬁnd a path-
invariance basis.
input: Directed graph G = (V , E ).
output: Path-invariance basis B .
Calculate SCCs G1 , . . . , GK for G and the resulting
contracted DAG Gdag .
Calculate a path-invriance basis Bdag for Gdag and
transform Bdag to Bdag that collect path pairs on G .
Calculate a path-invariance basis Bi for Gi .
Calculate path-invirance pairs Bij whenever Gi can
reach Gj in Gdag .
return B = Bdag S (cid:0) ∪K

i=1 Bi (cid:1) S (cid:0) ∪ij Bij (cid:1)

2:

3:

4:

5:

For each newly visited edge (u, v) ∈ E , we collect a set
of candidate vertices P ⊂ V such that every vertex w ∈ P
can reach both u and v in Gcur . Next we construct a set P
by removing from P all w ∈ P such that w can reach some
distinct w ′ ∈ P . In other words, w is redundant because of
w ′ in this case. For each vertex w ∈ P , we collect a new
path-pair (p′ , p ∼ uv), where p and p′ are shortest paths
from w to u and v , respectively. After collecting path pairs,
we augment Gcur with (u, v). With Bdag (σ) we denote the
resulting path-pair set after Ecur = E .

Theorem 3.1. Every topological order σ of G returns a
path-invariance basis Bdag (σ) whose size is at most |V ||E |.

Strongly connected graph (or SCG). To construct a path-
invariance basis of a SCG G , we run a slightly-modiﬁed
depth-ﬁrst search on G from arbitrary vertex. Since G is
strongly connected, the resulting spanning forest must be
a tree, denoted by T . The path pair set B is the result we
obtain. In addition, we use a Gdag to collect a acyclic sub-
graph of G and initially it is set as empty. When traversing
edge (u, v), if v is visited for the ﬁrst time, then we add
(u, v) to both T and Gdag . Otherwise, there can be two
possible cases:

• v is an ancestor of u in T . In this case we add cycle
pair (P ∼ (u, v), ∅), where P is the tree path from v
to u, into B .

• Otherwise, add (u, v) into Gdag .

We can show that Gdag is indeed an acyclic graph (See
Section A.3 in the supplementary material). Thus we can
obtain a path-invariance basis on Gdag by running the con-
struction procedure introduced for DAG. We add this basis
into B .

Proposition 2. The path pair set B constructed above is a
path-invariance basis of G .

General directed graph. Given path-invariance bases con-
structed on DAGs and SCGs, constructing path-invariance
bases on general graphs is straight-forward. Speciﬁcally,

11087

consider strongly connected components Gi , 1 ≤ i ≤ K of
a graph G . With Gdag we denote the DAG among Gi , 1 ≤
i ≤ K . We ﬁrst construct path-invariance bases Bdag and
Bi for Gdag and each Gi , respectively. We then construct a
path-invariance basis B of G by collecting three groups of
path pairs. The ﬁrst group simply combines Bi , 1 ≤ i ≤ K .
The second group extends Bdag to the original graph. This
is done by replacing each edge (Gi , Gj ) ∈ Edag through a
shortest path on G that connects the representatives of Gi
and Gj where representatives are arbitrarily chosen at ﬁrst
for each component. To calculate the third group, consider
all oriented edges between each (Gi , Gj ) ∈ Edag :

Eij = {uv ∈ E : u ∈ Vi , v ∈ Vj }.

Note that when constructing Bdag , all edges in Eij are
shrinked to one edge in Edag . This means when construct-
ing B , we have to enforce the consistency among Eij on the
original graph G . This can be done by constructing a tree
Tij where V (Tij ) = Eij , E (Tij ) ⊂ E 2
ij . Tij is a minimum
spanning tree on the graph whose vertex set is Eij and the
weight associated with edge (uv , u′ v ′ ) ∈ E 2
ij is given by
the sum of lengths of cuu′ and cvv ′ . This strategy encourages
reducing the total length of the resulting path pairs in Bij :
Bij := {( cuu′ ∼ u′ v ′ , uv ∼ cvv ′ ) : (uv , u′ v ′ ) ∈ E (Tij )},
where cuu′ and cvv ′ denote the shortest paths from u to u′
on Gi and from v to v ′ on Gj , respectively. Algorithm 1
presents the high-level pesudo code of our approach.

Theorem 3.2. The path-pairs B derived from Bdag ,
{Bi : 1 ≤ i ≤ K }, and {Bij : (Gi , Gj ) ∈ Edag } using the
algorithm described above is a path-invariance basis for G .

Proposition 3. The size of B is upper bounded by |V ||E |.

4. Joint Map Network Optimization

θij

In this section, we present a formulation for jointly op-
timizing a map network using the path-variance basis com-
puted in the preceding section.
Consider the map network deﬁned in Def. 1. We assume
the map associated with each edge (i, j ) ∈ E is a parametric
map f
ij , where θij denotes hyper-parameters or network
parameters of fij . We assume the supervision of map net-
work is given by a superset E ⊃ E . As we will see later,
such instances happen when there exist paired data between
two domains, but we do not have a direct neural network
between them. To utilize such supervision, we deﬁne the in-
duced map along an edge (i, j ) ∈ E as the composition map
(deﬁned in (1)) f Θ
along the short path dvi vj from vi to vj .
Here Θ = {θij , (i, j ) ∈ E } collects all the parameters. We
deﬁne each supervised loss term as lij (f Θ
ij ), ∀(i, j ) ∈ E .
The speciﬁc deﬁnition of lij will be deferred to Section 5.
Besides the supervised loss terms, the key component of
joint map network optimization utilizes a self-supervision

dvi vj

loss induced from the path-invariance basis B . Let dDi (·, ·)
be a distance measure associated with domain Di . Consider
an empirical distribution Pi of Di . We deﬁne the total loss
objective for joint map network optimization as

min

Θ

X

lij (f Θ

dvi vj )+λ X

(i,j )∈E

(p,q)∈B

E

v∼Ppt

dDpt

(f Θ
p (v), f Θ

q (v))

(3)
where pt denotes the index of the end vertex of p. Es-
sentially, (3) combines the supervised loss terms and an
unsupervised regularization term that ensures the learned
representations are consistent when passing unlabeled in-
stances across the map network. We employ the ADAM
optimizer [27] for optimization. In addition, we start with a
small value of λ, e.g., λ = 10−2 , to solve (3) for 40 epochs.
We then double the value of λ every 10 epochs. We stop the
training procedure when λ ≥ 103 . The training details are
deferred to the Appendix.

5. Experimental Evaluation

This section presents an experimental evaluation of our
approach across three settings, namely, shape matching
(Section 5.1), dense image maps (Section 5.2), and 3D se-
mantic segmentation (Section 5.3).

5.1. Map Network of Shape Maps

We begin with the task of joint shape matching [31, 25,
15, 16, 10], which seeks to jointly optimize shape maps to
improve the initial maps computed between pairs of shapes
in isolation. We utilize the functional map representation
described in [33, 47, 16]. Speciﬁcally, each domain Di is
given by a linear space spanned by the leading m eigenvec-
tors of a graph Laplacian [16] (we choose m = 30 in our
experiments). The map from Di to Dj is given by a ma-
trix Xij ∈ Rm×m . Let B be a path-invariance basis for the
associated graph G . Adapting (3), we solve the following
optimization problem for joint shape matching:
ij k1 + λ X
kXij − X in

kXp − Xq k2

X

F

(4)

(i,j )∈E

(p,q)∈B

where k · k1 and k · kF are the element-wise L1-norm and the
matrix Frobenius norm, respectively. X in
ij denotes the ini-
tial functional map converted from the corresponding initial
shape map using [33].
Dataset.
We perform experimental evaluation on
SHREC07–Watertight
[13].
Speciﬁcally, SHREC07-
Watertight contains 400 shapes across 20 categories.
Among them, we choose 11 categories (i.e., Human,
Glasses, Airplane, Ant, Teddy, Hand, Plier, Fish, Bird,
Armadillo, Fourleg) that are suitable for inter-shape map-
ping. We also test our approach on two large-scale
datasets Aliens (200 shapes) and Vase (300 shapes) from
ShapeCOSEG [50]. For initial maps, we employ blended
intrinsic maps [26], a state-of-the-art method for shape

11088

0.02

0.04

0.06

0.08

0.1

10

20

30

40

50

60

70

80

Geodesic Error

%

C

o

r
r

s
e
c
n
e
d
n
o
p
s
e

Shape Matching (SHREC07, Clique)

BIM
Cosmo17
Zhou15
Huang14

Rand−Cycle
Fund.−Cycle

Ours

0.02

0.04

0.06

0.08

0.1

10

20

30

40

50

60

70

80

Geodesic Error

%

C

o

r
r

s
e
c
n
e
d
n
o
p
s
e

Shape Matching (SHREC07, Sparse)

Input
Cosmo17
Zhou15
Huang14

Rand−Cycle
Fund.−Cycle

Ours

0.02

0.04

0.06

0.08

0.1

10

20

30

40

50

60

70

80

Geodesic Error

%

C

o

r
r

s
e
c
n
e
d
n
o
p
s
e

Shape Matching (SPCoSeg, Clique)

Input
Cosmo17
Zhou15
Huang14

Rand−Cycle
Fund.−Cycle

Ours

0.02

0.04

0.06

0.08

0.1

10

20

30

40

50

60

70

80

Geodesic Error

%

C

o

r
r

s
e
c
n
e
d
n
o
p
s
e

Shape Matching (SPCoSeg, Sprase)

Input
Cosmo17
Zhou15
Huang14

Rand−Cycle
Fund.−Cycle

Ours

Figure 3: Baseline comparison on benchmark datasets. We show
cumulative distribution functions (or CDFs) of each method with
respect to annotated feature correspondences.

matching. We test our approach under two graphs G . The
ﬁrst graph is a clique graph. The second graph connects
each shape with k-nearest neighbor with respect to the
GMDS descriptor [42] (k = 10 in our experiments).
Evaluation setup. We compare our approach to ﬁve base-
line approaches, including three state-of-the-art approaches
and two variants of our approach. Three state-of-the-art ap-
proaches are 1) functional-map based low-rank matrix re-
covery [16], 2) point-map based low-rank matrix recovery
via alternating minimization [58], and 3) consistent partial
matching via sparse modeling [10]. Two variants are 4) us-
ing a set of randomly sampled cycles [54] whose size is the
same as |B |, and 5) using the path-invariance basis derived
from the fundamental cycle-basis of G (c.f. [22]) (which
may contain long cycles).
We evaluate the quality of each map through annotated
key points (Please refer to the supplementary material). Fol-
lowing [26, 15, 16], we report the cumulative distribution
function (or CDF) of geodesic errors of predicted feature
correspondences.
Analysis of results. Figure 3 shows CDFs of our ap-
proach and baseline approaches. All participating methods
exhibit considerable improvements from the initial maps,
demonstrating the beneﬁts of joint matching. Compared
to state-of-the-art approaches, our approach is comparable
when G is a clique and exhibits certain performance gains
when G is sparse. One explanation is that low-rank ap-
proaches are based on relaxations of the cycle-consistency
constraint (c.f. [15]), and such relaxations become loose
on sparse graphs. Compared to the two variants, our ap-
proach delivers the best results on both clique graphs and
knn-graphs. This is because the two alternative strategies
generate many long paths and cycles in B , making the to-
tal objective function (3) hard to optimize. On knn-graphs,

both our approach and the baseline of using the fundamental
cycle-basis outperform the baseline of randomly sampling
path pairs, showing the importance of computing a path-
invariance basis for enforcing the consistency constraint.

5.2. Map Network of Dense Image Maps

In the second setting, we consider the task of optimizing
dense image ﬂows across a collection of relevant images.
We again model this task using a map network F , where
each domain Di is given by an image Ii . Our goal is to com-
pute a dense image map fij : Ii → Ij (its difference to the
identity map gives a dense image ﬂow) between each pair
of input images. To this end, we precompute initial dense
maps f in
ij , ∀(i, j ) ∈ E using DSP [24], which is a state-
of-the-art approach for dense image ﬂows. Our goal is to
obtain improved dense image maps fij , ∀(i, j ) ∈ E , which
lead to dense image maps between all pairs of images in F
via map composition (See (1)). Due to scalability issues,
state-of-the-art approaches for this task [28, 23, 36, 57] are
limited to a small number of images. To address this issue,
we encode dense image maps using the neural network f θ
described in [56]. Given a ﬁxed map network F and the
initial dense maps f in
ij , (i, j ) ∈ E , we formulate a similar
optimization problem as (4) to learn θ :
ij k1 + λ X

min

θ

X

(i,j )∈E

kf θ

ij − f in

(p,q)∈B

kf θ
p − f θ

q k2

F (5)

where B denotes a path-invariance basis associated with F ;
ps is the index of the start vertex of p; f θ
p is the composite
network along path p.
Dataset. The image sets we use are sampled from 12 rigid
categories of the PASCAL-Part dataset [6]. To generate im-
age sets that are meaningful to align, we pick the most pop-
ular view for each category (who has the smallest variance
among 20-nearest neighbors). We then generate an image
set for that category by collecting all images whose poses
are within 30◦ of this view. We construct the map network
by connecting each image with 20-nearest neighbors with
respect to the DSP matching score [24]. Note that the re-
sulting F is a directed graph as DSP is directed. The longest
path varies between 4(Car)-6(Boat) in our experiments.
Evaluation setup. We compare our approach with Con-
gealing [28], Collection Flow [23], RASL [36], and
FlowWeb [57]. Note that both Flowweb and our ap-
proach use DSP as input. We also compare our approach
against [56] under a different setup (See supplementary ma-
terial). To run baseline approaches, we follow the protocol
of [57] to further break each dataset into smaller ones with
maximum size of 100. In addition, we consider two variants
of our approach: Ours-Dense and Ours-Undirected. Ours-
Dense uses the clique graph for F . Ours-Undirected uses
an undirected knn-graph, where each edge weight averages
the bi-directional DSP matching scores (c.f. [24]). We em-
ploy the PCK measure [51], which reports the percentage of

11089

aero bike boat bottle
Congealing
0.13 0.24 0.05
0.21
RASL
0.18 0.20 0.05
0.36
CollectionFlow 0.17 0.18 0.06
0.33
DSP
0.19 0.33 0.07
0.21
FlowWeb
0.31 0.42 0.08
0.37
Ours-Dense
0.29 0.42 0.07
0.39
Ours-Undirected 0.32 0.43 0.07
0.43
Ours
0.35 0.45 0.07
0.45

bus
car
chair
0.22 0.11 0.09
0.33 0.19 0.14
0.31 0.15 0.15
0.36 0.37 0.12
0.56 0.51 0.12
0.53 0.55 0.11
0.56 0.55 0.18
0.63 0.62 0.19

table mbike sofa train
tv mean
0.05
0.14
0.09 0.10 0.09
0.13
0.06
0.19
0.13 0.14 0.29
0.19
0.04
0.12
0.11 0.10 0.11
0.12
0.07
0.19
0.13 0.15 0.21
0.20
0.06
0.23
0.18 0.19 0.34
0.28
0.06
0.22
0.18 0.21 0.31
0.28
0.06
0.26
0.21 0.25 0.37
0.31
0.06
0.27
0.22 0.23 0.38
0.33

50

40

30

20

10

s
e
c
n
e
d
n
o
p
s
e

r
r

o

C

%

Dense Image Map

0.02 0.04 0.06 0.08 0.1

PCK Error

Congealing
RASL
CollectionFlow
DSP
FlowWeb

Ours−Dense
Ours−Undirected

Ours

Figure 4: (Left) Keypoint matching accuracy (PCK) on 12 rigid PASCAL VOC categories (α = 0.05). Higher is better. (Right) Plots of
the mean PCK of each method with varying α

Source

Target

Congealing

RASL

CollectionFlow

DSP

FlowWeb

Ours

Figure 5: Visual comparison between our approach and state-of-the-art approaches. This ﬁgure is best viewed in color, zoomed in. More
examples are included in the supplemental material.

keypoints whose prediction errors fall within α · max(h, w)
(h and w are image height and width respectively).
Analysis of results. As shown in Figure 4 and Figure 5, our
approach outperforms all existing approaches across most
of the categories. Several factors contribute to such im-
provements. First, our approach can jointly optimize more
images than baseline approaches and thus beneﬁts more
from the data-driven effect of joint matching [15, 7]. This
explains why all variants of our approach are either compa-
rable or superior to baseline approaches. Second, our ap-
proach avoids ﬁtting a neural network directly to dissimilar
images and focuses on relatively similar images (other maps
are generated by map composition), leading to additional
performance gains. In fact, all existing approaches, which
operate on sub-groups of similar images, also implicitly
beneﬁt from map composition. This explains why FlowWeb
exhibits competing performance against Ours-Dense. Fi-
nally, Ours-Directed is superior to Ours-Undirected. This is
because the outlier-ratio of f in
ij in Ours-Undirected is higher
than that of Ours-Directed, which selects edges purely
based on matching scores.

5.3. Map Network of 3D Representations

In the third setting, we seek to jointly optimize a net-
work of neural networks to improve the performance of in-
dividual networks. We are particularly interested in the task
of semantic segmentation of 3D scenes. Speciﬁcally, we
consider a network with seven 3D representations (See Fig-
ure 1). The ﬁrst representation is the input mesh. The last
representation is the space of 3D semantic segmentations.
The second to fourth 3D representations are point clouds
with different number of points: PCI (12K), PCII (8K), and
PCIII(4K). The motivation of varying the number of points

PCI PCII PCIII VOLI VOLII ENS
100% Label (Isolated)
84.2 83.3 83.4 81.9
81.5
85
8% Label (Isolated)
79.2 78.3 78.4 78.7
77.4 81.4
8% Label + Unlabel (Joint) 82.3 82.5 82.3 81.6
79.0 83.4
30% Label (Isolated)
80.8 81.9 81.2 80.3
79.5 83.2

Table 1: Semantic surface voxel label prediction accuracy on
ScanNet test scenes (in percentages), following [37]. We also
show the ensembled prediction accuracy with ﬁve representations
in the last column.

is that the patterns learned under different number of points
show certain variations, which are beneﬁcial to each other.
In a similar fashion, the ﬁfth and sixth are volumetric repre-
sentations under two resolutions: VOLI(32 × 32 × 32) and
VOLII(24 × 24 × 24). The directed maps between differ-
ent 3D representations fall into three categories, which are
summarized below:
1. Segmentation networks. We use PointNet++ [37] and 3D
U-Net[9] for the segmentation networks under point cloud
and volumetric representations, respectively.
2. Pointcloud sub-sampling maps. We have six pointcloud
sub-sampling maps among the mesh representation (we uni-
formly sample 24K points using [32]) and three point cloud
representations. For each point sub-sampling map, we force
the down-sampled point cloud to align with the feature
points of the input point cloud [35]. Note that this down-
sampled point cloud is also optimized through a segmenta-
tion network to maximize the segmentation accuracy.
3. Generating volumetric representations. Each volumetric
representation is given by the signed-distance ﬁeld (or SDF)
described in [45]. These SDFs are precomputed.
Experimental setup. We have evaluated our approach on
ScanNet semantic segmentation benchmark [12]. Our goal

11090

Ground Truth

8% Label

30% Label

100% Label

8% Label + 92%Unlabel

Figure 6: Qualitative comparisons of 3D semantic segmentation results on ScanNet [12]. Each row represents one testing instance, where
ground truth and top sub-row show prediction for 21 classes and bottom sub-row only shows correctly labeled points. (Green indicates
correct predictions, while red indicates false predictions.) This ﬁgure is best viewed in color, zoomed in.

is to evaluate the effectiveness of our approach when using
a small labeled dataset and a large unlabeled dataset. To this
end, we consider three baseline approaches, which train the
segmentation network under each individual representation
using 100%, 30%, and 8% of the labeled data. We then test
our approach by utilizing 8% of the labeled data, which de-
ﬁnes the data term in (3), and 92% of the unlabeled data,
which deﬁnes the regularization term of (3). We initialize
the segmentation network for point clouds using uniformly
sampled points trained on labeled data. We then ﬁne-tune
the entire network using both labeled and unlabeled data.
Note that unlike [55], our approach leverages essentially
the same labeled data but under different 3D representa-
tions. The boost in performance comes from unlabeled data.
Code is publicly available at https://github.com/

zaiweizhang/path_invariance_map_network.

Analysis of results. Figure 6 and Table 1 present quali-
tative and quantitative comparisons between our approach
and baselines. Across all 3D representations, our approach
leads to consistent improvements, demonstrating the robust-
ness of our approach. Speciﬁcally, when using 8% labeled
data and 92% unlabeled data, our approach achieves com-
peting performance as using 30% to 100% labeled data
when trained on each individual representation. Moreover,

the accuracy on VOLI is competitive against using 100% of
labeled data, indicating that the patterns learned under the
point cloud representations are propagated to train the vol-
umetric representations. We also tested the performance of
applying popular vote [39] on the predictions of using dif-
ferent 3D representations. The relative performance gains
remain similar (See the last column in Table1). Please re-
fer to Appendix C for more experimental evaluations and
baseline comparisons.

6. Conclusions

We have studied the problem of optimizing a directed
map network while enforcing the path-invariance constraint
via path-invariance bases. We have described an algorithm
for computing a path-invariance basis with polynomial time
and space complexities. The effectiveness of this approach
is demonstrated on three groups of map networks with di-
verse applications.
Acknowledgement. Qixing Huang would like to acknowl-
edge support from NSF DMS-1700234, NSF CIP-1729486,
NSF IIS-1618648, a gift from Snap Research and a GPU
donation from Nvidia Inc. Xiaowei Zhou is supported in
part by NSFC (No. 61806176) and Fundamental Research
Funds for the Central Universities.

11091

References

[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si-
mon, Brian Curless, Steven M. Seitz, and Richard Szeliski.
Building rome in a day. Commun. ACM, 54(10):105–112,
Oct. 2011. 1
[2] Federica Arrigoni, Beatrice Rossi, Pasqualina Fragneto, and
Andrea Fusiello. Robust synchronization in SO(3) and SE(3)
via low-rank and sparse matrix decomposition. Computer
Vision and Image Understanding, 174:95–113, 2018. 2
[3] Chandrajit Bajaj, Tingran Gao, Zihang He, Qixing Huang,
and Zhenxiao Liang. Smac: Simultaneous mapping and clus-
tering via spectral decompositions. In ICML, pages 100–108,
2018. 1
[4] Jrgen Bang-Jensen and Gregory Z. Gutin. Digraphs - theory,
algorithms and applications. Springer, 2002. 4
[5] Avishek Chatterjee and Venu Madhav Govindu. Efﬁcient and
robust large-scale rotation averaging. In ICCV, pages 521–
528. IEEE Computer Society, 2013. 1, 2
[6] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler,
Raquel Urtasun, and Alan L. Yuille. Detect what you can:
Detecting and representing objects using holistic models and
body parts.
In CVPR, pages 1979–1986. IEEE Computer
Society, 2014. 6
[7] Yuxin Chen, Leonidas J. Guibas, and Qi-Xing Huang. Near-
optimal joint object matching via convex relaxation. In Pro-
ceedings of the 31th International Conference on Machine
Learning, ICML 2014, Beijing, China, 21-26 June 2014,
pages 100–108, 2014. 1, 2, 7
[8] Minsu Cho, Suha Kwak, Cordelia Schmid, and Jean Ponce.
Unsupervised object discovery and localization in the wild:
Part-based matching with bottom-up region proposals.
In
IEEE Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages
1201–1210, 2015. 1
¨Ozg ¨un C¸ ic¸ ek, Ahmed Abdulkadir, Soeren S Lienkamp,
Thomas Brox, and Olaf Ronneberger. 3d u-net:
learning
dense volumetric segmentation from sparse annotation.
In
International Conference on Medical Image Computing and
Computer-Assisted Intervention, pages 424–432. Springer,
2016. 7, 16
[10] Luca Cosmo, Emanuele Rodol `a, Andrea Albarelli, Facundo
M ´emoli, and Daniel Cremers. Consistent partial matching
of shape collections via sparse modeling. Comput. Graph.
Forum, 36(1):209–221, 2017. 5, 6
[11] David J. Crandall, Andrew Owens, Noah Snavely, and
Daniel P. Huttenlocher. Sfm with mrfs: Discrete-continuous
optimization for large-scale structure from motion.
IEEE
Trans. Pattern Anal. Mach. Intell., 35(12):2841–2853, 2013.
1
[12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas A. Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes, 2017.
1, 2, 7, 8, 16, 19
[13] Daniela Giorgi, Silvia Biasotti, and Laura Paraboschi. Shape
retrieval contest 2007: Watertight models track, 2007. 5
[14] Qi-Xing Huang, Yuxin Chen, and Leonidas J. Guibas. Scal-
able semideﬁnite relaxation for maximum A posterior esti-
mation. In Proceedings of the 31th International Conference

[9]

on Machine Learning, ICML 2014, Beijing, China, 21-26
June 2014, pages 64–72, 2014. 2
[15] Qixing Huang and Leonidas Guibas. Consistent shape
maps via semideﬁnite programming. In Proceedings of the
Eleventh Eurographics/ACMSIGGRAPH Symposium on Ge-
ometry Processing, pages 177–186, 2013. 1, 2, 3, 5, 6, 7
[16] Qixing Huang, Fan Wang, and Leonidas J. Guibas. Func-
tional map networks for analyzing and exploring large shape
collections. ACM Trans. Graph., 33(4):36:1–36:11, 2014. 1,
2, 5, 6
[17] Qi-Xing Huang, Simon Fl ¨ory, Natasha Gelfand, Michael
Hofer, and Helmut Pottmann.
Reassembling fractured
objects by geometric matching.
ACM Trans. Graph.,
25(3):569–578, July 2006. 1, 2
[18] Qi-Xing Huang, Guo-Xin Zhang, Lin Gao, Shi-Min Hu,
Adrian Butscher, and Leonidas Guibas. An optimization
approach for extracting and encoding consistent maps in a
shape collection. ACM Trans. Graph., 31(6):167:1–167:11,
Nov. 2012. 1
[19] Xiangru Huang, Zhenxiao Liang, Chandrajit Bajaj, and Qix-
ing Huang. Translation synchronization via truncated least
squares. In NIPS, page to appear, 2017. 1, 2
[20] Daniel F. Huber and Martial Hebert. Fully automatic reg-
istration of multiple 3d data sets.
Image Vision Comput.,
21(7):637–650, 2003. 1, 2
[21] Melvin Johnson, Mike Schuster, Quoc Le, Maxim Krikun,
Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vigas,
Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jef-
frey Dean. Google’s multilingual neural machine transla-
tion system: Enabling zero-shot translation. Transactions of
the Association for Computational Linguistics, 5:339–351,
2017. 2
[22] Telikepalli Kavitha, Christian Liebchen, Kurt Mehlhorn,
Dimitrios Michail, Romeo Rizzi, Torsten Ueckerdt, and
Katharina A. Zweig. Survey: Cycle bases in graphs char-
acterization, algorithms, complexity, and applications. Com-
put. Sci. Rev., 3(4):199–243, Nov. 2009. 2, 4, 6
[23] Ira Kemelmacher-Shlizerman and Steven M Seitz. Col-
lection ﬂow.
In Computer Vision and Pattern Recogni-
tion (CVPR), 2012 IEEE Conference on, pages 1792–1799.
IEEE, 2012. 6
[24] Jaechul Kim, Ce Liu, Fei Sha, and Kristen Grauman. De-
formable spatial pyramid matching for fast dense correspon-
dences. In CVPR, pages 2307–2314. IEEE Computer Soci-
ety, 2013. 1, 6
[25] Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen Di-
Verdi, and Thomas Funkhouser. Exploring collections of 3d
models using fuzzy correspondences. ACM Trans. Graph.,
31(4):54:1–54:11, July 2012. 1, 5
[26] Vladimir G. Kim, Yaron Lipman, and Thomas Funkhouser.
Blended intrinsic maps. In ACM SIGGRAPH 2011 Papers,
SIGGRAPH ’11, pages 79:1–79:12, New York, NY, USA,
2011. ACM. 5, 6, 17
[27] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. CoRR, abs/1412.6980, 2014. 5, 15,
16
[28] Erik G. Learned-Miller. Data driven image models through
continuous joint alignment. IEEE Trans. Pattern Anal. Mach.
Intell., 28(2):236–250, Feb. 2006. 6

11092

[29] Spyridon Leonardos, Xiaowei Zhou, and Kostas Daniilidis.
Distributed consistent data association via permutation syn-
chronization. In ICRA, pages 2645–2652. IEEE, 2017. 1
[30] Ce Liu, Jenny Yuen, and Antonio Torralba. Sift ﬂow: Dense
correspondence across scenes and its applications.
IEEE
Trans. Pattern Anal. Mach. Intell., 33(5):978–994, May
2011. 1
[31] Andy Nguyen, Mirela Ben-Chen, Katarzyna Welnicka,
Yinyu Ye, and Leonidas J. Guibas. An optimization approach
to improving collections of shape maps. Comput. Graph. Fo-
rum, 30(5):1481–1491, 2011. 1, 2, 5
[32] Robert Osada, Thomas Funkhouser, Bernard Chazelle, and
David Dobkin. Shape distributions. ACM Trans. Graph.,
21(4):807–832, Oct. 2002. 7
[33] Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian
Butscher, and Leonidas Guibas. Functional maps: A ﬂexible
representation of maps between shapes. ACM Transactions
on Graphics, 31(4), 2012. 5
[34] Deepti Pachauri, Risi Kondor, and Vikas Singh. Solving the
multi-way matching problem by permutation synchroniza-
tion. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahra-
mani, and K. Q. Weinberger, editors, Advances in Neural In-
formation Processing Systems 26, pages 1860–1868. Curran
Associates, Inc., 2013. 2
[35] Mark Pauly, Richard Keiser, and Markus Gross. Multi-scale
Feature Extraction on Point-Sampled Surfaces. Computer
Graphics Forum, 2003. 7
[36] Yigang Peng, Arvind Ganesh, John Wright, Wenli Xu, and
Yi Ma. Rasl: Robust alignment by sparse and low-rank de-
composition for linearly correlated images. IEEE Trans. Pat-
tern Anal. Mach. Intell., 34(11):2233–2246, Nov. 2012. 6
[37] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J.
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In Advances in Neural Informa-
tion Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, pages 5105–5114, 2017. 7, 16
[38] Ilija Radosavovic, Piotr Doll ´ar, Ross B. Girshick, Georgia
Gkioxari, and Kaiming He. Data distillation: Towards omni-
supervised learning.
In CVPR, pages 4119–4128. IEEE
Computer Society, 2018. 2
[39] Lior Rokach. Ensemble-based classiﬁers. Artif. Intell. Rev.,
33(1-2):1–39, Feb. 2010. 8
[40] Michael Rubinstein, Armand Joulin, Johannes Kopf, and Ce
Liu. Unsupervised joint object discovery and segmentation
in internet images. In 2013 IEEE Conference on Computer
Vision and Pattern Recognition, Portland, OR, USA, June 23-
28, 2013, pages 1939–1946. IEEE Computer Society, 2013.
1
[41] Michael Rubinstein, Ce Liu, and William T. Freeman. Joint
inference in weakly-annotated image datasets via dense cor-
respondence.
Int. J. Comput. Vision, 119(1):23–45, Aug.
2016. 1
[42] Raif M. Rustamov. Laplace-beltrami eigenfunctions for de-
formation invariant shape representation. In Proceedings of
the Fifth Eurographics Symposium on Geometry Process-
ing, SGP ’07, pages 225–233, Aire-la-Ville, Switzerland,
Switzerland, 2007. Eurographics Association. 6
[43] Yanyao Shen, Qixing Huang, Nati Srebro, and Sujay Sang-
havi. Normalized spectral map synchronization.
In D. D.

Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
editors, Advances in Neural Information Processing Systems
29, pages 4925–4933. Curran Associates, Inc., 2016. 1, 2
[44] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo
tourism: Exploring photo collections in 3d. ACM Trans.
Graph., 25(3):835–846, July 2006. 1
[45] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. Proceedings of 30th IEEE
Conference on Computer Vision and Pattern Recognition,
2017. 7
[46] Robert Endre Tarjan.
Edge-disjoint spanning trees and
depth-ﬁrst search. Acta Inf., 6(2):171–185, June 1976. 4
[47] Fan Wang, Qixing Huang, and Leonidas J. Guibas. Image
co-segmentation via consistent functional maps. In Proceed-
ings of the 2013 IEEE International Conference on Com-
puter Vision, ICCV ’13, pages 849–856, Washington, DC,
USA, 2013. IEEE Computer Society. 1, 5
[48] Fan Wang, Qixing Huang, Maks Ovsjanikov, and Leonidas J.
Guibas. Unsupervised multi-class joint image segmentation.
In CVPR, pages 3142–3149. IEEE Computer Society, 2014.
1
[49] Lanhui Wang and Amit Singer. Exact and stable recovery of
rotations for robust synchronization. Information and Infer-
ence: A Journal of the IMA, 2:145–193, Dec. 2013. 2
[50] Yunhai Wang, Shmulik Asaﬁ, Oliver van Kaick, Hao Zhang,
Daniel Cohen-Or, and Baoquan Chen. Active co-analysis of
a set of shapes. ACM Trans. Graph., 31(6):165:1–165:10,
Nov. 2012. 5, 17
[51] Yi Yang and Deva Ramanan. Articulated human detection
with ﬂexible mixtures of parts.
IEEE Trans. Pattern Anal.
Mach. Intell., 35(12):2878–2890, Dec. 2013. 6
[52] Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong.
Dualgan: Unsupervised dual learning for image-to-image
translation. In ICCV, pages 2868–2876. IEEE Computer So-
ciety, 2017. 1, 2
[53] Yuxin Chen and Emmanuel Candes. The projected power
method: An efﬁcient algorithm for joint alignment from pair-
wise differences. https://arxiv.org/abs/1609.05820, 2016. 2
[54] Christopher Zach, Manfred Klopschitz, and Marc Pollefeys.
Disambiguating visual relations using loop constraints.
In
CVPR, pages 1426–1433. IEEE Computer Society, 2010. 2,
6
[55] Amir Roshan Zamir, Alexander Sax, William B. Shen,
Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. In CVPR,
pages 3712–3722. IEEE Computer Society, 2018. 2, 8
[56] Tinghui Zhou, Philipp Kr ¨ahenb ¨uhl, Mathieu Aubry, Qi-Xing
Huang, and Alexei A. Efros. Learning dense correspon-
dence via 3d-guided cycle consistency. In 2016 IEEE Con-
ference on Computer Vision and Pattern Recognition, CVPR
2016, Las Vegas, NV, USA, June 27-30, 2016, pages 117–
126, 2016. 1, 2, 6, 15, 16
[57] Tinghui Zhou, Yong Jae Lee, Stella X. Yu, and Alexei A.
Efros. Flowweb: Joint image set alignment by weaving con-
sistent, pixel-wise correspondences. In CVPR, pages 1191–
1200. IEEE Computer Society, 2015. 1, 2, 6
[58] Xiaowei Zhou, Menglong Zhu, and Kostas Daniilidis. Multi-
image matching via fast alternating minimization. In ICCV,

11093

pages 4032–4040, Santiago, Chile, 2015. IEEE Computer
Society. 1, 2, 6
[59] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, pages 2242–2251.
IEEE Computer Society, 2017. 1, 2

11094

Probabilistic Permutation Synchronization using the Riemannian Structure of
the Birkhoff Polytope

Tolga Birdal 1, 2
Umut S¸ ims¸ekli 3
1 Computer Science Department, Stanford University, CA 94305 Stanford, US
2 Fakult ¨at f ¨ur Informatik, Technische Universit ¨at M ¨unchen, 85748 M ¨unchen, Germany
3 LTCI, T ´el ´ecom ParisTech, Universit ´e Paris-Saclay, 75013 Paris, France

Probability Simplex  Δ 
Orthogonal Hypersphere   :     
Permutation Matrices   =     ∩  
Birkhoff Polytope    
Tangent Space       
Common center of mass

Abstract

(a) Initialization

(b) Solution

(c) Certainty/Conf idence

(d) Multiple Hypotheses Solution

 

We present an entirely new geometric and probabilis-
tic approach to synchronization of correspondences across
multiple sets of objects or images. In particular, we present
two algorithms: (1) Birkhoff-Riemannian L-BFGS for op-
timizing the relaxed version of the combinatorially in-
tractable cycle consistency loss in a principled manner, (2)
Birkhoff-Riemannian Langevin Monte Carlo for generating
samples on the Birkhoff Polytope and estimating the con-
ﬁdence of the found solutions. To this end, we ﬁrst intro-
duce the very recently developed Riemannian geometry of
the Birkhoff Polytope. Next, we introduce a new probabilis-
tic synchronization model in the form of a Markov Random
Field (MRF). Finally, based on the ﬁrst order retraction op-
erators, we formulate our problem as simulating a stochas-
tic differential equation and devise new integrators. We
show on both synthetic and real datasets that we achieve
high quality multi-graph matching results with faster con-
vergence and reliable conﬁdence/uncertainty estimates.

1. Introduction

Correspondences fuel a large variety of computer vi-
sion applications such as structure-from-motion (SfM) [62],
SLAM [53], 3D reconstruction [20, 8, 6], camera re-
localization [60], image retrieval [44] and 3D scan stitch-
ing [38, 22].
In a typical scenario, given two scenes, an
initial set of 2D/3D keypoints is ﬁrst identiﬁed. Then the
neighborhood of each keypoint is summarized with a lo-
cal descriptor
[47, 23] and keypoints in the given scenes
are matched by associating the mutually closest descrip-
tors. In a majority of practical applications, multiple images
or 3D shapes are under consideration and ascertaining such
two-view or pairwise correspondences is simply not sufﬁ-
cient. This necessitates a further reﬁnement ensuring global
consistency. Unfortunately, at this stage even the well de-
veloped pipelines acquiesce either heuristic/greedy reﬁne-
ment [21] or incorporate costly geometric cues related to
the linking of individual correspondence estimates into a

Top-N
Certainty

Top-2
Certainty

 

Figure 1. Our algorithm robustly solves the multiway image
matching problem (a, b) and provides conﬁdence maps (c) that
can be of great help in further improving the estimates (d). The
bar on the right is used to assign colors to conﬁdences. For the
rest, incorrect matches are marked in red and correct ones in blue.

globally coherent whole [30, 62, 73].
In this paper, by using the fact that correspondences
are cycle consistent 1 , we propose two novel algorithms
for reﬁning the assignments across multiple images/scans
(nodes) in a multi-way graph and for estimating assignment
conﬁdences, respectively. We model the correspondences
between image pairs as relative, total permutation matri-
ces and seek to ﬁnd absolute permutations that re-arrange
the detected keypoints to a single canonical, global order.
This problem is known as map or permutation synchro-
nization [56, 70]. Even though in many practical scenar-
ios matches are only partially available, when shapes are
complete and the density of matches increases, total permu-
tations can sufﬁce [36].
Similar to many well received works [84, 61], we re-
lax the sought permutations to the set of doubly-stochastic
(DS) matrices. We then consider the geometric structure of
DS, the Birkhoff Polytope [9]. We are - to the best of our
knowledge, for the ﬁrst time introducing and applying the
recently developed Riemannian geometry of the Birkhoff
Polytope [26] to tackle challenging problems of computer
vision. Note that lack of this geometric understanding
caused plenty of obstacles for scholars dealing with our
problem [61, 77]. By the virtue of a ﬁrst order retraction, we

1Composition of correspondences for any circular path arrives back at
the start node.

111105

can use the recent Riemannian limited-memory BFGS (LR-
BFGS) algorithm [82] to perform a maximum-a-posteriori
(MAP) estimation of the parameters of the consistency loss.
We coin our variation as Birkhoff-LRBFGS.
At the next stage, we take on the challenge of conﬁ-
dence/uncertainty estimation for the problem at hand by
drawing samples on the Birkhoff Polytope and estimating
the empirical posterior distribution. To achieve this, we ﬁrst
formulate a new geodesic stochastic differential equation
(SDE). Our SDE is based upon the Riemannian Langevin
Monte Carlo (RLMC) [31, 78, 58] that is efﬁcient and ef-
fective in sampling from Riemannian manifolds with true
exponential maps. Note that similar stochastic gradient
geodesic MCMC (SG-MCMC) [46, 11] tools have already
been used in the context of synchronization of spatial rigid
transformations whose parameters admit an analytically de-
ﬁned geodesic ﬂow [7]. Unfortunately, for our manifold the
retraction map is only up to ﬁrst order and hence we can-
not use off-the-shelf schemes. Alleviating this nuisance, we
further contribute a novel numerical integrator to solve our
SDE by replacing the intractable exponential map of DS
matrices by the approximate retraction map. This leads to
another new algorithm: Birkhoff-RLMC.
In a nutshell, our contributions are:
1. We function as an ambassador and introduce the Rie-
mannian geometry of the Birkhoff Polytope [26] to
solve problems in computer vision.
2. We propose a new probabilistic model for the permu-
tation synchronization problem.
3. We minimize the cycle consistency loss via a
Riemannian-LBFGS algorithm and outperfom the
state-of-the-art both in recall and in runtime.
4. Based upon the Langevin mechanics, we introduce a
new SDE and a numerical integrator to draw samples
on the high dimensional and complex manifolds with
approximate retractions, such as the Birkhoff Poly-
tope. This lets us estimate the conﬁdence maps, which
can aid in improving the solutions and spotting consis-
tency violations or outliers.
Note that the tools developed herewith can easily ex-
tend beyond our application and would hopefully facilitate
promising research directions regarding the combinatorial
optimization problems in computer vision.

2. Related Work

Permutation synchronization is an emerging domain of
study due to its wide applicability, especially for the prob-
lems in computer vision. We now review the developments
in this ﬁeld, as chronologically as possible. Note that multi-
way graph matching problem formulations involving spatial
geometry are well studied [18, 50, 43, 29, 79, 19], as well
as transformation synchronization [76, 13, 72, 75, 3, 4, 33].
For brevity, we omit these literature and focus on works that
explicitly operate on correspondence matrices.

The ﬁrst applications of synchronization, a term coined
by Singer [67, 66], to correspondences only date back to
early 2010s [54]. Pachauri et al. [56] gave a formal def-
inition and devised a spectral technique. The same au-
thors quickly extended their work to Permutation Diffu-
sion Maps [55] ﬁnding correspondence between images.
Unfortunately, this ﬁrst method was quadratic in the num-
ber of images and hence was not computationally friendly.
In a sequel of works called MatchLift, Huang, Chen and
Guibas [36, 15] were the ﬁrsts to cast the problem of es-
timating cycle-consistent maps as ﬁnding the closest pos-
itive semideﬁnite matrix to an input matrix. They also
addressed the case of partial permutations. Due to the
semideﬁnite programming (SDP) involved, this perspec-
tive suffered from high computational cost in real applica-
tions. Similar to Pachauri [56], for N images and M edges,
this method required computing an eigendecomposition of
an N M × N M matrix. Zhou et al. [85] then introduced
MatchALS, a new low-rank formulation with nuclear-norm
relaxation, globally solving the joint matching of a set of
images without the need of SDP. Yu et al. [81] formulated
a synchronization energy similar to our method and pro-
posed proximal Gauss-Seidel methods for solving a relaxed
problem. However, unlike us, this paper did not use the ge-
ometry of the constraints or variables and thereby resorted
to complicated optimization procedures involving Frank-
Wolfe subproblems for global constraint satisfaction. Ar-
rigoni et al. [2] and Maset et al. [2] extended Pachauri [56]
to operate on partial permutations using spectral decompo-
sition. To do so, they considered the symmetric inverse
semigroup of the partial matches that are typically hard to
handle. Their closed form methods did not need initializa-
tion steps to synchronize, but also did not establish an ex-
plicit cycle consistency. Tang et al. [71] opted to use or-
dering heuristics improving upon Pachauri [56]. Cosmo et
al. [19] brought an interesting solution to the problem of es-
timating consistent correspondences between multiple 3D
shapes, without requiring initial pairwise solutions as in-
put. Schiavinato and Torsello [61] tried to overcome the
lack of group structure of the Birkhoff polytope by trans-
forming any graph-matching problem into a multi-graph
matching one. Bernard et al. [5] used an NMF-based ap-
proach to generate a cycle-consistent synchronization. Park
and Yoon [57] used multi-layer random walks framework to
address the global correspondence search problem of multi-
attributed graphs. Starting from a multi-layer random-walks
initialization, the authors proposed a robust solver by itera-
tive reweighting. Hu et al. [35] revisited the MatchLift and
developed a scalable, distributed solution with the help of
ADMMs, called DMatch. Their idea of splitting the input
into sub-collections can still lead to global consistency un-
der mild conditions while improving the efﬁciency. Finally,
Wang et al. [77] made use of the domain knowledge and
added the geometric consistency of image coordinates as a

11106

low-rank term to increase the recall.
The aforementioned works have neither considered the
Riemennian structure of the common Birkhoff convex re-
laxation nor have they provided a probabilistic framework,
which can pave the way to uncertainty estimation while
simultaneously solving the optimization problem. This is
what we propose in this work.

3. Preliminaries and Technical Background

Deﬁnition 1 (Permutation Matrix). A permutation matrix
is deﬁned as a sparse, square binary matrix, where each
column and each row contains only a single true (1) value:

Pn := {P ∈ {0, 1}n×n : P1n = 1n , 1⊤

n

P = 1⊤

n }.

(1)

where 1n denotes a n-dimensional ones vector. Every P ∈
P n is a total permutation matrix and Pij = 1 implies that
element i is mapped to element j . Permutation matrices
are the only strictly non-negative elements of the orthogonal
group Pn ∈ On = {O : O⊤O = I}, a special case of the
Stiefel manifold of m—frames in Rn when m = n.

Deﬁnition 2 (Center of Mass). The center of mass for all
the permutations on n objects is deﬁned in Rn×n as [59]:

Cn =

1

n! XPi∈Pn

Pi =

1
n!

(n − 1)!1n1⊤
n =

1
n

1n1⊤

n .

(2)

Notice that Cn /∈ P n as shown in Fig. 2.
Deﬁnition 3 (Relative Permutation). We deﬁne a permuta-
tion matrix to be relative if it is the ratio (or difference) of
two group elements (i → j ): Pij = PiP⊤
j .
Deﬁnition 4 (Permutation Synchronization Problem).
Given a redundant set of measures of ratios {Pij } :
the set of the edges of a directed graph of N nodes, the
permutation synchronization [56] can be formulated as the
problem of recovering {Pi } for i = 1, . . . , N such that the
group consistency constraint is satisﬁed: Pij = PiP−1
.

(i, j ) ∈ E ⊂ {1, . . . , N } × {1, . . . , N }, where E denotes

j

If the input data is noise-corrupted, this consistency will
not hold and to recover the absolute permutations {Pi },
some form of a consistency error is minimized. Typically,
any form of minimization on the discrete space of permu-
tations is intractable and these matrices are relaxed by their
doubly-stochastic counterparts [10, 84, 61] (see Fig. 2).

Deﬁnition 5 (Doubly Stochastic (DS) Matrix). A DS matrix
is a non-negative, square matrix whose rows and columns
sum to 1. The set of DS matrices is deﬁned as:

DP n = { X ∈ Rn×n
+

:

n

Xi=1

xij = 1 ∧

n

Xj=1

xij = 1 }.

(3)

Figure 2. Simpliﬁed (matrices are vectorized) illustration of ge-
ometries we consider: (i) ∆n is convex, (ii) DP n is strictly con-
tained in ∆n . In low dimensions, such conﬁguration cannot exist
as there is no convex shape that touches ∆n only on the corners.

Theorem 1 (Birkhoff-von Neumann Theorem). The con-
vex hull of the set of all permutation matrices is the set
of doubly-stochastic matrices and there exists a potentially
non-unique θ such that any DS matrix can be expressed as
a linear combination of k permutation matrices [9, 39]:

X = θ1P1 + · · · + θkPk , θi > 0 ∧ θ⊤1k = 1.

(4)

While ﬁnding the minimum k is shown to be NP-hard [27],
by Marcus-Ree theorem, we know that there exists one con-
structible decomposition where k < (n − 1)2 + 1.
Deﬁnition 6 (Birkhoff Polytope). The multinomial mani-
fold of DS matrices is incident to the convex object called
the Birkhoff Polytope [9], an (n − 1)2 dimensional con-
vex submanifold of the ambient Rn×n with n! vertices:
Bn ≡ DP n . We use DP n to refer to the Birkhoff Polytope.
It
is interesting to see that
this convex polytope is
co-centered with Pn at Cn , Cn ∈ DP n and over-
parameterizes the convex hull of the permutation vectors,
the permutahedron [32]. Pn can now be considered as an
orthogonal subset of DP n : Pn = {X ∈ DP n : XX⊤ =
I}, i.e. the discrete set of permutation matrices is the inter-
section of the convex set of DS matrices and the On .

3.1. Riemannian Geometry of the Birkhoff Polytope

Recently, Douik et al. [26] endowed DP n with the
Fisher information metric, resulting in the Riemannian
manifold of DP n . To the best of our knowledge, we are the
ﬁrst to exploit this manifold in the domain of computer vi-
sion, and hence will now recall the main results of Douik et
al. [26] and summarize the main constructs of Riemannian
optimization on DP n . The proofs can be found in [26].
Deﬁnition 7 (Tangent Space and Bundle). The tangent
bundle is referred to as the union of all tangent spaces
T DP n = ∪X∈DP n TXDP n one of which is deﬁned as:

TXDP n := {Z ∈ Rn×n : Z1n = 0n , Z⊤1n = 0n }. (5)

Theorem 2. The projection operator ΠX (Y), Y ∈ DP n
onto the tangent space of X ∈ DP n , TXDP n is written as:
ΠX (Y) = Y − (α1⊤ + 1β⊤ ) ⊙ X, with
(6)

11107

α = (I − XX⊤ )+ (Y − XY⊤ )1, β = Y⊤1 − X⊤α,

+ depicts the left pseudo-inverse and ⊙ the Hadamard prod-
uct. Note that there exists a numerically more stable way to
compute the same concise formulation of ΠX (Y) [26].

Theorem 3. For a vector ξX ∈ TXDP n lying on the tan-
gent space of X ∈ DP n , the ﬁrst order retraction map RX
is given as follows:

RX (ξX ) = Π(X ⊙ exp(ξX ⊘ X)),

(7)

where the operator Π denotes the projection onto DP n , ef-
ﬁciently computed using the Sinkhorn algorithm [68] and
⊘ is the Hadamard division.
Plis et al. [59] showed that on the n-dimensional
Birkhoff Polytope all permutations are equidistant from the
center of mass Cn , and thus the extreme points of DP n , that
are the permutation matrices, are located on an (n − 1)2 -
of radius √n − 1, cen-
dimensional hypersphere S (n−1)2
tered at Cn . This hypersphere is incident to the Birkhoff
Polytope on the vertices.

S (n−1)2

Proposition 1. The gap as a ratio between DP n and both
and On grows to inﬁnity as n grows.
The proof is given in the supplementary document. While
there exists polynomial time projections of the n!-element
permutation space onto the continuous hypersphere repre-
sentation and back [59], Prop. 1 prevents us from using hy-
persphere relaxations, as done in preceding works [59, 83].

4. Proposed Probabilistic Model

We assume that we are provided a set of pairwise, total
permutations Pij ∈ Pn for (i, j ) ∈ E and we are inter-
ested in ﬁnding the underlying absolute permutations Xi
for i ∈ {1, . . . , N } with respect to a common origin (e.g.
X1 = I, the identity matrix). We seek absolute permuta-
tions that would respect the consistency of the underlying
graph structure. For conciseness, we also restrict our set-
ting to total permutations, and leave the extension to partial
permutations, which live on the monoid, as a future study.
Because operating directly on Pn would require us to solve
a combinatorial optimization problem and because of the
lack of a manifold structure for Pn , we follow the popular
approach [45, 80, 48] and relax the domain of the absolute
permutations by assuming that each Xi ∈ DP n .
We formulate the permutation synchronization problem
in a probabilistic context where we treat the pairwise rela-
tive permutations as observed random variables and the ab-
solute ones as latent random variables. In particular, our
probabilistic construction enables us to cast the synchro-
nization problem as inferential in the model. With a slight
abuse of notation, in the rest of the paper, we will denote

P ≡ {Pij }(i,j )∈E and X ≡ {Xi }N

i=1 , all the observations
and all the latent variables, respectively.
A typical way to build a probabilistic model is to ﬁrst
choose the prior distributions on DP n for each Xi and
then choose a conditional distribution on Pn for each Xij
given the latent variables. Unfortunately, standard paramet-
ric distributions neither exist on DP n nor on Pn . The vari-
ational stick breaking [45] yields an implicitly deﬁned PDF
on DP n and is not able to provide direct control on the re-
sulting distribution. Deﬁning Kantorovich distance-based
distributions over the permutation matrices is possible [17],
yet these models incur high computational costs since they
would require solving optimal transport problems during in-
ference. For these reasons, instead of constructing a hierar-
chical probabilistic model, we will directly model the full
joint distribution of P and X.
We propose a probabilistic model where we assume the
full joint distribution admits the following factorized form:

p(P, X) =

1

Z Y(i,j )∈E

ψ(Pij , Xi , Xj ),

(8)

where Z denotes the normalization constant with

Z := XP∈P |E |

n

ZDP N
n Y(i,j )∈E

ψ(Pij , Xi , Xj ) dX,

(9)

and ψ is called the ‘clique potential’ that is deﬁned as:

F ).

(10)

j k2

ψ(Pij , Xi , Xj ) , exp(−β kPij − XiX⊤
Here k · kF denotes the Frobenius norm, β ∈ R+ is the
dispersion parameter that controls the spread of the distri-
bution. Note that the model is a Markov random ﬁeld [40].
Let us take a closer look at the proposed model. If we
deﬁne Xij := XiX⊤
j ∈ DP n , then by Thm. 1, we have the
following decomposition for each Xij :

Xij = XBij

b=1

θij,bMij,b , XBij

b=1

θij,b = 1,

(11)

where Bij is a positive integer, each θij,b ≥ 0, and Mij,b ∈
Pn . The next result states that we have an equivalent hier-
archical interpretation for the proposed model:

Proposition 2. The probabilistic model deﬁned in Eq. 8 im-
plies the following hierarchical decomposition:

p(X) =

1
C

exp(cid:16)−β X(i,j )∈E
1
exp(cid:16)2β tr(P⊤

kXij k2(cid:17) Y(i,j )∈E
Xij )(cid:17)

Zij

ij

p(Pij |Xi , Xj ) =

Zij

(12)

(13)

where C and Zij are normalization constants. Besides, for
all i, j , Zij ≥ QBij
b=1 f (β , θij,b ), where f is a positive func-
tion that is increasing in both β and θij,b .

11108

ij

The proof is given in the supplementary and is based on
the simple decomposition p(P, X) = p(X)p(P|X). This
hierarchical point of view lets us observe some interest-
ing properties: (1) the likelihood p(Pij |Xi , Xj ) mainly de-
pends on the term tr(P⊤
Xij ) that measures the data ﬁt-
ness. We aptly call this term the ‘soft Hamming distance’
between Pij and Xij since it would correspond to the ac-
tual Hamming distance between two permutations if Xi , Xj
were permutation matrices [41]. (2) On the other hand, the
prior distribution contains two competing terms: (i) the term
Zij favors large θij,b , which would push Xij towards the
corners of the Birkhoff polytope, (ii) the term kXij k2
F acts
as a regularizer on the latent variables and attracts them to-
wards the center of the Birkhoff polytope Cn (cf. Dfn. 2),
which will be numerically beneﬁcial for the inference algo-
rithms that will be developed in the following section.

5. Inference Algorithms

We can now formulate the permutation synchronization
problem as a probabilistic inference problem, where we will
be interested in the following quantities:

1. Maximum a-posteriori (MAP):

X⋆ = arg max

X∈DP N

n

log p(X|P)

(14)

F ,

j k2

where log p(X|P) =+ −β P(i,j )∈E kPij − XiX⊤
and =+ denotes equality up to an additive constant.
2. The full posterior distribution: p(X|P) ∝ p(P, X).
The MAP estimate is often easier to obtain and useful in
practice. On the other hand, characterizing the full poste-
rior can provide important additional information, such as
uncertainty; however, not surprisingly it is a much harder
task.
In addition to the usual difﬁculties associated with
these tasks, in our context we are facing extra challenges
due to the non-standard manifold of our latent variables.

5.1. Maximum A(cid:173)Posteriori Estimation

The MAP estimation problem can be cast as a minimiza-
tion problem on DP n , given as follows:

X⋆ = arg min

X∈DP N

n nU (X) := X(i,j )∈E kPij − XiX⊤
j k2

Fo

where U is called the potential energy function. We observe
that the choice of the dispersion parameter has no effect on
the MAP estimate. Although this optimization problem re-
sembles conventional norm minimization, the fact that X
lives in the cartesian product of Birkhoff polytopes renders
the problem very complicated.
Thanks to the retraction operator over the Birkhoff poly-
tope (cf. Thm. 3), we are able to use several Riemannian op-
timization algorithms [69], without resorting to projection-
based updates. In this study, we use the recently proposed

Riemannian limited-memory BFGS (LR-BFGS) [37], a
powerful optimization technique that attains faster conver-
gence rates by incorporating local geometric information in
an efﬁcient manner. This additional piece of information is
obtained through an approximation of the inverse Hessian,
which is computed on the most recent values of the past
iterates with linear time- and space-complexity in the di-
mension of the problem. We give more detail on LR-BFGS
in our supp. material. The detailed description of the algo-
rithm can be found in [37, 82].
Finally, we round the resulting approximate solutions
into a feasible one via Hungarian algorithm [52], obtaining
binary permutation matrices.

5.2. Posterior Sampling via Riemannian Langevin
Monte Carlo with Retractions

In this section we will develop a Markov Chain Monte
Carlo (MCMC) algorithm for generating samples from the
posterior distribution p(X|P), by borrowing ideas from [64,
46, 7]. Once such samples are generated, we will be able
to quantify the uncertainty in our estimation by using the
generated samples.
The dimension and complexity of the Birkhoff manifold
makes it very challenging to generate samples on DP n or
its product spaces and to the best of our knowledge there is
no Riemannian MCMC algorithm that is capable of achiev-
ing this. There are existing Riemannian MCMC algorithms
[11, 46], which are able to draw samples on embedded man-
ifolds; however, they require the exact exponential map to
be analytically available, which in our case, can only be ap-
proximated by the retraction map at best.
To this end, we develop an algorithmically simpler yet
effective algorithm. Let the posterior density of interest

be πH (X) := p(X|P) ∝ exp(−βU (X)) with respect

ξ : RN (n−1)2
RN (n−1)2

to the Hausdorff measure. We then deﬁne an embedding
n such that ξ ( ˜X) = X for ˜X ∈
. By the area formula (cf. Thm. 1 in [25]), we
have the following expression for the embedded posterior
density πλ (with respect to the Lebesgue measure):

7→ DP N

πH (x) = πλ ( ˜X)/q|G( ˜X)|,

(15)

where G denotes the Riemann metric tensor.
We then consider the following stochastic differential
equation (SDE), which is a slight modiﬁcation of the SDE
that is used to develop the Riemannian Langevin Monte
Carlo algorithm [31, 78, 58]:

d ˜Xt = (−G−1∇ ˜XUλ ( ˜Xt ) + Γt )dt + p2/βG−1dBt ,

where Bt denotes the standard Brownian motion and Γt
is called the correction term that is deﬁned as follows:
∂ [G−1
By Thm. 1 of [49], it is easy to show that the solution
process ( ˜Xt )t≥0 leaves the embedded posterior distribution

[Γt ( ˜X)]i = PN (n−1)2
j=1

t ( ˜X)]ij /∂ ˜Xj .

11109

Figure 3. Sample images and manually annotated correspondences
from the challenging Willow dataset [16]. Images are plotted in
pairs (there are multiple) and in gray for better viewing.

πλ invariant. Informally, this result means that if we could
exactly simulate the continuous-time process ( ˜Xt )t≥0 , the
distribution of the sample paths would converge to the em-
bedded posterior distribution πλ , and therefore the distribu-
tion of ξ ( ˜Xt ) would converge to πH (X). However, unfor-
tunately it is not possible to exactly simulate these paths and
therefore we need to consult approximate algorithms.

A possible way to numerically simulate the SDE would
be to use standard discretization tools, such as the Euler-
Maruyama integrator [14]. However, this would require
knowing the analytical expression of ξ and constructing
Gt and Γt at each iteration. On the other hand, recent
results have shown that we can simulate SDEs directly
on their original manifolds by using geodesic integrators
[11, 46, 34], which bypasses these issues altogether. Yet,
these approaches require the exact exponential map of the
manifold to be analytically available, restricting their appli-
cability in our context.

Inspired by the recent manifold optimization algorithms
[74], we propose to replace the exact, intractable exponen-
tial map arising in the geodesic integrator with the tractable
retraction operator given in Thm. 3. We develop our recur-
sive scheme, we coin as retraction Euler integrator:

V(k+1)

i

X(k+1)

i

= ΠX

(k)

i

= RX

(k)

i

(h∇Xi U (X(k)
(V(k+1)

),

i

i

) + p2h/βZ(k+1)
∀i ∈ {1, . . . , N } (17)

) (16)

i

i

i

Z(k)
X(0)

where h > 0 denotes the step-size, k denotes the iterations,
denotes standard Gaussian random variables in Rn×n ,
denotes the initial absolute permutations. The deriva-
tion of this scheme is similar to [46] and we provide more
detailed information in the supplementary material. To the
best of our knowledge, the convergence properties of the
geodesic integrator that is approximated with a retraction
operator have not yet been analyzed. We leave this analysis
as a futurework, which is beyond the scope of this study.
We note that the term kXij k2
F plays an important role
in the overall algorithm since it prevents the latent variables
Xi to go the extreme points of the Birkhoff polytope, where
the retraction operator becomes inaccurate. We also note
that, when β → ∞, the distribution πH concentrates on
the global optimum X⋆ and the proposed retraction Euler
integrator becomes the Riemannian gradient descent with a
retraction operator.

6. Experiments and Evaluations

6.1. Real Data

2D Multi-image Matching We run our method to per-
form multiway graph matching on two datasets, CMU [12]
and Willow Object Class [16]. CMU is composed of
House and Hotel objects viewed under constant illumina-
tion and smooth motion. Initial pairwise correspondences
as well as ground truth (GT) absolute mappings are pro-
vided within the dataset. Object images in Willow dataset
include pose, lighting, instance and environment variation
as shown in Fig. 3, rendering naive template matching in-
feasible. For our evaluations, we follow the same design
as Wang et al. [77]. We ﬁrst extract local features from
a set of 227 × 227 patches centered around the annotated
landmarks, using the prosperous Alexnet [42] pretrained on
ImageNet [24]. Our descriptors correspond to the feature
map responses of Conv4 and Conv5 layers anchored on the
hand annotated keypoints. These features are then matched
by the Hungarian algorithm [52] to obtain initial pairwise
permutation matrices P0 .
We initialize our algorithm by the closed form
MatchEIG [51] and evaluate it against the state of the art
methods of Spectral [56], MatchALS [85], MatchLift [36],
MatchEIG [51], and Wang et al. [77]. The size of the uni-
verse is set to the number of features per image. We assume
that this number is ﬁxed and partial matches are not present.
Handling partialities while using the Birkhoff structure is
left as a future work. Note that [77] uses a similar cost func-
tion to ours in order to initialize an alternating procedure
that in addition exploits the geometry of image coordinates.
Authors also use this term as an extra bit of information
during their initialization. The standard evaluation metric,
recall, is deﬁned over the pairwise permutations as:

R({ ˆPi }|Pgnd ) =

1

n|E | X(i,j )∈E

Pgnd

ij ⊙ ( ˆPi ˆP⊤

j )

(18)

where Pgnd
ij are the GT relative transformations and ˆPi is an
estimated permutation. R = 0 in the case of no correctly
found correspondences and R = 1 for a perfect solution.
Tab. 1 shows the results of different algorithms as well as
ours. Note that our Birkhoff-LRBFGS method that oper-
ates solely on pairwise permutations outperforms all meth-
ods, even the ones which make use of geometry during ini-
tialization. Moreover, when our method is used to initialize
Wang et al. [77] and perform geometric optimization, we
attain the top results. These ﬁndings validate that walking
on the Birkhoff Polytope, even approximately, and using
Riemannian line-search algorithms constitute a promising
direction for optimizing the problem at hand.

Uncertainty Estimation in Real Data We now run our
conﬁdence estimator on the same Willow Object Class [16].

11110

Table 1. Our results on the WILLOW Object Class graph matching dataset. Wang− refers to running Wang [77] without the geometric con-
sistency term. The vanilla version of our method, Ours, already lacks this term. Ours-Geom then refers to initializing Wang’s veriﬁcation
method with our algorithm. For all the methods, we use the original implementation of the authors.

Dataset

Initial Spectral [56] MatchLift [36] MatchALS [85] MatchEig [51] Wang− [77] Ours Wang [77] Ours-Geom

Car
0.48
Duck
0.43
Face
0.86
Motorbike
0.30
Winebottle
0.52
CMU-House 0.68
CMU-Hotel
0.64

Average

0.52

0.55
0.59
0.92
0.25
0.64
0.90
0.81

0.59

0.65
0.56
0.94
0.27
0.72
0.94
0.87

0.65

0.69
0.59
0.93
0.34
0.70
0.92
0.86

0.66

0.66
0.56
0.93
0.28
0.71
0.94
0.92

0.71

0.72
0.63
0.95
0.40
0.73
0.98
0.94

0.76

0.71
0.67
0.95
0.37
0.73
0.98
0.96

0.77

1.00
0.932
1.00
1.00
1.00
1.00
1.00

0.99

1.00
0.96
1.00
1.00
1.00
1.00
1.00

0.99

To do that, we ﬁrst ﬁnd the optimal point where synchro-
nization is at its best. Then, we set h ← 0.0001, β ←
[0.075, 0.1] and automatically start sampling the posterior
around this mode for 1000 iterations. Note that β is a criti-
cal parameter which can also be dynamically controlled [7].
Larger values of β cannot provide enough variation for a
good diversity of solutions. Smaller values cause greater
random perturbations leading to samples far from the opti-
mum. This can cause divergence or samples not explaining
the local mode. Nevertheless, all our tests worked well with
values in the given range.
The generated samples are useful in many applications,
e.g. ﬁtting distributions or providing additional solution
hints. We address the case of multiple hypotheses genera-
tion for the permutation synchronization problem and show
that generating an additional per-edge candidate with high
certainty helps to improve the recall. Tab. 2 shows the top-
K scores we achieve by simply incorporating K likely sam-
ples. Note that, when 2 matches are drawn at random and
contribute as potentially correct matches, the recall is in-
creased only by 2%, whereas including our samples instead
boosts the multi-way matching by 6%.

Table 2. Using top-K errors to rank by uncertainty. Based on the
conﬁdence information we could retain multiple hypotheses. This
is not possible by the other approaches such as Wang et al. [51,
77]. Rand-K refers to using K − 1 additional random hypotheses
to complement the found solution. Ours-K ranks assignments by
our probabilistic certainty and retains top-K candidates per point.

Dataset

Wang Ours Rand-2 Ours-2 Rand-3 Ours-3

Car
0.72 0.71
Duck
0.63 0.67
Face
0.95 0.95
Motorbike
0.40 0.37
Winebottle 0.73 0.74

0.73
0.67
0.96
0.45
0.77

Avg.

0.69 0.69

0.71

0.76
0.69
0.97
0.49
0.82

0.75

0.76
0.67
0.96
0.52
0.79

0.74

0.81
0.72
0.98
0.60
0.85

0.79

We further present illustrative results for our conﬁdence
prediction in Fig. 4. There, unsatisfactory solutions arising
in certain cases are improved by analyzing the uncertainty

2 [77] reports a value of 0.88, but for their method, we attained 0.93
and therefore report this value.

map. The column (e) of the ﬁgure depicts the top-2 as-
signments retained in the conﬁdence map and (e) plots the
assignments that overlap with the true solution. Note that,
we might not have access to such an oracle in real applica-
tions and only show this to illustrate potential use cases of
the estimated conﬁdence map.

6.2. Evaluations on Synthetic Data

We synthetically generate 28 different problems with
varying sizes: M ∈ [10, 100] nodes and n ∈ [16, 100]
points in each node. For the scenario of image matching,
this would correspond to M cameras and N features in each
image. We then introduce 15% − 35% random swaps to the
GT absolute permutations and compute the observed rela-
tive ones. Details of this dataset are given in suppl. material.
Among all 28 sets of synthetic data, we attain an overall re-
call of 91% whereas MatchEIG [51] remains about 83%.

Runtime Analysis Next, we assess the computational
cost of our algorithm against the state of the art methods,
on the dataset explained above. All of our experiments are
run on a MacBook computer with an Intel i7 2.8GhZ CPU.
Our implementation uses a modiﬁed Ceres solver [1]. All
the other algorithms use highly vectorized MATLAB 2017b
code making our comparisons reasonably fair. Fig. 5 tab-
ulates runtimes for different methods excluding initializa-
tion. MatchLift easily took more than 20min. for moderate
problems and hence we choose to exclude it from this eval-
uation. It is noticeable that thanks to the ability of using
more advanced solvers such as LBFGS, our method con-
verges much faster than Wang et al. and runs on par with
the fastest yet least accurate spectral synchronization [56].
The worst case theoretical computational complexity of our
algorithm is OB-LRBFGS := O(K |E |KS (n2 + (2n)3 ) where
K is the number of LBFGS iterations and KS the number
of Sinkhorn iterations. While KS can be a bottleneck, in
practice our matrices are already restricted to the Birkhoff
manifold and Sinkhorn early-terminates, letting KS remain
small. The complexity is: (1) linearly-dependent upon the
number of edges, which in the worst case relates quadrati-
cally to the number of images |E | = N (N −1), (2) cubically
dependent on n. This is due to the fact that projection onto

11111

(a) Initialization

(b) Solution

(c) Confusion

(d) Certainty Map

(e) Top-2
Confusion 

(f) Top-2 Solution with Uncertainty

Figure 4. Results from our conﬁdence estimation. Given potentially erroneous solutions (b) to the problems initialized as in (a), our latent
samples discover the uncertain assignments as shown in the middle three columns (c-e). When multiple top-2 solutions are accepted as
potential positives, our method can suggest high quality hypotheses (f). The edges in the last column (f) is colored by their conﬁdence
value. Note that even though, for the sake of space we show pairs of images, the datasets contain multiple sets of images.

the tangent space solves a system of 2n × 2n equations.

7. Conclusion

n

I

d
e
z

In this work we have proposed two new frameworks
for relaxed permutation synchronization on the manifold of
doubly stochastic matrices. Our novel model and formula-
tion paved the way to using sophisticated optimizers such as
Riemannian limited-memory BFGS. We further integrated a
manifold-MCMC scheme enabling posterior sampling and
thereby conﬁdence estimation. We have shown that our
conﬁdence maps are informative about cycle inconsisten-
cies and can lead to new solution hypotheses. We used these
hypotheses in a top-K evaluation and illustrated its beneﬁts.
In the future, we plan to (i) address partial permutations, the
inner region of the Birkhoff Polytope (ii) investigate more
sophisticated MCMC schemes such as [28, 34, 63, 46, 65]

O

i

m

n

I

l

a

i
t

p

i
t
i

l

a

i
t
i

d
e
z

300

200

100

)

s
d
n
o
c
e
s

(

e

m

i
t

n
u

R

0

0

Spectral
MatchEIG
MatchALS
Wang et al.
Ours

5

10
15
Problem Complexity

20

25

Figure 5. Running times of different methods with increasing
problem size: N ∈ [10, 100] and n ∈ [16, 100].

(iii) seek better use cases for our conﬁdence estimates such
as outlier removal.

Acknowledgements: Supported by the grant ANR-16-CE23-0014 (FBI-

MATRIX). Authors thank Haowen Deng for the initial 3D correspon-

dences and, Benjamin Busam and Jesus Briales for fruitful discussions.

i

m

i
t

p

O

11112

References

[1] Sameer Agarwal, Keir Mierle, and Others. Ceres

solver. http://ceres-solver.org.

[2] Federica Arrigoni, Eleonora Maset, and Andrea
Fusiello. Synchronization in the symmetric inverse
semigroup.
In International Conference on Im-
age Analysis and Processing, pages 70–81. Springer,
2017.

[3] Federica Arrigoni, Beatrice Rossi,
and Andrea
Fusiello. Spectral synchronization of multiple views
in se (3).
SIAM Journal on Imaging Sciences,
9(4):1963–1990, 2016.

[4] Florian Bernard, Johan Thunberg, Peter Gemmar,
Frank Hertel, Andreas Husch, and Jorge Goncalves.
A solution for multi-alignment by transformation syn-
chronisation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
2161–2169, 2015.

[5] Florian Bernard, Johan Thunberg, Jorge Goncalves,
and Christian Theobalt.
Synchronisation of par-
tial multi-matchings via non-negative factorisations.
CoRR, abs/1803.06320, 2018.

[6] Tolga Birdal, Emrah Bala, Tolga Eren, and Slobodan
Ilic. Online inspection of 3d parts via a locally over-
lapping camera network. In 2016 IEEE Winter Con-
ference on Applications of Computer Vision (WACV),
pages 1–10. IEEE, 2016.

[7] Tolga Birdal, Umut S¸ ims¸ekli, M. Onur Eken, and
Slobodan Ilic. Bayesian Pose Graph Optimization
via Bingham Distributions and Tempered Geodesic
MCMC. In Advances in Neural Information Process-
ing Systems (NeurIPS), 2018.

[8] Tolga Birdal and Slobodan Ilic. Cad priors for accu-
rate and ﬂexible instance reconstruction. In Proceed-
ings of the IEEE International Conference on Com-
puter Vision, 2017.

[9] Garrett Birkhoff. Tres observaciones sobre el algebra
lineal. Univ. Nac. Tucum ´an Rev. Ser. A, 5:147–151,
1946.

[10] Benjamin Busam, Marco Esposito, Simon Che’Rose,
Nassir Navab, and Benjamin Frisch. A stereo vision
approach for cooperative robotic movement therapy.
In IEEE International Conference on Computer Vision
Workshop (ICCVW), December 2015.

[11] Simon Byrne and Mark Girolami. Geodesic monte
carlo on embedded manifolds. Scandinavian Journal
of Statistics, 40(4):825–845, 2013.

[12] Tib ´erio S Caetano, Julian J McAuley, Li Cheng,
Quoc V Le, and Alex J Smola. Learning graph match-
ing. IEEE transactions on pattern analysis and ma-
chine intelligence, 31(6):1048–1058, 2009.

[13] Kunal N Chaudhury, Yuehaw Khoo, and Amit Singer.
Global registration of multiple point clouds using
semideﬁnite programming. SIAM Journal on Opti-
mization, 25(1):468–501, 2015.

[14] Changyou Chen, Nan Ding, and Lawrence Carin. On
the convergence of stochastic gradient MCMC algo-
rithms with high-order integrators.
In Advances in
Neural Information Processing Systems, pages 2278–
2286, 2015.

[15] Yuxin Chen, Leonidas Guibas, and Qixing Huang.
Near-optimal joint object matching via convex re-
laxation.
In Proceedings of the 31st International
Conference on International Conference on Machine
Learning - Volume 32, ICML’14, pages II–100–II–
108. JMLR.org, 2014.

[16] Minsu Cho, Karteek Alahari, and Jean Ponce. Learn-
ing graphs to match. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 25–
32, 2013.

[17] St ´ephan Cl ´emenc¸ on and J ´er ´emie Jakubowicz. Kan-
torovich distances between rankings with applications
to rank aggregation.
In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases. Springer, 2010.

[18] Luca Cosmo, Andrea Albarelli, Filippo Bergamasco,
Andrea Torsello, Emanuele Rodol `a, and Daniel Cre-
mers. A game-theoretical approach for joint matching
of multiple feature throughout unordered images. In
Pattern Recognition (ICPR), 2016 23rd International
Conference on, pages 3715–3720. IEEE, 2016.

[19] Luca Cosmo, Emanuele Rodol `a, Andrea Albarelli, Fa-
cundo M ´emoli, and Daniel Cremers. Consistent par-
tial matching of shape collections via sparse model-
ing. In Computer Graphics Forum, volume 36, pages
209–221. Wiley Online Library, 2017.

[20] Angela Dai, Matthias Nießner, Michael Zollh ¨ofer,
Shahram Izadi, and Christian Theobalt. Bundlefusion:
Real-time globally consistent 3d reconstruction using
on-the-ﬂy surface reintegration. ACM Transactions on
Graphics (TOG), 36(4):76a, 2017.

[21] Joseph DeGol, Timothy Bretl, and Derek Hoiem. Im-
proved structure from motion using ﬁducial marker
matching. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 273–288, 2018.

[22] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppf-
foldnet: Unsupervised learning of rotation invariant
3d local descriptors. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 602–
618, 2018.

[23] Haowen Deng, Tolga Birdal, and Slobodan Ilic.
Ppfnet: Global context aware local features for robust

11113

3d point matching. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June
2018.

[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. Imagenet: A large-scale hierarchi-
cal image database. In IEEE Conference on Computer
Vision and Pattern Recognition. Ieee, 2009.

[25] Persi Diaconis, Susan Holmes, Mehrdad Shahshahani,
et al. Sampling from a manifold. In Advances in Mod-
ern Statistical Theory and Applications: A Festschrift
in honor of Morris L. Eaton. Institute of Mathematical
Statistics, 2013.

[26] A. Douik and B. Hassibi. Manifold Optimization Over
the Set of Doubly Stochastic Matrices: A Second-
Order Geometry. ArXiv e-prints, Feb. 2018.

[27] Fanny Dufoss ´e, Kamer Kaya, Ioannis Panagiotas, and
Bora Uc¸ ar. Further notes on Birkhoff-von Neumann
decomposition of doubly stochastic matrices. PhD
thesis, Inria-Research Centre Grenoble–Rh ˆone-Alpes,
2017.

[28] Alain Durmus, Umut Simsekli, Eric Moulines, Roland
Badeau, and Ga ¨el Richard.
Stochastic gradient
Richardson-Romberg Markov Chain Monte Carlo. In
Advances in Neural Information Processing Systems,
pages 2047–2055, 2016.

[29] Rizal Fathony, Sima Behpour, Xinhua Zhang, and
Brian Ziebart. Efﬁcient and consistent adversarial bi-
partite matching. In International Conference on Ma-
chine Learning, pages 1456–1465, 2018.

[30] P. Gargallo. Using opensfm. Online. Accessed May-
2018.

[31] Mark Girolami and Ben Calderhead. Riemann mani-
fold Langevin and Hamiltonian Monte Carlo methods.
Journal of the Royal Statistical Society: Series B 118
(Statistical Methodology), 73(2):123–214, 2011.

[32] Michel X Goemans. Smallest compact formulation
for the permutahedron. Mathematical Programming,
2015.

[33] Venu Madhav Govindu. Lie-algebraic averaging for
globally consistent motion estimation.
In Proceed-
ings of the 2004 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition, 2004.
CVPR 2004., volume 1, pages I–I. IEEE, 2004.

[34] Andrew Holbrook. Note on the geodesic monte carlo.
arXiv preprint arXiv:1805.05289, 2018.

[35] Nan Hu, Qixing Huang, Boris Thibert, UG Alpes,
and Leonidas Guibas. Distributable consistent multi-
object matching.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition,
2018.

[36] Qi-Xing Huang and Leonidas Guibas.
Consis-
tent shape maps via semideﬁnite programming.
In Proceedings
of
the Eleventh Eurograph-
ics/ACMSIGGRAPH Symposium on Geometry
Processing. Eurographics Association, 2013.

[37] Wen Huang, Kyle A Gallivan, and P-A Absil. A
broyden class of quasi-newton methods for rieman-
nian optimization. SIAM Journal on Optimization,
25(3):1660–1685, 2015.

[38] Daniel F Huber and Martial Hebert. Fully automatic
registration of multiple 3d data sets. Image and Vision
Computing, 21(7):637–650, 2003.

[39] GLENN Hurlbert. A short proof of the birkhoff-von
neumann theorem. preprint (unpublished), 2008.

[40] Ross Kindermann. Markov random ﬁelds and their
applications. American mathematical society, 1980.

[41] Anna Korba, Alexandre Garcia, and Florence d’Alch ´e
Buc. A structured prediction approach for label rank-
ing. arXiv preprint arXiv:1807.02374, 2018.

[42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. Imagenet classiﬁcation with deep convolutional
neural networks. In Advances in neural information
processing systems, pages 1097–1105, 2012.

[43] Hongdong Li and Richard Hartley. The 3d-3d regis-
tration problem revisited. In Computer Vision, 2007.
ICCV 2007. IEEE 11th International Conference on,
pages 1–8. IEEE, 2007.

[44] Xinchao Li, Martha Larson, and Alan Hanjalic. Pair-
wise geometric matching for large-scale object re-
trieval.
In Proceedings of
the IEEE Conference
on Computer Vision and Pattern Recognition, pages
5153–5161, 2015.

[45] Scott Linderman, Gonzalo Mena, Hal Cooper, Liam
Paninski, and John Cunningham. Reparameterizing
the birkhoff polytope for variational permutation in-
ference. In International Conference on Artiﬁcial In-
telligence and Statistics, pages 1618–1627, 2018.

[46] Chang Liu, Jun Zhu, and Yang Song. Stochastic gra-
dient geodesic mcmc methods. In Advances in Neural
Information Processing Systems, pages 3009–3017,
2016.

[47] David G Lowe. Distinctive image features from scale-
invariant keypoints. International journal of computer
vision, 60(2):91–110, 2004.

[48] Vince Lyzinski, Donniell E Fishkind, Marcelo Fiori,
Joshua T Vogelstein, Carey E Priebe, and Guillermo
Sapiro. Graph matching: Relax at your own risk.
IEEE transactions on pattern analysis and machine
intelligence, 38(1):60–73, 2016.

[49] Yi-An Ma, Tianqi Chen, and Emily Fox. A complete
recipe for stochastic gradient MCMC. In Advances in

11114

Neural Information Processing Systems, pages 2917–
2925, 2015.

[50] Jo ˜ao Maciel and Jo ˜ao Paulo Costeira. A global solu-
tion to sparse correspondence problems. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
25(2), 2003.

[51] E. Maset, F. Arrigoni, and A. Fusiello. Practical and
efﬁcient multi-view matching.
In 2017 IEEE Inter-
national Conference on Computer Vision (ICCV), Oct
2017.

[52] James Munkres. Algorithms for the assignment and
transportation problems. Journal of the society for in-
dustrial and applied mathematics, 5(1):32–38, 1957.

[53] Ra ´ul Mur-Artal and Juan D. Tard ´os. ORB-SLAM2:
an open-source SLAM system for monocular, stereo
and RGB-D cameras. IEEE Transactions on Robotics,
33(5):1255–1262, 2017.

[54] Andy Nguyen, Mirela Ben-Chen, Katarzyna Wel-
nicka, Yinyu Ye, and Leonidas Guibas. An opti-
mization approach to improving collections of shape
maps.
In Computer Graphics Forum, volume 30,
pages 1481–1491. Wiley Online Library, 2011.

[55] Deepti Pachauri, Risi Kondor, Gautam Sargur, and
Vikas Singh. Permutation diffusion maps (pdm) with
application to the image association problem in com-
puter vision. In Advances in Neural Information Pro-
cessing Systems, pages 541–549, 2014.

[56] Deepti Pachauri, Risi Kondor, and Vikas Singh. Solv-
ing the multi-way matching problem by permutation
synchronization.
In Advances in neural information
processing systems, pages 1860–1868, 2013.

[57] Han-Mu Park and Kuk-Jin Yoon. Consistent multiple
graph matching with multi-layer random walks syn-
chronization. Pattern Recognition Letters, 2018.

[58] Sam Patterson and Yee Whye Teh. Stochastic gradi-
ent Riemannian Langevin dynamics on the probability
simplex. In Advances in Neural Information Process-
ing Systems, pages 3102–3110, 2013.

[59] Sergey Plis, Stephen McCracken, Terran Lane, and
Vince Calhoun. Directional statistics on permutations.
In Proceedings of the Fourteenth International Con-
ference on Artiﬁcial Intelligence and Statistics, pages
600–608, 2011.

[60] Torsten Sattler, Tobias Weyand, Bastian Leibe, and
Leif Kobbelt. Image retrieval for image-based local-
ization revisited, 2012.

[61] Michele Schiavinato and Andrea Torsello. Synchro-
nization over the birkhoff polytope for multi-graph
matching. In International Workshop on Graph-Based
Representations in Pattern Recognition, pages 266–
275. Springer, 2017.

[62] Johannes L Schonberger and Jan-Michael Frahm.
Structure-from-motion revisited.
In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, pages 4104–4113, 2016.

[63] Umut S¸ ims¸ekli. Fractional Langevin Monte Carlo:
Exploring L ´evy driven stochastic differential equa-
tions for Markov Chain Monte Carlo. In International
Conference on Machine Learning, pages 3200–3209.
JMLR. org, 2017.

[64] Umut Simsekli, Roland Badeau, Taylan Cemgil, and
Ga ¨el Richard.
Stochastic quasi-Newton Langevin
Monte Carlo. In International Conference on Machine
Learning (ICML), 2016.

[65] Umut S¸ ims¸ekli, C¸ a ˘gatay Yıldız, Thanh Huy Nguyen,
Ga ¨el Richard, and A Taylan Cemgil. Asynchronous
stochastic quasi-Newton MCMC for non-convex op-
timization.
In International Conference on Machine
Learning, 2018.

[66] Amit Singer. Angular synchronization by eigenvectors
and semideﬁnite programming. Applied and computa-
tional harmonic analysis, 30(1):20, 2011.

[67] Amit Singer and Yoel Shkolnisky. Three-dimensional
structure determination from common lines in cryo-
em by eigenvectors and semideﬁnite programming.
SIAM journal on imaging sciences, 4(2):543–572,
2011.

[68] Richard Sinkhorn and Paul Knopp. Concerning non-
negative matrices and doubly stochastic matrices. Pa-
ciﬁc Journal of Mathematics, 21(2):343–348, 1967.

[69] Steven T Smith. Optimization techniques on Rie-
mannian manifolds. Fields institute communications,
3(3):113–135, 1994.

[70] Yifan Sun, Zhenxiao Liang, Xiangru Huang, and Qix-
ing Huang. Joint map and symmetry synchronization.
In Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 251–264, 2018.

[71] Da Tang and Tony Jebara. Initialization and coordi-
nate optimization for multi-way matching. In Artiﬁcial
Intelligence and Statistics, pages 1385–1393, 2017.

[72] Johan Thunberg,
Florian Bernard,
and Jorge
Goncalves. Distributed methods for synchronization
of orthogonal matrices over graphs.
Automatica,
80:243–252, 2017.

[73] Bill Triggs, Philip F McLauchlan, Richard I Hartley,
and Andrew W Fitzgibbon. Bundle adjustment—a
modern synthesis.
In International workshop on vi-
sion algorithms, pages 298–372. Springer, 1999.

[74] Nilesh Tripuraneni, Nicolas Flammarion, Francis
Bach, and Michael I Jordan. Averaging stochastic gra-
dient descent on Riemannian manifolds.
In Confer-
ence on Learning Theory (COLT), 2018.

11115

[75] Roberto Tron and Rene Vidal. Distributed 3-d lo-
calization of camera sensor networks from 2-d image
measurements. IEEE Transactions on Automatic Con-
trol, 59(12):3325–3340, 2014.

[76] Lanhui Wang and Amit Singer. Exact and stable re-
covery of rotations for robust synchronization.
In-
formation and Inference: A Journal of
the IMA,
2(2):145–193, 2013.

[77] Qianqian Wang, Xiaowei Zhou, and Kostas Daniilidis.
Multi-image semantic matching by mining consistent
features. In CVPR, 2018.

[78] Tatiana Xifara, Chris Sherlock, Samuel Livingstone,
Simon Byrne, and Mark Girolami. Langevin dif-
fusions and the Metropolis-adjusted Langevin algo-
rithm.
Statistics & Probability Letters, 91:14–19,
2014.

[79] Junchi Yan, Minsu Cho, Hongyuan Zha, Xiaokang
Yang, and Stephen M Chu. Multi-graph matching via
afﬁnity optimization with graduated consistency regu-
larization. IEEE transactions on pattern analysis and
machine intelligence, 38(6):1228–1242, 2016.

[80] Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng,
Hongyuan Zha, and Xiaokang Yang. A short survey of
recent advances in graph matching. In Proceedings of
the 2016 ACM on International Conference on Multi-
media Retrieval, pages 167–174. ACM, 2016.

[81] Jin-Gang Yu, Gui-Song Xia, Ashok Samal, and Jin-
wen Tian. Globally consistent correspondence of mul-
tiple feature sets using proximal gauss–seidel relax-
ation. Pattern Recognition, 51:255–267, 2016.

[82] Xinru Yuan, Wen Huang, P-A Absil, and Kyle A Gal-
livan. A riemannian limited-memory BFGS algorithm
for computing the matrix geometric mean. Procedia
Computer Science, 80:2147–2157, 2016.

[83] Andrei Zanﬁr and Cristian Sminchisescu. Deep learn-
ing of graph matching.
In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR),
June 2018.

[84] Mikhail Zaslavskiy, Francis Bach, and Jean-Philippe
Vert. A path following algorithm for the graph match-
ing problem. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 31(12):2227–2242, 2009.

[85] Xiaowei Zhou, Menglong Zhu, and Kostas Daniilidis.
Multi-image matching via fast alternating minimiza-
tion. In Proceedings of the IEEE International Con-
ference on Computer Vision, pages 4032–4040, 2015.

11116

Pushing the Boundaries of View Extrapolation with Multiplane Images

Pratul P. Srinivasan1
Richard Tucker2
Ravi Ramamoorthi3
Ren Ng1

Jonathan T. Barron2
Noah Snavely2

1UC Berkeley, 2Google Research, 3UC San Diego

Abstract

We explore the problem of view synthesis from a nar-
row baseline pair of images, and focus on generating high-
quality view extrapolations with plausible disocclusions.
Our method builds upon prior work in predicting a multi-
plane image (MPI), which represents scene content as a set
of RGBα planes within a reference view frustum and ren-
ders novel views by projecting this content into the target
viewpoints. We present a theoretical analysis showing how
the range of views that can be rendered from an MPI in-
creases linearly with the MPI disparity sampling frequency,
as well as a novel MPI prediction procedure that theoret-
ically enables view extrapolations of up to 4× the lateral
viewpoint movement allowed by prior work. Our method
ameliorates two speciﬁc issues that limit the range of views
renderable by prior methods: 1) We expand the range of
novel views that can be rendered without depth discretiza-
tion artifacts by using a 3D convolutional network architec-
ture along with a randomized-resolution training procedure
to allow our model to predict MPIs with increased disparity
sampling frequency. 2) We reduce the repeated texture arti-
facts seen in disocclusions by enforcing a constraint that the
appearance of hidden content at any depth must be drawn
from visible content at or behind that depth.

1. Introduction

View synthesis, the problem of predicting novel views
of a scene from a set of captured images, is a central prob-
lem in computer vision and graphics. The ability to render
nearby views from a single image or a stereo pair can en-
able compelling photography effects such as 3D parallax
and synthetic defocus blur. Furthermore, given a collection
of images of a scene taken from different viewpoints, view
synthesis could enable free-viewpoint navigation for virtual
and augmented reality.
However, there is still a long way to go. State-of-the-art
view synthesis algorithms use their input images to estimate
a 3D scene representation, which can then be reprojected to
render novel views. This approach works well for content

s
e
g
a

m

I

t

u
p
n

I

w

e

i

V

l

e
v
o

N

]

8
3

[

I

P

M

l

a
n

i

g

i
r

O

w

e

i

V

s

t

c
a

f
i
t
r

A

n
o

i
t

a
z

i
t

e

r

c
s

i

D

h

t

p
e

D

s

t

c
a

f
i
t
r

A

”
e

r

u

t

x
e

T

d
e

t

a
e
p
e

R

“

l

e
v
o

N

r

u

O

Figure 1. Given two input images taken from nearby viewpoints,
our algorithm predicts an MPI scene representation that can render
view extrapolations with disocclusions. Our model improves upon
prior work in two speciﬁc ways: 1) We reduce depth discretization
artifacts due to insufﬁcient depth sampling, as seen in the red zoom
of the wood table. 2) We mitigate the repeated texture artifacts
produced by prior methods by predicting plausible hidden scene
content, as shown in the blue and green zooms where we predict
realistic textures behind the fruit bowl and lamp.

visible in the input images, but the quality of novel views
degrades rapidly as the target viewpoint moves further away
from the input views, thereby revealing more previously-
occluded scene content. In this work, we study the prob-
lem of view extrapolation where regions of the rendered im-
ages observe disoccluded content, and focus speciﬁcally on
demonstrating view synthesis from a stereo input.

We build upon a state-of-the-art deep learning approach
for view synthesis [38] that predicts a scene representation

175

called a multiplane image (MPI) from an input narrow-
baseline stereo pair. An MPI consists of a set of fronto-
parallel RGBα planes sampled within a reference view
camera frustum, as illustrated by Figure 2. Diffuse volu-
metric scene representations such as the MPI are becom-
ing increasingly popular for view synthesis for a number of
reasons: 1) They can represent geometric uncertainty in am-
biguous regions as a distribution over depths, thereby trad-
ing perceptually-distracting artifacts in those ambiguous re-
gions for a more visually-pleasing blur [17, 21]. 2) They are
able to convincingly simulate non-Lambertian effects such
as specularities [38]. 3) They are straightforward to repre-
sent as the output of a CNN and they allow for differentiable
rendering, which enables us to train networks for MPI pre-
diction using only triplets of frames from videos for input
and supervision [38]. In this work, we extend the MPI pre-
diction framework to enable rendering high-quality novel
views up to 4× further from the reference view than was
possible in prior work. Our speciﬁc contributions are:

Theoretical analysis of MPI limits (Section 3.2). We
present a theoretical framework, inspired by Fourier theory
of volumetric rendering and light ﬁelds, to analyze the lim-
its of views that can be rendered from diffuse volumetric
representations such as the MPI. We show that the extent of
renderable views is limited by the MPI’s disparity sampling
frequency, even for content visible in both the input and ren-
dered views, and that this “renderable range” increases lin-
early with the MPI’s disparity sampling frequency.

Improved view extrapolation for visible content (Sec-
tion 3.3). View extrapolation in previous work on MPIs is
limited in part by a network architecture that ﬁxes the num-
ber of disparity planes during training and testing. Increas-
ing the renderable range of an MPI by simply increasing its
ﬁxed number of planes during training is not computation-
ally feasible due to the memory limits of current GPUs. We
present a simple solution that increases disparity sampling
frequency at test time by replacing the previously used 2D
convolutional neural network (CNN) with a 3D CNN archi-
tecture and a randomized-resolution training procedure. We
demonstrate that this change reduces the depth discretiza-
tion artifacts found in distant views rendered by prior work,
as shown in Figure 1.

Predicting disoccluded content for view extrapolation
(Section 4). We observe and explain why MPIs predicted
by prior work [38] contain approximately the same RGB
content at each plane, and differ only in α. This behav-
ior results in unrealistic disocclusions with repeated texture
artifacts, as illustrated in Figure 1. In general, the appear-
ance of hidden scene content is inherently ambiguous, so
training a network to simply minimize the distance between
rendered and ground truth target views tends to result in
unrealistic hallucinations of this occluded content. We pro-

pose to improve the realism of predicted disocclusions by
constraining the appearance of occluded scene content at
every depth to be drawn from visible scene points at or be-
yond that depth, and present a two-step MPI prediction pro-
cedure that enforces this constraint. We demonstrate that
this strategy forces predicted disocclusions to contain plau-
sible textures, alleviates the artifacts found in prior work,
and produces more compelling extrapolated views than al-
ternative approaches, as illustrated in Figures 1 and 5.

2. Related Work

Traditional approaches for view synthesis. View synthe-
sis is an image-based rendering (IBR) task, with the goal of
rendering novel views of scenes given only a set of sampled
views. It is useful to organize view synthesis algorithms by
the extent to which they use explicit scene geometry [24].
At one extreme are light ﬁeld rendering [11, 18] techniques,
which require many densely sampled input images so that
they can render new views by simply slicing the sampled
light ﬁeld without relying on accurate geometry estimation.
At the other extreme are techniques such as view dependent
texture mapping that rely entirely on an accurate estimated
global mesh and then reproject and blend the texture from
nearby input views to render new views [5].
Many successful modern approaches to view synthe-
sis [3, 13, 21, 40] follow a strategy of computing detailed
local geometry for each input view followed by forward
projecting and blending the local texture from multiple in-
put views to render a novel viewpoint. This research has
traditionally focused on interpolating between input views
and therefore does not attempt to predict content that is oc-
cluded in all input images. In contrast, we focus on the case
of view extrapolation, where predicting hidden scene con-
tent is crucial for rendering compelling images.

Deep learning approaches for view synthesis. Recently, a
promising line of work has focused on training deep learn-
ing pipelines end-to-end to render novel views. One class of
methods focuses on the challenging problem of training net-
works to learn about geometry and rendering from scratch
and synthesize arbitrarily-distant views from such limited
input as a single view [7, 20, 39]. However, the lack of built-
in geometry and rendering knowledge limits these methods
to synthetic non-photorealistic scenarios.
Other end-to-end approaches have focused on photore-
alistic view synthesis by learning to model local geometry
from a target viewpoint and using this geometry to back-
wards warp and blend input views. This includes algorithms
for interpolating between views along a 1D camera path [9],
interpolating between four input corner views sampled on a
plane [15], and expanding a single image into a local light
ﬁeld of nearby views [28]. These methods separately pre-
dict local geometry for every novel viewpoint and are not

176

able to guarantee consistency between these predictions, re-
sulting in temporal artifacts when rendering a sequence of
novel views. Furthermore, the use of backward projection
means that disoccluded regions must be ﬁlled in with repli-
cas of visible pixels, so these techniques are limited in their
ability to render convincing extrapolated views.
The most relevant methods to our work are algorithms
that predict a 3D scene representation from a source im-
age viewpoint and render novel views by differentiably for-
ward projecting this representation into each target view-
point. This approach ensures consistency between rendered
views and allows for the prediction of hidden content. Tul-
siani et al. and Dhamo et al. predict a layered depth image
(LDI) representation [6, 31], but this approach is unable to
approximate non-Lambertian reﬂectance effects. Further-
more, training networks to predict LDIs using view synthe-
sis as supervision has proven to be difﬁcult, and the train-
ing procedure requires a regularization term that encourages
hidden content to resemble occluding content [31], limit-
ing the quality of rendered disocclusions. Zhou et al. pro-
posed the MPI scene representation [38], where novel views
are rendered by forward projecting and alpha compositing
MPI layers, and a deep learning pipeline is used to train an
MPI prediction network using held-out views as supervi-
sion. They demonstrated that the MPI scene representation
can convincingly render parallax and non-Lambertian ef-
fects for a small range of rendered views. We build upon
this work and present a theoretical analysis of limits on
views rendered from MPIs as well as a new MPI predic-
tion framework that is able to render more compelling view
extrapolations with disocclusions.

Inpainting occluded content. Predicting the appearance
of content hidden behind visible surfaces can be thought of
as 3D scene inpainting. The problem of inpainting in 2D
images has an extensive history [12], ranging from early
propagation techniques [2] to modern CNN-based inpaint-
ing [36]. However, such algorithms must be applied sepa-
rately to each rendering and therefore do not ensure consis-
tency between different views of the same occluded content.
A few recent works [1, 14, 22, 29] focus on multi-view
inpainting, i.e. removing objects and inpainting the result-
ing empty pixels in a collection of multiple input images.
This strategy operates on input image collections instead of
scene representations, so it cannot be used to predict oc-
cluded content that only appears during view extrapolation.
Finally, a recent line of work [8, 27, 34] focuses on scene
shape completion. These methods require an input depth
image and only focus on inpainting the shape and seman-
tics of hidden content and not its appearance, so the pre-
dicted scenes cannot be used for rendering novel views. In
contrast to prior methods, our work addresses the problem
of jointly inpainting the geometry, color, and opacity of hid-
den content in scenes to render convincing disocclusions.

3D MPI Scene Representation

Input Image 1 (Reference Viewpoint)

Input Image 2

MPI Prediction

Figure 2. MPI scene representation. Our work builds on the MPI
scene representation and prediction procedure introduced in [38].
We train a deep network that takes two narrow-baseline images
of a scene as input (captured at the blue and green camera poses
shown above), and predicts an MPI scene representation, consist-
ing of a set of fronto-parallel RGBα planes within a reference
camera frustum (signiﬁed by the green camera above). Novel
views are rendered by alpha compositing along rays from the MPI
voxels into the novel viewpoint.

3. View Extrapolation for Visible Content

3.1. MPI scene representation

The multiplane image (MPI) scene representation, intro-
duced by Zhou et al. [38] and illustrated in Figure 2, con-
sists of a set of fronto-parallel RGBα planes within a ref-
erence camera’s view frustum. An MPI can be thought of
as a frustum-shaped volumetric scene representation where
each “voxel” consists of a diffuse RGB color and opacity
α. Novel views are rendered from an MPI by alpha com-
positing the color along rays into the novel view using the
“over” operator [17, 23], which is easily implemented as
homography-warping each MPI plane onto the sensor plane
of the novel view (see Equation 2 in Zhou et al. [38]), and
alpha compositing the resulting images from back to front.

3.2. Theoretical signal processing limits for render(cid:173)
ing visible content

Perhaps surprisingly, there is a limit on views that can
be rendered with high ﬁdelity from an MPI, even if we just
consider mutually-visible content, i.e., content visible from
all input and target viewpoints. Rendering views beyond
this limit results in depth discretization artifacts similar to
aliasing artifacts seen in volume rendering [17].
We formalize this effect in the context of MPI render-
ings, and make use of Fourier theory to derive a bound
on viewpoints that can be rendered from an MPI with
high ﬁdelity. Our model of rendering mutually-visible con-
tent from an MPI is conceptually similar to Frequency do-
main volume rendering [30] using a shear-warp factoriza-

177

1

2Δd

1

2Δx

Δ

d

= a Δ

d

= b Δ

d

= c

(a) MPI Spatial Domain

(b) MPI Fourier Domain

(c) Renderable Range

Figure 3. Viewpoint limits for rendering visible content from an MPI. Views rendered from an MPI without occlusions can be expressed
as sheared integral projections of that MPI. (a) Here, we visualize a 2D slice from an MPI, where the y dimension is constant and only
the x and z dimensions vary. This MPI is in the reference viewpoint v0 . (b) In the frequency domain, rendered views are equivalent
to 1D slices of the 2D MPI spectrum, where views further from the reference viewpoint correspond to Fourier slices at steeper slopes.
The MPI spectrum is bandlimited due to its spatial and disparity sampling frequencies, so there is a range of viewpoints outside which
rendered views will have a lower spatial bandwidth than the original MPI plane images. Viewpoint v1 represents the maximum extent of
this “renderable range”, and v2 represents a viewpoint outside this range. (c) The renderable range of views is shaped like a truncated cone,
and we visualize how the range of renderable views shrinks linearly as we increase the disparity sampling interval ∆d from a < b < c.

tion [16]. Additionally, our derivation of an MPI’s “ren-
derable range” is inspired by derivations for a 3D display’s
depth-of-ﬁeld [41] and light ﬁeld photography’s “refocus-
able range” [19]. Our main insight is that the 2D Fourier
Transform of a view rendered from an MPI can be con-
sidered as a 2D slice through the 3D Fourier Transform
of the MPI. An MPI is bandlimited by its ﬁxed sampling
frequency, so there exists a range of viewpoints outside of
which rendered views will have a smaller spatial frequency
bandwidth than the input images, potentially resulting in
aliasing artifacts. We cover the main steps of this deriva-
tion below. Please refer to our supplementary materials for
detailed intermediate steps and diagrams.
Let us consider rendering views from an MPI in the sim-
pliﬁed case where (a) the camera is translated but not ro-
tated, and (b) there is no occlusion, so all content is equally
visible from every viewpoint. The rendered view ru,s (x) at
a lateral translation u and axial translation s relative to the
reference camera center can then be expressed as:

ru,s (x) = Xd∈D

c(x

′ , d) = Xd∈D

c ((1 − sd) x + ud, d) (1)

where c(x, d) is the pre-multiplied RGBα at each pixel co-
ordinate x and disparity plane d within the set of MPI dis-
parity planes D . Note that u and s are in units of pixels
(such that the camera focal length f = 1), and we limit s to
the range −∞ < s < 1/dmax because renderings are not de-
ﬁned for viewpoints within the MPI volume. Additionally,
note that the disparity d is in units 1/pixel .
To study the limits of views rendered from an MPI, let
us consider a worst-case MPI with content in the subset of
closest planes, for which we make a locally linear approxi-
mation to the coordinate transformation (x, d) → (x

′ , d):

where dmax is a constant. Now, we have expressed the ren-
dering of mutually-visible content as a sheared and dilated
integral projection of the MPI. We apply the generalized
Fourier slice theorem [19] to interpret the Fourier trans-
form of this integral projection as a 2D slice through the
3D MPI’s Fourier transform. The resulting rendered view is
the slice’s inverse Fourier transform:

ru,s (x) = F −1 (cid:26)C (cid:18)

kx
1 − sdmax

,

1 − sdmax (cid:19)(cid:27) (3)
−ukx

where F −1 is the inverse Fourier transform and C (kx , kd )
is the Fourier transform of c(x, d).
An MPI is a discretized function, so the frequency sup-
port of C lies within a box bounded by +−1/2∆x and +−1/2∆d ,
where ∆x is the spatial sampling interval (set by the num-
ber of pixels in each RGBα MPI plane image) and ∆d is
the disparity sampling interval (set by the number of MPI
planes within the MPI disparity range).
Figures 3a and 3b illustrate Fourier slices through the
MPI’s Fourier transform that correspond to rendered views
from different lateral positions. Rendered views further
from the reference view correspond to slices at steeper
slopes. There is a range of slice slopes within which the
spatial bandwidth of the rendered views is equal to that of
the MPI, and outside of which the spatial bandwidth of the
rendered views decreases linearly with the slice slope.
We can solve for the worst-case “renderable range” by
determining the set of slopes whose slices intersect the box
in Figure 3b at the spatial frequency boundary +−1/2∆x .
This provides constraints on camera positions (u, s), within
which rendered views enjoy the full image bandwidth:

s ≤ 0,

|u| ≤

∆x (1 − sdmax )
∆d

(4)

ru,s (x) = Xd∈D

c ((1 − sdmax ) x + ud, d)

(2)

Figure 3c plots the renderable ranges with varying dis-
parity intervals ∆d , for an MPI with disparities up to dmax .

178

The allowed lateral motion extent increases linearly as the
target viewpoint moves further axially from the MPI, start-
ing at the reference viewpoint. Decreasing ∆d linearly in-
creases the amount of allowed lateral camera movement. In-
tuitively, when rendering views at lateral translations from
the reference viewpoint, the renderable range boundary cor-
responds to views in which adjacent MPI planes are shifted
by a single pixel relative to each other before compositing.

3.3. Increasing disparity sampling frequency with
3D CNN and randomized(cid:173)resolution training

Section 3.2 establishes that additional MPI planes in-
creases the view extrapolation ability, and that this rela-
tionship is linear. Accordingly, the range of extrapolated
views rendered by the original MPI method [38] is limited
because it uses a 2D CNN to predict a small ﬁxed number
of planes (32 planes at a spatial resolution of 1024×576).
Simply increasing this ﬁxed number of planes in the net-
work is computationally infeasible during training due to
GPU memory constraints. Additionally, training on smaller
spatial patches to allow for increased disparity sampling fre-
quency prevents the network from utilizing larger spatial
receptive ﬁelds, which is important for resolving depth in
ambiguous untextured regions.
We propose a simple solution to predict MPIs at full res-
olution with up to 128 planes at test time by using a 3D
CNN architecture, theoretically increasing the view extrap-
olation ability by 4×. The key idea is that because our net-
work is fully 3D convolutional along the height, width, and
depth planes dimensions, it can be trained on inputs with
varying height, width, and number of depth planes. We use
training examples across a spectrum of MPI spatial and dis-
parity sampling frequencies that ﬁt in GPU memory, rang-
ing from MPIs with low spatial and high disparity sampling
frequency (128 planes) to MPIs with high spatial and low
disparity sampling frequency (32 planes). Perhaps surpris-
ingly, we ﬁnd that the trained network learns to utilize a
receptive ﬁeld equal to the maximum number of spatial and
disparity samples it sees during training, even though no in-
dividual training example is of that size.
Our MPI prediction network takes as input a plane-
sweep-volume tensor of size [H, W, D , 3N ], where H and
W are the image height and width, D = |D | is the num-
ber of disparity planes, and N is the number of input im-
ages (N = 2 in our experiments). This tensor is created
by reprojecting each input image to disparity planes D in a
reference view frustum. We use a 3D encoder-decoder net-
work with skip connections and dilated convolutions [35] in
the network bottleneck, so that the network’s receptive ﬁeld
can encompass the maximum spatial and disparity sampling
frequencies used during training. Please refer to our sup-
plementary materials for a more detailed description of our
network architecture and training procedure.

4. View Extrapolation for Hidden Content

In the previous section, we described how view extrapo-
lation is limited by the disparity sampling frequency, which
is a fundamental property of the MPI scene representation.
View extrapolation is also limited by the quality of hid-
den content, which is instead a property of the MPI pre-
diction model. Models that train a CNN to directly predict
an MPI from an input plane-sweep-volume (which contains
homography-warped versions of the same RGB content at
each plane) learn the undesirable behavior of predicting ap-
proximately the same RGB content at each MPI plane with
variation only in α (see Figure 5 in Zhou et al. [38]). We
observe that this behavior is consistent for models that use
either the original 2D CNN architecture or our 3D CNN ar-
chitecture (Section 3.3). Copies of the same RGB content
at different MPI layers lead to “repeated texture” artifacts
in extrapolated views, where disoccluded content contains
repeated copies of the occluder, as visualized in Figure 1.
We believe that this undesirable learned behavior is due
to both the inductive bias of CNNs that directly predict an
MPI from a plane-sweep-volume and the output uncertainty
for disocclusions. The probability distribution over hidden
scene content, conditioned on observed content, is highly
multimodal—there may be many highly plausible versions
of the hidden content. As a result, training a network to
minimize the distance between rendered and ground truth
views produces unrealistic predictions of disocclusions that
are some mixture over the space of possible outputs.
We propose to reduce the output uncertainty by con-
straining the predicted hidden content at any depth, such
that its appearance is limited to re-using visible scene con-
tent at or behind that depth. This effectively forces the
network to predict occluded scene content by copying tex-
tures and colors from nearby visible background content.
One possible limitation is that this constraint will have dif-
ﬁculty predicting the appearance of self-occlusions where
an object extends backwards perpendicular to the viewing
direction. However, as argued by the generic viewpoint as-
sumption [10], it is unlikely that our reference viewpoint
happens to view an object exactly at the angle at which it
extends backwards along the viewing direction. In general,
the majority of disoccluded pixels view background content
instead of self-occlusions.
We enforce our constraint on the appearance of occluded
content with a two-step MPI prediction procedure. The ﬁrst
step provides an initial estimate of the geometry and appear-
ance of scene content visible from the reference viewpoint.
The second step uses this to predict a ﬁnal MPI where the
color at each voxel is parameterized by a ﬂow vector that
points to a visible surface’s color to copy.
In the ﬁrst step, an input plane-sweep volume p is con-
structed by reprojecting j input images ivj , each captured at
a viewpoint vj , to disparity planes d ∈ D . The 3D CNN Φ1

179

Remove 
Occluded 
Content

3D CNN

Φ1

Final Opacities 
fin(x,y,d)

α

2D Flows f(x,y,d)

3D CNN

Φ2

Accumulated  
Renderings rvis(x,y,d)

Input PSVs p(x,y,d,j)

Initial MPI:  
cinit(x,y,d), 
 init(x,y,d)

α

Visible MPI:  
cvis(x,y,d), 
 vis(x,y,d)

α

Flow-Based Gather

Final MPI:  
cfin(x,y,d), 
 fin(x,y,d)

α

Figure 4. Two-step MPI prediction pipeline. We propose a two-step procedure to predict convincing hidden content in an MPI for view
extrapolation. In the ﬁrst step, a 3D CNN predicts an initial MPI from the input images’ plane-sweep-volumes. Next, occluded content
in this MPI is softly removed, resulting in a “ﬁrst-visible-surface” MPI. In the second step, another 3D CNN predicts ﬁnal MPI opacities
and a 2D ﬂow vector for each MPI voxel. The ﬁnal MPI RGB colors are computed by using these predicted ﬂows to gather RGB colors
from back-to-front cumulative renderings of the visible content. This encourages hidden content at any depth to be synthesized by copying
textures of visible content at or behind the same depth, which reduces the output space uncertainty for hidden content and thereby enables
convincing view extrapolation with realistic disocclusions.

of Section 3.3 takes this plane-sweep volume and predicts
an initial MPI’s RGB and α values, cinit and αinit :

(5)

cinit (x, y , d), αinit (x, y , d) = Φ1 (cid:0)p(x, y , d, j )(cid:1).

This initial MPI typically contains repeated foreground tex-
tures in occluded regions of the scene. In the second step
of our procedure, we aim to preserve the predicted geome-
try and appearance of the ﬁrst visible surface from the ini-
tial MPI while re-predicting the appearance and geometry
of hidden content and enforcing our ﬂow-based appearance
constraint. We softly remove hidden RGB content from this
initial MPI by multiplying each MPI RGB value by its trans-
mittance t relative to the reference viewpoint v0 :

tv0 (x, y , d) = αinit (x, y , d) Yd′>d
cvis (x, y , d) = cinit (x, y , d)tv0 (x, y , d)
αvis (x, y , d) = tv0 (x, y , d)

[1 − αinit (x, y , d′ )] (6)

(7)

where cvis and αvis are the MPI RGBα planes from which
content that is occluded from the reference view has been
softly removed. Intuitively, a voxel’s transmittance (Equa-
tion 6) describes the extent to which an MPI voxel’s color
contributes to the rendered reference view.
A second CNN Φ2 takes this reference-visible MPI,
consisting of cvis and αvis , as input and predicts opaci-
ties αﬁn (x, y , d) and a 2D ﬂow vector for each MPI voxel

f (x, y , d) = [fx (x, y , d), fy (x, y , d)]:

αﬁn (x, y , d), f (x, y , d) = Φ2 (cid:0)cvis (x, y , d), αvis (x, y , d)(cid:1). (8)
The ﬁnal MPI’s colors cﬁn (x, y , d) are computed by using
these predicted ﬂows to gather colors from renderings of the
visible content at or behind each plane rvis (x, y , d):

rvis (x, y , d) = Xd′≤d (cid:2)cvis (x, y , d′ )(cid:3)
cﬁn (x, y , d) = rvis (x + fx (x, y , d), y + fy (x, y , d), d) .

(9)

We gather the color from rvis using bilinear interpolation
for differentiability. This constraint restricts the appearance
of hidden content at each depth to be drawn from visible
scene points at or beyond that depth.

5. Training Loss

As in Zhou et al. [38], we train our MPI prediction
pipeline using view synthesis as supervision. Our training
loss is simply the sum of reconstruction losses for render-
ing a held-out novel view rgt at target camera pose vt , us-
ing both our initial and ﬁnal predicted MPIs. These MPIs
are predicted from input images iv0 and iv1 . We use a
deep feature matching loss LVGG for layers from the VGG-
19 network [26], using the implementation of Chen and
Koltun [4]. The total loss L for each training example is:

L =LVGG (rinit (iv0 , iv1 , vt ), rgt )+

LVGG (rﬁn (iv0 , iv1 , vt ), rgt )

(10)

where rinit and rﬁn are rendered views from the initial and
ﬁnal predicted MPIs.

6. Results

The following section presents quantitative and qualita-
tive evidence to validate the beneﬁts of our method. Please
view our supplementary video for rendered camera paths
that demonstrate our predicted MPIs’ ability to render high
quality extrapolated views that are consistent with a 3D
scene representation and contain realistic disocclusions.

6.1. Experiment details

We train and evaluate on the open-source YouTube Real
Estate 10K dataset [38]1 , which contains approximately
10,000 YouTube videos of indoor and outdoor real estate

1 https://google.github.io/realestate10k/

180

Algorithm
Original MPI [38]

Our rinit

rinit + Adversarial Disocclusion
Disocclusion Inpainting [36]
Our rﬁn

SSIMfov SSIMocc NATocc
0.838
0.803
0.805
0.858
0.811
0.904
0.853
0.791
0.849
0.808
0.691
0.227
0.853
0.814
0.931

Table 1. Quantitative evaluation. Images rendered from our pre-
dicted MPIs are quantitatively superior to those rendered from the
original MPI model [38]. Furthermore, our method predicts dis-
occlusions that are both closer to the ground truth hidden content
and more perceptually plausible than alternative methods.

scenes along with computed camera poses for each video
frame. We generate training examples on the ﬂy by sam-
pling two source frames and a target frame from a randomly
chosen video, so that the target image is not in between the
source images (and therefore requires view extrapolation,
not view interpolation) for ∼87% of the training examples.
The dataset is split into 9,000 videos for training and
1,000 for testing, where the test set videos do not overlap
with those in the training dataset. From these test videos,
we randomly sample 6,800 test triplets, each consisting of
two input frames and a single target frame.

6.2. Evaluation metrics

We use three metrics for our quantitative comparisons:
SSIMfov : To evaluate the overall quality of rendered im-
ages, we use the standard SSIM [32] metric computed over
the region of the target image that views all MPI planes.
SSIMocc : To speciﬁcally assess the accuracy of predicted
disocclusions, we evaluate SSIM over the subset of pixels
that were not visible from the input reference viewpoint.
We determine whether a pixel in a rendered target image
is disoccluded by examining the MPI voxels that contribute
to the rendered pixel’s value, and thresholding the maxi-
mum change in transmittance of these contributing voxels
between the reference and target viewpoint. Similarly to
Equation 6, we can compute the transmittance of each MPI
voxel from a target viewpoint vt as:

tvt (x, y , d) = αvt (x, y , d) Yd′>d

[1 − αvt (x, y , d′ )]

(11)

where αvt is an MPI α plane homography-warped onto the
sensor plane of viewpoint vt . We consider a pixel (x, y)
in the target rendered view as a member of the disoccluded
pixels set H if the transmittance t of any contributing MPI
voxel is some threshold value greater than the same voxel’s
transmittance when rendering the reference viewpoint:
H = (cid:26)(x, y) : max
d (cid:0)tvt (x, y , d) − tv0→vt (x, y , d)(cid:1) ≥ ǫ(cid:27) (12)
where tv0→vt is the transmittance relative to the reference
viewpoint, warped into the target viewpoint so that both

transmittances are in the same reference frame. We com-
pute disoccluded pixels using αinit for all models, to ensure
that each model is evaluated on the same set of pixels. We
set ǫ = 0.075 in our experiments. Please see our supple-
mentary materials for visualizations of disoccluded pixels.
NATocc : To quantify the perceptual plausibility of predicted
disoccluded content, we evaluate a simple image prior over
disoccluded pixels. We use the negative log of the Earth
Mover’s (Wasserstein-1) distance between gradient magni-
tude histograms of the rendered disoccluded pixels and the
ground-truth pixels in each target image.
Intuitively, re-
alistic rendered image content should have a distribution
of gradients that is similar to that of the true natural im-
age [25, 33], and therefore a higher NATocc score.

6.3. Comparison to baseline MPI prediction

We ﬁrst show that renderings from both our initial and
ﬁnal predicted MPIs (rinit and rﬁn ) are superior to those
from the original MPI method [38], which was demon-
strated to signiﬁcantly outperform other recent view synthe-
sis methods [15, 37]. The increase in SSIMfov from “Orig-
inal MPI” (Table 1 row 2) to “Our rinit ” (row 3) demon-
strates the improvement from our method’s increased dis-
parity sampling frequency. Furthermore, the increase in
SSIMocc and NATocc from “Original MPI” (row 2) to “Our
rﬁn ” (row 6) demonstrates that our method predicts disoc-
cluded content that is both closer to the ground truth and
more plausible. Figure 5 qualitatively demonstrates that
renderings from our method contain fewer depth discretiza-
tion artifacts than renderings from the original MPI work,
and that renderings from our ﬁnal MPI contain more realis-
tic disocclusions without “repeated texture” artifacts.

6.4. Evaluation of hidden content prediction

We compare occluded content predicted by our model to
the following alternative disocclusion prediction strategies:
Our rinit : We ﬁrst compare renderings “Our rﬁn ” from our
full method to the ablation “Our rinit ”, which does not
enforce our ﬂow-based occluded content appearance con-
straint. The improvement in SSIMocc and NATocc from Ta-
ble 1 row 3 to row 6 and the qualitative results in Figure 5
demonstrate that our full method renders disocclusions that
are both closer to the ground truth and more perceptually
plausible with fewer “repeated texture” artifacts.
“rinit + Adversarial Disocclusions”: Next, we compare to
an alternative two-step MPI prediction strategy. We use an
identical Φ1 to predict the initial MPI, but Φ2 directly pre-
dicts RGBα planes instead of α and ﬂow planes. We apply
an adversarial loss to the resulting rendered target image to
encourage realistic disocclusions (additional details in our
supplementary materials). Table 1 row 4 demonstrates that
this strategy renders disocclusions that are less accurate but
more perceptually plausible than the original MPI method,

181

Input View 1

Our Rendered Novel View rfin

Truth

Orig. MPI [38]

Input View 2

Inpaint [36]

Adversarial

Our rinit

Our rfin

Input View 1

Our Rendered Novel View rfin

Truth

Orig. MPI [38]

Input View 2

Inpaint [36]

Adversarial

Our rinit

Our rfin

Figure 5. Qualitative comparison of rendered novel views. Our method predicts MPIs with convincing hidden content, as demonstrated
by the disoccluded foliage textures to the left of the wooden pole in the top example, and the disoccluded region to the left of the grey pillow
in the bottom example. Renderings from alternative methods contain depth discretization artifacts, implausible colors, blurry textures, and
repeated textures in disoccluded regions.

due to the adversarial loss. However, Figure 5 demonstrates
that the renderings from our full method contain sharper
content and more accurate colors than those of the “rinit +
Adversarial Disocclusions” strategy. We hypothesize that
this is due to the difﬁculty of training a discriminator net-
work when the number and location of “fake” disoccluded
pixels varies drastically between training examples.

“Disocclusion Inpainting”: Finally, we compare to an
image-based disocclusion prediction strategy. We remove
the disoccluded pixels from our ﬁnal MPI renderings and
re-predict them using a state-of-the-art deep learning image
inpainting model [36]. Table 1 row 5 shows that this strat-
egy results in an overall quality reduction, especially for the
accuracy and plausibility of disoccluded regions. Figure 5
visualizes the unrealistic inpainting results. Furthermore, as
shown in our video, predicting disocclusions separately for
each rendered image creates distracting temporal artifacts
in rendered camera paths because the appearance of disoc-
cluded content changes with the viewpoint.

7. Conclusion

We have presented a theoretical signal processing anal-
ysis of limits for views that can be rendered from an MPI
scene representation, and a practical deep learning method
to predict MPIs that theoretically allow for 4× more lat-
eral movement in rendered views than prior work. This im-
provement is due to our method’s ability to predict MPIs
with increased disparity sampling frequency and our ﬂow-
based hidden content appearance constraint to predict MPIs
that render convincing disocclusion effects. However, there
is still a lot of room for improvement in predicting scene
representations for photorealistic view synthesis that con-
tain convincing occluded 3D content and are amenable to
deep learning pipelines, and we hope that this work inspires
future progress along this exciting research direction.

Acknowledgments We thank John Flynn and Ben Mildenhall
for fruitful discussions, and acknowledge support from ONR grant
N000141712687, NSF grant 1617234, and an NSF fellowship.
This work was done while PPS interned at Google Research.

182

References

[1] Seung-Hwan Baek, Inchang Choi, and Min H. Kim. Mul-
tiview image completion with space structure propagation.
CVPR, 2016.
[2] Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and
Coloma Ballester. Image inpainting. In Proceedings of SIG-
GRAPH, 2000.
[3] Gaurav Chaurasia, Sylvain Duch ˆene, Olga Sorkine-
Hornung, and George Drettakis. Depth synthesis and local
warps for plausible image-based navigation. In ACM Trans-
actions on Graphics (SIGGRAPH), 2013.
[4] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reﬁnement networks. ICCV, 2017.
[5] Paul Debevec, Camillo J. Taylor, and Jitendra Malik. Mod-
eling and rendering architecture from photographs: A hy-
brid geometry-and image-based approach. In Proceedings of
SIGGRAPH, 1996.
[6] Helisa Dhamo, Keisuke Tateno, Iro Laina, Nassir Navab, and
Federico Tombari. Peeking behind objects: Layered depth
prediction from a single image. arXiv:1807.08776, 2018.
[7] S. M. Ali Eslami, Danilo J. Rezende, Frederic Besse, Fabio
Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruderman,
Andrei A. Rusu, Ivo Danihelka, Karol Gregor, et al. Neural
scene representation and rendering. In Science, 2018.
[8] Michael Firman, Oisin Mac Aodha, Simon Julier, and
Gabriel J. Brostow. Structured prediction of unobserved vox-
els from a single depth image. CVPR, 2016.
[9] John Flynn, Ivan Neulander, James Philbin, and Noah
Snavely. DeepStereo: Learning to predict new views from
the world’s imagery. CVPR, 2016.
[10] William T. Freeman. Exploiting the generic viewpoint as-
sumption. In IJCV, 1996.
[11] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and
Michael F. Cohen. The lumigraph. In Proceedings of SIG-
GRAPH, 1996.
[12] Christine Guillemot and Olivier Le Meur. Image inpainting:
Overview and recent advances. In IEEE Signal Processing
Magazine, 2014.
[13] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,
George Drettakis, and Gabriel Brostow. Deep blending for
free-viewpoint image-based rendering. In ACM Transactions
on Graphics (SIGGRAPH Asia), 2018.
[14] Joel Howard, Bryan S. Morse, Scott Cohen, and Brian L.
Price. Depth-based patch scaling for content-aware stereo
image completion. WACV, 2014.
[15] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-
mamoorthi. Learning-based view synthesis for light ﬁeld
cameras.
In ACM Transactions on Graphics (SIGGRAPH
Asia), 2016.
[16] Philippe Lacroute and Marc Levoy. Fast volume rendering
using a shear-warp factorization of the viewing transforma-
tion. In Proceedings of SIGGRAPH, 1994.
[17] Marc Levoy. Display of surfaces from volume data. In IEEE
Computer Graphics and Applications, 1988.
[18] Marc Levoy and Pat Hanrahan. Light ﬁeld rendering.
Proceedings of SIGGRAPH, 1996.

In

[19] Ren Ng. Fourier slice photography. In ACM Transactions on
Graphics (SIGGRAPH), 2005.
[20] Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan,
and Alexander C. Berg. Transformation-grounded image
generation network for novel 3D view synthesis. ECCV,
2016.
[21] Eric Penner and Li Zhang. Soft 3D reconstruction for view
synthesis. In ACM Transactions on Graphics (SIGGRAPH
Asia), 2017.
[22] Julien Philip and George Drettakis. Plane-based multi-view
inpainting for image-based rendering in large scenes. I3D,
2018.
[23] Thomas Porter and Tom Duff. Compositing digital images.
SIGGRAPH, 1984.
[24] Heung-Yeung Shum and Sing Bing Kang. A review of
image-based rendering techniques.
In Visual Communica-
tions and Image Processing, 2000.
[25] Eero Simoncelli. Statistical models for images:compression
restoration and synthesis. Asilomar Conference on Signals,
Systems, and Computers, 1997.
[26] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. ICLR,
2015.
[27] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene comple-
tion from a single depth image. CVPR, 2017.
[28] Pratul P. Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi
Ramamoorthi, and Ren Ng. Learning to synthesize a 4D
RGBD light ﬁeld from a single image. ICCV, 2017.
[29] Theo Thonat, Eli Shechtman, Sylvain Paris, and George
Drettakis. Multi-view inpainting for image-based scene edit-
ing and rendering. 3DV, 2016.
[30] Takashi Totsuka and Marc Levoy. Frequency domain volume
rendering. In Proceedings of SIGGRAPH, 1993.
[31] Shubham Tulsiani, Richard Tucker, and Noah Snavely.
Layer-structured 3D scene inference via view synthesis.
ECCV, 2018.
[32] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simon-
celli. Image quality assessment: from error visibility to struc-
tural similarity. In IEEE Transactions on Image Processing,
2004.
[33] Yair Weiss and William T. Freeman. What makes a good
model of natural images? CVPR, 2007.
[34] Bo Yang, Zihang Lai, Xiaoxuan Lu, Shuyu Lin, Hongkai
Wen, Andrew Markham, and Niki Trigoni. Learning 3d
scene semantics and structure from a single depth image.
CVPR Workshops, 2018.
[35] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-
tion by dilated convolutions. ICLR, 2016.
[36] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and
Thomas S Huang. Generative image inpainting with contex-
tual attention. CVPR, 2018.
[37] Zhoutong Zhang, Yebin Liu, and Qionghai Dai. Light ﬁeld
from micro-baseline image pair. CVPR, 2015.
[38] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely. Stereo magniﬁcation: Learning view
synthesis using multiplane images. In ACM Transactions on
Graphics (SIGGRAPH), 2018.

183

[39] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-
lik, and Alexei A. Efros. View synthesis by appearance ﬂow.
ECCV, 2016.
[40] C. Lawrence Zitnick, Sing Bing Kang, Matthew Uytten-
daele, Simon Winder, and Richard Szeliski. High-quality
video view interpolation using a layered representation. In
ACM Transactions on Graphics (SIGGRAPH), 2004.
[41] Matthias Zwicker, Wojciech Matusik, Fredo Durand, and
Hanspeter Pﬁster. Antialiasing for automultiscopic 3D dis-
plays. In Proceedings of Eurographics Symposium on Ren-
dering, 2006.

184

Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning
for Vision-Language Navigation

Xin Wang1 Qiuyuan Huang2 Asli Celikyilmaz2
Jianfeng Gao2 Dinghan Shen3
Yuan-Fang Wang1 William Yang Wang1
Lei Zhang2
1University of California, Santa Barbara
2Microsoft Research, Redmond

3Duke University

{qihua,aslicel,jfgao,leizhang}.microsoft.com, dinghan.shen@duke.edu

{xwang,yfwang,william}@cs.ucsb.edu

Abstract

Vision-language navigation (VLN) is the task of navi-
gating an embodied agent to carry out natural language
instructions inside real 3D environments.
In this paper,
we study how to address three critical challenges for this
task: the cross-modal grounding, the ill-posed feedback,
and the generalization problems. First, we propose a novel
Reinforced Cross-Modal Matching (RCM) approach that
enforces cross-modal grounding both locally and globally
via reinforcement learning (RL). Particularly, a matching
critic is used to provide an intrinsic reward to encourage
global matching between instructions and trajectories, and
a reasoning navigator is employed to perform cross-modal
grounding in the local visual scene. Evaluation on a VLN
benchmark dataset shows that our RCM model signiﬁcantly
outperforms previous methods by 10% on SPL and achieves
the new state-of-the-art performance. To improve the gen-
eralizability of the learned policy, we further introduce a
Self-Supervised Imitation Learning (SIL) method to explore
unseen environments by imitating its own past, good deci-
sions. We demonstrate that SIL can approximate a better
and more efﬁcient policy, which tremendously minimizes the
success rate performance gap between seen and unseen en-
vironments (from 30.7% to 11.7%).

1. Introduction

Recently, vision-language grounded embodied agents
have received increased attention [36, 22, 7] due to their
popularity in many intriguing real-world applications, e.g.,
in-home robots and personal assistants. Meanwhile, such
an agent pushes forward visual and language grounding by
putting itself in an active learning scenario through ﬁrst-
person vision.
In particular, Vision-Language Navigation
(VLN) [2] is the task of navigating an agent inside real envi-
ronments by following natural language instructions. VLN

Local	
visual	
scene	

Instruction

Turn right and head
towards the kitchen.
Then turn left, pass a
table and enter the
hallway. Walk down
the hallway and turn
into the entry way to
your
right without
doors. Stop in front
of the toilet.

Initial	Position

Target	Position

Demonstration	Path	A
Executed	Path	B
Executed	Path	C	

Global	
trajectories	
in	top-down	
view	

Figure 1: Demonstration of the VLN task. The instruction, the
local visual scene, and the global trajectories in a top-down view
is shown. The agent does not have access to the top-down view.
Path A is the demonstration path following the instruction. Path B
and C are two different paths executed by the agent.

requires a deep understanding of linguistic semantics, vi-
sual perception, and most importantly, the alignment of the
two. The agent must reason about the vision-language dy-
namics in order to move towards the target that is inferred
from the instructions.
VLN presents some unique challenges. First, reasoning
over visual images and natural language instructions can be
difﬁcult. As we demonstrate in Figure 1, to reach a desti-
nation, the agent needs to ground an instruction in the local
visual scene, represented as a sequence of words, as well as
match the instruction to the visual trajectory in the global
temporal space. Secondly, except for strictly following ex-
pert demonstrations, the feedback is rather coarse, since
the “Success” feedback is provided only when the agent
reaches a target position, completely ignoring whether the
agent has followed the instructions (e.g., Path A in Figure 1)

6629

or followed a random path to reach the destination (e.g.,
Path C in Figure 1). Even a “good” trajectory that matches
an instruction can be considered unsuccessful if the agent
stops marginally earlier than it should be (e.g., Path B in
Figure 1). An ill-posed feedback can potentially deviate
from the optimal policy learning. Thirdly, existing work
suffers from the generalization problem, causing a huge per-
formance gap between seen and unseen environments.
In this paper, we propose to combine the power of re-
inforcement learning (RL) and imitation learning (IL) to
address the challenges above. First, we introduce a novel
Reinforced Cross-Modal Matching (RCM) approach that
enforces cross-modal grounding both locally and globally
via RL. Speciﬁcally, we design a reasoning navigator that
learns the cross-modal grounding in both the textual in-
struction and the local visual scene, so that the agent can
infer which sub-instruction to focus on and where to look
at. From the global perspective, we equip the agent with
a matching critic that evaluates an executed path by the
probability of reconstructing the original instruction from
it, which we refer to as the cycle-reconstruction reward.
Locally, the cycle-reconstruction reward provides a ﬁne-
grained intrinsic reward signal to encourage the agent to
better understand the language input and penalize the tra-
jectories that do not match the instructions. For instance,
using the proposed reward, Path B is considered better than
Path C (see Figure 1).
Being trained with the intrinsic reward from the match-
ing critic and the extrinsic reward from the environment, the
reasoning navigator learns to ground the natural language
instruction on both local spatial visual scene and global tem-
poral visual trajectory. Our RCM model signiﬁcantly out-
performs the existing methods and achieves new state-of-
the-art performance on the Room-to-Room (R2R) dataset.
Our experimental results indicate a large performance
gap between seen and unseen environments. To narrow the
gap, we propose an effective solution to explore unseen en-
vironments with self-supervision. This technique is valu-
able because it can facilitate lifelong learning and adaption
to new environments. For example, domestic robots can
explore a new home it arrives at and iteratively improve
the navigation policy by learning from previous experience.
Motivated by this fact, we introduce a Self-Supervised Imi-
tation Learning (SIL) method in favor of exploration on un-
seen environments that do not have labeled data. The agent
learns to imitate its own past, good experience. Speciﬁcally,
in our framework, the navigator performs multiple roll-outs,
of which good trajectories (determined by the matching
critic) are stored in the replay buffer and later used for the
navigator to imitate. In this way, the navigator can approxi-
mate its best behavior that leads to a better policy. To sum-
marize, our contributions are mainly three-fold:

ing (RCM) framework that utilizes both extrinsic and
intrinsic rewards for reinforcement learning, of which
we introduce a cycle-reconstruction reward as the in-
trinsic reward to enforce the global matching between
the language instruction and the agent’s trajectory.

• Experiments show that RCM achieves the new state-
of-the-art performance on the R2R dataset, and among
the prior art, is ranked ﬁrst1 in the VLN Challenge in
terms of SPL, the most reliable metric for the task.

• We introduce a new evaluation setting for VLN, where
exploring unseen environments prior to testing is al-
lowed, and then propose a Self-Supervised Imita-
tion Learning (SIL) method for exploration with self-
supervision, whose effectiveness and efﬁciency are
validated on the R2R dataset.

2. Related Work

Vision-and-Language Grounding Recently, researchers
in both computer vision and natural language processing
are striving to bridge vision and natural language towards
a deeper understanding of the world [51, 45, 20, 6, 27, 17,
41, 19], e.g., captioning an image or a video with natural
language [9, 10, 44, 46, 52, 53, 47] or localizing desired
objects within an image given a natural language descrip-
tion [35, 18, 54, 55]. Moreover, visual question answer-
ing [3] and visual dialog [8] aim to generate one-turn or
multi-turn response by grounding it on both visual and tex-
tual modalities. However, those tasks focus on passive vi-
sual perception in the sense that the visual inputs are usually
ﬁxed. In this work, we are particularly interested in solving
the dynamic multi-modal grounding problem in both tem-
poral and spatial spaces. Thus, we focus on the task of
vision-language navigation (VLN) [2] which requires the
agent to actively interact with the environment.

Embodied Navigation Agent Navigation in 3D environ-
ments [56, 28, 29, 14] is an essential capability of a mobile
intelligent system that functions in the physical world. In
the past two years, a plethora of tasks and evaluation proto-
cols [36, 22, 38, 50, 2] have been proposed as summarized
in [1]. VLN [2] focuses on language-grounded navigation
in the real 3D environment.
In order to solve the VLN
task, Anderson et al. [2] set up an attention-based sequence-
to-sequence baseline model. Then Wang et al. [48] in-
troduced a hybrid approach that combines model-free and
model-based reinforcement learning (RL) to improve the
model’s generalizability. Lately, Fried et al. [11] proposed
a speaker-follower model that adopts data augmentation,
panoramic action space and modiﬁed beam search for VLN,
establishing the current state-of-the-art performance on the

• We propose a novel Reinforced Cross-Modal Match-

1As of November 16th, 2018.

6630

Room-to-Room dataset. Extending prior work, we propose
a Reinforced Cross-Modal Matching (RCM) approach to
VLN. The RCM model is built upon [11] but differs in
many signiﬁcant aspects: (1) we combine a novel multi-
reward RL with imitation learning for VLN while Speaker-
Follower models [11] only uses supervised learning as in
[2].
(2) Our reasoning navigator performs cross-modal
grounding rather than the temporal attention mechanism on
single-modality input. (3) Our matching critic is similar to
Speaker in terms of the architecture design, but the former
is used to provide the cycle-reconstruction intrinsic reward
for both RL and SIL training while the latter is used to aug-
ment training data for supervised learning. Moreover, we
introduce a self-supervised imitation learning method for
exploration in order to explicitly address the generalization
issue, which is a problem not well-studied in prior work.
Concurrent to our work, [42, 24, 25, 26] studies the VLN
tasks from various aspects, and [30] introduces a variant of
the VLN task to ﬁnd objects by requesting language assis-
tance when needed. Note that we are the ﬁrst to propose to
explore unseen environments for the VLN task.

Exploration Much work has been done on improving ex-
ploration [4, 12, 16, 32, 40] because the trade-off between
exploration and exploitation is one of the fundamental chal-
lenges in RL. The agent needs to exploit what it has learned
to maximize reward and explore new territories for better
policy search. Curiosity or uncertainty has been used as a
signal for exploration [37, 39, 23, 33]. Most recently, Oh
et al. [31] proposed to exploit past good experience for bet-
ter exploration in RL and theoretically justiﬁed its effective-
ness. Our Self-Supervised Imitation Learning (SIL) method
shares the same spirit. But instead of testing on games, we
adapt SIL and validate its effectiveness and efﬁciency on the
more practical task of VLN.

3. Reinforced Cross-Modal Matching

3.1. Overview

Here we consider an embodied agent that learns to nav-
igate inside real indoor environments by following natural
language instructions. The RCM framework mainly con-
sists of two modules (see Figure 2): a reasoning naviga-
tor πθ and a matching critic Vβ . Given the initial state s0
and the natural language instruction (a sequence of words)
X = x1 , x2 , ..., xn , the reasoning navigator learns to per-
form a sequence of actions a1 , a2 , ..., aT ∈ A, which gen-
erates a trajectory τ , in order to arrive at the target location
starget indicated by the instruction X . The navigator inter-
acts with the environment and perceives new visual states
as it executes actions. To promote the generalizability and
reinforce the policy learning, we introduce two reward func-
tions: an extrinsic reward that is provided by the environ-

Labeled	Target	
Location

Environment

State

Action

Instruction

Navigator

Trajectory

Matching Critic

Extrinsic
Reward

Intrinsic
Reward

Figure 2: Overview of our RCM framework.

ment and measures the success signal and the navigation er-
ror of each action, and an intrinsic reward that comes from
our matching critic and measures the alignment between the
language instruction X and the navigator’s trajectory τ .

3.2. Model

Here we discuss the reasoning navigator and matching
critic in details, both of which are end-to-end trainable.

3.2.1 Cross-Modal Reasoning Navigator

The navigator πθ is a policy-based agent that maps the input
instruction X onto a sequence of actions {at }T
t=1 . At each
time step t, the navigator receives a state st from the envi-
ronment and needs to ground the textual instruction in the
local visual scene. Thus, we design a cross-modal reason-
ing navigator that learns the trajectory history, the focus of
the textual instruction, and the local visual attention in or-
der, which forms a cross-modal reasoning path to encourage
the local dynamics of both modalities at step t.
Figure 3 shows the unrolled version of the navigator
at time step t. Similar to [11], we equip the navigator
with a panoramic view, which is split into image patches
of m different viewpoints, so the panoramic features that
are extracted from the visual state st can be represented as
j=1 , where vt,j denotes the pre-trained CNN feature
of the image patch at viewpoint j .

{vt,j }m

History Context Once the navigator runs one step, the
visual scene would change accordingly. The history of the
trajectory τ1:t till step t is encoded as a history context vec-
tor ht by an attention-based trajectory encoder LSTM [15]:

ht = LST M ([vt , at−1 ], ht−1 )

(1)

where at−1 is the action taken at previous step, and vt =
Pj αt,j vt,j , the weighted sum of the panoramic features.
αt,j is the attention weight of the visual feature vt,j , repre-
senting its importance with respect to the previous history
context ht−1 . Note that we adopt the dot-product atten-
tion [43] hereafter, which we denote as (taking the attention

6631

turn completely around until you 
face an open door with a window to 
the left and a patio to the right, 
walk forward though the door and 
into a dinning room, … …

Panoramic			Features

,
{𝒗) ,+ }	
+%&

Attention

𝒂)/&

Language Encoder

…

𝒉)/&

Trajectory Encoder

𝒉)

…

𝒉).&

{𝒘# }	

’
#%&

Attention

)12)

𝒄)

4#5678

𝒄𝒕

Attention

Action
Predictor

𝒂𝒕

Figure 3: Cross-modal reasoning navigator at step t.

over visual features above for an example)

vt = attention(ht−1 , {vt,j }m

j=1 )

= X

j

sof tmax(ht−1Wh (vt,jWv )T )vt,j

(2)

(3)

where Wh and Wv are learnable projection matrices.

Visually Conditioned Textual Context Memorizing the
past can enable the recognition of the current status and thus
understanding which words or sub-instructions to focus on
next. Hence, we further learn the textual context ctext
con-
ditioned on the history context ht . We let a language en-
coder LSTM to encode the language instruction X into a
set of textual features {wi }n
i=1 . Then at every time step, the
textual context is computed as

t

ctext

t = attention(ht , {wi }n

i=1 )

(4)

Note that ctext
t weighs more on the words that are more rel-
evant to the trajectory history and the current visual state.

Textually Conditioned Visual Context Knowing where
to look at requires a dynamic understanding of the language
instruction; so we compute the visual context cvisual
based
on the textual context ctext
:

t

t

cvisual
t

= attention(ctext

t

, {vj }m

j=1 )

(5)

Action Prediction In the end, our action predictor con-
siders the history context ht , the textual context ctext
, and
the visual context cvisual
, and decides which direction to go
next based on them. It calculates the probability pk of each
navigable direction using a bilinear dot product as follows:

t

t

pk = sof tmax([ht , ctext

t

, cvisual
t

]Wc (ukWu )T )

(6)

where uk is the action embedding that represents the k-
th navigable direction, which is obtained by concatenat-
ing an appearance feature vector (CNN feature vector ex-
tracted from the image patch around that view angle or

Navigator	

𝜋#

𝜏

Matching Critic	𝑉%

Trajectory	Encoder

𝜒

𝑉% 𝜒, 𝜏 = 𝑝% (𝜒|𝜏)

Language	Decoder

Figure 4: Cross-modal matching critic that provides the cycle-
reconstruction intrinsic reward.

direction) and a 4-dimensional orientation feature vector
[sinψ ; cosψ ; sinω ; cosω ], where ψ and ω are the heading
and elevation angles respectively. The learning objectives
for training the navigator are introduced in Section 3.3.

3.2.2 Cross-Modal Matching Critic

In addition to the extrinsic reward signal from the environ-
ment, we also derive an intrinsic reward Rintr provided by
the matching critic Vβ to encourage the global matching be-
tween the language instruction X and the navigator πθ ’s tra-
jectory τ = {< s1 , a1 >, < s2 , a2 >, ..., < sT , aT >}:

Rintr = Vβ (X , τ ) = Vβ (X , πθ (X ))

(7)

One way to realize this goal is to measure the cycle-
reconstruction reward p( ˆX = X |πθ (X )), the probability of
reconstructing the language instruction X given the trajec-
tory τ = πθ (X ) executed by the navigator. The higher the
probability is, the better the produced trajectory is aligned
with the instruction.
Therefore as shown in Figure 4, we adopt an attention-
based sequence-to-sequence language model as our match-
ing critic Vβ , which encodes the trajectory τ with a trajec-
tory encoder and produces the probability distributions of
generating each word of the instruction X with a language
decoder. Hence the intrinsic reward

Rintr = pβ (X |πθ (X )) = pβ (X |τ )

(8)

which is normalized by the instruction length n.
In our
experiments, the matching critic is pre-trained with hu-
man demonstrations (the ground-truth instruction-trajectory
pairs < X ∗ , τ ∗ >) via supervised learning.

3.3. Learning

In order to quickly approximate a relatively good pol-
icy, we use the demonstration actions to conduct supervised
learning with maximum likelihood estimation (MLE). The
training loss Lsl is deﬁned as

Lsl = −E[log(πθ (a∗
t |st ))]

(9)

where a∗
t is the demonstration action provided by the sim-
ulator. Warm starting the agent with supervised learning

6632

can ensure a relatively good policy on the seen environ-
ments. But it also limits the agent’s generalizability to re-
cover from erroneous actions in unseen environments, since
it only clones the behaviors of expert demonstrations.
To learn a better and more generalizable policy, we then
switch to reinforcement learning and introduce the extrin-
sic and intrinsic reward functions to reﬁne the policy from
different perspectives.

Extrinsic Reward A common practice in RL is to di-
rectly optimize the evaluation metrics. Since the objective
of the VLN task is to successfully reach the target loca-
tion starget , we consider two metrics for the reward design.
The ﬁrst metric is the relative navigation distance similar
to [48]. We denote the distance between st and starget as
Dtarget (st ). Then the immediate reward r(st , at ) after tak-
ing action at at state st (t < T ) becomes:

r(st , at ) = Dtarget (st ) − Dtarget (st+1 ),

t < T (10)

This indicates the reduced distance to the target location af-
ter taking action at . Our second choice considers the “Suc-
cess” as an additional criterion. If the agent reaches a point
within a threshold measured by the distance d from the tar-
get (d is preset as 3m in the R2R dataset), then it is counted
as “Success”. Particularly, the immediate reward function
at last step T is deﬁned as

r(sT , aT ) = ✶(Dtarget (sT ) ≤ d)

(11)

where ✶() is an indicator function. To incorporate the in-
ﬂuence of the action at on the future and account for the
local greedy search, we use the discounted cumulative re-
ward rather than the immediate reward to train the policy:

Rextr (st , at ) = r(st , at )

+

| {z }

immediate reward

T

X

t′=t+1

γ t′

−t r(st′ , at′ )

{z

}

(12)
where γ is the discounted factor (0.95 in our experiments).

|

discounted future reward

Intrinsic Reward As discussed in Section 3.2.2, we pre-
train a matching critic to calculate the cycle-reconstruction
intrinsic reward Rintr (see Equation 8), promoting the
alignment between the language instruction X and the tra-
jectory τ . It encourages the agent to respect the instruction
and penalizes the paths that deviate from what the instruc-
tion indicates.
With both the extrinsic and intrinsic reward functions,
the RL loss can be written as

Lrl = −Eat∼πθ [At ]

(13)

where the advantage function At = Rextr + δRintr . δ
is a hyperparameter weighing the intrinsic reward. Based

Unlabeled
Instruction	!

Navigator	"#

{&' ,	&( ,…,	&) }

Imitation	
Learning

Replay	
Buffer

Matching Critic	

&̂ =

$%

argmax $% 	(!, &)

Figure 5: SIL for exploration on unlabeled data.

on REINFORCE algorithm [49],
the gradient of non-
differentiable, reward-based loss function can be derived as

∇θ Lrl = −At∇θ log πθ (at |st )

(14)

4. Self-Supervised Imitation Learning

The last section introduces the effective RCM method
for generic vision-language navigation task, whose standard
setting is to train the agent on seen environments and test it
on unseen environments without exploration. In this section
we discuss a different setting where the agent is allowed to
explore unseen environments without ground-truth demon-
strations. This is of practical beneﬁt because it facilitates
lifelong learning and adaption to new environments.
To this end, we propose a Self-Supervised Imitation
Learning (SIL) method to imitate the agent’s own past good
decisions. As shown in Figure 5, given a natural language
instruction X without paired demonstrations and ground-
truth target location, the navigator produces a set of possi-
ble trajectories and then stores the best trajectory ˆτ that is
determined by matching critic Vβ into a replay buffer, in
formula,

ˆτ = arg max

τ

Vβ (X , τ )

(15)

The matching critic evaluates the trajectories with the cycle-
reconstruction reward as introduced in Section 3.2.2. Then
by exploiting the good trajectories in the replay buffer, the
agent is indeed optimizing the following objective with self-
supervision. The target location is unknown and thus there
is no supervision from the environment.

Lsil = −Rintr log πθ (at |st )

(16)

Note that Lsil can be viewed as the loss for policy gradient
except that the off-policy Monte-Carlo return Rintr is used
instead of on-policy return. Lsil can also be interpreted as
the supervised learning loss with ˆτ as the “ground truths”:

Lsil = −E[log(πθ ( ˆat |st ))]

(17)

where ˆat is the action stored in the replay buffer using Equa-
tion 15. Paired with a matching critic, the SIL method can
be combined with various learning methods to approximate
a better policy by imitating the previous best of itself.

6633

5. Experiments and Analysis

5.1. Experimental Setup

R2R Dataset We evaluate our approaches on the Room-
to-Room (R2R) dataset [2] for vision-language navigation
in real 3D environments, which is built upon the Matter-
port3D dataset [5]. The R2R dataset has 7,189 paths that
capture most of the visual diversity and 21,567 human-
annotated instructions with an average length of 29 words.
The R2R dataset is split into training, seen validation, un-
seen validation, and test sets. The seen validation set shares
the same environments with the training set. While both the
unseen validation and test sets contain distinct environments
that do not appear in the other sets.

Testing Scenarios The standard testing scenario of the
VLN task is to train the agent in seen environments and
then test it in previously unseen environments in a zero-shot
fashion. There is no prior exploration on the test set. This
setting is preferred and able to clearly measure the general-
izability of the navigation policy, so we evaluate our RCM
approach under the standard testing scenario.
Furthermore, exploration in unseen environments is cer-
tainly meaningful in practice, e.g., in-home robots are ex-
pected to explore and adapt to a new environment. So we
introduce a lifelong learning scenario where the agent is en-
couraged to learn from trials and errors on the unseen envi-
ronments. In this case, how to effectively explore the unseen
validation or test set where there are no expert demonstra-
tions becomes an important task to study.

Evaluation Metrics We report ﬁve evaluation metrics as
used by the VLN Challenge: Path Length (PL), Navigation
Error (NE), Oracle Success Rate (OSR), Success Rate (SR),
and Success rate weighted by inverse Path Length (SPL).2
Among those metrics, SPL is the recommended primary
measure of navigation performance [1], as it considers both
effectiveness and efﬁciency. The other metrics are also re-
ported as auxiliary measures.

Implementation Details Following prior work [2, 48,
11], ResNet-152 CNN features [13] are extracted for all im-
ages without ﬁne-tuning. The pretrained GloVe word em-
beddings [34] are used for initialization and then ﬁne-tuned
during training. We train the matching critic with human
demonstrations and then ﬁx it during policy learning. Then

2 PL: the total length of the executed path. NE: the shortest-path dis-
tance between the agent’s ﬁnal position and the target. OSR: the success
rate at the closest point to the goal that the agent has visited along the tra-
jectory. SR: the percentage of predicted end-locations within 3m of the
target locations. SPL: SPL trades-off Success Rate against Path Length,
which is deﬁned in [1].

Model

Test Set (VLN Challenge Leaderboard)
PL ↓
NE ↓ OSR ↑ SR ↑ SPL ↑

Random
seq2seq [2]
RPA [48]
Speaker-Follower [11]
+ beam search

9.89
8.13
9.15
14.82
1257.38

Ours
RCM
RCM + SIL (train)
RCM + SIL (unseen)3

15.22
11.97

9.48

9.79
7.85
7.53
6.62
4.87

6.01
6.12

4.21

18.3
26.6
32.5
44.0
96.0

50.8
49.5

66.8

13.2
20.4
25.3
35.0
53.5

43.1
43.0

60.5

12
18
23
28
1

35
38

59

Table 1: Comparison on the R2R test set [2]. Our RCM model sig-
niﬁcantly outperforms the SOTA methods, especially on SPL (the
primary metric for navigation tasks [1]). Moreover, using SIL to
imitate itself on the training set can further improve its efﬁciency:
the path length is shortened by 3.25m. Note that with beam search,
the agent executes K trajectories at test time and chooses the most
conﬁdent one as the ending point, which results in a super long
path and is heavily penalized by SPL.

we warm start the policy via SL with a learning rate 1e-
4, and then switch to RL training with a learning rate 1e-5
(same for SIL). Adam optimizer [21] is used to optimize all
the parameters. More details can be found in the appendix.

5.2. Results on the Test Set

Comparison with SOTA We compare the performance
of RCM to the previous state-of-the-art (SOTA) methods
on the test set of the R2R dataset, which is held out as the
VLN Challenge. The results are shown in Table 1, where
we compare RCM to a set of baselines: (1) Random: ran-
domly take a direction to move forward at each step until
ﬁve steps. (2) seq2seq: the best-performing sequence-to-
sequence model as reported in the original dataset paper [2],
which is trained with the student-forcing method. (3) RPA:
a reinforced planning-ahead model that combines model-
free and model-based reinforcement learning for VLN [48].
(4) Speaker-Follower: a compositional Speaker-Follower
method that combines data augmentation, panoramic action
space, and beam search for VLN [11].
As can be seen in Table 1, RCM signiﬁcantly outper-
forms the existing methods, improving the SPL score from
28% to 35%4 . The improvement is consistently observed
on the other metrics, e.g., the success rate is increased by
8.1%. Moreover, using SIL to imitate the RCM agent’s pre-
vious best behaviors on the training set can approximate a

3 The results of using SIL to explore unseen environments are only used
to validate its effectiveness for lifelong learning, which is not directly com-
parable to other models due to different learning scenarios.
4Note that our RCM model also utilizes the panoramic action space and
augmented data in [11] for a fair comparison.

6634

Seen Validation

Unseen Validation

# Model

PL ↓

NE ↓ OSR ↑

SR ↑

PL ↓

NE ↓ OSR ↑

SR ↑

0

Speaker-Follower (no beam search) [11]

-

1 RCM + SIL (train)
2 RCM
3
− intrinsic reward
4
− extrinsic reward = pure SL
5
− cross-modal reasoning

10.65
11.92
12.08
11.99
11.88

3.36

3.53
3.37
3.25
3.22
3.18

6 RCM + SIL (unseen)

10.13

2.78

73.8

75.0
76.6
77.2
76.7
73.9

79.7

66.4

66.7
67.4
67.6
66.9
66.4

73.0

-

11.46
14.84
15.00
14.83
14.51

9.12

6.62

6.09
5.88
6.02
6.29
6.47

4.17

45.0

50.1
51.9
50.5
46.5
44.8

35.5

42.8
42.5
40.6
37.7
35.7

69.31

61.3

Table 2: Ablation study on seen and unseen validation sets. We report the performance of the speaker-follower model without beam search
as the baseline. Row 1-5 shows the inﬂuence of each individual component by successively removing it from the ﬁnal model. Row 6
illustrates the power of SIL on exploring unseen environments with self-supervision. Please see Section 5.3 for more detailed analysis.

more efﬁcient policy, whose average path length is reduced
from 15.22m to 11.97m and which achieves the best result
(38%) on SPL. Therefore, we submit the results of RCM +
SIL (train) to the VLN Challenge, ranking ﬁrst among prior
work in terms of SPL. It is worth noticing that beam search
is not practical in reality, because it needs to execute a very
long trajectory before making the decision, which is pun-
ished heavily by the primary metric SPL. So we are mainly
comparing the results without beam search.

Self-Supervised Imitation Learning As mentioned
above, for a standard VLN setting, we employ SIL on the
training set to learn an efﬁcient policy. For the lifelong
learning scenario, we test
the effectiveness of SIL on
exploring unseen environments (the validation and test
sets). It is noticeable in Table 1 that SIL indeed leads to
a better policy even without knowing the target locations.
SIL improves RCM by 17.5% on SR and 21% on SPL.
Similarly, the agent also learns a more efﬁcient policy that
takes less number of steps (the average path length is re-
duced from 15.22m to 9.48m) but obtains a higher success
rate. The key difference between SIL and beam search
is that SIL optimizes the policy itself by play-and-imitate
while beam search only makes a greedy selection of the
rollouts of the existing policy. But we would like to point
out that due to different learning scenarios, the results of
RCM + SIL (unseen) cannot be directly compared with
other methods following the standard settings of the VLN
challenge.

5.3. Ablation Study

Effect of Individual Components We conduct an abla-
tion study to illustrate the effect of each component on both
seen and unseen validation sets in Table 2. Comparing Row
1 and Row 2, we observe the efﬁciency of the learned pol-
icy by imitating the best of itself on the training set. Then

we start with the RCM model in Row 2, and successively
remove the intrinsic reward, extrinsic reward, and cross-
modal reasoning to demonstrate their importance.

Removing the intrinsic reward (Row 3), we notice that
the success rate (SR) on unseen environments drops 1.9
points while it is almost ﬁxed on seen environments (0.2↑).
It evaluates the alignment between instructions and trajecto-
ries, serving as a complementary supervision besides of the
feedback from the environment, therefore it works better
for the unseen environments that require more supervision
due to lack of exploration. This also indirectly validates the
importance of exploration on unseen environments.

Furthermore, the results of Row 4 (the RCM model with
only supervised learning) validate the superiority of rein-
forcement learning compared to purely supervised learning
on the VLN task. Meanwhile, since eventually the results
are evaluated based on the success rate (SR) and path length
(PL), directly optimizing the extrinsic reward signals can
guarantee the stability of the reinforcement learning and
bring a big performance gain.

We then verify the strength of our cross-modal reasoning
navigator by comparing it (Row 4) with an attention-based
sequence-to-sequence model (Row 5) that utilizes the previ-
ous hidden state ht−1 to attend to both the visual and textual
features at decoding time. Everything else is exactly the
same except the cross-modal attention design. Evidently,
our navigator improves upon the baseline by considering
history context, visually-conditioned textual context, and
textually-conditioned visual context for decision making.

In the end, we demonstrate the effectiveness of the pro-
posed SIL method for exploration in Row 6. Considerable
performance boosts have been obtained on both seen and
unseen environments, as the agent learns how to better exe-
cute the instructions from its own previous experience.

6635

Instruction: Exit the door and turn left towards the 
staircase. Walk all the way up the stairs, and stop at 
the top of the stairs.

Instruction: Turn right and go down the stairs. Turn 
left and go straight until you get to the laundry room. 
Wait there.

Intrinsic	Reward:	0.53				Result:	Success	(error	=	0m)	

Intrinsic	Reward:	0.54				Result:	Failure	(error	=	5.5m)	

step	1	panorama	view

step	1	panorama	view

step	2	panorama	view

step	2	panorama	view

step	3	panorama	view

step	3	panorama	view

step	4	panorama	view

step	4	panorama	view

:

step	6	panorama	view

Above	steps	are	all	good,	but	it	stops	at	a	wrong	place	in	the	end.	

step	5	panorama	view

(a) A successful case

(b) A failure case

Figure 6: Qualitative examples from the unseen validation set.

Generalizability Another observation from the experi-
ments (e.g., see Table 2) is that our RCM approach is much
more generalizable to unseen environments compared with
the baseline. The improvements on the seen and unseen
validation sets are 0.3 and 7.1 points, respectively. So is
the SIL method, which explicitly explores the unseen en-
vironments and tremendously reduces the success rate per-
formance gap between seen and unseen environments from
30.7% (Row 5) to 11.7% (Row 6).

Qualitative Analysis For a more intuitive view of how
our model works for the VLN task, we visualize two qual-
itative examples in Figure 6. Particularly, we choose two
examples, both with high intrinsic rewards. In (a), the agent
successfully reaches the target destination, with a compre-
hensive understanding of the natural language instruction.
While in (b), the intrinsic reward is also high, which indi-
cates most of the agent’s actions are good, but it is also no-
ticeable that the agent fails to recognize the laundry room
at the end of the trajectory, which shows the importance of
more precise visual grounding in the navigation task.

6. Conclusion

In this paper we present two novel approaches, RCM and
SIL, which combine the strength of reinforcement learn-
ing and self-supervised imitation learning for the vision-
language navigation task. Experiments illustrate the ef-
fectiveness and efﬁciency of our methods under both the
standard testing scenario and the lifelong learning scenario.
Moreover, our methods show strong generalizability in un-
seen environments. The proposed learning frameworks are
modular and model-agnostic, which allow the components
to be improved separately. We also believe that the idea of
learning more ﬁne-grained intrinsic rewards, in addition to
the coarse external signals, is commonly applicable to vari-
ous embodied agent tasks, and the idea SIL can be generally
adopted to explore other unseen environments.

Acknowledgment

This work was partly performed when the ﬁrst author
was interning at Microsoft Research. The authors thank Pe-
ter Anderson and Pengchuan Zhang for their helpful discus-
sions, and Ronghang Hu for his visualization code.

6636

References

[1] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy,
S. Gupta, V. Koltun, J. Kosecka, J. Malik, R. Mottaghi,
M. Savva, et al. On evaluation of embodied navigation
agents. arXiv preprint arXiv:1807.06757, 2018.

[2] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson,
N. S ¨underhauf, I. Reid, S. Gould, and A. van den Hen-
gel. Vision-and-language navigation: Interpreting visually-
grounded navigation instructions in real environments.
In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), volume 2, 2018.

[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zit-
nick, and D. Parikh. VQA: Visual Question Answering. In
International Conference on Computer Vision (ICCV), 2015.

[4] M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul,
D. Saxton, and R. Munos. Unifying count-based exploration
and intrinsic motivation. In Advances in Neural Information
Processing Systems, pages 1471–1479, 2016.

[5] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Nießner,
M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d:
Learning from rgb-d data in indoor environments. arXiv
preprint arXiv:1709.06158, 2017.

[6] X. Chen and C. Lawrence Zitnick. Mind’s eye: A recur-
rent visual representation for image caption generation. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 2422–2431, 2015.

[7] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Ba-
tra. Embodied Question Answering. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2018.

[8] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M.
Moura, D. Parikh, and D. Batra. Visual Dialog.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017.

[9] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,
S. Venugopalan, K. Saenko, and T. Darrell. Long-term recur-
rent convolutional networks for visual recognition and de-
scription. In CVPR, 2015.

[10] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng,
P. Doll ´ar, J. Gao, X. He, M. Mitchell, J. C. Platt, et al. From
captions to visual concepts and back. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1473–1482, 2015.

[11] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-
P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, and
T. Darrell. Speaker-follower models for vision-and-language
navigation. In Advances in Neural Information Processing
Systems (NIPS), 2018.

[12] J. Gao, M. Galley, and L. Li. Neural approaches to conver-
sational ai. arXiv preprint arXiv:1809.08267, 2018.

[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016.

[14] S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy,
A. Stentz, and M. R. Walter. Learning models for following

natural language directions in unknown environments. arXiv
preprint arXiv:1503.05079, 2015.
[15] S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997.
[16] R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck,
and P. Abbeel. Vime: Variational information maximizing
exploration. In Advances in Neural Information Processing
Systems, pages 1109–1117, 2016.
[17] R. Hu, M. Rohrbach, and T. Darrell. Segmentation from
natural language expressions.
In European Conference on
Computer Vision, pages 108–124. Springer, 2016.
[18] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Dar-
rell. Natural language object retrieval. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 4555–4564, 2016.
[19] Q. Huang, P. Zhang, D. Wu, and L. Zhang. Turbo learning
for captionbot and drawingbot. In Advances in Neural Infor-
mation Processing Systems (NIPS), 2018.
[20] A. Karpathy and L. Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 3128–3137, 2015.
[21] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
[22] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and
A. Farhadi. Ai2-thor: An interactive 3d environment for vi-
sual ai. arXiv preprint arXiv:1712.05474, 2017.
[23] Z. C. Lipton, J. Gao, L. Li, X. Li, F. Ahmed, and
L. Deng. Efﬁcient exploration for dialogue policy learning
with bbq networks & replay buffer spiking. arXiv preprint
arXiv:1608.05081, 2016.
[24] Y. B. A. H. Z. G. J. L. J. G. Y. C. S. S. Liyiming Ke, Xi-
ujun Li. Tactical rewind: Self-correction via backtracking
in vision-and-language navigation.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2019.
[25] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher,
and C. Xiong. Self-monitoring navigation agent via auxiliary
progress estimation. arXiv preprint arXiv:1901.03035, 2019.
[26] C.-Y. Ma, Z. Wu, G. AlRegib, C. Xiong, and Z. Kira. The
regretful agent: Heuristic-aided navigation through progress
estimation. arXiv preprint arXiv:1903.01602, 2019.
[27] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and
K. Murphy. Generation and comprehension of unambiguous
object descriptions. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 11–20,
2016.
[28] P. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. J. Ballard,
A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu,
et al. Learning to navigate in complex environments. arXiv
preprint arXiv:1611.03673, 2016.
[29] A. Mousavian, A. Toshev, M. Fiser, J. Kosecka, and J. David-
son. Visual representations for semantic target driven navi-
gation. arXiv preprint arXiv:1805.06066, 2018.
[30] K. Nguyen, D. Dey, C. Brockett, and B. Dolan. Vision-
based navigation with language-based assistance via imi-
tation learning with indirect intervention. arXiv preprint
arXiv:1812.04155, 2018.

6637

Association for Computational Linguistics (Volume 1: Long
Papers), 2018.
[46] X. Wang, W. Chen, J. Wu, Y.-F. Wang, and W. Y. Wang.
Video captioning via hierarchical reinforcement learning.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018.
[47] X. Wang, Y.-F. Wang, and W. Y. Wang. Watch, listen, and de-
scribe: Globally and locally aligned cross-modal attentions
for video captioning.
In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies,
Volume 2 (Short Papers), 2018.
[48] X. Wang, W. Xiong, H. Wang, and W. Y. Wang. Look before
you leap: Bridging model-free and model-based reinforce-
ment learning for planned-ahead vision-and-language navi-
gation.
In The European Conference on Computer Vision
(ECCV), September 2018.
[49] R. J. Williams. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
learning, 8(3-4):229–256, 1992.
[50] F. Xia, A. R. Zamir, Z.-Y. He, A. Sax, J. Malik, and
S. Savarese. Gibson Env: real-world perception for em-
bodied agents. In Computer Vision and Pattern Recognition
(CVPR), 2018 IEEE Conference on. IEEE, 2018.
[51] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
image caption generation with visual attention. In Interna-
tional Conference on Machine Learning, pages 2048–2057,
2015.
[52] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked
attention networks for image question answering.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 21–29, 2016.
[53] H. Yu, J. Wang, Z. Huang, Y. Yang, and W. Xu. Video
paragraph captioning using hierarchical recurrent neural net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4584–4593, 2016.
[54] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Mod-
eling context in referring expressions. In European Confer-
ence on Computer Vision, pages 69–85. Springer, 2016.
[55] D. Zhang, X. Dai, X. Wang, Y.-F. Wang, and L. S. Davis.
Man: Moment alignment network for natural language mo-
ment retrieval via iterative graph adjustment. arXiv preprint
arXiv:1812.00087, 2018.
[56] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-
Fei, and A. Farhadi. Target-driven visual navigation in in-
door scenes using deep reinforcement learning. In Robotics
and Automation (ICRA), 2017 IEEE International Confer-
ence on, pages 3357–3364. IEEE, 2017.

[31] J. Oh, Y. Guo, S. Singh, and H. Lee. Self-imitation learning.
arXiv preprint arXiv:1806.05635, 2018.
[32] G. Ostrovski, M. G. Bellemare, A. v. d. Oord, and R. Munos.
Count-based exploration with neural density models. arXiv
preprint arXiv:1703.01310, 2017.
[33] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-
driven exploration by self-supervised prediction.
In Inter-
national Conference on Machine Learning (ICML), volume
2017, 2017.
[34] J. Pennington, R. Socher, and C. Manning. Glove: Global
vectors for word representation. In Proceedings of the 2014
conference on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543, 2014.
[35] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo,
J. Hockenmaier, and S. Lazebnik. Flickr30k entities: Col-
lecting region-to-phrase correspondences for richer image-
to-sentence models.
In Proceedings of the IEEE interna-
tional conference on computer vision, pages 2641–2649,
2015.
[36] M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser,
and V. Koltun. Minos: Multimodal
indoor simulator
for navigation in complex environments.
arXiv preprint
arXiv:1712.03931, 2017.
[37] J. Schmidhuber. Adaptive conﬁdence and adaptive curiosity.
In Institut fur Informatik, Technische Universitat Munchen,
Arcisstr. 21, 800 Munchen 2. Citeseer, 1991.
[38] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and
T. Funkhouser. Semantic scene completion from a single
depth image. IEEE Conference on Computer Vision and Pat-
tern Recognition, 2017.
[39] A. L. Strehl and M. L. Littman. An analysis of model-based
interval estimation for markov decision processes. Journal
of Computer and System Sciences, 74(8):1309–1331, 2008.
[40] H. Tang, R. Houthooft, D. Foote, A. Stooke, O. X. Chen,
Y. Duan, J. Schulman, F. DeTurck, and P. Abbeel. # ex-
ploration: A study of count-based exploration for deep re-
inforcement learning.
In Advances in Neural Information
Processing Systems, pages 2753–2762, 2017.
[41] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Ur-
tasun, and S. Fidler. Movieqa: Understanding stories in
movies through question-answering. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4631–4640, 2016.
[42] J. Thomason, D. Gordan, and Y. Bisk. Shifting the base-
line: Single modality performance on visual navigation &
qa. arXiv preprint arXiv:1811.00613, 2018.
[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all
you need.
In Advances in Neural Information Processing
Systems, pages 5998–6008, 2017.
[44] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In Computer Vision
and Pattern Recognition (CVPR), 2015 IEEE Conference on,
pages 3156–3164. IEEE, 2015.
[45] X. Wang, W. Chen, Y.-F. Wang, and W. Y. Wang. No met-
rics are perfect: Adversarial reward learning for visual sto-
rytelling. In Proceedings of the 56th Annual Meeting of the

6638

Relation-Shape Convolutional Neural Network for Point Cloud Analysis

Yongcheng Liu† ‡
Bin Fan∗†
Shiming Xiang† ‡
Chunhong Pan†
† National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences
‡ School of Artiﬁcial Intelligence, University of Chinese Academy of Sciences

Email: {yongcheng.liu, bfan, smxiang, chpan}@nlpr.ia.ac.cn

Abstract

Point cloud analysis is very challenging, as the shape
implied in irregular points is difﬁcult
to capture.
In
this paper, we propose RS-CNN, namely, Relation-Shape
Convolutional Neural Network, which extends regular grid
CNN to irregular conﬁguration for point cloud analysis.
The key to RS-CNN is learning from relation, i.e., the ge-
ometric topology constraint among points. Speciﬁcally, the
convolutional weight for local point set is forced to learn
a high-level relation expression from predeﬁned geometric
priors, between a sampled point from this point set and the
others. In this way, an inductive local representation with
explicit reasoning about the spatial layout of points can be
obtained, which leads to much shape awareness and robust-
ness. With this convolution as a basic operator, RS-CNN,
a hierarchical architecture can be developed to achieve
contextual shape-aware learning for point cloud analysis.
Extensive experiments on challenging benchmarks across
three tasks verify RS-CNN achieves the state of the arts.

1. Introduction

Recently, the analysis of 3D point cloud has drawn a lot
of attention, as it has many applications such as autonomous
driving and robot manipulation. However, this task is very
challenging, since it is difﬁcult to infer the underlying shape
formed by these irregular points (see Fig. 1 for detail).
For this issue, much effort is focused on replicating the
remarkable success of convolutional neural network (CNN)
on regular grid data (e.g., image) analysis [17, 32], to irregu-
lar point cloud processing [26, 15, 45, 29, 27, 34, 38]. Some
works transform point cloud to regular voxels [42, 22, 3] or
multi-view images [35, 2, 5] for easy application of clas-
sic grid CNN. These transformations, however, usually lead
to much loss of inherent geometric information in 3D point
cloud, as well as high complexity.
To directly process point cloud, PointNet [24] indepen-
dently learns on each point and gathers the ﬁnal features

∗Corresponding author: Bin Fan

Figure 1. Left part: Point cloud. Right part: Underlying shape
formed by this point cloud.

for a global representation. Though impressive, this de-
sign ignores local structures that have been proven to be
important for abstracting high-level visual concepts in im-
age CNN [49]. To solve this problem, some works parti-
tion point cloud into several subsets by sampling [26] or
superpoint [18]. Then a hierarchy is built to learn contex-
tual representation from local to global. Nevertheless, this
extremely relies on effective inductive learning of local sub-
sets, which is quite intractable to achieve.

Generally, there are mainly three challenges for learning
from point set P ⊂ R3 : (1) P is unordered, thus requir-
ing the learned representation being permutation invariant;
(2) P distributes in 3D geometric space, thus demanding
the learned representation being robust to rigid transforma-
tion (e.g., rotation and translation); (3) P forms an underly-
ing shape, therefore, the learned representation should be of
discriminative shape awareness. The issue (1) has been well
resolved by symmetric function [24, 27, 48], while (2) and
(3) still demand for a full exploration. The goal of this work
is to extend regular grid CNN to irregular conﬁguration for
handling these issues together.

To this end, we propose a relation-shape convolutional
neural network (aliased as RS-CNN). The key to RS-CNN
is learning from relation, i.e., the geometric topology con-
straint among points, which in our view can encode mean-
ingful shape information in 3D point cloud.

Speciﬁcally, each local convolutional neighborhood is
constructed by taking a sampled point x as the centroid and

8895

the surrounding points as its neighbors N (x). Then, the
convolutional weight is forced to learn a high-level relation
expression from predeﬁned geometric priors, i.e., intuitive
low-level relation between x and N (x). By convoluting in
this way, an inductive representation with explicit reason-
ing about the spatial layout of points can be obtained. It
discriminatively reﬂects the underlying shape that irregular
points form thus is shape-aware. Furthermore, it can bene-
ﬁt from geometric priors, including the invariance to points
permutation and the robustness to rigid transformation (e.g.,
translation and rotation). With this convolution as a ba-
sic operator, a hierarchical CNN-like architecture, i.e., RS-
CNN, can be developed to achieve contextual shape-aware
learning for point cloud analysis.
The key contributions are highlighted as follows:

• A novel
learn-from-relation convolution operator
called relation-shape convolution is proposed. It can
explicitly encode geometric relation of points, thus re-
sulting in much shape awareness and robustness;

• A deep hierarchy equipped with the relation-shape
convolution, i.e., RS-CNN, is proposed. It can extend
regular grid CNN to irregular conﬁguration for achiev-
ing contextual shape-aware learning of point cloud;

• Extensive experiments on challenging benchmarks
across three tasks, as well as thorough empirical and
theoretical analysis, demonstrate RS-CNN achieves
the state of the arts.

2. Related Work

View-based and volumetric methods. View-based meth-
ods represent a 3D shape as a group of 2D views from dif-
ferent angles. Recently, many works [35, 2, 5, 43, 6, 25]
have been proposed to recognize these view images with
deep neural networks. They often ﬁnetune a pre-trained
image-based architecture for accurate recognition. How-
ever, 2D projections could cause loss of shape information
due to self-occlusions, and it often demands a huge number
of views for decent performance.
Volumetric methods convert the input 3D shape into
a regular 3D grid, over which classic CNN can be em-
ployed [42, 22, 3]. The main limitation is the quantization
loss of the shape due to the low resolution enforced by 3D
grid. Recent space partition methods like K-d trees [16] or
octrees [39, 36, 28] rescue some resolution issues but still
rely on the subdivision of a bounding volume rather than
a local geometric shape. In contrast to these methods, our
work aims to process 3D point cloud directly.

a hierarchical application of PointNet to multiple subsets of
point cloud. Local structure exploitation with PointNet is
also investigated in [4, 30]. In addition, Superpoint [18] is
proposed to partition point cloud into geometric elements.
Graph convolution network is applied on a local graph cre-
ated by neighboring points [38, 37, 20]. However, these
methods do not explicitly model the local spatial layout of
points, thus acquiring less shape awareness. By contrast,
our work captures the spatial layout of points by learning a
high-level relation expression among points.
Some works map point cloud to a high-dimensional
space to facilitate the application of classic CNN. SPLAT-
Net [34] maps the input points onto a sparse lattice, then
processing with bilateral convolution [14]. PCNN [1] ex-
tends the function over point cloud to a continuous volu-
metric function over ambient space. These methods could
cause loss of geometric information, while our method di-
rectly operates on point cloud without introducing such loss.
Another key issue is the irregularity of points. Some
works focus on analyzing symmetric functions that are
equivariant to point sets learning [24, 27, 48, 19]. Some
other works [24, 21] develop alignment network for the
robustness to rigid transformation in 3D space. However,
the alignment learning is a suboptimal solution for this is-
sue. Some traditional descriptors like Fast Point Feature
Histograms can be invariant to translation and rotation, yet
they are often less effective for high-level shape understand-
ing. Our method that learns on geometric relation among
points is naturally robust to rigid transformation, whilst be-
ing highly effective due to the powerfulness of deep nets.

Relation learning. To learn a data-dependent weight from
relation has been explored in the ﬁeld of image and video
analysis. Spatial transformer [13] learns a transition matrix
to align 2D images. Non-local network [40] learns long-
term relation across video frames. Relation networks [9]
learn position relation across objects.
There are also some works focusing on the relation learn-
ing in 3D point cloud. DGCNN [41] captures similar lo-
cal shapes by learning point relation in a high-dimensional
feature space, yet this relation could be unreliable in some
cases. Wang et al. propose a parametric continuous convo-
lution that is based on computable relation among points,
but they do not explicitly learn from local to global like
classic CNN. By contrast, our method learns a high-level
relation expression from geometric priors in 3D space, and
performs contextual local-to-global shape learning.

3. Shape-Aware Representation Learning

Deep learning on point cloud. PointNet [24] pioneers this
route by independently learning on each point and gathering
the ﬁnal features with max pooling. Yet this design neglects
local structures, which have been proven important for the
success of CNN. To remedy this, PointNet++ [26] suggests

The core of point cloud analysis is to discriminatively
represent the underlying shape with robustness. Here we
learn contextual shape-aware representation for this goal,
by extending regular grid CNN to irregular conﬁguration
with a novel relation-shape convolution (RS-Conv).

8896

r

Channel-raising mapping.
In Eq. (3), the channel num-
ber of f Psub is the same as the input feature f xj . This is
inconsistent with classic image CNN that increases channel
number while decreasing image resolution for a more ab-
stract representation. For example, the channel number of
64-128-256-512 is set in VGG network [32]. Accordingly,
we add a shared MLP on f Psub for further channel-raising
mapping. It is illustrated in the middle part of Fig. 2.

3.2. Properties

RS-Conv in Eq. (3) can maintain four decent properties:

Permutation invariance.
In the inner mapping function
M(h), both the low-level relation h and the shared MLP
M are invariant to the input order of points. Therefore,
with the outer aggregation function A being symmetric, the
permutation invariance can be satisﬁed.

Robustness to rigid transformation. This property is well
held in the high-level relation encoding M(h). It can be
robust to rigid transformation, e.g., translation and rotation,
when a suitable h (e.g., 3D Euclidean distance) is deﬁned.

Points interaction. Points are not isolated and nearby
points form a meaningful shape in geometric space. Thus
their inherent interaction is critical for discriminative shape
awareness. Our solution of relation learning explicitly en-
code the geometric relation among points, naturally captur-
ing the interaction of points.

Weight sharing. This is the key property that allows apply-
ing the same learning function over different irregular point
subsets for robustness, as well as low complexity. In Eq. (3),
the symmetric A, the shared MLP M and the predeﬁned
geometric priors h are all independent to the irregularity of
points. Hence, this property is also satisﬁed.

3.3. Revisiting 2D Grid Convolution

The proposed RS-Conv is a generic formulation of 2D
grid convolution for relation reasoning. We clarify this with
a neighborhood (convolution kernel) of 3 × 3 on a 2D-grid
feature map, as illustrated in Fig. 3. Speciﬁcally, the sum-
mation function P is a speciﬁc instance of the aggregation
function A. Moreover, note that wj always implies a ﬁxed
positional relation between xi and its neighbor xj in the reg-
ular grid. For example, w1 always implies the top-left rela-
tion with xi , and w2 implies the right-above relation with
xi . In other words, wj is actually constrained to encode one
kind of regular grid relation in the learning process. There-
fore, our RS-Conv with relation learning is more general
and can be applied to model 2D grid spatial relationship.

3.4. RS(cid:173)CNN for Point Cloud Analysis

Using RS-Conv (Fig. 2) as a basic operator and adopt-
ing a uniform sampling strategy, a hierarchical shape-aware

output

: top left
: right above
: top right

.
.
.

grid relation

convolution kernel

feature map

Figure 3. Illustration of 2D grid convolution with a kernel of 3 × 3.

RS-CNN

RS-CNN

fully connected layers

labels

per-point predictions
feature propagation layers

(a)

...

...

long-range connections

(b)

Figure 4. The architectures of RS-CNN applied in the classiﬁca-
tion (a) and segmentation (b) of point cloud. N is the number of
points and C is the channel number.

learning architecture like classic CNN, namely, RS-CNN,
can be developed for point cloud analysis as

Fℓ

PNℓ

= RS -CONV(Fℓ−1

PNℓ−1

),

(4)

PNℓ

where Fℓ
, features in layer ℓ of the sampled point set PNℓ
with number Nℓ , are obtained by applying RS-Conv on the
features in the previous layer ℓ − 1.
Our RS-CNN applied in the classiﬁcation and segmen-
tation of point cloud is illustrated in Fig. 4. In both tasks,
RS-CNN is used for learning a group of hierarchical shape-
aware representation. The ﬁnal global representation fol-
lowed by three fully connected (FC) layers is conﬁgured
for classiﬁcation. For segmentation, the learned multi-level
representation is successively upsampled by feature propa-
gation [26] to generate per-point predictions. Both of them
can be trained in an end-to-end manner.

3.5. Implementation Details

RS-Conv in Eq. (3). Symmetric function max pooling is
applied as aggregation function A. ReLU [23] is used as
nonlinear activator σ . For mapping function M, a three-
layer shared MLP is deployed since theoretically it can ﬁt
arbitrary continuous mappings [8]. Low-level relation hij
is deﬁned as a compact vector with 10 channels, i.e., (3D
Euclidean distance, xi − xj , xi , xj ). The channel-raising
mapping is achieved by a single-layer shared MLP. Batch
normalization [12] is applied in each MLP.

RS-CNN for points analysis. The farthest points are
picked from point cloud for sampling local subsets to per-
form RS-Conv. In each neighborhood, a ﬁxed number of
neighbors are randomly sampled for batch processing, and

8898

they are normalized to take the centroid as the origin. To
capture more sufﬁcient geometric relation, we force RS-
CNN to learn over three-scale neighborhoods centered on
a sampled point with a shared weight. This is different
from multi-scale grouping (MSG) [26] that learns multi-
scale features using multiple groups of weight. RS-CNN
with 3 layers and 4 layers is deployed for classiﬁcation and
segmentation, respectively. Note that only 3D coordinates
xyz are used as the input features to RS-CNN.
Our RS-CNN is implemented using Pytorch2 . The Adam
optimization algorithm is employed for training, with a
mini-batch size of 32. The momentum for BN starts with
0.9 and decays with a rate of 0.5 every 20 epochs. The
learning rate begins with 0.001 and decays with a rate of
0.7 every 20 epochs. The weight of RS-CNN is initialized
using the techniques introduced by He et al. [7].

4. Experiment

In this section, we arrange comprehensive experiments
to validate the proposed RS-CNN. First, we evaluate RS-
CNN for point cloud analysis on three tasks (Sec 4.1). We
then provide detailed experiments to carefully study RS-
CNN (Sec 4.2). Finally, we visualize the shape features that
RS-CNN captures and analyze the complexity (Sec 4.3).

4.1. Point Cloud Analysis

Shape classiﬁcation. We evaluate RS-CNN on Model-
Net40 classiﬁcation benchmark [42].
It is composed of
9843 train models and 2468 test models in 40 classes. The
point cloud data is sampled from these models by [24]. We
uniformly sample 1024 points and normalize them to a unit
sphere. During training, we augment the input data with
random anisotropic scaling in the range [-0.66, 1.5] and
translation in the range [-0.2, 0.2], as in [16]. Meanwhile,
dropout technique [33] with 50% ratio is applied in FC lay-
ers. During testing, similar to [24, 26], we perform ten vot-
ing tests with random scaling and average the predictions.
The quantitative comparisons with the state-of-the-art
point-based methods are summarized in Table 1, where
RS-CNN outperforms all the xyz-input methods. Speciﬁ-
cally, RS-CNN reduces the error rate of PointNet++ [26] by
31.2%, and surpasses its advanced version that uses addi-
tional normal data as well as very dense points (5k). More-
over, even using only xyz as the input, RS-CNN can also
achieve a superior result (93.6%) compared with the best
additional-input method SO-Net [19] (93.4%). This con-
vincingly veriﬁes the effectiveness of our RS-CNN.
We test the robustness of RS-CNN on sampling den-
sity, by using sparser points of number 1024, 512, 256, 128
and 64 as the input to a model trained with 1024 points.
As in [26], random input dropout technique is applied for
a fair comparison. Fig. 5 shows the test results, where

2 https://github.com/Yochengliu/Relation-Shape-CNN

1024

512

256

128

64

Figure 5. Left part: Point cloud with random point dropout. Right
part: Test results of using sparser points as the input to a model
trained with 1024 points.

Table 1. Shape classiﬁcation results (%) on ModelNet40 bench-
mark (nor: normal, “-”: unknown).
method
input
Pointwise-CNN [10]
Deep Sets [48]
ECC [31]
PointNet [24]
SCN [44]
Kd-Net(depth=10) [16]
PointNet++ [26]
KCNet [30]
MRTNet [3]
Spec-GCN [38]
PointCNN [21]
DGCNN [41]
PCNN [1]
Ours
SO-Net [19]
Kd-Net(depth=15) [16]
O-CNN [39]
Spec-GCN [38]
PointNet++ [26]
SpiderCNN [45]
SO-Net [19]

#points
1k
1k
1k
1k
1k
1k
1k
1k
1k
1k
1k
1k
1k
1k
2k
32k
-
1k
5k
5k
5k

acc.
86.1
87.1
87.4
89.2
90.0
90.6
90.7
91.0
91.2
91.5
91.7
92.2
92.3
93.6
90.9
91.8
90.6
91.8
91.9
92.4
93.4

xyz, nor
xyz, nor
xyz, nor
xyz, nor
xyz, nor

xyz
xyz
xyz
xyz
xyz
xyz
xyz
xyz
xyz
xyz
xyz
xyz
xyz

xyz

xyz
xyz

the compared methods are PointNet [24], PointNet++ [26],
PCNN [1] and DGCNN [41]. As can be seen, it is more dif-
ﬁcult for shape recognition when points get sparser. Even
so, RS-CNN is still considerably robust. It achieves nearly
consistent robustness as PointNet++, whilst showing supe-
rior performance on each density.

Shape part segmentation. Part segmentation is a chal-
lenging task for ﬁne-grained shape analysis. We evaluate
RS-CNN for this task on ShapeNet part benchmark [46] and
follow the data split in [24]. This dataset contains 16881
shapes with 16 categories, and is labeled in 50 parts in total.
As in [24], we randomly pick 2048 points as the input and
concatenate the one-hot encoding of the object label to the
last feature layer. During testing, we also apply ten voting
tests using random scaling. Except for standard IoU (Inter-
over-Union) on each category, we also report two types of
mean IoU (mIoU) that are averaged across all classes and
all instances, respectively.

8899

Table 2. Shape part segmentation results (%) on ShapeNet part benchmark (nor: normal, “-”: unknown).

method

Kd-Net [16]
PointNet [24]
RS-Net [11]
SCN [44]
PCNN [1]
SPLATNet [34]
KCNet [30]
DGCNN [41]
Ours

PointNet++ [26]

SyncCNN [47]
SO-Net [19]

SpiderCNN [45]

input

4k
2k
-
1k
2k
-
2k
2k
2k
2k,nor
mesh
1k,nor
2k,nor

class
mIoU
77.4
80.4
81.4
81.8
81.8
82.0
82.2
82.3
84.0
81.9
82.0
80.8
82.4

instance
mIoU
82.3
83.7
84.9
84.6
85.1
84.6
84.7
85.1
86.2
85.1
84.7
84.6
85.3

car

table

cap

bag

air
chair ear
guitar knife lamp laptop motor
mug pistol rocket skate
plane
phone
bike
board
80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3
83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6
82.7 86.4 84.1 78.2 90.4 69.3 91.4 87.0 83.5 95.4 66.0 92.6 81.8 56.1 75.8 82.2
83.8 80.8 83.5 79.3 90.5 69.8 91.7 86.5 82.9 96.0 69.2 93.8 82.5 62.9 74.4 80.8
82.4 80.1 85.5 79.5 90.8 73.2 91.3 86.0 85.0 95.7 73.2 94.8 83.3 51.0 75.0 81.8
81.9 83.9 88.6 79.5 90.1 73.5 91.3 84.7 84.5 96.3 69.7 95.0 81.7 59.2 70.4 81.3
82.8 81.5 86.4 77.6 90.3 76.8 91.0 87.2 84.5 95.5 69.2 94.4 81.6 60.1 75.2 81.3
84.2 83.7 84.4 77.1 90.9 78.5 91.5 87.3 82.9 96.0 67.8 93.3 82.6 59.7 75.5 82.0
83.5 84.8 88.8 79.6 91.2 81.1 91.6 88.4 86.0 96.0 73.7 94.1 83.4 60.5 77.7 83.6
82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6
81.6 81.7 81.9 75.2 90.2 74.9 93.0 86.1 84.7 95.6 66.7 92.7 81.6 60.6 82.9 82.1
81.9 83.5 84.8 78.1 90.8 72.2 90.1 83.6 82.3 95.2 69.3 94.2 80.0 51.6 72.1 82.6
83.5 81.0 87.2 77.5 90.7 76.8 91.1 87.3 83.3 95.8 70.2 93.5 82.7 59.7 75.8 82.8

Table 3. Normal estimation error on ModelNet40 dataset.

dataset
ModelNet40

method
PointNet [1]
PointNet++ [1]
PCNN [1]
Ours

#points
1k
1k
1k
1k

error
0.47
0.29
0.19
0.15

k

s
e

d

e

n

a

l

p

r
i

a

n
o

s

r

e

p

s

r
i

a

t

s

ground truth
ground truth

ours

pointnet
< 30° normal

pointnet++
> 90° normal

Figure 7. Normal estimation on ModelNet40 dataset. For clear-
ness, we only show predictions with angle less than 30◦ in blue,
and angle greater than 90◦ in red between ground truth normals.

a lower error of 0.15. This signiﬁcantly reduces the er-
ror of PointNet++ (0.29) by 48.3%. Fig. 7 shows some
normal estimation examples, where our RS-CNN with geo-
metric relation learning can obtain more decent predictions.
However, RS-CNN could also be less effective for some in-
tractable shapes, such as spiral stairs and intricate plants.

8900

Figure 6. Segmentation examples on ShapeNet part benchmark.

Table 2 summarizes the quantitative comparisons with
the state-of-the-art methods, where RS-CNN achieves the
best performance with class mIoU of 84.0% and instance
mIoU of 86.2%. This considerably surpasses the second
best xyz-based methods, i.e., DGCNN [41] with 82.3%
(1.7↑) in class mIoU and PCNN [1] with 85.1% (1.1↑) in
instance mIoU, respectively. Noticeably, RS-CNN sets new
state of the arts in the xyz-based methods over ten cate-
gories. These improvements demonstrate the robustness of
RS-CNN to diverse shape structures. Fig. 6 shows some
segmentation examples. One can see that although the part
shapes implied in irregular points are varied and they may
be very confusing to recognize, RS-CNN can also segment
them out with decent accuracy.

Normal estimation. Normal estimation in point cloud is
a crucial step for numerous applications, such as surface
reconstruction and rendering. This task is very challeng-
ing since it requires a higher level of reasoning, which goes
beyond the underlying shape recognition. We take normal
estimation as a supervised regression task, and achieve it us-
ing the segmentation network. The cosine-loss between the
normalized output and ground truth normal is applied for
regression training. ModelNet40 dataset is used for evalua-
tion, with uniformly sampled 1024 points as the input.
The quantitative results are summarized in Table 3. RS-
CNN outperforms other advanced methods on this task with

Table 4. Ablation study of RS-CNN (%).
“DP” indicates the
dropout technique in FC layers of the classiﬁcation network.

Table 5. The results (%) of different designs on aggregation func-
tion A and mapping function M (Eq. (3)) (M(k) : k-layer MLP).

model
A
B
C
D
E
F
G
H
I

#points
1k
1k
1k
1k
1k
1k
1k
2k
1k

relation

BN

DP

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

scale
1
1
1
1
2
3
3
3
3

voting

X

X

X

acc.
87.2
89.9
91.9
92.2
92.5
92.9
93.6
93.6
90.1

4.2. RS(cid:173)CNN Design Analysis

In this section, we ﬁrst perform a detailed ablation study
on RS-CNN. Then, we discuss the choices of aggregation
function A, mapping function M and low-level relation h
in Eq. (3). Finally, we validate the robustness of RS-CNN
on point permutation and rigid transformation. All experi-
ments are conducted on ModelNet40 classiﬁcation dataset.

Ablation study. The results are summarized in Table 4.
The baseline (model A) is set to learn without geometric re-
lation encoding, but with a shared three-layer MLP as fea-
ture transformation function T in Eq. (1).
The baseline only gets an accuracy of 87.2%. Yet with
geometric relation learning, it is signiﬁcantly improved to
89.9% (model B). This convincingly veriﬁes the effective-
ness of our RS-CNN. Then, a great improvement of 2%
is gained after using BN (model C), maybe because it can
greatly ease the network training. Moreover, dropout tech-
nique improves the result by 0.3% (model D). As men-
tioned in Sec 3.5, RS-CNN should be able to beneﬁt from
sufﬁcient geometric relation. This is veriﬁed by model E
(92.5%) and model F (92.9%) that perform two-scale and
three-scale relation learning, respectively. Eventually, with
ten voting tests, an impressive accuracy of 93.6% (model
G) can be obtained with only xyz features.
To investigate the impact of the number of input points
on RS-CNN, we also train the network with 2048 points but
ﬁnd no improvement (model H). In addition, to compare
with the baseline (model A) more fairly, we set a new base-
line (model I) that works with all the techniques but rela-
tion learning. It gets an accuracy of 90.1%, which RS-CNN
can also surpass by 3.5%. We speculate that RS-CNN with
geometric relation reasoning can acquire more discrimina-
tive shape awareness, and this awareness can be greatly en-
hanced by multi-scale relation learning.

Aggregation function A. Three symmetric functions: max
pooling (max), average pooling (avg.)
and summation
(sum), are employed to study the effect of A on RS-CNN.
Table 5 summarizes the results. As can be seen, with M
using three layers, max pooling achieves the best perfor-
mance while average pooling and summation get the same

A M(2) M(3) M(4)
X

max
max
max
avg.
sum

acc.
92.4
93.6
92.7
91.6
91.6

X

X

X

X

Table 6. The results (%) of ﬁve intuitive low-level relations h (Ed:
Euclidean distance, cosd: cosine distance, xnor : normal of x, x′ :
2D projection of x). Model A applies only 3D Euclidean distance
as h; Model B adds the coordinates difference to model A; Model
C adds the coordinates of two points to model B; Model D utilizes
the normals of two points and their cosine distance as h; Model E
projects 3D points onto a 2D plane of XY , XZ and YZ.
model
low-level relation h
channels
acc.
A
(3D-Ed)
1
92.5
B
(3D-Ed, xi − xj )
4
93.0
C
(3D-Ed, xi − xj , xi , xj )
10
93.6
(3D-cosd, xnor
D
j )
7
92.8
E
(2D-Ed, x′
i − x′
j , x′
i , x′
j )
10
≈ 92.2

, xnor

i

accuracy. The reason may be that max pooling can select
the biggest feature response, thus keeping the most expres-
sive representation and removing redundant information.

Mapping function M. The results of M deployed with
different layers are summarized in the ﬁrst three rows of
Table 5. One can see that the best accuracy of 93.6% is
obtained by a shared three-layer MLP, and it decreases by
0.9% when increasing the number of layers. The reason
might be that M with four layers brings some difﬁculty for
network training. Noticeably, RS-CNN can also get a de-
cent accuracy of 92.4% with M using only two layers. This
veriﬁes the powerfulness of relation learning for underlying
shape capturing from point cloud.

Low-level relation h. The key to RS-CNN is learning from
relation, thus how to deﬁne h is an issue worth exploring.
Actually, h can be deﬁned ﬂexibly, as long as it could dis-
criminatively reﬂect the underlying shape. To validate this
claim and facilitate the understanding, we experiment with
ﬁve intuitive relation deﬁnitions as examples, whose results
are summarized in Table 6.
As can be seen, using only 3D Euclidean distance as
h, the accuracy can also reach 92.5% (model A). This
demonstrates the effectiveness of our RS-CNN for high-
level geometric relation learning. Moreover, the perfor-
mance is gradually improved with additional relation, in-
cluding coordinates difference (model B) and coordinates
themselves (model C). We also utilize the normal vectors of
two points and their cosine distance as h, the result (model
D) is 92.8%. This indicates RS-CNN is also able to abstract
shape information from the relation in normals.

8901

Table 7. Robustness to point permutation and rigid transformation
(%). During testing, we perform random permutation (perm.) of
points, add a small translation of ±0.2 and counterclockwise rotate
the input point cloud by 90◦ and 180◦ around Y axis.

method
acc.
perm.
+0.2
-0.2
90◦
180◦
PointNet [24]
88.7
88.7
70.8
70.6
42.5
38.6
88.2†
PointNet++ [26]
88.2
88.2
88.2
47.9
39.7
Ours
90.3†
90.3
90.3
90.3
90.3
90.3
† The accuracy drops a lot mainly because the forcible normalization
of each local point subset could bring difﬁculty for shape recognition.

Intuitively, the relation among points in the 2D view of
point cloud can also reﬂect the underlying shape. There-
fore, to validate our RS-CNN for shape abstraction on 2D
relation, we forcibly set the value of one dimension in 3D
coordinates to be zero, i.e., projecting 3D points onto a 2D
plane of XY , XZ and YZ. The results are all around 92.2%
(model E), which is quite impressive. This further veriﬁes
the effectiveness of the proposed relation learning method.

Robustness to point permutation and rigid transforma-
tion. We compare the robustness of our RS-CNN with
PointNet [24] and PointNet++ [26]. Note that all the models
are trained without related data augmentations, e.g., transla-
tion or rotation, to avoid confusion in this test. In addition,
although relation learning in RS-CNN is robust to rotation,
the initial input features of 3D coordinates are affected. We
address this issue by normalizing each sampled point sub-
set to corresponding local coordinate system, which is de-
termined by each sampled point and its normal. For a fair
comparison, we also perform this normalization for Point-
Net++, as it learns over local subsets as well. The 3D Eu-
clidean distance is applied as geometric relation h in RS-
CNN for this test. Table 7 summarizes the test results.
As can be seen, all the methods are invariant to permu-
tation. However, PointNet is vulnerable to both translation
and rotation while PointNet++ is sensitive to rotation. By
contrast, our RS-CNN with geometric relation learning is
invariant to these perturbations, making it powerful for ro-
bust shape recognition.

4.3. Visualization and Complexity Analysis

Visualization. Fig. 8 visualizes the shape features learned
by the ﬁrst two layers of RS-CNN on ModelNet40 dataset.
As it shows, the features learned by the ﬁrst layer mostly
respond to edges, corners and arcs, while the ones in the
second layer capture more semantical shape parts like air-
foils and heads. This veriﬁes RS-CNN can learn progressive
shape-aware representation for point cloud analysis.

Complexity Analysis. Table 8 summarizes the space
(number of params) and the time (ﬂoating point opera-
tions/sample) complexity of RS-CNN in classiﬁcation with
1024 points as the input. Compared with PointNet [24],
RS-CNN reduces the params by 59.7% and the FLOPs by
32.9%, which shows its great potential for real-time appli-
cations, e.g., scene parsing in autonomous driving.

Figure 8. Visualization of the shape features learned by the ﬁrst
two layers of RS-CNN on ModelNet40 dataset. The features
learned by the ﬁrst layer mostly respond to edges, corners and arcs,
while the ones in the second layer capture more semantical shape
parts like airfoils and heads.

Table 8. Complexity of RS-CNN in point cloud classiﬁcation.

method
PointNet [24]
PointNet++ [21]
PCNN [21]
Ours

#params
3.50M
1.48M
8.20M
1.41M

#FLOPs/sample
440M
1684M
294M
295M

5. Conclusion

In this work, RS-CNN, namely, Relation-Shape Convo-
lutional Neural Network, which extends regular grid CNN
to irregular conﬁguration for point cloud analysis, has been
proposed. The core to RS-CNN is a novel convolution op-
erator, which learns from relation, i.e., the geometric topol-
ogy constraint among points. In this way, explicit reasoning
about the spatial layout of points can be made to obtain dis-
criminative shape awareness. Moreover, the decent prop-
erties of geometric relation can also be acquired, such as
robustness to rigid transformation. As a consequence, RS-
CNN equipped with this operator can achieve contextual
shape-aware learning, making it highly effective. Extensive
experiments on challenging benchmarks across three tasks,
as well as thorough empirical and theoretical analysis, have
demonstrated RS-CNN achieves the state of the arts.

Acknowledgement

The authors thank anonymous reviewers very much for
their valuable comments that greatly improve this paper.
This work is supported by the National Natural Science
Foundation of China under Grants 61573352, 91646207
and 61773377, the Young Elite Scientists Sponsorship Pro-
gram by CAST under Grant 2018QNRC001 and the Beijing
Natural Science Foundation under Grant L172053.

8902

References

[1] M. Atzmon, H. Maron, and Y. Lipman. Point convolutional
neural networks by extension operators.
In SIGGRAPH,
pages 1–14, 2018. 2, 5, 6

[2] Y. Feng, Z. Zhang, X. Zhao, R. Ji, and Y. Gao. GVCNN:
Group-view convolutional neural networks for 3D shape
recognition. In CVPR, pages 264–272, 2018. 1, 2

[3] M. Gadelha, R. Wang, and S. Maji. Multiresolution tree net-
works for 3D point cloud processing. In ECCV, pages 105–
122, 2018. 1, 2, 5

[4] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra.
PCPNet: Learning local shape properties from raw point
clouds. Comput. Graph. Forum, 37(2):75–85, 2018. 2

[5] H. Guo, J. Wang, Y. Gao, J. Li, and H. Lu. Multi-view 3D
object retrieval with deep embedding network. IEEE Trans.
Image Processing, 25(12):5526–5537, 2016. 1, 2

[6] Z. Han, M. Shang, Z. Liu, C. Vong, Y. Liu, M. Zwicker,
J. Han, and C. L. P. Chen. SeqViews2SeqLabels: Learning
3D global features via aggregating sequential views by RNN
with attention. IEEE Trans. Image Processing, 28(2):658–
672, 2019. 2

[7] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rec-
tiﬁers: Surpassing human-level performance on ImageNet
classiﬁcation. In ICCV, pages 1026–1034, 2015. 5

[8] K. Hornik. Approximation capabilities of multilayer feed-
forward networks. Neural Networks, 4(2):251–257, 1991.
4

[9] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks
for object detection. In CVPR, pages 3588–3597, 2018. 2

[10] B.-S. Hua, M.-K. Tran, and S.-K. Yeung. Pointwise convo-
lutional neural networks.
In CVPR, pages 974–993, 2018.
5

[11] Q. Huang, W. Wang, and U. Neumann. Recurrent slice net-
works for 3D segmentation of point clouds. In CVPR, pages
2626–2635, 2018. 6

[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML, pages 448–456, 2015. 4

[13] M.
Jaderberg, K. Simonyan, A. Zisserman,
K. Kavukcuoglu.
Spatial
transformer networks.
NeurIPS, pages 2017–2025, 2015. 2

and
In

[14] V. Jampani, M. Kiefel, and P. V. Gehler. Learning sparse high
dimensional ﬁlters: Image ﬁltering, dense CRFs and bilateral
neural networks. In CVPR, pages 4452–4461, 2016. 2

[15] M. Jiang, Y. Wu, and C. Lu. PointSIFT: A SIFT-like network
module for 3D point cloud semantic segmentation. arXiv
preprint arXiv:1807.00652, 2018. 1

[16] R. Klokov and V. S. Lempitsky. Escape from cells: Deep
Kd-Networks for the recognition of 3D point cloud models.
In ICCV, pages 863–872, 2017. 2, 5, 6

[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
ImageNet
classiﬁcation with deep convolutional neural networks.
In
NeurIPS, pages 1106–1114, 2012. 1, 3

[18] L. Landrieu and M. Simonovsky. Large-scale point cloud
semantic segmentation with superpoint graphs.
In CVPR,
pages 4558–4567, 2018. 1, 2

[19] J. Li, B. M. Chen, and G. H. Lee. SO-Net: Self-organizing
network for point cloud analysis.
In CVPR, pages 9397–
9406, 2018. 2, 5, 6
[20] R. Li, S. Wang, F. Zhu, and J. Huang. Adaptive graph convo-
lutional neural networks. In AAAI, pages 3546–3553, 2018.
2
[21] Y. Li, R. Bu, M. Sun, and B. Chen. PointCNN: Convolution
on X-transformed points. In NeurIPS, pages 828–838, 2018.
2, 5, 8
[22] D. Maturana and S. Scherer. VoxNet: A 3D convolutional
neural network for real-time object recognition.
In IROS,
pages 922–928, 2015. 1, 2
[23] V. Nair and G. E. Hinton. Rectiﬁed linear units improve
restricted boltzmann machines.
In ICML, pages 807–814,
2010. 4
[24] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. PointNet: Deep
learning on point sets for 3D classiﬁcation and segmentation.
In CVPR, pages 77–85, 2016. 1, 2, 5, 6, 8
[25] C. R. Qi, H. Su, M. Nießner, A. Dai, M. Yan, and L. J.
Guibas. Volumetric and multi-view CNNs for object clas-
siﬁcation on 3D data. In CVPR, pages 5648–5656, 2016. 2
[26] C. R. Qi, L. Yi, H. su, and L. J. Guibas. PointNet++: Deep
hierarchical feature learning on point sets in a metric space.
In NeurIPS, pages 5099–5108, 2017. 1, 2, 4, 5, 6, 8
[27] S. Ravanbakhsh, J. Schneider, and B. Poczos. Deep learning
with sets and point clouds. In ICLR, pages 1–12, 2017. 1, 2
[28] G. Riegler, A. O. Ulusoy, and A. Geiger. OctNet: Learning
deep 3D representations at high resolutions. In CVPR, pages
6620–6629, 2017. 2
[29] A. Savchenkov. Generalized convolutional neural networks
for point cloud data. In ICMLA, pages 930–935, 2017. 1
[30] Y. Shen, C. Feng, Y. Yang, and D. Tian. Mining point cloud
local structures by kernel correlation and graph pooling. In
CVPR, pages 4548–4557, 2018. 2, 5, 6
[31] M. Simonovsky and N. Komodakis.
Dynamic edge-
conditioned ﬁlters in convolutional neural networks on
graphs. In CVPR, pages 29–38, 2017. 5
[32] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, pages
1–14, 2015. 1, 3, 4
[33] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov. Dropout: A simple way to prevent neu-
ral networks from overﬁtting. Journal of Machine Learning
Research., 15(1):1929–1958, 2014. 5
[34] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H.
Yang, and J. Kautz. SPLATNet: Sparse lattice networks for
point cloud processing. In CVPR, pages 2530–2539, 2018.
1, 2, 6
[35] H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller.
Multi-view convolutional neural networks for 3D shape
recognition. In ICCV, pages 945–953, 2015. 1, 2
[36] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree gen-
erating networks: Efﬁcient convolutional architectures for
high-resolution 3D outputs.
In ICCV, pages 2107–2115,
2017. 2
[37] G. Te, W. Hu, A. Zheng, and Z. Guo. RGCNN: Regular-
ized graph CNN for point cloud segmentation. In MM, pages
746–754, 2018. 2

8903

[38] C. Wang, B. Samari, and K. Siddiqi. Local spectral graph
convolution for point set feature learning. In ECCV, pages
1–16, 2018. 1, 2, 5
[39] P. Wang, Y. Liu, Y. Guo, C. Sun, and X. Tong. O-CNN:
octree-based convolutional neural networks for 3D shape
analysis. ACM Trans. Graph., 36(4):72:1–72:11, 2017. 2,
5
[40] X. Wang, R. B. Girshick, A. Gupta, and K. He. Non-local
neural networks. In CVPR, pages 7794–7803, 2018. 2
[41] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and
J. M. Solomon. Dynamic graph CNN for learning on point
clouds. arXiv preprint arXiv:1801.07829, 2018. 2, 5, 6
[42] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and
J. Xiao. 3D ShapeNets: A deep representation for volumetric
shapes. In CVPR, pages 1912–1920, 2015. 1, 2, 5
[43] J. Xie, G. Dai, F. Zhu, E. K. Wong, and Y. Fang. DeepShape:
Deep-learned shape descriptor for 3D shape retrieval. IEEE
Trans. Pattern Anal. Mach. Intell., 39(7):1335–1345, 2017.
2
[44] S. Xie, S. Liu, Z. Chen, and Z. Tu. Attentional ShapeCon-
textNet for point cloud recognition. In CVPR, pages 4606–
4615, 2018. 5, 6
[45] Y. Xu, T. Fan, M. Xu, L. Zeng, and Y. Qiao. SpiderCNN:
Deep learning on point sets with parameterized convolu-
tional ﬁlters. In ECCV, pages 90–105, 2018. 1, 5, 6
[46] L. Yi, V. G. Kim, D. Ceylan, I. Shen, M. Yan, H. Su, C. Lu,
Q. Huang, A. Sheffer, and L. J. Guibas. A scalable active
framework for region annotation in 3D shape collections.
ACM Trans. Graph., 35(6):210:1–210:12, 2016. 5
[47] L. Yi, H. Su, X. Guo, and L. J. Guibas. SyncSpecCNN:
Synchronized spectral CNN for 3D shape segmentation. In
CVPR, pages 6584–6592, 2017. 6
[48] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. P ´oczos, R. R.
Salakhutdinov, and A. J. Smola. Deep sets.
In NeurIPS,
pages 3394–3404, 2017. 1, 2, 5
[49] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional networks. In ECCV, pages 818–833, 2014. 1

8904

Relational Action Forecasting

Chen Sun1 , Abhinav Shrivastava2 , Carl Vondrick1 ,
Rahul Sukthankar1 , Kevin Murphy1 , and Cordelia Schmid1

1Google Research
2University of Maryland

Abstract

This paper focuses on multi-person action forecasting
in videos. More precisely, given a history of H previous
frames, the goal is to detect actors and to predict their fu-
ture actions for the next T frames. Our approach jointly
models temporal and spatial interactions among different
actors by constructing a recurrent graph, using actor pro-
posals obtained with Faster R-CNN as nodes. Our method
learns to select a subset of discriminative relations with-
out requiring explicit supervision, thus enabling us to tackle
challenging visual data. We refer to our model as Discrimi-
native Relational Recurrent Network (DR2N). Evaluation of
action prediction on AVA demonstrates the effectiveness of
our proposed method compared to simpler baselines. Fur-
thermore, we signiﬁcantly improve performance on the task
of early action classiﬁcation on J-HMDB, from the previous
SOTA of 48% to 60%.

1. Introduction

In this paper, we consider the task of forecasting what
high level actions people will perform in the future, given
noisy visual evidence. For example, consider Figure 1:
given the current video frame, and a sequence of past
frames, we would like to detect (localize) the people (man
and woman), and classify their current actions (woman rides
horse, man and woman are talking), as well as predict future
plausible action sequences for each person (woman will get
off horse, man will hold horse reigns).
More formally, our task is to compute the probability
p(N , b0
1:N |V −H :0 ), where V t is the frame at time t
(where t = 0 is the present), V = V −H :0 is the visual his-
tory of H previous frames, N is the number of predicted
actors, b0
n is the predicted location (bounding box) of actor
n at time 0, and at
n is the predicted action label for actor
n at time t, which we compute for t = 0 : T , where T

1:N , a0:T

Figure 1: Action prediction from a single frame (middle)
is ambiguous, but requires temporal context for the actors
and their interactions.
It only becomes apparent that the
lady will get off the horse, if we know that she was riding
towards the man, and the man is holding the horse.

1:N , b0:T
1:N |V 0:T );

is the maximum forecasting horizon. This formulation is
closely related to but different than prior work. In particular,
video classiﬁcation focuses on computing a single global
label in the ofﬂine scenario, p(c|V 0:T ); spatio-temporal ac-
tion detection focuses on multi-agent localization and clas-
siﬁcation, but in the ofﬂine scenario, p(a0:T
action anticipation focuses on the future, but for a single
class label, p(c0:T |V −H :0 ); and trajectory forecasting fo-
cuses on multiple agents in the future, but generally fo-
cuses on locations, not actions, and assumes that the past
locations (and hence the number of agents) is observed:
p(b1:T
1:N , N ). By contrast, we only observe
past frames, and want to predict future high level actions of
agents. This could be useful for self-driving cars, human-
robot interaction, etc. See Section 2 for a more detailed
discussion of related work.

1:N |V −H :0 , b−H :0

1273

Figure 2: Overview of our approach Discriminative Relational Recurrent Network (DR2N). Given actor proposals and their
spatio-temporal descriptors at a given time T=0, we model their relations by a graph neural network and its recurrence over
time (here with a GRU).

Our proposed approach is to create a graph-structured
recurrent neural network (GRNN), in which nodes corre-
spond to candidate person detections (from an object de-
tector trained on person examples), and edges represent po-
tential interactions between the people. Each node has an
action label and bounding box location associated with it.
(Note that some nodes may be false positive detections aris-
ing from the detector, and should be labeled as such.) We
use a modiﬁed version of graph attention networks [61] ap-
plied to a fully connected pairwise graph to capture inter-
action effects. Nodes are also linked over time via RNNs
to capture temporal patterns in the data. We name our pro-
posed framework Discriminative Relational Recurrent Net-
work (DR2N). See Figure 2 for an illustration.

We train the model in a weakly supervised way, in the
sense that we have a ground truth set of labeled bounding
boxes for people in frame 0, and we have action labels (but
not location labels) for the people for frames 1 : T . How-
ever, we do not observe any edge information. Our model
is able to learn which edges are useful, so as to maximize
node classiﬁcation performance. On the challenging AVA
dataset [18], we show that our model outperforms various
strong baselines at the task of predicting person action la-
bels for up to 5 seconds into the future.

To be more directly comparable to prior work, we also
consider another task, namely “early classiﬁcation” of video
clips, where the task is to compute p(c|V 0:t ), where c is the
class label for the video clip of length T , and t < T is
some preﬁx of the clip (we use the ﬁrst 10% to 50% of the
frames). We modify our model for this task, and train it on
the J-HMDB dataset, as used in prior works [54, 55]. We
achieve signiﬁcant improvements over the previous state of
the art, increasing the classiﬁcation accuracy given a 10%
preﬁx from 48% (obtained by [54]) to 60%.

2. Related work

Action recognition. Human action recognition in videos
is dominated primarily by three well-established tasks: ac-
tion classiﬁcation [52, 7, 38, 26, 56, 1, 28, 41, 29], tempo-
ral action localization [7, 24, 75, 53], and spatio-temporal
action detection [30, 76, 48, 26, 56, 71, 40, 18]. Given
a video (a set of frames), the goal of action classiﬁcation
is to assign action labels to the entire video, whereas tem-
poral action localization assigns labels to only a subset of
frames representing the action. Spatio-temporal action de-
tection combines the temporal action localization with ac-
tor detection, i.e., detecting actors per-frame in a subset
of frames and assigning action labels to per-actor spatio-
temporal tubes [30, 76, 26].

The three standard tasks discussed above assume that the
entire video is observed, therefore prediction in any frame
can utilize past or future frames. In contrast, this paper in-
troduces the task of actor-level action prediction, i.e., given
a video predict or anticipate what actions each actor will
perform in the future. This task operates in a more prac-
tical setup where only the past and present frames are ob-
served, and predictions have to be made for unobserved fu-
ture frames. Note that the proposed task inherently requires
spatio-temporal actor detection in the observed frames.

Future prediction. Our work follows a large number of
works studying future prediction [64, 44, 39, 45, 14, 3,
27, 35, 73, 66, 57, 67, 77, 63, 32, 16, 78, 2, 47, 68, 62,
15, 33, 13, 75, 36]. Broadly speaking, the research on fu-
ture prediction follows two main themes: generating future
frame(s) [64, 44, 39, 45, 14, 3, 27, 35, 73, 57] and predicting
future labels or state(s) [67, 77, 63, 32, 16, 78, 2, 47, 62, 15,
33, 13, 75, 36, 66, 49]. For future frame generation, there
is a wide variety of approaches ranging from predicting in-
termediate representations (e.g., optical ﬂow [45, 15, 68],

274

human pose [62, 69]) and using it to generate future pix-
els, to directly generating future pixels by extending gener-
ative models for images to videos [64, 44, 14, 3]. Though
the quality of generated frames has improved over the past
few years [65, 3], it is arguably solving a much harder task
than necessary (after all, humans can predict likely future
actions, but cannot predict likely future pixels).

The second theme in future prediction circumvents pixel
generation and directly predicts future states [67, 77, 63, 32,
16, 78, 2, 47, 62, 15, 33, 13, 75, 36, 66, 49]. These future
states can vary from low-level trajectories [67, 32, 2, 47, 49,
33] for different agents to high-level semantic outputs [36,
13, 75, 37, 58]. The trajectory-based approaches rely on
modeling and predicting agent behavior in the future, where
an agent can be an object (such as human [32, 2, 47] or
car [49]) or an image patch [67]. Most of these methods
require a key assumption that the scene is static; i.e., no
camera motion (e.g., VIRAT dataset [42]) and no movement
of non-agent entities [32, 2, 47, 49]. Our method is similar
to these trajectory-based methods, in the sense that our state
prediction is also about agents. However, we do not assume
anything about the scene.
In particular, the AVA dataset
has signiﬁcant camera motion and scene cuts, in addition to
agent motion and cluttered, dynamic backgrounds.

Much work on future forecasting focuses on the spatop-
temporal extent of detected actions [54, 55, 20, 37]. How-
ever, there is some work on forecasting high-level seman-
tic states, ranging from semantic segmentation masks [36]
to action classes [13, 75]. However, our method has sev-
eral key distinctions. First, many works require the se-
mantic states to be part of the input to the forecasting al-
gorithm, whereas our method detects actors and actions
from past/present frames. Second, most of the state pre-
diction methods operate on MoCap data (e.g., 3D human
keypoints) [16, 25], whereas our approach works from pix-
els. Third, many method assume static cameras and pos-
sibly single agents, whereas we can forecasting labels for
multiple agents in unconstrained video.

Relational reasoning. Our proposed approach builds on
the ﬁeld of relational reasoning [6, 51, 43, 31, 50, 61, 21,
70, 12, 19, 5, 4, 43]. This is natural because the current
and future actions of an actor rely heavily on the dynam-
ics it shares with other actors [2]. Relational reasoning [6],
in general, can capture relationships between a wide array
of entities. For example, relationship between abstract en-
tities or features [51, 70], different objects [4], humans and
objects [10, 9, 17, 74], humans and context [59], humans
and humans [2, 23], etc. Our work aims to capture human-
human relationships to reason about future actions.

In terms of modeling these relationships, the standard
tools include Interaction Network (IN) [5], Relation Net-
work (RN) [51], Graph Neural Network (GNN) [19], Graph
ATtenion networks [61], as well as their contemporary ex-

tensions to videos, such as Actor-centric Relation Network
(ACRN) [59] and Object Relation Network (ORN) [4].
Similar to ACRN and ORN, our proposed DR2N tries
to capture relation between different entity in videos for
spatio-temporal reasoning. However, as opposed to mod-
eling exhaustive relationships (RN [51] and ORN [4]), our
method discovers discriminative relationships for the task
of actor-level action prediction. Compared to ACRN, our
method focuses on forecasting future actor labels given past
visual information. In addition, we model interaction be-
tween the agents, but ignore any objects in the scene.

3. Approach

In this section, we describe our approach in more detail.

3.1. Creating the nodes in the graph

To generate actor proposals and extract the initial actor-
level features for action prediction, we build our system on
top of the two-stage Faster RCNN [46] detector. The ﬁrst
stage is a region proposal network (RPN) that generates
a large set (e.g. hundreds) of candidate actor proposals in
terms of 2D bounding boxes on a single frame. We apply
this RPN module on the last observed frame V 0 to locate ac-
tors whose actions are to be predicted. These become nodes
in the graph.
The second stage associates features with these nodes.
We ﬁrst extract visual features from video inputs, V −H :0 ,
using a 3D CNN (see 3.6 for details), and then crop out
features inside the bounding boxes for all (candidate) actors
using ROIPooling. Let vi be the visual features for actor i.
The third stage is to connect the nodes together into a
fully connected graph. However, since not all nodes are
equally useful for prediction, in Section 3.3 we explain how
to learn discriminative edge weights.

3.2. Modeling node dynamics

We model the action dynamics of individual actors using
RNNs. Given ht
i as the latent representation for actor i at
time t and at
i as the set of actor labels, we have

ht

i = fRNN (ht−1
i

, at−1
i

)

at

i = fCLS (ht

i )

(1)

(2)

where fRNN (·) is the RNN update function, and fCLS (·)
is an action classiﬁer that decodes the latent states h into
action labels (we use a simple MLP for this, see 3.6 for
details). The initial state h0
i is set to vi , the visual features
extracted for this bounding box.
To make the model scalable over a varying number of
actors, the RNN function fRNN (·) is shared over all actors.
Similarly, the action classiﬁer fCLS (·) is shared over all ac-
tors and all time steps.

275

p(N , b0

1:N , a0:T

1:N |V −H :0 ) = δ(N , b0
1:N |fRPN (V 0 ))p(a0

1:N , h0
1:N |b0
1:N , V −H :0 )

T

Y

t=1

p(at

1:N , ht
1:N |a1:t−1
1:N , ht−1
1:N )

p(a0

1:N , h0
1:N |b0

1:N , V ) =

p(at

1:N , ht
1:N |at−1
1:N , ht−1
1:N ) =

N

Y

n=1

N

Y

n=1

Cat(a0

n |fCLS (h0

n ))δ(h0

n |fROI (fS3D (V −H :0 ), b0

n ))

Cat(at

n |fCLS (ht

n ))δ(ht

n |fRNN (˜ht−1
n , at−1

n ))δ(˜ht−1

n |fGNN (ht−1
1:N ))

Table 1: Formal speciﬁcation of DR2N model. ht
n is the hidden state of RNN n at time t; δ(a|b) denotes a deterministic
probability distribution. Here fRPN is a region proposal network applied to frame V 0 which predicts the location of N
boxes b0
1:N . fROI is a region of interest feature extractor applied to S3D features derived from frames V −H :0 at the locations
speciﬁed by the boxes. fCLS is an MLP classiﬁer with softmax output, and Cat(a|p) is a categorical distribution over action
labels a with parameters p. fGNN is a graph neural network where the input nodes are the old RNN hidden states, ht−1
1:N , and
the output nodes are denoted ˜ht−1
1:N ; its deﬁnition is given in Table 2. Finally, fRNN is a recurrent neural net update function
applied to the previous predicted label and GNN output.

˜h1:N = fGNN (h1:N ) =

N

Y

i=1

δ(˜hi |fnode ([hi , zi ]))δ(zi | X

j

αij hj )

N

Y

j=1

δ(αij |Sj (fattn (ei,1:N ))δ(eij |fedge (hi , hj ))

Table 2: Formal speciﬁcation of the graph neural network. fnode is an MLP that computes node states, fedge is an MLP that
computes edge states, fattn is a self-attention network, and Sj (l) is the j ’th output of the softmax function with logits l.

3.3. Modeling the edges

mented as

Our current model captures action dynamics for a set
of independent actors. Many actions involve interactions
among actors (e.g. hugging and kissing). To capture such
interactions, it is important to model the relations between
different actors. Motivated by the recent success on rela-
tional reasoning, we combine graph neural networks (GNN)
with recurrent models. We treat each actor as a node and use
the RNN’s latent representation ht
i as node feature.
We ﬁrst consider a general graph network deﬁnition. For
simplicity, we ignore the time step t, and denote hi as the
feature representation for node i. Let’s also denote Ni as the
neighbors of i in the graph. We compute the output repre-
sentation ˜hi from the input features of the other connected
nodes:

eij = fedge (hi , hj )

˜hi = fnode ({eij : j ∈ Ni })

(3)

(4)

where eij is the derived features for edge (i, j ). Both
fedge (·) and fnode (·) can be implemented with neural net-
works. Note that fnode (·) is a function mapping a set to a
vector. To make it permutation invariant, it is often imple-

˜hi = fnode




1
|Ni | X

j∈Ni

eij


 .

(5)

i.e. the output node feature is the average over all edge fea-
tures connected to the node.
The overall graph-RNN update function can thus be ex-
pressed as

ht

i = fRNN (˜ht−1
i

, at−1
i

)

(6)

i.e. we apply GNN on the hidden states at time t − 1, then
run the standard RNN update function.
In practice, the number of actors are not known before
hand, neither are the graph structures (relations) provided
as supervision. Besides, the outputs from person detec-
tors are typically over-complete and noisy (e.g. hundreds to
thousands of proposals per frame are generated by Faster-
RCNN). One method to handle unknown graph is via “re-
lation networks” [51, 59], which assumes the graph is fully
connected (i.e. Ni = {vj | j 6= i}). According to Equa-
tion 5, this leads to an average over all edge features, and is
sensitive to noisy nodes.
To mitigate this problem, we introduce the concept of
“virtual node” zi for node i. The virtual node is connected

276

to all nodes in Ni , and aggregates the node features with a
weighted sum:

3.6. Implementation details

zi = X

j

αij hj

(7)

The distributions of soft weights for neighboring nodes
are given by

αij = softmax(fattn (eij ))

(8)

where fattn (·) is a attention function that measures the im-
portance of node j to node i. fattn (·) can be efﬁciently
implemented as the self-attention mechanism [60, 61] with
neural networks. Its parameters can be jointly learned with
the target task using back propagation, and thus requires no
additional supervision.
Once zi is computed, we assume node i is connected
only to this virtual node, and updates the output feature by

˜hi = fnode ([hi ; zi ])

(9)

The difference from graph attention networks [61] is that
they use ˜hi = fnode (zi ). We have found this gives worse
results (see Section 4), perhaps because the model is not
sure if it should focus on features from itself or features
from neighbors.

3.4. Summary of model

We call our overall model Discriminative Relational Re-
current Network or DR2N for short. See Figure 2 for a
sketch, and Tables 1 and 2 for a precise speciﬁcation of the
model.

3.5. Training

The overall framework is jointly optimized end-to-end,
where the loss function is

Ltotal = αLloc +

T

X

t=0

βtLcls
t

(10)

t

Here Lloc is the box localization loss given by the region
proposal network and the bounding box reﬁnement network
computed for the last observed frame. Lcls
is the action clas-
siﬁcation loss at time t. α and βt are scalars which balances
the two sets of losses. In practice, one may want to down-
weight βt for larger t as the prediction task becomes more
challenging.
Note that we do not use teacher forcing during training,
to encourage the model to predict multiple steps into the
future. That is, when computing the predicted labels at
i , we
condition on previous predicted labels a0:t−1
rather than the
ground truth predicted labels. (We use the soft logit scores
for the predictions, to avoid the need to sample from the
model during training.)

1:t

Our implementation of the Faster-RCNN [46, 22] pro-
posal extractor largely follows the design choices of [59].
The region proposal network (RPN) uses a 2D ResNet-50
network and takes a single image as input.
It is jointly
trained with the whole framework, using the human an-
notations from target dataset. We apply RPN on the ﬁnal
observed frame of each example to generate actor propos-
als. To handle temporal context, the feature network uses
an S3D-G [72] backbone, which is a type of “inﬂated” 3D
convolutional neural network [8], and takes sequences of
frames as inputs. We apply the feature network to frames at
−H : 0, where 0 is the frame number of the last observed
frame, and H is the duration of temporal context. Once the
features are extracted, we apply a temporal convolutional
layer after the Mixed 4f layer of S3D-G to aggregate the
3D feature maps into 2D and then apply the standard 2D
ROIPooling to crop out the features inside actor proposals.
Each cropped feature map is passed to the remaining layers
of S3D-G. The ﬁnal outputs are average-pooled into 1024-
dim feature vectors.

The weights of ResNet-50 used by proposal network
are initialized with an ImageNet pre-trained model. We
keep top 300 proposals per image. The weights of S3D-
G used by feature network are pre-trained from Kinetics-
400 [29]. The weights of newly added layers are randomly
initialized from truncated normal distributions. Unlike oth-
erwise mentioned, the inputs to the feature network at 10
RGB frames resized to 400 by 400. We use gated recur-
rent units (GRU) [11] as the particular RNN architecture to
model action dynamics. We set the number of hidden units
to 1024, which is the same dimension as the visual input
features. For the discriminative relation network, we imple-
ment fedge (·) and fattn (·) as single fully-connected layers.

i

During both training and inference, the input actions at
to the GRU are generated by the model rather than provided
by the ground truth. We set the localization weight α = 1.
The classiﬁcation weight β0 is set to 1, we linearly anneal βt
such that βt = 0.5. To compute classiﬁcation loss, we use
softmax cross entropy for J-HMDB and the sum of sigmoid
cross entropies for AVA (since the action labels of AVA are
not mutually exclusive). We optimize the model with syn-
chronous SGD and batch size of 4 per GPU, and disable
batch norm updates during training. We use 10 GPUs in to-
tal. Two techniques are used to stabilize and speed up train-
ing: ﬁrst, we warm-start the learning rate from 0.008 to 0.08
for Tw steps and use cosine learning rate decay [34] starting
from 0.08 for another Tc steps; second, we apply a gradient
multiplier of 0.01 to gradients computed from DR2N to the
feature map. For AVA, we set Tw to 5K and Tc to 300K.
For J-HMDB, we set Tw to 1K and Tc to 40K.

277

Method

Dynamics Model Relation Model

t = 0

t = 1

t = 2

t = 3

t = 4

t = 5

Single-head
Multi-head
GRU
Graph-GRU
Graph-GRU
Graph-GRU

-
-
GRU
GRU
GRU
GRU

-
-
-
RN [51]
GAT [61]
DR2N (Us)

19.1
16.0
18.7
17.3
16.4
20.4

7.8
9.4
13.1
12.3
12.3
14.4

5.3
6.8
10.3
9.9
9.3
11.2

4.2
5.4
8.0
7.7
7.3
9.3

2.6
4.3
6.7
6.5
6.2
7.5

1.8
3.6
5.7
5.3
5.2
6.8

Table 3: Ablation study on the AVA dataset. We report mean AP@.5 for six different time steps. t = 0 corresponds to the
last observed frame, i.e., standard action detection. Each step is one second long.

Figure 3: Change in AP performance from T = 0 to t = 1. The actions with the biggest change are the hardest to predict in
the future.

4. Experiments

In this section, we conduct experiments to study the im-
pact of different design choices for relational reasoning and
temporal dynamics modeling for the action prediction task.

4.1. Experimental setup

In the following we present the two datasets used in
our experiments, Atomic Visual Actions (AVA) [18] and J-
HMDB [26] as well as the metrics used for evaluating action
prediction.
AVA [18] is a recently released large-scale action detec-
tion dataset with 60 action classes. AVA is sparsely labeled
at 1 FPS and each frame may contain multiple actors with
multiple action labels. We use the most recent AVA ver-
sion 2.1, which contains 210K training examples and 57K
validation examples. To get the ground-truth for future
action labels, we use the bounding box identities (tracks)
semi-automatic annotated for this dataset [18]. For actor-

level action prediction on AVA, we measure the IoU of all
the actor detections with the ground-truth boxes. If the IoU
is above 0.5, and the action label is correct, we consider it a
true positive. For prediction, the ground-truth labels come
from future boxes that are linked to the last observed ground
truth boxes. We compute the average precision, and report
per frame-level mean AP for different time steps.

J-HMDB is an action detection dataset with 928 clips
and 21 categories. Clips have an average length of 1.4 sec-
onds. Every frame in J-HMDB contains one annotated ac-
tor with a single action label. There are three train/val splits
for J-HMDB. We report results by averaging over the three
splits, which is the standard practice on this dataset. Since
there is only one action performed in each example, we fol-
low the setup of Soomro et al. [54, 55] and treat it as an
early action prediction problem. We report accuracy@K,
which is the action classiﬁcation accuracy by watching the
ﬁrst K% of the videos.

278

Figure 4: Change in AP performance from adding graph connections at t = 0. The actions with the biggest change beneﬁt
the most from contextual modeling.

Figure 5: Visualizations of top 3 relations (blue boxes) selected for an actor proposal (orange box) by DR2N on AVA. We see
that the attended regions provide useful contextual information.

4.2. Action prediction on AVA

This section presents and discusses quantitative and
qualitative results for action prediction on the AVA dataset.
We consider the following approaches:

• Single-head: for each future step t, train a separate
model that directly classiﬁes the future actions from
visual features vi derived from V 0 .

• Multi-head:
similar to the single-head model, but
jointly trains the classiﬁers for all t in the same model.
The visual features vi are shared with all classiﬁcation
heads.

• GRU: future actions are predicted from hidden states
of GRU, where the states are initialized from visual
features of the actor proposals. Model is trained jointly
for all t. However, there are no edges in the graph, so
all nodes evolve independenty.

• Graph-GRU: Same as GRU, but with a fully con-
nected graph. We consider 3 versions:
the Relation
Network (RN) [51], which assigns equal weights to
all pairwise relations; Graph Attention Network [61],

which uses a weighted sum of features from itself and
all its neighbors; and our proposed method, which uses
Equation 5.

The results are shown in Table 3. The ﬁrst three rows
compare the impact of different approaches for dynamics
modeling. Our ﬁrst observation is that, as t grows larger, the
mean AP declines accordingly. This is expected since the
further away in time the prediction is, the harder is the task.
Our second observation is that the single-head baseline per-
forms worse on all t except for t = 0, where the frames are
observed. The lower performance of t = 0 for multi-head
can be explained by the fact that the joint model has less
model capacity compared with 6 independent single-head
models. However, we can see that by sharing the visual
features in a multi-task setting, the multi-head baseline out-
performs its single-head counterpart for future prediction at
t > 0. Our third observation is that using GRU to model
action dynamics offers better future prediction performance
without sacriﬁcing detection performance at t = 0, since it
can capture patterns in the sequence of action labels.
The last three rows of Table 3 compares the impact of
different relational models. We can see DRN outperforms

279

Figure 6: Example predictions on the AVA validation set at t = 1. We show last observed frames at t = 0 and render the
detected actor boxes on the frames. To the right of each example, we also show the unobserved future frames half and one
second ahead. We show top one detections if above threshold of 0.1, and remove the most frequent categories, such as sitting
and standing. The top row shows examples where the model can predict what will happen in the future based on the current
scene context. The second row shows examples where the model can predict what will happen in the future based on the
other actors in the scene. The third row highlights the challenges in action forecasting (e.g. multiple possible futures).

the other two consistently. For RN, one possible explana-
tion for the performance gap is that it assigns equal weights
to all edges, which is prone to noise in our case, since many
nodes correspond to background detections. For GAT, we
notice that the performance at t = 0 is much lower, indicat-
ing that it has difﬁculty distinguishing node features from
neigbhor features.
Figure 3 compares the classes with biggest performance
drops from detection (t = 0) to prediction (t = 1). We
see that it is challenging for the model to capture actions
with short durations, such as kissing or ﬁghting. Actions
with longer durations, sich as talking, are typically easier to
predict. Figure 4 compares the effectiveness of DR2N over
the GRU baseline, without any edges in the graph. We can
see that the categories with the most gains are those with
explicit interactions (e.g. hand clap, dance, martial art), or
where other actors provide useful context (e.g. eat and ride).
In Figure 5, we show the top 3 boxes ( blue) with the high-
est attention weights to the actor being classiﬁed (orange).
We can see that they typically correspond to other actors.
Finally, we visualize example predictions in Figure 6.

4.3. Early action prediction on J(cid:173)HMDB

Finally, we demonstrate the effectiveness of DR2N on
the early clip classiﬁcation. During training, we feed 10
RGB frames to the feature network, and predict one step
into the future. During inference, we feed the ﬁrst K%

Model

10% 20% 30% 40% 50%

Soomro et al. [55] ≈ 5 ≈ 12 ≈ 21 ≈ 25 ≈ 30
Singh et al. [54]
GRU
GAT [61]
DR2N

≈ 48 ≈ 59 ≈ 62 ≈ 66 ≈ 66

61.1
64.4
68.1

52.5
58.1
60.6

56.2
61.8
65.8

65.2
68.7
71.4

65.9
68.8
71.8

Table 4: Early action prediction performance on J-HMDB.

frames to the feature network, and take the most conﬁdent
prediction as the label of the clip. Table 4 shows the results,
we can see that our approach signiﬁcantly outperforms pre-
vious state-of-the-art methods. To study the impact of rela-
tion models, we also compare with the GRU only and the
GAT baselines, and ﬁnd DR2N outperforms both. By in-
specting the edge attentions, we observe that some of the
RPN proposals cover objects in the scene, which are uti-
lized by DR2N to model human-object relations.

5. Conclusion

We address the multi-person action forecasting task in
videos. We propose a model that jointly models temporal
and spatial interactions among different actors with Dis-
criminative Relational Recurrent Network. Quantitative
and qualitative evaluations on AVA and J-HMDB datasets
demonstrate the effectiveness of our proposed method.

280

References

[1] S. Abu-El-Haija, N. Kothari, J. Lee, P. Natsev, G. Toderici,
B. Varadarajan, and S. Vijayanarasimhan. YouTube-8M: A
large-scale video classiﬁcation benchmark. arXiv preprint
arXiv:1609.08675, 2016. 2
[2] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei,
and S. Savarese. Social lstm: Human trajectory prediction in
crowded spaces. In CVPR, 2016. 2, 3
[3] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and
S. Levine. Stochastic variational video prediction. arXiv
preprint arXiv:1710.11252, 2017. 2, 3
[4] F. Baradel, N. Neverova, C. Wolf, J. Mille, and G. Mori.
Object level visual reasoning in videos. In ECCV, 2018. 3
[5] P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al.
In-
teraction networks for learning about objects, relations and
physics. In NIPS, 2016. 3
[6] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-
Gonzalez, V. F. Zambaldi, M. Malinowski, A. Tacchetti,
D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song,
A. J. Ballard, J. Gilmer, G. E. Dahl, A. Vaswani, K. R.
Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wier-
stra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and R. Pas-
canu. Relational inductive biases, deep learning, and graph
networks. arXiv preprint arXiv:1806.01261, 2018. 3
[7] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. C. Niebles.
ActivityNet: A large-scale video benchmark for human ac-
tivity understanding. In CVPR, 2015. 2
[8] J. Carreira and A. Zisserman. Quo vadis, action recognition?
A new model and the Kinetics dataset. In CVPR, 2017. 5
[9] Y.-W. Chao, Y. Liu, X. Liu, H. Zeng, and J. Deng. Learning
to detect human-object interactions. In WACV, 2018. 3
[10] Y.-W. Chao, Z. Wang, Y. He, J. Wang, and J. Deng. HICO:
A benchmark for recognizing human-object interactions in
images. In ICCV, 2015. 3
[11] K. Cho, B. van Merrienboer, C¸ . G ¨ulc¸ ehre, F. Bougares,
H. Schwenk, and Y. Bengio. Learning phrase representations
using RNN encoder-decoder for statistical machine transla-
tion. In EMNLP, 2014. 5
[12] B. Dai, Y. Zhang, and D. Lin. Detecting visual relationships
with deep relational networks. In CVPR, 2017. 3
[13] D. Damen, H. Doughty, G. M. Farinella, S. Fidler,
A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Per-
rett, W. Price, and M. Wray. Scaling egocentric vision: The
EPIC-KITCHENS dataset. In ECCV, 2018. 2, 3
[14] C. Finn, I. Goodfellow, and S. Levine. Unsupervised learn-
ing for physical interaction through video prediction.
In
NIPS, 2016. 2, 3
[15] K. Fragkiadaki, J. Huang, A. Alemi, S. Vijayanarasimhan,
S. Ricco, and R. Sukthankar. Motion prediction under
multimodality with conditional stochastic networks. arXiv
preprint arXiv:1705.02082, 2017. 2, 3
[16] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik. Recurrent
network models for human dynamics. In ICCV, 2015. 2, 3
[17] G. Gkioxari, R. Girshick, P. Doll ´ar, and K. He. Detecting
and recognizing human-object intaractions. In CVPR, 2018.
3

[18] C. Gu, C. Sun, S. Vijayanarasimhan, C. Pantofaru, D. A.
Ross, G. Toderici, Y. Li, S. Ricco, R. Sukthankar, C. Schmid,
and J. Malik. AVA: A video dataset of spatio-temporally lo-
calized atomic visual actions. In CVPR, 2018. 2, 6
[19] W. L. Hamilton, R. Ying, and J. Leskovec. Representation
learning on graphs: Methods and applications. IEEE Data
Engineering Bulletin, 2017. 3
[20] M. Hoai and F. De la Torre. Max-margin early event detec-
tors. IJCV, 2014. 3
[21] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei. Relation networks
for object detection. arXiv preprint arXiv:1711.11575, 2017.
3
[22] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, and
K. Murphy. Speed/accuracy trade-offs for modern convolu-
tional object detectors. In CVPR, 2017. 5
[23] M. S. Ibrahim and G. Mori. Hierarchical relational networks
for group activity recognition and retrieval. In ECCV, 2018.
3
[24] H. Idrees, A. R. Zamir, Y. Jiang, A. Gorban, I. Laptev,
R. Sukthankar, and M. Shah. The THUMOS challenge on
action recognition for videos “in the wild”. CVIU, 2017. 2
[25] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena. Structural-
RNN: Deep learning on spatio-temporal graphs. In CVPR,
2016. 3
[26] H. Jhuang, J. Gall, S. Zufﬁ, C. Schmid, and M. Black. To-
wards understanding action recognition. In ICCV, 2013. 2,
6
[27] N. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka,
O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel
networks. arXiv preprint arXiv:1610.00527, 2016. 2
[28] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar,
and L. Fei-Fei. Large-scale video classiﬁcation with convo-
lutional neural networks. In CVPR, 2014. 2
[29] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier,
S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev,
M. Suleyman, and A. Zisserman. The Kinetics human action
video dataset. arXiv preprint arXiv:1705.06950, 2017. 2, 5
[30] Y. Ke, R. Sukthankar, and M. Hebert. Efﬁcient visual event
detection using volumetric features. In ICCV, 2005. 2
[31] T. Kipf, E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel.
Neural relational inference for interacting systems. In ICML,
2018. 3
[32] K. M. Kitani, B. D. Ziebart, J. A. Bagnell, and M. Hebert.
Activity forecasting. In ECCV, 2012. 2, 3
[33] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. S. Torr, and
M. K. Chandraker. DESIRE: distant future prediction in dy-
namic scenes with interacting agents.
In CVPR, 2017. 2,
3
[34] I. Loshchilov and F. Hutter. SGDR: stochastic gradient de-
scent with restarts. arXiv preprint arXiv:1608.03983, 2016.
5
[35] W. Lotter, G. Kreiman, and D. Cox. Deep predictive cod-
ing networks for video prediction and unsupervised learning.
arXiv preprint arXiv:1605.08104, 2016. 2
[36] P. Luc, N. Neverova, C. Couprie, J. Verbeek, and Y. LeCun.
Predicting deeper into the future of semantic segmentation.
In ICCV, 2017. 2, 3

281

[37] S. Ma, L. Sigal, and S. Sclaroff. Learning activity progres-
sion in lstms for activity detection and early detection.
In
CVPR, 2016. 3
[38] M. Marszalek, I. Laptev, and C. Schmid. Actions in context.
In CVPR, 2009. 2
[39] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale
video prediction beyond mean square error. arXiv preprint
arXiv:1511.05440, 2015. 2
[40] P. Mettes, J. van Gemert, and C. Snoek. Spot On: Action
localization from pointly-supervised proposals.
In ECCV,
2016. 2
[41] M. Monfort, B. Zhou, S. A. Bargal, A. Andonian, T. Yan,
K. Ramakrishnan, L. Brown, Q. Fan, D. Gutfruend, C. Von-
drick, et al. Moments in time dataset: one million videos
for event understanding. arXiv preprint arXiv:1801.03150,
2018. 2
[42] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T.
Lee, S. Mukherjee, J. Aggarwal, H. Lee, L. Davis, et al.
A large-scale benchmark dataset for event recognition in
surveillance video. In CVPR, 2011. 3
[43] R. B. Palm, U. Paquet, and O. Winther. Recurrent relational
networks for complex relational reasoning. arXiv preprint
arXiv:1711.08028, 2017. 3
[44] N. Petrovic, A. Ivanovic, and N. Jojic. Recursive estimation
of generative models of video. In CVPR, 2006. 2, 3
[45] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert,
and S. Chopra. Video (language) modeling: a baseline
for generative models of natural videos.
arXiv preprint
arXiv:1412.6604, 2014. 2
[46] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-
wards real-time object detection with region proposal net-
works. In NIPS, 2015. 3, 5
[47] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese.
Learning social etiquette: Human trajectory understanding
in crowded scenes. In ECCV, 2016. 2, 3
[48] M. Rodriguez, J. Ahmed, and M. Shah. Action MACH: a
spatio-temporal maximum average correlation height ﬁlter
for action recognition. In CVPR, 2008. 2
[49] A. Sadeghian, F. Legros, M. Voisin, R. Vesel, A. Alahi, and
S. Savarese. Car-net: Clairvoyant attentive recurrent net-
work. In ECCV, 2018. 2, 3
[50] A. Santoro, R. Faulkner, D. Raposo,
J. W. Rae,
M. Chrzanowski, T. Weber, D. Wierstra, O. Vinyals, R. Pas-
canu, and T. P. Lillicrap. Relational recurrent neural net-
works. arXiv preprint arXiv:1806.01822, 2018. 3
[51] A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. P. Lillicrap. A simple neural
network module for relational reasoning. In NIPS, 2017. 3,
4, 6, 7
[52] C. Schuldt, I. Laptev, and B. Caputo. Recognizing human
actions: a local SVM approach. In ICPR, 2004. 2
[53] G. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and
A. Gupta. Hollywood in homes: Crowdsourcing data collec-
tion for activity understanding. In ECCV, 2016. 2
[54] G. Singh, S. Saha, M. Sapienza, P. Torr, and F. Cuzzolin.
Online real-time multiple spatiotemporal action localisation
and prediction. In ICCV, 2017. 2, 3, 6, 8

[55] K. Soomro, H. Idrees, and M. Shah. Online localization and
prediction of actions and interactions. IEEE PAMI, 2018. 2,
3, 6, 8
[56] K. Soomro, A. Zamir, and M. Shah. UCF101: A dataset of
101 human actions classes from videos in the wild. Technical
Report CRCV-TR-12-01, 2012. 2
[57] N. Srivastava, E. Mansimov, and R. Salakhudinov. Unsuper-
vised learning of video representations using lstms. In ICML,
2015. 2
[58] S. Su, J. Pyo Hong, J. Shi, and H. Soo Park. Predicting be-
haviors of basketball players from ﬁrst person videos.
In
CVPR, 2017. 3
[59] C. Sun, A. Shrivastava, C. Vondrick, K. Murphy, R. Suk-
thankar, and C. Schmid. Actor-centric relation network. In
ECCV, 2018. 3, 4, 5
[60] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all
you need. In NIPS, 2017. 5
[61] P. Veli ˇckovi ´c, G. Cucurull, A. Casanova, A. Romero, P. Li `o,
and Y. Bengio. Graph attention networks. In ICLR, 2018. 2,
3, 5, 6, 7, 8
[62] R. Villegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee.
Learning to generate long-term future via hierarchical pre-
diction. arXiv preprint arXiv:1704.05831, 2017. 2, 3
[63] C. Vondrick, H. Pirsiavash, and A. Torralba. Anticipating
visual representations from unlabeled video. In CVPR, 2016.
2, 3
[64] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating
videos with scene dynamics. In NIPS, 2016. 2, 3
[65] C. Vondrick and A. Torralba. Generating the future with ad-
versarial transformers. In CVPR, 2017. 3
[66] J. Walker, C. Doersch, A. Gupta, and M. Hebert. An uncer-
tain future: Forecasting from static images using variational
autoencoders. In ECCV, 2016. 2, 3
[67] J. Walker, A. Gupta, and M. Hebert. Patch to the future:
Unsupervised visual prediction. In CVPR, 2014. 2, 3
[68] J. Walker, A. Gupta, and M. Hebert. Dense optical ﬂow pre-
diction from a static image. In ICCV, 2015. 2
[69] J. Walker, K. Marino, A. Gupta, and M. Hebert. The pose
knows: Video forecasting by generating pose futures.
In
ICCV, 2017. 3
[70] X. Wang, R. B. Girshick, A. Gupta, and K. He. Non-local
neural networks. In CVPR, 2018. 3
[71] P. Weinzaepfel, Z. Harchaoui, and C. Schmid. Learning to
track for spatio-temporal action localization. In ICCV, 2015.
2
[72] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy. Rethinking
spatiotemporal feature learning for video understanding. In
ECCV, 2018. 5
[73] T. Xue, J. Wu, K. Bouman, and B. Freeman. Visual dynam-
ics: Probabilistic future frame synthesis via cross convolu-
tional networks. In NIPS, 2016. 2
[74] B. Yao and L. Fei-Fei. Modeling mutual context of object
and human pose in human-object interaction activities.
In
CVPR, 2010. 3
[75] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori,
and L. Fei-Fei. Every moment counts: Dense detailed label-
ing of actions in complex videos. IJCV, 2017. 2, 3

282

[76] J. Yuan, Z. Liu, and Y. Wu. Discriminative subvolume search
for efﬁcient action detection. In CVPR, 2009. 2
[77] J. Yuen and A. Torralba. A data-driven approach for event
prediction. In ECCV, 2010. 2, 3
[78] Y. Zhou and T. L. Berg. Temporal perception and prediction
in ego-centric video. In ICCV, 2015. 2, 3

283

Revealing Scenes by Inverting Structure from Motion Reconstructions

Francesco Pittaluga1

Sanjeev J. Koppal1

Sing Bing Kang2

Sudipta N. Sinha2

1 University of Florida

2 Microsoft Research

(a) SfM point cloud (top view)

(b) Projected 3D points

(c) Synthesized Image

(d) Original Image

Figure 1: SYNTH E S I Z ING IMAGERY FROM A S FM PO IN T C LOUD : From left to right: (a) Top view of a SfM reconstruction
of an indoor scene, (b) 3D points projected into a viewpoint associated with a source image, (c) the image reconstructed using
our technique, and (d) the source image. The reconstructed image is very detailed and closely resembles the source image.

Abstract

Many 3D vision systems localize cameras within a scene
using 3D point clouds. Such point clouds are often obtained
using structure from motion (SfM), after which the images
are discarded to preserve privacy. In this paper, we show,
for the ﬁrst time, that such point clouds retain enough in-
formation to reveal scene appearance and compromise pri-
vacy. We present a privacy attack that reconstructs color
images of the scene from the point cloud. Our method is
based on a cascaded U-Net that takes as input, a 2D multi-
channel image of the points rendered from a speciﬁc view-
point containing point depth and optionally color and SIFT
descriptors and outputs a color image of the scene from
that viewpoint. Unlike previous feature inversion meth-
ods [46, 9], we deal with highly sparse and irregular 2D
point distributions and inputs where many point attributes
are missing, namely keypoint orientation and scale, the de-
scriptor image source and the 3D point visibility. We evalu-
ate our attack algorithm on public datasets [24, 39] and an-
alyze the signiﬁcance of the point cloud attributes. Finally,
we show that novel views can also be generated thereby en-
abling compelling virtual tours of the underlying scene.

1. Introduction

Emerging AR technologies on mobile devices based on
ARCore [2], ARKit [3], 3D mapping APIs [1], and new
devices such as HoloLens [15] have set the stage for de-
ployment of devices with always-on cameras in our homes,

workplaces, and other sensitive environments. Image-based
localization techniques allow such devices to estimate their
precise pose within the scene [18, 37, 23, 25]. However,
these localization methods requires persistent storage of 3D
models of the scene which contains sparse 3D point clouds
reconstructed using images and SfM algorithms [38].
SfM source images are usually discarded to safeguard
privacy. Surprisingly, however, we show that the SfM point
cloud and the associated attributes such as color and SIFT
descriptors contain enough information to reconstruct de-
tailed comprehensible images of the scene (see Fig. 1 and
Fig. 3). This suggests that the persistent point cloud storage
poses serious privacy risks that have been widely ignored
so far but will become increasingly relevant as localization
services are adopted by a larger user community.
While privacy issues for wearable devices have been
studied [16], to the best of our knowledge, a systematic
analysis of privacy risk of storing 3D point cloud maps has
never been reported. We illustrate the privacy concerns by
proposing the problem of synthesizing color images from
an SfM model of a scene. We assume that the reconstructed
model contains a sparse 3D point cloud with optional at-
tributes such as descriptors, color, point visibility and asso-
ciated camera poses but not the source images.
We make the following contributions:
(1) We intro-
duce the problem of inverting a sparse SfM point cloud
and reconstructing detailed views of the scene from arbi-
trary viewpoints. This problem differs from the previously
studied single-image feature inversion problem due to the
need to deal with highly sparse point distributions and a

1145

higher degree of missing information in the input, namely
unknown keypoint orientation and scale, unknown image
source of descriptors, and unknown 3D point visibilities.
(2) We present a new approach based on three neural net-
works where the ﬁrst network performs visibility estima-
tion, the second network reconstructs the image and the
third network uses an adversarial framework to further re-
ﬁne the image quality. (3) We systematically analyze vari-
ants of the inversion attack that exploits additional attributes
that may be available, namely per-point descriptors, color
and information about the source camera poses and point
visibility and show that even the minimalist representation
(descriptors only) are prone to the attack. (4) We demon-
strate the need for developing privacy preserving 3D repre-
sentations, since the reconstructed images reveal the scene
in great details and conﬁrm the feasibility of the attack in a
wide range of scenes. We also show that novel views of the
scene can be synthesized without any additional effort and
a compelling virtual tour of a scene can be easily generated.
The three networks in our cascade are trained on 700+
indoor and outdoor SfM reconstructions generated from
500k+ multi-view images taken from the NYU2 [39] and
MegaDepth [24] datasets. The training data for all three
networks including the visibility labels were generated au-
tomatically using COLMAP [38]. Next we compare our
approach to previous work on inverting image features
[46, 9, 8] and discuss how the problem of inverting SfM
models poses a unique set of challenges.

2. Related Work

In this section, we review existing work on inverting im-
age features and contrast them to inverting SfM point cloud
models. We then broadly discuss image-to-image transla-
tion, upsampling and interpolation, and privacy attacks.

Inverting features. The task of reconstructing images from
features has been explored to understand what is encoded
by the features, as was done for SIFT features by Weinza-
epfel et al. [46], HOG features by Vondrick et al. [45] and
bag-of-words by Kato and Harada [20]. Recent work on
the topic has been primarily focused on inverting and inter-
preting CNN features [49, 48, 29]. Dosovitskiy and Brox
proposed encoder-decoder CNN architectures for inverting
many different features (DB1) [9] and later incorporated ad-
versarial training with perceptual loss functions (DB2) [8].
While DB1 [9] showed some qualitative results on inverting
sparse SIFT, both papers focused primarily on dense fea-
tures. In contrast to these feature inversion approaches, we
focus solely on inverting SIFT descriptors stored along with
SfM point clouds. While the projected 3D points on a cho-
sen viewpoint may resemble single image SIFT features,
there are some key differences. First, our input 2D point
distributions can be highly sparse and irregular, due to the

typical inherent sparsity of SfM point clouds. Second, the
SIFT keypoint scale and orientation are unknown since SfM
methods retain only the descriptors for the 3D points. Third,
each 3D point typically has only one descriptor sampled
from an arbitrary source image whose identity is not stored
either, entailing descriptors with unknown perspective dis-
tortions and photometric inconsistencies. Finally, the 3D
point visibilities are also unknown and we will demonstrate
the importance of visibility reasoning in the paper.

Image-to-Image Translation. Various methods such as
Pix2Pix [19], CycleGan [50], CoGAN [27] and related un-
supervised approaches [7, 26, 34] use conditional adversar-
ial networks to transform between 2D representations, such
as edge to color, label to color, and day to night images.
While such networks are typically dense (without holes)
and usually low-dimensional (single channel or RGB), Con-
tour2Im [5] takes sparse 2D points sampled along gradients
along with low-dimensional input features. In contrast to
our work, these approaches are trained on speciﬁc object
categories and semantically similar images. While we use
similar building blocks to these methods (encoder-decoder
networks, U-nets, adversarial loss, and perceptual loss), our
networks can generalize to arbitrary images, and are trained
on large scale indoor and outdoor SfM datasets.

Upsampling. When the input and output domains are iden-
tical, deep networks have shown excellent results on up-
sampling and superresolution tasks for images, disparity,
depth maps and active range maps [4, 28, 43, 36, 17]. How-
ever, prior upsampling methods typically focus on inputs
with uniform sparsity. Our approach differs due to the non-
uniform spatial sampling in the input data which also hap-
pens to be high dimensional and noisy since the input de-
scriptors are from different source images and viewpoints.

Novel view synthesis and image-based rendering. Deep
networks can signiﬁcantly improve photorealism in free
viewpoint image-based rendering [12, 14]. Additionally,
several works have also explored monocular depth estima-
tion and novel view synthesis using U-Nets [11, 24, 31].
Our approach arguably provides similar photorealistic visu-
ally quality – remarkably, from sparse SfM reconstructions
instead of images. This is disappointing news from a pri-
vacy perspective but could be useful in other settings for
generating photorealistic images from 3D reconstructions.

CNN-based privacy attacks and defense techniques. Re-
cently, McPherson et al. [30] and Vasiljevic et al. [44]
showed that deep models could defeat existing image obfus-
cation methods. Further more, many image transformations
can be considered as adding noise and undoing them as de-
noising, and here deep networks have been quite success-
ful [47]. To defend against CNN-based attacks, attempts at
learning CNN-resistant transformations have shown some

146

promise [33, 10, 35, 13]. Concurrent to our work, Speciale
et al. [41] introduced the privacy preserving image-based
localization problem to address the privacy issues we have
brought up. They proposed a new camera pose estimation
technique using an obfuscated representation of the map ge-
ometry which can defend against our inversion attack.

3. Method

The input to our pipeline is a feature map generated from
a SfM 3D point cloud model given a speciﬁc viewpoint i.e.
a set of camera extrinsic parameters. We obtain this fea-
ture map by projecting the 3D points on the image plane
and associating the 3D point attributes (SIFT descriptor,
color, etc.) with the discrete 2D pixel where the 3D point
projects in the image. When multiple points project to the
same pixel, we retain the attributes for the point closest to
the camera and store its depth. We train a cascade of three
encoder-decoder neural networks for visibility estimation,
coarse image reconstruction and the ﬁnal reﬁnement step
which recovers ﬁne details in the reconstructed image.

Visibility Estimation. Since SfM 3D point clouds are of-
ten quite sparse and the underlying geometry and topology
of the surfaces in the scene are unknown, it is not possi-
ble to easily determine which 3D points should be consid-
ered as visible from a speciﬁc camera viewpoint just us-
ing z-buffering. This is because a sufﬁcient number of 3D
points may not have been reconstructed on the foreground
occluding surfaces. This produces 2D pixels in the input
feature maps which are associated with 3D points in the
background i.e. lie on surfaces which are occluded from
that viewpoint. Identifying and removing such points from
the feature maps is critical to generating high-quality im-
ages and avoiding visual artifacts. We propose to recover
point visibility using a data-driven neural network-based
approach, which we refer to as V I S IBN E T. We also evaluate
two geometric methods which we refer to as V I S IBS PAR SE
and V I S IBD EN S E. Both geometric methods however re-
quire additional information which might be unavailable.

Coarse Image Reconstructon and Reﬁnement. Our tech-
nique for image synthesis from feature maps consists of a
coarse image reconstruction step followed by a reﬁnement
step. COAR S EN E T is conditioned on the input feature map
and produces an RGB image of the same width and height
as the feature map. R E FIN EN E T outputs the ﬁnal color im-
age which has the same size, given the input feature map
along with the image output of COAR S EN ET as its input.

3.1. Visibility Estimation

If we did not perform explicit visibility prediction in
our pipeline, some degree of implicit visibility reasoning
would still be carried out by the image synthesis network
COAR SEN ET. In theory, this network has access to the input

depths and could learn to reason about visibility. However,
in practice, we found that this approach to be inaccurate,
especially in regions where the input feature maps contain
a low ratio of visible to occluded points. Qualitative exam-
ples of these failure cases are shown in Figure 5. Therefore
we explored explicit visibility estimation approaches based
on geometric reasoning as well as learning.

VisibSparse. We explored a simple geometric method that
we refer to as V I S IBS PAR SE. It is based on the “point splat-
ting” paradigm used in computer graphics. By considering
only the depth channel in the input, we apply a min ﬁlter
with a k × k kernel on the feature map to obtain a ﬁltered
depth map. Here, we used k = 3 based on empirical test-
ing. Each entry in the feature map whose depth value is no
greater than 5% of the depth value in the ﬁltered depth map
is retained as visible. Otherwise, the point is considered
occluded and the associated entry in the input is removed.

VisibDense. When the camera poses for the source im-
ages computed during SfM and the image measurements
are stored along with the 3D point cloud, it is often possible
to exploit that data to compute a dense scene reconstruc-
tion. Labatut et al. [21] proposed such a method to com-
pute a dense triangulated mesh by running space carving on
the tetrahedral cells of the 3D Delaunay triangulation of the
sparse SfM points. We used this method, implemented in
COLMAP [38] and computed 3D point visibility based on
the reconstructed mesh model using traditional z-buffering.

VisibNet. A geometric method such as V I S IBD EN S E can-
not be used when the SfM cameras poses and image mea-
surements are unavailable. We therefore propose a general
regression-based approach that directly predicts the visibil-
ity from the input feature maps, where the predictive model
is trained using supervised learning. Speciﬁcally, we train
an encoder-decoder neural network which we refer to as
V I S IBN ET to classify each input point as either “visible”
or “occluded”. Ground truth visibility labels were gener-
ated automatically by leveraging V I S IBD EN S E on all train,
test, and validation scenes. Using V I S IBN ET’s predictions
to “cull” occluded points from the input feature maps prior
to running COAR S EN E T signiﬁcantly improves the quality
of the reconstructed images, especially in regions where the
input feature map contains fewer visible points compared to
the number of points that are actually occluded.

3.2. Architecture

A sample input feature map as well as our complete net-
work architecture consisting of V I S IBN ET, COAR SEN ET,
and R E FINEN E T is shown in Figure 2. The input to our
network is an H × W × n dimensional feature map consist-
ing of n-dimensional feature vectors with different combi-
nations of depth, color, and SIFT features at each 2D loca-
tion. Except for the number of input/output channels in the

147

nD 
Input Tensor

=

z  RGB     SIFT descriptor

encoder

decoder

conv. layers

nD Input

VisibNet

Visibility
Map

CoarseNet

RGB image

RefineNet

RGB image
(output)

Figure 2: N E TWORK ARCH I T EC TUR E : Our network has three sub-networks – V I S IBN E T, COAR SEN ET and R E FIN EN ET.
The upper left shows that the input to our network is a multi-dimensional nD array. The paper explores network variants where
the inputs are different subsets of depth, color and SIFT descriptors. The three sub-networks have similar architectures. They
are U-Nets with encoder and decoder layers with symmetric skip connections. The extra layers at the end of the decoder
layers (marked in orange) are there to help with high-dimensional inputs. See the text and supplementary material for details.

ﬁrst/ﬁnal layers, each sub-network has the same architec-
ture consisting of U-Nets with a series of encoder-decoder
layers with skip connections. Compared to conventional U-
Nets, our network has a few extra convolutional layers at
the end of the decoder layers. These extra layers facilitate
propagation of information from the low-level features, par-
ticularly the information extracted from SIFT descriptors,
via the skip connections to a larger pixel area in the out-
put, while also helping to attenuate visual artifacts resulting
from the highly sparse and irregular distribution of these
features. We use nearest neighbor upsampling followed by
standard convolutions instead of transposed convolutions as
the latter are known to produce artifacts [32].

3.3. Optimization

We separately train the sub-networks in our architecture,
V I S IBN ET, COAR SEN ET, and R E FIN EN E T. Batch normal-
ization was used in every layer, except the ﬁnal one in each
network. We applied Xavier initialization and projections
were generated on-the-ﬂy to facilitate data augmentation
during training and novel view generation after training.
V I S IBN ET was trained ﬁrst to classify feature map points
as either visible or occluded, using ground-truth visibility
masks generated automatically by running V I S IBD EN S E for
all train, test, and validation samples. Given training pairs
of input feature maps Fx ∈ RH×W ×N and target source
images x ∈ RH×W ×3 , V I S IBN ET’s objective is

LV (x) = −

M

X

i=1

(cid:2)Ux log(cid:0)(V (Fx ) + 1)/2(cid:1)+

(1)

(1 − Ux )log(cid:0)(1 − V (Fx ))/2(cid:1)(cid:3)i ,

where V : RH×W ×N → RW ×H×1 denotes a differen-
tiable function representing V I S IBN ET, with learnable pa-
rameters, Ux ∈ RH×W ×1 denotes the ground-truth visibil-
ity map for feature map Fx , and the summation is carried
out over the set of M non-zero spatial locations in Fx .
COAR SEN ET was trained next, using a combination of
an L1 pixel loss and an L2 perceptual loss (as in [22, 8])
over the outputs of layers relu1 1, relu2 2, and relu3 3 of
VGG16 [40] pre-trained for image classiﬁcation on the Im-
ageNet [6] dataset. The weights of V I S IBN E T remained
ﬁxed while COAR S EN E T was being trained using the loss

LC = ||C (Fx ) − x||1 + α

3

X

i=1

||φi (C (Fx )) − φi (x)||2

2 , (2)

H
2

×

H
8

H
4

where C : RH×W ×N → RH×W ×3 denotes a differentiable
function representing COAR SEN ET, with learnable param-
eters, and φ1 : RH×W ×3 → R

×64 , φ2 : RH×W ×3 →
×128 , and φ3 : RH×W ×3 → R

W
4

W
8

W
2

×

×

R

×256 denote
the layers relu1 1, relu2 2, and relu2 2, respectively, of the
pre-trained VGG16 network.
R E FINEN ET was trained last using a combination of an
L1 pixel loss, the same L2 perceptual loss as COAR SEN ET,
and an adversarial loss. While training R E FIN EN ET, the
weights of V I S IBN ET and COAR SEN ET remained ﬁxed.
For adversarial training, we used a conditional discrimi-
nator whose goal was to distinguish between real source
images used to generate the SfM models and images syn-
thesized by R E FINEN ET. The discriminator trained using
cross-entropy loss similar to Eq. (1). Additionally, to sta-
bilize adversarial training, φ1 (R(Fx ))1 , φ2 (R(Fx ))1 , and

148

Desc.
Src.

Si
Si
Si
Si

M

Inp. Feat.
MAE
SSIM
D O S
20% 60% 100% 20% 60% 100%
X X X .126
.105
.101
.539
.605
.631
X X × .133
.111
.105
.499
.568
.597
X × X .129
.107
.102
.507
.574
.599
X × × .131
.113
.109
.477
.550
.578
X × × .147
.128
.123
.443
.499
.524

INVERT ING S INGL E IMAG E S I F T F EATUR E S :

Table 1:
The top four rows compare networks designed for differ-
ent subsets of single image (Si) inputs: descriptor (D), key-
point orientation (O) and scale (S). Test error (MAE) and
accuracy (SSIM) obtained when 20%, 60% and all the SIFT
features are used. Lower MAE and higher SSIM values are
better. The last row is for when the descriptors originate
from multiple (M) different and unknown source images.

φ3 (R(Fx ))1 were concatenated before the ﬁrst, second, and
third convolutional layers of the discriminator as done in
[42]. R E FIN EN ET denoted as R() has the following loss.

LR =||R(Fx ) − x||1 + α

3

X

i=1

||φi (R(Fx )) − φi (x)||2

2

(3)

+ β [log(D(x)) + log(1 − D(R(Fx )))].

Here, the two functions, R : RH×W ×N +3 → RH×W ×3
and D : RH×W ×N +3 → R denote differentiable functions
representing R E FINEN ET and the discriminator, respec-
tively, with learnable parameters. We trained R E FIN EN ET
to minimize LR by applying alternating gradient updates
to R E FIN EN E T and the discriminator. The gradients were
computed on mini-batches of training data, with different
batches used to update R E FIN EN E T and the discriminator.

4. Experimental Results

We now report a systematic evaluation of our method.
Some of our results are qualitatively summarized in Fig.
3, demonstrating robustness to various challenges, namely,
missing information in the point clouds, effectiveness of our
visibility estimation, and the sparse and irregular distribu-
tion of input samples over a large variety of scenes.

Dataset. We use the MegaDepth [24] and NYU [39]
datasets in our experiments. MegaDepth (MD) is an In-
ternet image dataset with ∼150k images of 196 landmark
scenes obtained from Flickr. NYU contains ∼400k images
of 464 indoor scenes captured with the Kinect (we only used
the RGB images). These datasets cover very different scene
content, image resolution, and generate very different dis-
tribution of SfM points and camera poses. Generally, NYU
scenes produce far fewer SfM points than the MD scenes.

Data

MD

NYU

Inp. Feat.
Accuracy
z
D C 20% 60% 100%
X × × .948
.948
.946
X × X .938
.943
.941
X X × .949
.951
.948
X X X .952
.952
.950
X × × .892
.907
.908
X × X .897
.908
.910
X X × .895
.907
.909
X X X .906
.916
.917

Table 2: EVALUAT ION O F V I S IBN E T: We trained four ver-
sion of V I S IBN E T, each with a different set of input at-
tributes, namely, z (depth), D (SIFT) and C (color) to eval-
uate their relative importance. Ground truth labels were ob-
tained with VisibDense. The table reports mean classiﬁca-
tion accuracy on the test set for the NYU and MD datasets.
The results show that V I S IBN E T achieves accuracy greater
than 93.8% and 89.2% on MD and NYU respectively and is
not very sensitive to sparsity levels and input attributes.

partitioned the scenes into training, validation, and testing
sets with 441, 80, and 139 scenes respectively. All images
of one scene were included only in one of the three groups.
We report results using both the average mean absolute error
(MAE), where color values are scaled to the range [0,1].
and average structured similarity (SSIM). Note that lower
MAE and higher SSIM values indicate better results.

Inverting Single Image SIFT Features. Consider the sin-
gle image scenario, with trivial visibility estimation and
identical input to [9]. We performed an ablation study in
this scenario, measuring the effect of inverting features with
unknown keypoint scale, orientation, and multiple unknown
image sources. Four variants of COAR SEN ET were trained,
then tested at three sparsity levels. The results are shown
in Table 1 and Figure 4. Table 1 reports MAE and SSIM
across a combined MD and NYU dataset. The sparsity per-
centage refers to how many randomly selected features were
retained in the input, and our method handles a wide range
of sparsity reasonably well. From the examples in Figure 4,
we observe that the networks are surprisingly robust at in-
verting features with unknown orientation and scale; while
the accuracy drops a bit as expected, the reconstructed im-
ages are still recognizable. Finally, we quantify the effect
of unknown and different image sources for the SIFT fea-
tures. The last row of Table 1 shows that indeed the feature
inversion problem becomes harder but the results are still re-
markably good. Having demonstrated that our work solves
a harder problem than previously tackled, we now report
results on inverting SfM points and their features.

4.1. Visibility Estimation

Preprocessing. We processed the 660 scenes in MD and
NYU using the SfM implementation in COLMAP [38]. We

We ﬁrst independently evaluate the performance of the
proposed V I S IBN ET model and compare it to the geomet-

149

Figure 3: QUAL I TAT IV E R E SU LT S : Each result is a 3 × 1 set of square images, showing point clouds (with occluded points
in red), image reconstruction and original. The ﬁrst four columns (top and bottom) show results from the MegaDepth dataset
(internet scenes) and the last four columns (top and bottom) show results from indoor NYU scenes. Sparsity: Our network
handles a large variety in input sparsity (density decreases from left to right). In addition, perspective projection accentuates
the spatially-varying density differences, and the MegaDepth outdoor scenes have concentrated points in the input whereas
NYU indoor scenes have far samples. Further, the input points are non-homogeneous, with large holes which our method
gracefully ﬁlls in. Visual effects: For the ﬁrst four columns (MD scenes) our results give the pleasing effect of uniform
illumination (see top of ﬁrst column). Since our method relies on SfM, moving objects are not recovered. Scene diversity:
The fourth column is an aerial photograph, an unusual category that is still recovered well. For the last four columns (NYU
scenes), despite lower sparsity, we can recover textures in common household scenes such as bathrooms, classrooms and
bedrooms. The variety shows that our method does not learn object categories and works on any scene. Visibility: All scenes
beneﬁt from visibility prediction using V I S IBN ET which for example was crucial for the bell example (lower 2nd column).

ric methods V I S IBS PAR SE and V I S IBD EN S E. We trained
four variants of V I S IBN ET designed for different subsets of
input attributes to classify points in the input feature map
as “visible” or “occluded”. We report classiﬁcation accu-
racy separately on the MD and NYU test sets even though
the network was trained on the combined training set (see
Table 2). We observe that V I S IBN ET is largely insensitive
to scene type, sparsity levels, and choice of input attributes
such as depth, color, and descriptors. The V I S IBN E T vari-
ant designed for depth only has 94.8% and 89.2% mean

classiﬁcation accuracy on MD and NYU test sets, respec-
tively, even when only 20% of the input samples were used
to simulate sparse inputs. Table 3 shows that when points
predicted as occluded by V I S IBN ET are removed from the
input to COAR S EN E T, we observe a consistent improve-
ment when compared to COAR S EN ET carrying both the
burdens of visibility and image synthesis (denoted as Im-
plicit in the table). While the improvement may not seem
numerically large, in Figure 5 we show insets where visual
artifacts (bookshelf above, building below) are removed.

150

(a) Input

(b) SIFT

(c) SIFT + s

(d) SIFT + o

(e) SIFT + s + o

(f) Original

Figure 4: INVERT ING S I F T F EATUR E S IN A S INGL E IMAG E : (a) 2D keypoint locations. Results obtained with (b) only
descriptor, (c) descriptor and keypoint scale, (d) descriptor and keypoint orientation, (e) descriptor, scale and orientation. (f)
Original image. Results from using only descriptors (2nd column) are only slightly worse than the baseline (5th column).

(a) Input

(b) Pred. (VisibNet)

(c) Implicit

(d) VisibNet

(e) VisibDense

(f) Original

Figure 5: IM PORTANCE O F V I S IB I L I TY E S T IMAT ION : Examples showing (a) input 2D point projections (in blue), (b) pre-
dicted visibility from V I S IBN E T – occluded (red) and visible (blue) points, (c–e) results from IM P L IC I T (no explicit visibility
estimation), V I S IBN E T (uses a CNN) and V I S IBD EN S E (uses z-buffering and dense models), and (f) the original image.

4.2. Relative Signiﬁcance of Point Attributes

and SIFT descriptors signiﬁcantly improves visual quality.

We trained four variants of COAR S EN ET, each with a
different set of the available SfM point attributes. The goal
here is to measure the relative importance of each of the at-
tributes. This information could be used to decide which
optional attributes should be removed when storing SfM
model to enhance privacy. We report reconstruction error on
the test set for both indoor (NYU) and outdoor scenes (MD)
for various sparsity levels in Table 4 and show qualitative
evaluation on the test set in Figure 6. The results indicate
that our approach is largely invariant to sparsity and capable
of capturing very ﬁne details even when the input feature
map contains just depth, although, not surprisingly, color

4.3. Signiﬁcance of ReﬁneNet

In Figure 7 we qualitatively compare two scenes where
the feature maps had only depth and descriptors (left) and
when it had all the attributes (right). For privacy preser-
vation, these results are sobering. While Table 4 showed
that COAR S EN ET struggles when color is dropped (sug-
gesting an easy solution of removing color for privacy),
Figure 7 (left) unfortunately shows that R E FINEN ET recov-
ers plausible colors and improves results a lot. Of course,
R E FIN EN ET trained on all features also does better than
COAR S EN ET although less dramatically (Figure 7 (right)).

151

Data

MD

NYU

Visibility
Est.
Implicit
VisibSparse
VisibNet
VisibDense
Implicit
VisibSparse
VisibNet
VisibDense

MAE
SSIM
20% 60% 100% 20% 60% 100%
.201
.197
.195
.412
.436
.445
.202
.197
.196
.408
.432
.440
.201
.196
.195
.415
.440
.448
.201
.196
.195
.417
.442
.451
.121
.100
.094
.541
.580
.592
.122
.100
.094
.539
.579
.592
.120
.098
.092
.543
.583
.595
.120
.097
.090
.545
.587
.600

Table 3: IM PORTANCE O F V I S IB I L I TY E ST IMAT ION : Both

sub-tables show results obtained using IM P L IC I T i.e. no
explicit occlusion reasoning where of burden of visibility
estimation implicitly falls on COAR SEN ET, VisibNet and
the geometric methods V I S IBS PAR SE and V I S IBD EN S E.
Lower MAE and higher SSIM values are better.

z + D

z + D + C

IM PORTANCE O F R E FIN EN E T:

Figure 7:
(Top row)
COAR S EN ET results. (Bottom Row) R E FIN EN E T results.
(Left) Networks use depth and descriptors (z + D). (Right)
Networks use depth, descriptor and color (z + D + C).

Data

MD

NYU

Inp. Feat.
MAE
SSIM
z
D C 20% 60% 100% 20% 60% 100%
X × × .258
.254
.253
.264
.254
.250
X × X .210
.204
.202
.378
.394
.403
X X × .228
.223
.221
.410
.430
.438
X X X .201
.196
.195
.414
.439
.448
X × × .295
.290
.289
.244
.209
.197
X × X .148
.121
.111
.491
.528
.546
X X × .207
.179
.171
.493
.528
.539
X X X .121
.099
.093
.542
.582
.594

Table 4: E FFEC T O F PO IN T ATTR IBU T E S : Performance of
four networks designed for different sets of input attributes
– z (depth), D (SIFT) and C (color), on MD and NYU. Input
sparsity is simulated by applying random dropout to input
samples during training and testing.

z

z + D

z + C

z + D + C

orig

Figure 6: E FFEC T O F PO IN T AT TR IBUT E S : Results ob-
tained with different attributes. Left to right: depth [z],
depth + SIFT [z + D], depth + color [z + C], depth + SIFT
+ color [z + D + C] and the original image. (see Table 4).

4.4. Novel View Synthesis

Our technique can be used to easily generate realistic
novel views of the scene. While quantitatively evaluating

Figure 8: NOV EL V I EW SYNTH E S I S : Synthesized images
from virtual viewpoints in two NYU scenes [39] helps to
interpret the cluttered scenes (see supplementary video).

such results is more difﬁcult (in contrast to our experiments
where aligned real camera images are available), we show
qualitative results in Figure 8 and generate virtual tours
based on the synthesized novel views1 . Such novel view
based virtual tours can make scene interpretation easier for
an attacker even when the images contain some artifacts.

5. Conclusion

In this paper, we introduced a new problem, that of in-
verting a sparse SfM point cloud and reconstructing color
images of the underlying scene. We demonstrated that sur-
prisingly high quality images can be reconstructed from the
limited amount of information stored along with sparse 3D
point cloud models. Our work highlights the privacy and
security risks associated with storing 3D point clouds and
the necessity for developing privacy preserving point cloud
representations and camera localization techniques, where
the persistent scene model data cannot easily be inverted to
reveal the appearance of the underlying scene. This was
also the primary goal in concurrent work on privacy pre-
serving camera pose estimation [41] which proposes a de-
fense against the type of attacks investigated in our paper.
Another interesting avenue of future work would be to ex-
plore privacy preserving features for recovering correspon-
dences between images and 3D models.

1 see the video in the supplementary material.

152

References

[1] 6D.AI. http://6d.ai/, 2018. 1
[2] ARCore. developers.google.com/ar/, 2018. 1
[3] ARKit. developer.apple.com/arkit/, 2018. 1
[4] Z. Chen, V. Badrinarayanan, G. Drozdov, and A. Rabinovich.
Estimating depth from RGB and sparse sensing. In ECCV,
pages 167–182, 2018. 2
[5] T. Dekel, C. Gan, D. Krishnan, C. Liu, and W. T. Free-
man. Smart, sparse contours to represent and edit images.
In CVPR, 2018. 2
[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. ImageNet: A large-scale hierarchical image database.
In CVPR, pages 248–255, 2009. 4
[7] J. Donahue, P. Kr ¨ahenb ¨uhl, and T. Darrell. Adversarial fea-
ture learning. In ICLR, 2017. 2
[8] A. Dosovitskiy and T. Brox. Generating images with percep-
tual similarity metrics based on deep networks. In Advances
in Neural Information Processing Systems, pages 658–666,
2016. 2, 4
[9] A. Dosovitskiy and T. Brox. Inverting visual representations
with convolutional networks. In CVPR, pages 4829–4837,
2016. 1, 2, 5
[10] H. Edwards and A. Storkey. Censoring representations with
an adversary. In ICLR, 2016. 3
[11] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction
from a single image using a multi-scale deep network.
In
Advances in neural information processing systems, pages
2366–2374, 2014. 2
[12] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deep-
stereo: Learning to predict new views from the world’s im-
agery. In CVPR, pages 5515–5524, 2016. 2
[13] J. Hamm. Minimax ﬁlter: learning to preserve privacy from
inference attacks. The Journal of Machine Learning Re-
search, 18(1):4704–4734, 2017. 3
[14] P. Hedman, J. Philip, T. Price, J.-M. Frahm, G. Drettakis, and
G. Brostow. Deep blending for free-viewpoint image-based
rendering. ACM Transactions on Graphics (SIGGRAPH
Asia Conference Proceedings), 37(6), November 2018. 2
[15] Hololens. https://www.microsoft.com/en-us/
hololens, 2016. 1
[16] J. Hong. Considering privacy issues in the context of google
glass. Commun. ACM, 56(11):10–11, 2013. 1
[17] T.-W. Hui, C. C. Loy, and X. Tang. Depth map super-
resolution by deep multi-scale guidance.
In ECCV, 2016.
2
[18] A. Irschara, C. Zach, J.-M. Frahm, and H. Bischof. From
structure-from-motion point clouds to fast location recogni-
tion. In CVPR, pages 2599–2606, 2009. 1
[19] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In CVPR,
pages 1125–1134, 2017. 2
[20] H. Kato and T. Harada. Image reconstruction from bag-of-
visual-words. In CVPR, pages 955–962, 2014. 2
[21] P. Labatut, J.-P. Pons, and R. Keriven. Efﬁcient multi-view
reconstruction of large-scale scenes using interest points, De-
launay triangulation and graph cuts.
In ICCV, pages 1–8,
2007. 3

[22] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham,
A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et al.
Photo-realistic single image super-resolution using a genera-
tive adversarial network. In CVPR, pages 4681–4690, 2017.
4

[23] Y. Li, N. Snavely, D. Huttenlocher, and P. Fua. Worldwide
pose estimation using 3d point clouds. In ECCV, pages 15–
29. Springer, 2012. 1

[24] Z. Li and N. Snavely. Megadepth: Learning single-view
depth prediction from internet photos. In Computer Vision
and Pattern Recognition (CVPR), 2018. 1, 2, 5

[25] H. Lim, S. N. Sinha, M. F. Cohen, M. Uyttendaele, and
H. J. Kim. Real-time monocular image-based 6-dof localiza-
tion. The International Journal of Robotics Research, 34(4-
5):476–492, 2015. 1

[26] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In Advances in Neural Informa-
tion Processing Systems, pages 700–708, 2017. 2

[27] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-
works.
In Advances in neural information processing sys-
tems, pages 469–477, 2016. 2

[28] J. Lu and D. Forsyth. Sparse depth super resolution.
CVPR, pages 2245–2253, 2015. 2

In

[29] A. Mahendran and A. Vedaldi. Understanding deep image
representations by inverting them.
In CVPR, pages 5188–
5196, 2015. 2

[30] R. McPherson, R. Shokri, and V. Shmatikov. Defeat-
ing image obfuscation with deep learning. arXiv preprint
arXiv:1609.00408, 2016. 2

[31] M. Moukari, S. Picard, L. Simoni, and F. Jurie. Deep multi-
scale architectures for monocular depth estimation. In ICIP,
pages 2940–2944, 2018. 2

[32] A. Odena, V. Dumoulin, and C. Olah. Deconvolution and
checkerboard artifacts. Distill, 2016. 4

[33] F. Pittaluga, S. Koppal, and A. Chakrabarti. Learning privacy
preserving encodings through adversarial training. In 2019
IEEE Winter Conference on Applications of Computer Vision
(WACV), pages 791–799. IEEE, 2019. 3

[34] X. Qi, Q. Chen, J. Jia, and V. Koltun. Semi-parametric image
synthesis. In CVPR, pages 8808–8816, 2018. 2

[35] N. Raval, A. Machanavajjhala, and L. P. Cox. Protecting vi-
sual secrets using adversarial nets. In CV-COPS 2017, CVPR
Workshop, pages 1329–1332, 2017. 3

[36] G. Riegler, M. R ¨uther, and H. Bischof. ATGV-Net: Accurate
depth super-resolution. In ECCV, pages 268–284, 2016. 2

[37] T. Sattler, B. Leibe, and L. Kobbelt. Fast image-based lo-
calization using direct 2d-to-3d matching.
In ICCV, pages
667–674. IEEE, 2011. 1

[38] J. L. Sch ¨onberger and J.-M. Frahm. Structure-from-motion
revisited. In CVPR, pages 4104–4113, 2016. 1, 2, 3, 5

[39] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus.
Indoor
segmentation and support inference from rgbd images.
In
ECCV, 2012. 1, 2, 5, 8

[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
4

153

[41] P. Speciale, J. L. Sch ¨onberger, S. B. Kang, S. N. Sinha, and
M. Pollefeys. Privacy preserving image-based localization.
arXiv preprint arXiv:1903.05572, 2019. 3, 8
[42] D. Sungatullina, E. Zakharov, D. Ulyanov, and V. Lempit-
sky. Image manipulation with perceptual discriminators. In
ECCV, pages 579–595, 2018. 5
[43] J. Uhrig, N. Schneider, L. Schneider, U. Franke, T. Brox,
and A. Geiger. Sparsity invariant CNNs.
In International
Conference on 3D Vision (3DV), pages 11–20, 2017. 2
[44] I. Vasiljevic, A. Chakrabarti, and G. Shakhnarovich. Exam-
ining the impact of blur on recognition by convolutional net-
works. arXiv preprint arXiv:1611.05760, 2016. 2
[45] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba.
Hoggles: Visualizing object detection features.
In CVPR,
pages 1–8, 2013. 2
[46] P. Weinzaepfel, H. J ´egou, and P. P ´erez. Reconstructing an
image from its local descriptors. In CVPR, pages 337–344,
2011. 1, 2
[47] L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural
network for image deconvolution.
In Advances in Neural
Information Processing Systems, pages 1790–1798, 2014. 2
[48] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson.
Understanding neural networks through deep visualization.
In ICML Workshop on Deep Learning, 2015. 2
[49] M. D. Zeiler and R. Fergus. Visualizing and understanding
convolutional networks. In ECCV, pages 818–833. Springer,
2014. 2
[50] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In CVPR, pages 2223–2232, 2017. 2

154

SDRSAC: Semideﬁnite-Based Randomized Approach for
Robust Point Cloud Registration without Correspondences

Huu M. Le1 , Thanh-Toan Do2

,

3 , Tuan Hoang1 , and Ngai-Man Cheung1

1 Singapore University of Technology and Design 2University of Liverpool 3AIOZ Pte Ltd

Abstract

This paper presents a novel randomized algorithm for
robust point cloud registration without correspondences.
Most existing registration approaches require a set of pu-
tative correspondences obtained by extracting invariant de-
scriptors. However, such descriptors could become unreli-
able in noisy and contaminated settings. In these settings,
methods that directly handle input point sets are preferable.
Without correspondences, however, conventional random-
ized techniques require a very large number of samples in
order to reach satisfactory solutions. In this paper, we pro-
pose a novel approach to address this problem. In partic-
ular, our work enables the use of randomized methods for
point cloud registration without the need of putative cor-
respondences. By considering point cloud alignment as a
special instance of graph matching and employing an efﬁ-
cient semi-deﬁnite relaxation, we propose a novel sampling
mechanism, in which the size of the sampled subsets can be
larger-than-minimal. Our tight relaxation scheme enables
fast rejection of the outliers in the sampled sets, resulting
in high quality hypotheses. We conduct extensive experi-
ments to demonstrate that our approach outperforms other
state-of-the-art methods. Importantly, our proposed method
serves as a generic framework which can be extended to
problems with known correspondences. 1

1. Introduction

Point cloud registration is important in many computer
vision applications, including range scan alignment [37],
3D object recognition and localization [18, 48], large scale
reconstruction [2, 44]. Given two sets of points in three-
dimensional (3D) Euclidean space, the objective is to search
for the optimal rigid transformation, which comprises a ro-
tation matrix R∗ ∈ SO(3) and a translation vector t∗ ∈ R3 ,
that optimally aligns the two input point sets.
In many
practical applications, the input data contains a signiﬁcant
amount of noise. Moreover, the overlapping region between

1 Source code is available at: https://github.com/intellhave/SDRSAC.

the two point sets can be small, resulting in a large number
of outliers, i.e., non-overlapping points. Therefore, the reg-
istration needs to be conducted in a robust manner so that
the ﬁnal estimates are not affected by the contamination.
Formally, let S = {si ∈ R3 }Ns
i=1 and D = {dj ∈ R3 }Nd
denote the source and destination (target) point clouds, re-
spectively, the problem of robust rigid registration can be
formulated as

j=1

min

R∈SO(3),t∈R3

Ns

Xi=1

ρ (kRsi + t − dj k) ,

(1)

where the notation k · k represents the ℓ2 norm, dj is a point
in the target set D that is closest to the transformed point
Rsi + t, i.e.,

dj = arg min

dk ∈D

kRsi + t − dk k,

(2)

(3)

and ρ is a robust loss function. Here, SO(3) denotes the
space of rotation matrices. In order for the registration to be
robust, ρ is typically chosen from a set of robust kernels [27,
1, 33]. In this work, we make use of the popular maximum
consensus criterion [26], in which ρ is deﬁned as
ρ(x) = (0 if x ≤ ǫ,
1 otherwise.
The threshold ǫ > 0 is a user-deﬁned parameter that speci-
ﬁes the maximum allowable distance for a correspondence
pair to be considered as an inlier.
Intuitively, by solv-
ing (1) with ρ deﬁned as per (3), we search for the optimal
alignment (R∗ , t∗ ) that maximizes the set of overlapping
points, where si ∈ S overlaps dj ∈ D if the transforma-
tion (R∗ , t∗ ) brings si to a new location that lies within the
ball (dj , ǫ). The problem (1) is an active research topic in
computer vision due to its computational complexity.
Many existing algorithms need to take as input a set
of putative correspondences, which are usually obtained
(as a pre-processing step) by extracting local invariant fea-
tures on the given point sets, and executing multiple rou-
tines of nearest neighbor search to propose initial key-

1124

point matches [5, 25]. Several types of 3D local fea-
tures [45, 15, 52, 43] have demonstrated to provide promis-
ing results throughout a variety of challenging datasets.
However, noise and contamination would degrade the qual-
ity of extracted features. Furthermore, in order for the lo-
cal features to be precisely computed, many feature extrac-
tors require the surface representation to be dense; however,
sparse point clouds are common in practice. In particular,
it has been demonstrated in [4] that, for noisy datasets with
high proportion of outliers, alignments using features have
poorer results compared with using raw data. Therefore,
there is interest to develop registration algorithms that di-
rectly align raw point cloud data without the need of a priori
correspondences [11, 4, 9, 36, 21].

Due to the computational complexity of the problem
under different input settings, randomized hypothesize-
and-verify algorithms such as RANSAC [22] and its vari-
ants [17, 16, 47, 46, 42, 32] are popular approaches. Ran-
domized techniques have been employed to address prob-
lems with known correspondences. On the other hand, sim-
ilar random sampling strategy as that of RANSAC can also
be applied to problems of without correspondences. Specif-
ically, at each iteration, a minimal subset (of three points) on
each point cloud can be sampled to form three pairs of cor-
respondences, which are used for estimating and validating
one hypothesis. Such procedure can then be repeated un-
til a satisfactory solution is obtained. However, with noise
and outliers, the likelihood of picking outlier-free subsets
degrades rapidly. Therefore, much efforts have been de-
voted to develop better sampling strategies, notably the 4-
Points Congruent Sets (4PCS) method proposed in [4] and
its improved variant [38]. Although 4PCS provides consid-
erable advantages over conventional randomized methods,
the enumeration of (approximately) congruent sets that un-
derpins this algorithm is the main issue when working with
point clouds with large number of points and high outlier
rates. In fact, for dense point clouds, 4PCS and its variants
need to down-sample the point cloud before conducting the
sampling process to reduce processing time.

In this paper, we address the above-mentioned limita-
tions. Speciﬁcally, by employing a special instance of
graph matching formulation, we propose a new larger-than-
minimal sampling strategy for point cloud registration with-
out correspondences. The advantage of our method is that
the task of searching for correspondences is quickly approx-
imated by solving a relaxed convex problem, rather than
subset enumeration. This allows us to sample subsets with
arbitrarily large size, in which sets of correspondences are
obtained from solutions of convex semi-deﬁnite program-
ming. These correspondences can then be used for estimat-
ing and validating hypotheses. A large subset of points in
the source point cloud represent better its structure, and by
identifying its corresponding points on the target set, the

two point clouds can be coarsely align faster, which can
then be reﬁned by local methods, e.g., ICP [8]. Empirical
results show that the proposed method is very competitive.
Our main contributions are:

• We apply graph matching for the registration problem
without correspondences using a novel cost function
to enforce robustness and a tight semideﬁnite (SDP)
relaxation.

• From the SDP formulation, we then develop a new
larger-than-minimal subset sampling scheme, leading
to an effective randomized algorithm that outperforms
other state-of-the-art methods.

2. Related Work

In practice, if the two input point clouds are coarsely
aligned, the well-known Iterative Closest Point (ICP) [8]
is commonly employed. Like other iterative algorithms,
ICP alternates between establishing correspondences and
estimating the transformation. The main drawback of this
method lies in the fact that it requires a good starting point
(initial pose) to prevent itself from converging to poor align-
ments. Additionally, in terms of robustness, ICP suffers
from the same weakness as that of the least squares esti-
mator, i.e., it is easily biased by erroneous outliers. Sev-
eral works [13, 7, 40] have been proposed to improve the
such shortcomings of ICP. However, these variants still need
to be bootstrapped by a descent initial pose. ICP is there-
fore commonly employed as a local reﬁnement procedure,
which is executed after the point clouds are roughly regis-
tered by some type of global alignment algorithms.
Algorithms that offer globally optimal solutions are
also actively developed in the literature. To address the
initialization-dependent issue of ICP, Go-ICP [50] employs
the branch and bound strategy to search in the space of R
and t for the optimal transformation. However, due to the
least squares objective of Go-ICP, its returned optimal so-
lutions are still non-robust to outliers. The robustness of
Go-ICP can be improved by incorporating robust loss func-
tions in place of least squares. Another globally optimal
algorithm to tackle the robust registration without corre-
spondences was proposed by Bustos et al. [11]. Unlike
Go-ICP, [11] solves (1) directly and its solution is robust
to outliers. Branch and bound is also the mechanism be-
hind [11], with a novel steographic projection implementa-
tion for fast matching query and tighter bound computation.
Although the convergence to globally optimal solutions is
guaranteed for these methods, they are still impractical for
large datasets due to their expensive computational cost.
As previously mentioned, in some point cloud registra-
tion applications, randomized strategies – with the famous
RANSAC [22] representative – are still the dominant ap-
proaches. Our work also belongs to this category. Gener-

125

ally speaking, for most randomized methods, the underlying
sampling strategy signiﬁcantly affects the run time, since
the termination of a sampling mechanism depends largely
on its ability to quickly identify outlier-free subsets. For
the case with no correspondences, it is even harder since
two outlier-free subsets, one in each input point cloud, need
to be identiﬁed and the elements in these two subsets must
form correct correspondences. Different sampling strate-
gies have been proposed in the literature [17, 16, 49]. For
instance, in problems with known correspondences, one of
the notable improvements for RANSAC is LO-RANSAC
(Locally Optimal RANSAC) [17], which proposes to sam-
ple larger-than-minimal subsets when RANSAC solution is
updated. This improved strategy has been shown to sig-
niﬁcantly outperform conventional RANSAC. Randomized
methods can also be improved using local reﬁnement tech-
niques [31, 12, 41].

In the context of robust point cloud alignment without
correspondences, however, the idea of larger-than-minimal
sampling has yet to be thoroughly explored. Unlike the
case of known correspondences, the “inlier” set at each
RANSAC iteration may not be true inliers, and purely ap-
plying LO-RANSAC to these subsets may not be of any
help, while the run time is increased. Besides, even if true
inliers reside in the subset, a larger-than-minimal subset
may be contaminated with outliers, which can deteriorate
the estimation since it is done solely by solving least squares
over the sampled subsets. From the insight discussed above,
it can be seen that from any larger-than-minimal subsets, if
the outliers are efﬁciently rejected, the ability to discover
good hypotheses can be accelerated. This idea is analyzed
in our paper, in which we propose a new algorithm that en-
ables the sampling of any arbitrarily large subsets. Such
sampling scheme is incorporated with an oracle that allows
the outliers in the sampled point sets to be efﬁciently dis-
carded. Our experiments show that this newly-proposed al-
gorithm outperforms previous approaches.

Our work is closely related to 4PCS [3], which is the
state-of-the-art sampling approach that also solves the same
problem as ours. Instead of randomly picking minimal sub-
sets of three points, 4PCS works on sampled tuples of four
co-planar points. This method is later improved by Su-
per4PCS [38], where the complexity of congruent set ex-
traction is reduced from quadratic to linear with respect to
the number of points. Though efﬁcient, especially with the
improvements introduced in [38], 4PCS still suffers from
the same weakness as RANSAC, e.g., the low likelihood of
quickly sorting out the right subsets under the presence of
high outliers. In fact, for each subset of 4 co-planar points
on one point cloud, 4PCS needs to enumerate all congru-
ent subsets on the other point cloud. Therefore, this method
needs to subsample the point set into a smaller subset before
the sampling is conducted.

The technique developed in this work is also inspired by
the class of methods that solve 3D registration problem us-
ing graph matching [21, 35, 34] and recent semi-deﬁnite
relaxation for non-rigid registration [19, 28, 30]. These al-
gorithms, however, can only work for a small number of
data points, which is very inefﬁcient if applied directly on
large point clouds. We, on the other hand, propose to em-
ploy graph matching as sub-problems embedded in a ran-
dom sampling framework. This allows us to combine the
strengths of both classes of methods, i.e., randomized tech-
niques and graph matching, to derive an efﬁcient sampling
mechanism for point cloud registration.

3. Semideﬁnite-Based Randomized Approach

3.1. The Correspondence Problem

When the putative correspondences are not given, the
task of solving (1) can be viewed as jointly estimating the
transformation (R∗ , t∗ ) and the best subset of correspon-
dences C ∗ . Hence, if C ∗ is provided, the optimal align-
ment can be computed in a straightforward manner, and
vice versa. With that in mind, in this section, we ﬁrst in-
troduce the correspondence problem to search for C ∗ .
In
the latter sections, we then discuss its semideﬁnite (SDP)
relaxation, which can be employed as a fast hypothesis gen-
eration mechanism that lies at the core of our efﬁcient ran-
domized algorithm.

To simplify the formulation, let us for now assume that
we are given two 3D point sets P = {pi }N
i=1 and Q =
{qj }N
j=1 , each contains N data points. The task is to ﬁnd
the optimal set of correspondences C ∗
PQ that can be used to
robustly align P and Q. Note that we use different notations
for input data here compared to (1), since this new problem
will be used as sub-problem for solving (1), which will be
clariﬁed in the latter sections.
Let X ∈ {0, 1}N ×N be the permutation matrix in which
the element at the i−th row and j−th column (denoted by
Xi,j ) is assigned the value of 1 if the pair pi ∈ P and qj ∈
P belongs to the set of correspondences and 0 otherwise. To
account for outliers, let us further assume that the optimal
solution contains m ≤ N pairs of correspondences. The
value of m can be chosen to be greater than or equal to 3
(the size of the minimal subset), or estimated based on the
known outlier ratio of the problem at hand. Note that with
the introduction of m, X is now a sub-permutation matrix.
Denote by ˆX the vectorization of the matrix X obtained by
stacking its columns:

ˆX = [XT

:,1 XT

:,2 . . . XT

:,N ]T ,

(4)

where X:,j denotes the j−th column of X.
In order to
search for the best correspondence assignments, consider

126

the following optimization problem:

max

X

ˆXT A ˆX,

(5a)

subject to Xi,j ∈ {0, 1} ∀i, j ∈ {1, . . . , N },

(5b)

N

Xi,j ≤ 1 ∀i ∈ {1, . . . , N },

(5c)

Xi,j ≤ 1 ∀j ∈ {1, . . . , N },

(5d)

designed based on the fact that if pa ∈ P corresponds to
qb ∈ Q and pc ∈ P corresponds to qd ∈ Q, since the
transformation is rigid, the lengths of the two line segments
papc and qbqd must be approximately the same (due to the
effect of noise). With the new formulation (6), instead of
assigning the matching potentials to all the elements of A,
we only allow pairs of segments whose lengths differ by a
small value γ to be considered as candidate for matching,
while pairs having large gap are rejected.

3.2. Semideﬁnite Relaxation

Xi,j = m,

N

Xj=1

(5e)

Let us ﬁrst consider the equivalent formulation of (5).
Let Y = ˆX ˆXT ∈ RN 2×N 2
, the problem (5) becomes

where A ∈ RN 2×N 2
is the symmetric matrix that charac-
terizes the matching potentials between pairs of points (line
segments) in P and Q. In particular, we deﬁne the elements
of the matrix A as

max

X,Y

subject to

trace(AY),

Y = ˆX ˆXT ,
trace(Y) = m,
0 ≤ Xi,j ≤ 1 ∀i, j,
(5c), (5d), (5e).

(7a)

(7b)

(7c)

(7d)

(7e)

N

Xj=1
Xi=1
Xi=1

N

0

Aab,cd = (f (pa , pc , qb , qd ) if|δ(pa , pc ) − δ(qb , qd )| ≤ γ ,
otherwise,
(6)
where a, b, c, d ∈ {1..N } are point indexes, ab = a + (b −
1)N and cd = c + (d − 1)N are the indexes for the row
and column of A, respectively; pa , pc ∈ P ; qb , qd ∈ Q;
γ > 0 is a predeﬁned threshold and δ(p1 , p2 ) computes the
Euclidean distance between two 3D points p1 and p2 ; f is
a function that takes as input two pairs of points (pa , pc )
and (qb , qd ) and outputs a scalar that represents matching
potential for these two pairs. Typically, f in (6) is chosen
to be the function that penalizes the difference in length be-
tween the two line segments. For simplicity, we choose f
to be exp(−|δ(pa , pc ) − δ(qb , qd )|).
The constraints in (5) are to assure that X must lie in
the space of permutation matrices. Speciﬁcally, besides the
binary constraint (5b) to restrict Xi,j ∈ {0, 1}, it is also
required that each point pi ∈ P can only be assigned to at
most one point in Q and vice versa, which is reﬂected by the
constraints (5c) and (5d). Finally, by enforcing (5e), the op-
timal solution of (5) will only contain m pairs of correspon-
dences. The solution to (5) provides the optimal assignment
such that the sum of matching potentials gained yield from
the corresponding pairs is maximized.
While the use of graph matching has been explored in
several rigid and non-rigid registration problems [21, 35,
30, 28], most previous work consider solving graph match-
ing for the whole input data, which is infeasible for large
datasets. This work proposes to solve graph matching on
very small subsets of points, then embed them into a ran-
dom sampling framework. Additionally, we also propose
an better formulation of the matrix A for the special case
of robust rigid registration. Particularly, the matrix A is

Note that in (7), the constraint (5b) can be removed with-
out affecting the equivalence between (7) and (5). Indeed,
since trace(Y) = m, it must hold that Pi,j X2
i,j = m. On
the other hand, due to the constraint (5e), Pi,j Xi,j = m.
These conditions result in Pi,j X2
i,j = Pi,j Xi,j . Also,
due to the condition 0 ≤ Xi,j ≤ 1, Xi,j can only be ei-
ther 0 or 1. In other word, X can now be constrained in the
convex hull of the sub-permutation matrices [39].
By introducing Y , the problem is lifted into the domain
and the binary constraint of X can be relaxed
without changing the solution of the problem. However,
the problem (7) is still non-convex due to the rank-one con-
straint (7b).
In order to approximate (7), we employ the
common convex relaxation approach, in which (7b) is re-
laxed to the semideﬁnite constraint Y − ˆX ˆXT (cid:23) 0. Then,
we arrive at the following convex optimization problem:

of RN 2×N 2

max

X,Y

subject to

trace(AY),

Y − ˆX ˆXT (cid:23) 0,
(5c), (5d), (5e), (7c), (7d).

(8a)

(8b)

(8c)

The problem (8) introduced above is a convex semideﬁnite
program (SDP), whose globally optimal solution can be ob-
tained using many off-the-shelf SDP solvers. In this work,
we use SDPNAL+ [51] throughout all the experiments.

3.2.1 Tightening the Relaxation

As (8) is solved as an approximation for (7) with rank-one
constraint, one would expect that the two solutions to be as

127

0,

0,



if a = c, b 6= d,

if b = d, a 6= c,

min(Xab , Xcd ),

close as possible. Inspired by [30], we add the following
constraints to (8) to tighten the relaxation:
Yab,cd ≤ 
The intuition behind (9) is that one point in S is not al-
lowed to match with more than one point in D and vice
versa. Also, since Yab,cd = XabXcd and these are binary
numbers, it must hold that Yab,cd ≤ min(Xab , Xcd ).
In addition, thanks to the special case of robust registra-
tion, (8) can be further tightened. Observe that, based on
the discussion of formulating the matrix A, the following
constraints can be enforced:

otherwise.

(9)

Yab,cd = 0 if Aab,cd = 0,

(10)

which means if the pairs papc and pbpd differ too much
in length (more than γ , we directly disallow them to be
matched.
Finally, with the addition of (9) and (10), our SDP relax-
ation becomes:

max

X,Y

trace(AY)

(11a)

subject to

(8b), (7c), (7d), (8c), (9), (10)

(11b)

Note that (11) is still a convex SDP since the additional con-
straints (9) and (10) are linear.

3.3. Projection of Solutions to Permutation Matrix

After solving (11) up to its global optimality using a con-
vex solver, the remaining task is to project its solution back
to the space of permutation matrices. This task can be done
using different strategies [23]. In this work, we apply the
linear assignment problem [10], in which the projection can
be computed efﬁciently by solving a linear program (LP).
Speciﬁcally, let ˜X be the optimal solution of (11). The LP
for projection can be formulated as

max

X

hX, ˜Xi

Xi,j = 1

subject to 0 ≤ Xi,j ≤ 1 Xi
Xi,j = 1, Xj
(12)
where h·, ·i represents the inner product of two matrices.
The solution of (12) provides us with a set of N corre-
spondences. However, in (5), we only want to pick m pairs.
To tackle this with a simple heuristic, observe that in the
optimal solution X∗ of (5), only m rows/columns of X∗
contain the value of 1, while the rest of the rows/columns
contain all zeros. Therefore, from correspondence set ob-
tained from (12), we associate each pair of correspondence
(pi , qj ) with a score that is equal to ˜Xi,j . Finally, m pairs

with the highest scores are chosen as the approximate solu-
tion for (5). This approach has been empirically shown to
be very efﬁcient throughout our experiments.

3.4. Main Algorithm

Algorithm 1 SDRSAC
Require: Input
data
and D ,
size of sampled subsets Nsample

inner iters,

max iter,

S

1: iter← 0; best score← 0;
2: while iter< max iter do

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

S ′ ← Randomly sample Nsample points from S
for t = 1 to inner iters do
D ′ ← Randomly sample Nsample points from D
{M, R, t} ← SDRMatching (S , D , S ′ , D ′ )
/*Described in Alg. 2 */
if |M| > best score then
best score← |M|; R∗ ← R; t∗ ← t
end if
end for

iter← iter+ inner iters

T ← Number of iterations that satisﬁes the proba-
bilistic stopping criteria (Sec. 3.5)
if iter≥ T then

return

15:

end if
16: end while
17: return Best transformation (R∗ , t∗ ), best score

Algorithm 2 SDRMatching

Require: Input data S and D , sampled subsets S ′ and D ′ ,
threshold ǫ.
1: A ← Matrix generated using Eq. (6) with P = S ′ and

Q = D ′

2: ˜X ← Solve (11) with A generated from Step 1.
3: X ← Solve (12) with ˜X obtained from Step 2.
4: M′ ← {(s′
i ∈ S ′ , d′
5: ( ˜R, ˜t) ← Estimate transformation based on correspon-
dence set M′ [20, 29].
6: (R, t) ← Reﬁnement using ICP, initialized by ( ˜R, ˜t)

j ∈ D ′ )|Xi,j = 1}

7: M ← {(si ∈ S , dj ∈ D) | kRsi + t − dj k ≤ ǫ}

/*dj is deﬁned in (2) */
8: return M, R, t

Although (11) is convex, solving it for large value of N
is inefﬁcient since the lifted variable Y belongs to the do-
main of RN 2×N 2
. However, for a small value of N (typi-
cally, N ≤ 20), this problem can be optimized efﬁciently
and (11) provides a good approximation for the original
problem (7). Taking advantage of this observation, one can

128

develop a sampling approach where the number of points
in each sample can be of any size, up to the limit that the
employed solver can handle (11) efﬁciently. More specif-
ically, at each iteration, two subsets of points S ′ ⊆ S and
D ′ ⊆ D are randomly sampled from S and D , respectively.
The cardinality of S ′ and D ′ can be controlled depending
on the capability of the employed convex solver. By solv-
ing (11), the outliers in the sampled subsets can be rejected
and the best subset in each sample is retained for estimating
the transformation. Like other randomized paradigms, this
process can be repeated after a ﬁxed number of iterations
or until the stopping criteria is satisﬁed. The insight be-
hind this strategy is that by sampling large subsets at each
iteration, one is more likely to encounter the right subsets
that contains inliers, since if any sampled subset is contam-
inated, the outliers can be rejected efﬁciently by solving the
SDP approximation discussed in Section 3.1. Algorithm 1
summarizes our method.

3.5. Stopping Criterion

We follow [4] to derive our stopping criterion. Let us
denote by pI be the probability of selecting one inlier (cor-
rect correspondence pair), and by T the number of trials.
Note that from statistics [24], the expected inlier rate in a
sample of size Nsample is also pI . Moreover, after running
Alg. 2, only m pairs of correspondences remain, allowing
us to compute the stopping criterion based on m. Denote
by pf the probability that the algorithm fails to ﬁnd an all-
inlier correspondence set after T trials, pf can be computed
as

pf = (1 − pm

I )T ,

(13)

Therefore, in order to get a success probability to be greater
than ps , we must have the number of iterations greater than

T ≥

log(1 − ps )
log(1 − pm
I )

.

(14)

Since the real inlier rate pI is not known in advance, fol-
lowing common practice of several randomized methods,
we update this value during the sampling process, i.e., pI is
iteratively updated using the inlier ratio of the current best-
so-far solutions.

4. Experimental Results

To evaluate the performance of our proposed algo-
rithm (SDRSAC), we conduct experiments on multiple sets
of synthetic and real datasets and compare our approach
against several state-of-the-art algorithms that can be used
to solve point cloud registration without correspondences,
including ICP [8], Trimmed ICP (TrICP) [13], Iteratively
Re-weighted Least Squares (IRLS) [6], 4PCS [4] and its
improvement Super4PCS [38]. Within the class of globally

optimal algorithms, we also compare SDRSAC against Go-
ICP [50] and its robust version with trimming (TrGoICP).
Note that conventional RANSAC method [22] performs
poorly for this type of problem, and in many cases it turns
into a brute-force type algorithm. Thus, to save space, we
only show results from established methods listed above.
As we focus on validating the effectiveness of robust
global registration, throughout the experiments, we measure
and report the number of matches (objective function of (1))
and run time for each method.
All experiments are executed on a standard Ubuntu ma-
chine with 16GB of RAM. SDRSAC, ICP, TrimmedICP
were implemented using MATLAB. For 4PCS, Su-
per4PCS [38], Go-ICP [50], we use the released C++ code
and the parameters suggested by the authors. All results
of randomized methods are reported by averaging the out-
comes obtained from 20 different runs. In the following, we
only report representative results. More results, implemen-
tation details and extensions to the case with known corre-
spondences can be found in the supplementary material.

4.1. Synthetic Data

We ﬁrst evaluate the performance of SDRSAC on syn-
thetically generated data. The bunny point cloud from the
Stanford dataset2 is loaded and uniformly sampled to yield
a source point cloud S containing Ns = 10, 000 points. To
generate the target set D , we apply a random transforma-
tion ( ˜R, ˜t) to S . Each point in D is then perturbed with a
Gaussian noise of zero mean and variance of σnoise = 0.01.
To simulate partial overlapping, we randomly pick and re-
move r% of the points in D . In order to evaluate the perfor-
mance of the algorithms with different outlier ratios, we re-
peat the experiments with r = 10, 15, . . . , 50%. The thresh-
old ǫ in (3) was chosen to be 0.01 for all the methods. For
SDRSAC, we choose the sample size to be Nsample= 16,
and m = 4. (The choice of Nsample and m will be stud-
ied in Sec. 4.3). Figure 1 shows the number of matches
(inliers) and run time for all the methods.
It is evident
that SDRSAC outperforms other methods in terms of cor-
respondences obtained with faster (or comparable) execu-
tion time. Note that the run time of SDRSAC is very close
to Super4PCS, though SDRSAC consistently attains higher
solution quality. The performance of ICP, as anticipated, is
unstable due to the effect of initialization (i.e., wrong initial-
ization may lead to poor convergence), as shown in Fig. 1.
Moreover, as the outlier ratio increases, the consensus sizes
produced by most ICP-based methods are poor due to their
non-robustness.

4.2. Real Data

In this section, we evaluate the performance of our pro-
posed method on real datasets and compare it with existing

2 http://graphics.stanford.edu/data/3Dscanrep/

129

Figure 1. Results for experiments with synthetic data. Left: Number of correspondences (Objective of (1)). Right: Run time (in log scale).

Pairs

SDRSAC 4PCS

S-4PCS

ICP

TrICP

IRLS GoICP

TrGoICP

Ofﬁce1 1,2

Ofﬁce1 8, 9

Ofﬁce1 15, 16

Ofﬁce1 51, 52

Living1 1, 2

Living1 25, 26

Living1 54, 55

Living1 32, 33

#Corrs
Time(s)
#Corrs
Time(s)
#Corrs
Time(s)
#Corrs
Time(s)
#Corrs
Time(s)
#Corrs
Time(s)
#Corrs
Time(s)
#Corrs
Time(s)

6990
8.69
4353
9.53
4992
7.56
4490
10.05
5817
8.34
4768
9.27
5558
9.2
5570
8.32

5922
11.18
4184
15.08
4954
15.95
2998
15.45
5602
15.09
3615
15.07
5155
15.05
5459
15.15

6388
10.05
2901
15.06
4933
15.45
3714
15.03
4590
15.39
4327
15.95
5433
15.65
3269
15.23

2433
4.69
2132
4.44
655
4.35
60
4.29
3609
4.26
1350
4.34
4104
4.29
2028
4.65

4455
4.35
1205
4.45
487
4.63
612
4.55
4599
4.96
1364
4.42
4783
4.32
1519
4.44

2429
3.16
2125
4.06
673
4.19
178
4.39
4016
5.13
4240
4.35
4009
5.19
2808
5.1

2664
50.9
2253
63.2
1513
60.3
554
60.5
4060
32.5
4398
65.3
5120
61.2
2334
34.6

6828
33.4
4101
38.3
1413
37.6
848
38.5
5655
35.6
4567
36.1
5250
35.9
5319
33.3

Table 1. Results for real data experiments. For each pairs, the ﬁrst row is the number of correspondences (#Corrs) and the second row
shows the run time in second. Note that S-4PCS represent Super4PCS.

approaches. The input point clouds for this experiment are
obtained from the challenging Redwood 3D dataset [14].
We randomly pick eight pairs of point clouds from the Of-
ﬁce and Living Room repository, then uniformly sampled
the point sets to obtain 10, 000 points on each input set. The
input threshold ǫ in (3) was chosen to be in the range of 0.01
to 0.05 and for each set of input data, ǫ was chosen to be the
same for all benchmarking methods.

Table 1 shows the number of matches and run time for
each method. The sample qualitative results are displayed
in Figure 2. As can be seen, on average, SDRSAC ob-
tains higher matches compared to other competitors. We
also observe that local methods such as ICP or IRLS can
sometimes converge fast to very good results, but are un-
stable due to the effect of bad initializations. The robust
version TrGo-ICP performs slightly better than Go-ICP, but

130

Figure 2. Examples of alignments using SDRSAC. From top to
bottom: Ofﬁce 8 and Ofﬁce 9; LivingRoom 1 and LivingRoom 2.

also fails in some cases due to high outlier ratios residing in
the input data.

4.3. Ablation Studies

This section analyzes the effect of different parameter
settings to the performance of our proposed method, and
suggest the choices for the hyperparameters.

4.3.1 Effect of Sample Size Nsample

Figure 3. Performance analysis for different values of Nsample .
Left: Consensus size, Middle:# Iterations, Right: Run time.

Nsample

Fig. 3 plots the accuracy (consensus size), number of it-
erations (to satisfy the stopping criterion (14)), and the total
run time when Nsample increases from 8 to 18 and m = 4
for a synthetic dataset (generated as described in Sec. 4.1)
with N = 10, 000 and pI ≈ 30% (the plotted results are the
median over 20 runs). Our obtained results conform to sta-
tistical theory [24]. Speciﬁcally, the inlier rates in the sub-
sets of Nsample points have a mean of pI and standard devi-
ation (SD) of q pI (1−pI )
. From Fig. 3, we observe that for
small Nsample (Nsample< 16), the obtained consensus sizes
are low and the algorithm requires a large number of itera-
tions. In particular, if Nsample
is small, then SD of the in-
lier rates is large among sampled subsets, thus some subsets
may be contaminated with large numbers of outliers, which
affects the quality and stability of our algorithm. When
Nsample reaches around 16, SD is q 0.3∗0.7
16 ≈ 0.1, hence the
inlier rates vary only slightly among subsets, leading to a
stable performance, as shown in Fig. 3 (left, middle). While
it is good to use large Nsample
from the statistical point of
view, SDP solver would take more time at each iteration for
large Nsample , resulting in longer overall run time as shown
in Fig. 3 (right). Empirically, we found that Nsample = 16
provides a good trade-off between algorithm stability (ac-
curacy) and run time in many settings.

4.3.2 Effect of m

This experiment is conducted to study on the effect of m
(introduced in Sec. 3.1) on the solution quality and run time
of our approach. We repeat the experiment for a synthetic
dataset containing N = 5, 000 points per point cloud with
10% outliers. The sample size Nsample is set to 16. All
results are obtained as median over 20 runs. Fig. 4 (left)
plots the number of inliers obtained at termination (using

the stopping criterion (14)). Observe that the performance
is quite stable at different values of m, which demonstrates
the effectiveness of SDP to reject outliers and the strengths
of our proposed sampling scheme.
In Fig. 4 (right), we
also plot the run time required until termination. Appar-
ently, although the solution qualities for different values of
m are similar as discussed above, the run time is affected by
m. Speciﬁcally, as m increases, SDRSAC takes more itera-
tions before the stopping criterion (14) is met. This can be
explained by recalling from (13) that large m increase the
failure probability pf . Moreover, for any sampled subset,
the expected number of inliers is Nsample × pI , hence if m
is larger than this value, outliers may be included. In our
experiments, the choice of m = 4 works best in most sce-
narios (the minimal case of m = 3 may not be very stable
due to noise in 3D data points and the 3 correspondences
could be near each other spatially).

Figure 4. Plots of consensus sizes (left) and run times (right) over
different values of m.

5. Conclusions

We have presented a novel and efﬁcient randomized ap-
proach for robust point cloud registration without corre-
spondences. We apply graph matching formulation for the
correspondence problem. We propose a novel cost function
and a tight SDP relaxation scheme. We embed the formu-
lation into a new sampling strategy which samples larger-
than-minimal subsets. Extensive experiments with several
synthetic and real datasets show that our algorithm is com-
petitive. Our framework can also be extended for the prob-
lems with known correspondences.

Acknowledgement

This work was supported in part by both ST Electronics
and the National Research Foundation(NRF), Prime Minis-
ter’s Ofﬁce, Singapore under Corporate Laboratory @ Uni-
versity Scheme (Programme Title: STEE Infosec - SUTD
Corporate Laboratory).

References

[1] Khurrum Aftab and Richard Hartley. Convergence of iter-
atively re-weighted least squares to robust m-estimators. In
Applications of Computer Vision (WACV), 2015 IEEE Winter
Conference on, pages 480–487. IEEE, 2015.

131

[2] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si-
mon, Brian Curless, Steven M Seitz, and Richard Szeliski.
Building rome in a day. Communications of the ACM,
54(10):105–112, 2011.
[3] Sameer Agarwal, Noah Snavely, and Steven M Seitz. Fast
algorithms for l problems in multiview geometry. In Com-
puter Vision and Pattern Recognition, 2008. CVPR 2008.
IEEE Conference on, pages 1–8. IEEE, 2008.
[4] Dror Aiger, Niloy J Mitra, and Daniel Cohen-Or. 4-points
congruent sets for robust pairwise surface registration.
In
ACM Transactions on Graphics (TOG), volume 27, page 85.
ACM, 2008.
[5] Aitor Aldoma, Zoltan-Csaba Marton, Federico Tombari,
Walter Wohlkinger, Christian Potthast, Bernhard Zeisl,
Radu Bogdan Rusu, Suat Gedikli, and Markus Vincze. Tuto-
rial: Point cloud library: Three-dimensional object recogni-
tion and 6 dof pose estimation. IEEE Robotics & Automation
Magazine, 19(3):80–91, 2012.
[6] Per Bergstr ¨om and Ove Edlund. Robust registration of point
sets using iteratively reweighted least squares. Computa-
tional Optimization and Applications, 58(3):543–561, 2014.
[7] Per Bergstr ¨om and Ove Edlund. Robust registration of sur-
faces using a reﬁned iterative closest point algorithm with
a trust region approach. Numerical Algorithms, 74(3):755–
779, 2017.
[8] Paul J Besl and Neil D McKay. Method for registration of
3-d shapes.
In Sensor Fusion IV: Control Paradigms and
Data Structures, volume 1611, pages 586–607. International
Society for Optics and Photonics, 1992.
[9] Thomas M Breuel. Implementation techniques for geometric
branch-and-bound matching methods. Computer Vision and
Image Understanding, 90(3):258–294, 2003.
[10] Rainer E Burkard and Eranda Cela. Linear assignment prob-
lems and extensions.
In Handbook of combinatorial opti-
mization, pages 75–149. Springer, 1999.
´Alvaro Parra Bustos, Tat-Jun Chin, Anders Eriksson, Hong-
dong Li, and David Suter. Fast rotation search with stereo-
graphic projections for 3d registration.
IEEE Transactions
on Pattern Analysis and Machine Intelligence, 38(11):2227–
2240, 2016.
[12] Zhipeng Cai, Tat-Jun Chin, Huu Le, and David Suter. De-
terministic consensus maximization with biconvex program-
ming. In Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 685–700, 2018.
[13] Dmitry Chetverikov, Dmitry Svirko, Dmitry Stepanov, and
Pavel Krsek. The trimmed iterative closest point algorithm.
In Pattern Recognition, 2002. Proceedings. 16th Interna-
tional Conference on, volume 3, pages 545–548. IEEE, 2002.
[14] Sungjoon Choi, Qian-Yi Zhou, and Vladlen Koltun. Robust
reconstruction of indoor scenes. In Computer Vision and Pat-
tern Recognition (CVPR), 2015 IEEE Conference on, pages
5556–5565. IEEE, 2015.
[15] Chin Seng Chua and Ray Jarvis. Point signatures: A new
representation for 3d object recognition. International Jour-
nal of Computer Vision, 25(1):63–85, 1997.
[16] Ondrej Chum and Jiri Matas. Matching with prosac-
progressive sample consensus. In 2005 IEEE Computer So-

[11]

ciety Conference on Computer Vision and Pattern Recogni-
tion (CVPR’05), volume 1, pages 220–226. IEEE, 2005.
[17] Ondˇrej Chum, Jiˇr´ı Matas, and Josef Kittler. Locally opti-
mized ransac. In DAGM. Springer, 2003.
[18] Bertram Drost, Markus Ulrich, Nassir Navab, and Slobodan
Ilic. Model globally, match locally: Efﬁcient and robust 3d
object recognition. In Computer Vision and Pattern Recog-
nition (CVPR), 2010 IEEE Conference on, pages 998–1005.
Ieee, 2010.
[19] Nadav Dym, Haggai Maron, and Yaron Lipman. Ds++: a
ﬂexible, scalable and provably tight relaxation for match-
ing problems.
ACM Transactions on Graphics (TOG),
36(6):184, 2017.
[20] David W Eggert, Adele Lorusso, and Robert B Fisher. Esti-
mating 3-d rigid body transformations: a comparison of four
major algorithms. Machine vision and applications, 9(5-
6):272–290, 1997.
[21] Olof Enqvist, Klas Josephson, and Fredrik Kahl. Optimal
correspondences from pairwise constraints. In Computer Vi-
sion, 2009 IEEE 12th International Conference on, pages
1295–1302. IEEE, 2009.
[22] Martin A Fischler and Robert C Bolles. Random sample
consensus: a paradigm for model ﬁtting with applications to
image analysis and automated cartography. Communications
of the ACM, 24(6):381–395, 1981.
[23] Fajwel Fogel, Rodolphe Jenatton, Francis Bach, and Alexan-
dre d’Aspremont. Convex relaxations for permutation prob-
lems.
In Advances in Neural Information Processing Sys-
tems, pages 1016–1024, 2013.
[24] David Forsyth. Probability and Statistics for Computer Sci-
ence. Springer, 2018.
[25] Ran Gal and Daniel Cohen-Or. Salient geometric features
for partial shape matching and similarity. ACM Transactions
on Graphics (TOG), 25(1):130–150, 2006.
[26] Richard Hartley and Andrew Zisserman. Multiple view ge-
ometry in computer vision. Cambridge university press,
2003.
[27] Peter J Huber et al. Robust estimation of a location param-
eter. The Annals of Mathematical Statistics, 35(1):73–101,
1964.
[28] M Hullin, R Klein, T Schultz, and A Yao. Efﬁcient lifted
relaxations of the quadratic assignment problem. 2017.
[29] Wolfgang Kabsch. A solution for the best rotation to re-
late two sets of vectors. Acta Crystallographica Section A:
Crystal Physics, Diffraction, Theoretical and General Crys-
tallography, 32(5):922–923, 1976.
[30] Itay Kezurer, Shahar Z Kovalsky, Ronen Basri, and Yaron
Lipman. Tight relaxation of quadratic matching. In Com-
puter Graphics Forum, volume 34, pages 115–128. Wiley
Online Library, 2015.
[31] Huu Le, Tat-Jun Chin, and David Suter. An exact penalty
method for locally convergent maximum consensus. In Com-
puter Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on. IEEE, 2017.
[32] Huu Le, Tat-Jun Chin, and David Suter. Ratsac-random tree
sampling for maximum consensus estimation.
In 2017 In-
ternational Conference on Digital Image Computing: Tech-
niques and Applications (DICTA), pages 1–8. IEEE, 2017.

132

[33] Huu Le, Anders Eriksson, Michael Milford, Thanh-Toan Do,
Tat-Jun Chin, and David Suter. Non-smooth m-estimator for
maximum consensus estimation. 2018.
[34] D Khu ˆe L ˆe-Huu and Nikos Paragios. Alternating direction
graph matching.
In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 4914–4922.
IEEE, 2017.
[35] Marius Leordeanu and Martial Hebert. A spectral technique
for correspondence problems using pairwise constraints. In
Computer Vision, 2005. ICCV 2005. Tenth IEEE Interna-
tional Conference on, volume 2, pages 1482–1489. IEEE,
2005.
[36] Hongdong Li and Richard Hartley. The 3d-3d registration
problem revisited.
In Computer Vision, 2007. ICCV 2007.
IEEE 11th International Conference on, pages 1–8. IEEE,
2007.
[37] Feng Lu and Evangelos Milios. Globally consistent range
scan alignment for environment mapping.
Autonomous
robots, 4(4):333–349, 1997.
[38] Nicolas Mellado, Dror Aiger, and Niloy J Mitra. Super 4pcs
fast global pointcloud registration via smart indexing.
In
Computer Graphics Forum, volume 33, pages 205–215. Wi-
ley Online Library, 2014.
[39] NS Mendelsohn and AL Dulmage. The convex hull of sub-
permutation matrices. Proceedings of the American Mathe-
matical Society, 9(2):253–254, 1958.
[40] Jeff M Phillips, Ran Liu, and Carlo Tomasi. Outlier robust
icp for minimizing fractional rmsd. In 3-D Digital Imaging
and Modeling, 2007. 3DIM’07. Sixth International Confer-
ence on, pages 427–434. IEEE, 2007.
[41] Pulak Purkait, Christopher Zach, and Anders Eriksson. Max-
imum consensus parameter estimation by reweighted l1
methods.
In International Workshop on Energy Minimiza-
tion Methods in Computer Vision and Pattern Recognition,
pages 312–327. Springer, 2017.
[42] Rahul Raguram, Ondrej Chum, Marc Pollefeys, Jiri Matas,
and Jan-Michael Frahm. Usac: a universal framework for
random sample consensus.
IEEE transactions on pattern
analysis and machine intelligence, 35(8):2022–2038, 2013.
[43] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz.
Fast point feature histograms (fpfh) for 3d registration.
In
Robotics and Automation, 2009. ICRA’09. IEEE Interna-
tional Conference on, pages 3212–3217. IEEE, 2009.
[44] Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited.
In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4104–4113, 2016.
[45] Paul Scovanner, Saad Ali, and Mubarak Shah. A 3-
dimensional sift descriptor and its application to action
recognition. In Proceedings of the 15th ACM international
conference on Multimedia, pages 357–360. ACM, 2007.
[46] Ben J Tordoff and David W Murray. Guided-mlesac: Faster
image transform estimation by using matching priors. IEEE
transactions on pattern analysis and machine intelligence,
27(10):1523–1535, 2005.
[47] Philip HS Torr and Andrew Zisserman. Mlesac: A new ro-
bust estimator with application to estimating image geome-

try. Computer vision and image understanding, 78(1):138–
156, 2000.
[48] Ngoc-Trung Tran, Dang-Khoa Le Tan, Anh-Dzung Doan,
Thanh-Toan Do, Tuan-Anh Bui, Mengxuan Tan, and Ngai-
Man Cheung. On-device scalable image-based localization
via prioritized cascade search and fast one-many ransac.
IEEE Transactions on Image Processing, 28(4):1675–1690,
2019.
[49] Quoc Huy Tran, Tat-Jun Chin, Wojciech Chojnacki, and
David Suter. Sampling minimal subsets with large spans for
robust estimation. International journal of computer vision,
106(1):93–112, 2014.
[50] Jiaolong Yang, Hongdong Li, Dylan Campbell, and Yunde
Jia. Go-icp: a globally optimal solution to 3d icp point-set
registration. IEEE transactions on pattern analysis and ma-
chine intelligence, 38(11):2241–2254, 2016.
[51] Liuqin Yang, Defeng Sun, and Kim-Chuan Toh. Sdpnal
+ +: a majorized semismooth newton-cg augmented la-
grangian method for semideﬁnite programming with non-
negative constraints. Mathematical Programming Compu-
tation, 7(3):331–366, 2015.
[52] Hao Zhang, Wenjun Zhou, Christopher Reardon, and
Lynne E Parker. Simplex-based 3d spatio-temporal feature
description for action recognition.
In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 2059–2066, 2014.

133

Self-supervised 3D hand pose estimation through training by ﬁtting

Chengde Wan1 , Thomas Probst1 , Luc Van Gool1,3 , and Angela Yao2

1ETH Z ¨urich 2National University of Singapore 3KU Leuven

Abstract

We present a self-supervision method for 3D hand pose
estimation from depth maps. We begin with a neural net-
work initialized with synthesized data and ﬁne-tune it on
real but unlabelled depth maps by minimizing a set of data-
ﬁtting terms. By approximating the hand surface with a set
of spheres, we design a differentiable hand renderer to align
estimates by comparing the rendered and input depth maps.
In addition, we place a set of priors including a data-driven
term to further regulate the estimate’s kinematic feasibility.
Our method makes highly accurate estimates comparable to
current supervised methods which require large amounts of
labelled training samples, thereby advancing state-of-the-
art in unsupervised learning for hand pose estimation.

1. Introduction

Deep learning has signiﬁcantly advanced state-of-the-art
for 3D hand pose estimation. The improvement in the accu-
racy of learning-based approaches can be attributed to two
factors: choices in network design, and increased amounts
of labelled data [29, 49, 53]. However, acquiring accu-
rate 3D hand pose labels can be extremely difﬁcult; current
methods require marker-based motion capture [13], 6DoF
sensors [53], or multi-view model-based tracking [45]. In
all of these cases, careful supervision and manual cleaning
of the labels is additionally necessary for high quality anno-
tations. A commonly proposed alternative is to synthesize
training samples – this is much easier, comes at virtually
no cost and a variety of viewpoints, poses, and hand shapes
can be generated. Unfortunately, the domain shift in terms
of appearance, hand shape, and pose from synthesized to
real data samples is highly non-trivial. As we show in our
experiments, models trained on synthetic data exhibit a sig-
niﬁcant drop in accuracy when applied to real data.
A recent trend in hand pose estimation is to combine the
beneﬁts of learning-based discriminative approaches with
model-based tracking methods [2, 13, 33, 43]. Model-based
tracking casts pose estimation as a frame-wise model-ﬁtting

Figure 1. Method overview. (a) Outline of the proposed method;
(b) Rendered depth map from estimation through differentiable
renderer; (c) Corresponding input depth maps.

problem and requires no training data. Learning-based ap-
proaches can then be incorporated either by providing bet-
ter initialization [33, 43], or by supporting the ﬁtting with
key-point estimations [2, 13] to reduce the risk of getting
trapped in local minima.
In this paper, we provide an alternative way to exploit
the complementary beneﬁts of unsupervised model-ﬁtting
and data-driven learning approaches. Speciﬁcally, we pro-
pose a self-supervision method for learning 3D hand pose
from depth maps. Note that our approach does not re-
quire any manual annotation, either from labelling key-
points [24, 54] or from manual initialization of a model-
based tracker [40, 45]. At the same time, we are able to
achieve accuracy comparable with current state-of-the-art
that requires large amounts of labeled data.
Our proposed method supervises itself with a set of care-
fully designed differentiable model-ﬁtting terms. Follow-
ing a discriminative paradigm, hand poses are estimated
with a deep neural network. During training,
the net-
work learns to make more accurate estimates by back-

110853

propagating model-ﬁtting errors that update network pa-
rameters. Unlike conventional model-based tracking, the
hand poses in our method are not optimized in each frame
independently; instead, the model-ﬁtting error is minimized
jointly over a large set of unlabelled images. As training
progresses, the network learns to generalize from previous
optimization results. We additionally feed the network with
synthesized labelled data to avoid local minima and regu-
larize the learning process. Interestingly, although we train
without any (human-provided) labels, our method exhibits
behaviour similar to traditional supervised methods: the ac-
curacy steadily improves with increasing amounts of avail-
able data. For inference, our method is highly efﬁcient, with
only one forward pass through the network.
In contrast,
model-based tracking may need several optimization steps,
especially in scenes with fast motions and large movements.
Our proposed model-ﬁtting term penalizes the distance
between the estimated hand surface to 3D points from the
input depth map. For efﬁciency, we approximate the hand
surface with a set of spheres (see Fig. 2) as used in [30].
This enables a fast and differentiable computation of the
surface projection. For additional supervision, we train our
network under a multi-view setup and apply a consistency
loss term to overcome the ambiguities of self-occlusion. Fi-
nally, we penalize infeasible joint conﬁgurations by apply-
ing a variational auto-encoder (VAE) based prior.
We observe that previous methods which parameterize
poses with joint angles [6, 55] tend to be sensitive to errors
in parent node estimation. Further difﬁculty is introduced if
one attempts to solve these angles via regression. As such,
we directly estimate the 3D coordinates of the sphere cen-
ters. This conveniently allows us to work within a detection
framework of fully convolutional networks (FCNs), which
are used in many state-of-the-art methods [14, 41] and are
more accurate than regression approaches. In contrast,
Our contributions can be summarized as follows:

• We propose a self-supervised method for 3D hand pose
estimation from depth maps. Without any manual la-
bels, the method achieves results comparable to state-
of-the-art that requires large amounts of annotation.

• We propose a novel approach to couple unsupervised
model-based ﬁtting with supervised discriminative ap-
proaches for hand pose estimation.

• We provide a way to regularize kinematic feasibility
in FCNs by placing a set of carefully designed priors,
including a data-driven term learned by a VAE.

2. Related Works

Discriminative approaches. As a general trend, ever
deeper and more sophisticated neural network architectures
are dominating hand pose estimation methods. They are

highly accurate [4, 5, 9, 11, 12, 20, 23, 31, 50] when trained
with large amounts of labeled samples. However, given that
accurate 3D annotations are extremely difﬁcult to obtain, a
number of works approach the problem with deep genera-
tive models to leverage unlabelled data [1, 3, 21, 28, 29, 36,
49]. Synthesizing depth maps ensures accurate annotations
and seem to be a promising alternative but methods that rely
only on synthesized data suffer from the large domain shift
and actually perform much worse than when trained on less
accurate real data [31, 34, 6]. To reduce the domain gap,
Rad et al. [31] have proposed a domain adaptation method
that tries to minimize feature differences from synthesized
versus real images.

Simulated and unsupervised approaches. One line of
work [6, 34] considers the more challenging setting of not
using any manual labels. These models are trained on la-
beled but synthesized data and unlabeled real data. Shrivas-
tava et al. [34] train a generative adversarial network with
unlabelled depth maps to augment synthetic inputs with
more realistic noise patterns. While the synthesized images
better resemble images from real depth cameras, the domain
gap beyond appearance, especially in hand pose and shape
remain unsolved.

Dibra et al. [6] ﬁne-tune a network trained on synthe-
sized depth maps with unlabeled real data by minimizing a
differentiable model-ﬁtting error. While the ﬁne-tuning im-
proves the initial network accuracy, current state-of-the-art
still outperforms the ﬁne-tuned network by a large margin.
Our approach is similar to [6] in that we also initialize the
network with synthetic data and ﬁne-tune with a data-ﬁtting
error. However, our method has several key differences in-
cluding the hand model and model parameterization. We
discuss the differences in detail in Section 4.3.

Hybrid approaches. Conventional model-based track-
ers incorporate discriminative models either for robust ini-
tialization [33, 43] or to augment the observation-based
ﬁt [2, 37]. Tompson et al. [45] estimates accurate hand
poses with an expensive ofﬂine tracker and trains a network
with those estimations for efﬁcient online inference. More
recently, focus has shifted to the integration of differentiable
kinematic model representations such as forward kinemat-
ics [6, 16, 55] and linear blend skinning [6] into neural net-
works for end-to-end training.

Self supervision. A growing body of work tries to train
neural networks without any human supervision. Com-
mon strategies include leveraging spatial or temporal con-
text [7, 27], colour [18, 48], alignment [8] and privileged
information from other modalities[26]. Recently, supervi-
sion from multiple views has been applied successfully to
object key-point discovery [42], 3D reconstruction [47, 46],
body pose estimation [32], and hand pose estimation [35].
We follow this line of work using multi-view supervision
to resolve self-occlusions of the hand. In addition, beyond

210854

Figure 2. Hand model used during model ﬁtting. Left: Our hand
model approximation with 41 spheres; Middle: Real depth map;
Right: Rendered depth map from differentiable ray-tracing.

enforcing multi-view consistency as in [32, 35], our differ-
entiable depth renderer enables a dense and more detailed
consistency error term by projecting hypotheses from one
viewpoint to another.

3. Method

3.1. Hand model

We approximate the 3D hand surface with N=41 spheres
(11 for the palm, 6 for each ﬁnger, see Figure 2(a)) similar
to [30, 38, 39]. The sphere model M is parameterized as
M = {m(0) · · · m(N ) }, where m(i) = (x(i) , y (i) , z (i) , r(i) )
is the ith sphere centered at (x(i) , y (i) , z (i) ) with radius r(i) .
The radii are predeﬁned and remain unchanged during train-
ing. To this end, hand pose estimation is formulated as esti-
mating a set of 3D key point coordinates.

3.2. Pose estimation network

Given a depth map, the pose estimation network tries
to estimate the 3D coordinates of all N sphere centers. To
adapt the FCN for 3D coordinate estimation, we follow the
strategy of [14, 41, 42]. More speciﬁcally, the FCN re-
gresses a heatmap h2D of the 2D projections of the 3D
points and a latent depth map hdepth encoding the point’s
depth information. The 3D coordinates are then recovered
by integrating over the heatmap and latent depth map:
(x, y) =Xxi Xyi
z =Xxi Xyi
where xi and yi are the ith pixel’s coordinates on the 2D
grid and α is the annealing factor. We set as α = 10 in rest
of the paper. The softmax function serves as an approxi-
mation to argmax. We refer the reader to [14, 41, 42] for
more details. We use the hourglass network[22] as our FCN
architecture.

hdepth (xi , yi )softmax(αh2D )(xi , yi ) (2)

(xi , yi )softmax(αh2D )(xi , yi )

(1)

3.3. Network initialization

We initialize the network by training on a synthesized
dataset. Depth maps are synthesized from the hand model
provided by [45] according to the sampling strategy of [33]
for generating random hand poses. More details are pro-
vided in the Supplementary Materials. During training, 20
million synthesized depth maps with different poses and
view points are generated online and fed to the network.
The training loss is formulated as a mean squared error
between the estimated 3D coordinates integrated from h2D
and hdepth (see Equations 1 and 2), and the ground truth
coordinates, similar to [14, 19, 41]. Alternatively, as done
in [42], supervision can be provided directly at the level
of h2D and hdepth . The j th 3D ball’s label for h(j )
2D is a
2D heatmap centered around the 2D projection of the ball.
depth at the v th column and the uth row is generated as
depth (v , u) = (dj
where dj is the depth of the j th joint. In preliminary ex-
periments we found that both strategies perform similarly
and for the rest of the paper, we use the latter strategy to be
analogous to [42].

if h(j )
2D (v , u) > 0,
others;

h(j )

h(j )

0

(3)

3.4. Self(cid:173)supervision by model ﬁtting

For a given depth map of a hand, we begin by estimat-
ing the spheres’ center coordinates. From these coordinates
we render the spheres and evaluate how well the they ﬁt
with the input depth map according to an energy function.
We use a differentiable rendering process that allows back-
propagation of errors from the energy function for gradual
ﬁne-tuning of the estimations.
At ﬁrst glance, our overall process resembles conven-
tional model-based tracking. However, our method is fun-
damentally different in that we are optimizing over neural
network parameters rather than pose parameters. We share
the beneﬁts of data-driven approaches because we minimize
the model-ﬁtting error over an entire set of unlabeled depth
maps, rather than ﬁtting frames independently as done in
model-based tracking. As shown in our experiments (see
section 4.2), the joint optimization over a set of data gives
rise to accuracy improvements when the set size increases.
In this context, the model-ﬁtting energy can be directly in-
terpreted as a self-supervised training loss. The trained net-
work generalizes from previously estimated samples while
still leveraging supervision from synthesized labeled data.
Moreover, the method enables efﬁcient inference, using
only one forward pass, whereas model-based tracking needs
initialization and multiple iterations of optimization.
The self-supervised training loss, composed of data and

310855

prior terms, is deﬁned as follows,

L(θ) =Lm2d (θ) + λ1Ld2m (θ) + λ2Lmultiview (θ)

data terms
+ λ3Lvae (θ) + λ4Lbone length (θ) + λ5Lcollision

(4)

(θ),

|
|

{z
{z

prior terms

}

}

where θ are network parameters.

3.4.1 Data terms

(5)

|G(f (Di |θ)) − Di | ,

The model to data term Lm2d aligns the spheres as close as
possible to the surface points in the depth map. We deﬁne it
as the L1 distance between the input depth map Di and the
rendered depth map
Lm2d (θ) = Xi
where f (·|θ) estimates the coordinates of sphere centers
from input depth map and G(·) renders the estimate to a
depth map. The L1 loss makes the training robust to holes
in the input depth map. The rendered depth map is a com-
posite of all the spheres rendered independently followed by
a min pooling in the z dimension across the spheres. The
min pooling serves as a depth buffer check in a standard
rendering pipeline. For each pixel on the rendered image,
the rendering process checks if the corresponding ray inter-
sects with the sphere and calculates the depth at the point
of intersection. We apply an orthographic projection for the
rendering. The rays are perpendicular to the image plane,
and the depth at pixel (u, v) for j th sphere m(j ) with radius
r(j ) centered at (x(j ) , y (j ) , z (j ) ) w.r.t.
the camera coordi-
nate frame is calculated as
g(m(j ) )uv = (z (j ) −p(r(j ) )2 − d2 (u, v)
where

dfar

(6)

if d(u, v) ≤ r,
others,

d(u, v) = k(x(j ) , y (j ) ) − Π−1 (u, v)k2

(7)

measures the distance between the ray and sphere center
and Π−1 (u, v) is the inverse orthographic projection. The
rendering process G of an estimated set of spheres Mi can
be formulated as

are no gradients on the background of the rendered depth
map according to Equation 6, the modle to data term Lm2d
alone cannot push the model towards unexplained points on
the input depth map. This is taken care of by Ld2m which
works in the spirit of ICP by minimizing the distance be-
tween every point p from the depth map Di and its projec-
tion on to the estimated hand model surface Mi ,

Ld2m (θ) = Xi Xp∈Di

d(p, ΠMi (p)).

(9)

The distance d(p, ΠMi (p)) is estimated as the distance to
every single sphere m(j ) with radius r(j ) centered at c(j ) as

d(p, m(j ) ) = abs(kp − c(j ) k2 − r(j ) )

(10)

and takes the minimum among all balls as

d(p, ΠMi (p)) = min

j

d(p, m(j ) ).

(11)

The multi-view consistency term Lmultiview provides su-
pervision from multiple viewpoints; this mitigates the ambi-
guities and errors that arise from the frequent self-occlusion
of hands. For training we assume a calibrated multi-camera
set-up and maintain consistency between the viewpoints in
two ways. Firstly, as shown in Figure 3, we project the cen-
ters of spheres estimated from view vi to view vj and eval-
uate Ld2m and Lm2d with respect to the depth map captured
from view vj . To further propagate estimations from dif-
ferent viewpoints, an additional multi-view term is deﬁned
as:

kTv (c(j,v)

i

) − ¯c(j )

i k2

2 ,

(12)

Lmultiview (θ) = Xi Xj Xv
i

i

where c(j,v)
is the center of j th sphere estimated from the ith
depth map with the v th camera. Tv (·) projects points from
view v to a canonical frame while ¯c(j )
is the correspond-
ing robust average from the multi-view estimations in the
canonical frame. The robust average ¯c(j )
can be either the
median or a selected cj
i based on the heat map with mini-
mal variance. In preliminary tests, we found both strategies
perform equally well and we use the median for our ex-
periments. In contrast, simply averaging over the multiple
views degrades results as the mean is sensitive to outliers.

i

G(Mi ) = min

j

g(m(j )
i ).

(8)

3.4.2 Prior terms

Note that the entire rendering process is fully differen-
tiable and can be implemented easily with any deep learn-
ing framework. The rendered primitive depth map and its
input depth map counterpart are shown in Fig.2.
The data to model term Ld2m is a registration loss be-
tween the estimated model and input depth map. Since there

Because we do not have a skeleton model, we cannot en-
force conventional kinematic constraints such as joint an-
gle ranges. As an alternative, we adopt a data-driven ap-
proach to encourage the estimated sphere positions to form
a kinematically feasible pose. More speciﬁcally, we apply a
vae term Lvae which aims to maximize the likelihood lower

410856

bound of the hand pose conﬁguration. Since the sphere po-
sitions inherently populate only a subspace, we train a vari-
ational auto-encoder (VAE)[17] over the estimated sphere
centers to learn this latent space. The pose training samples
of the VAE are generated by the same kinematics model and
sampling strategy as used in Section 3.3 and is therefore un-
supervised. We use the standard variational lower bound to
ensure that the estimated pose parameters lie in the learned
subspace as follows,
Lvae (θ) = Xi
(13)
where f (·|θ) is the pose estimation network. We refer the
reader to [17] for more details on learning and inference in
the VAE. The VAE is trained in advance and its weights
remain unchanged during training.
The bone length term Lbone length ensures that distances
between two bone end points remain unchanged:

Ez∼Q [log P (cid:0)f (Di |θ)|z(cid:1)]−DKL [Q(z )||P (z )],

Lbone length = Xi,j,k

where

max(djk

i −ljk

max , 0)2+max(ljk

min−djk

i

djk

i = kc(j )
i − c(k)
i k2

, 0)2 ,

(14)

(15)

i

measures the estimated bone length with the estimated end
points c(j )
and c(k)
from the ith input. (cid:2)ljk
max (cid:3) are pre-
deﬁned ranges based on different skeleton sizes.
The collision term Lcollision penalizes self collisions be-
tween the j th and k th sphere as follows,

min , ljk

i

Lcollision = Xi,j,k

max(r(j ) + r(k) − kc(j )

i − c(k)
i k2

2 , 0). (16)

4. Experimentation

We evaluate our method on the NYU Hand Pose
Dataset [45], which is currently the only publicly available
multi-view depth dataset. The dataset, captured by 3 cal-
ibrated and synchronized PrimeSense depth cameras, con-
sists of 72757 × 3 training frames and 8252 × 3 for testing.
NYU is a challenging dataset with a wide coverage of hand
poses. We apply the ground-truth annotation only to calcu-
late camera extrinsics. In addition, we synthesize a dataset
of 20K depth maps to evaluate how well the trained network
can generalize to (new) synthesized samples.
We quantitatively evaluate with two standard metrics:
mean joint position error (in mm) averaged over all joints
and frames, and the percentage of successful frames, i.e.
those in which all joint predictions are within a certain
threshold [44]. Qualitative results of the estimated hand
poses and their sphere model renderings from other view-
points by the differentiable renderer are shown in Figure 3.
By default, we report the result using a single stack hour-
glass network.

One should note that in our self-supervised scenario, the
pose estimation error on the training set is fundamentally
different than the training error in standard supervised train-
ing. In self-supervised learning, we are minimizing a data-
ﬁtting error, which should be correlated with pose estima-
tion accuracy, but there are no guarantees that lower errors
will give rise to more accurate pose estimates.

4.1. Training with only synthesized data

We ﬁrst evaluate how a network trained on purely syn-
thesized data generalizes to new synthesized samples and as
well as real depth maps. Table 1 (synth, synth(test on test)
) and Figure 5 (cyan dotted line) shows that this network
is highly accurate on unseen synthesized samples; 78% of
frames have maximum errors less than 20mm and the mean
joint error is only 6.76mm. However, the accuracy deteri-
orates dramatically when testing on real-world depth maps
– the mean average joint error increases almost four-fold to
27.85mm. If we augment the synthesized depth maps with
random noise (synth, aug. with noise), we can reduce the
mean joint error to 22.65mm. This shows that the neural
network easily over-ﬁts to certain local patterns; we specu-
late that it is likely rasterization artifacts on the synthesized
depth maps. We use the network trained from synthesized
depth maps augmented with noise as the initial network for
subsequent experiments in this section.

4.2. Ablation studies

Impact of multi-view supervision. To what extent do
multiple viewpoints help with self-supervision? We ﬁrst de-
sign a single-view baseline (100% single view in Figure 6
and Table 1) without the multi-view consistency term and
without projecting the pose estimates to other viewpoints.
Since we use the median as the robust average in measuring
multi-view consistency (see Equ. 12), using only 2 views is
not applicable. In the subsequent experiments, we denote
the setting of using 3 views, all prior terms, and the data
ﬁtting losses Lm2d and Ld2m , as ‘multi-view’.
When comparing to the results of training under a multi-
view set up (100% multi-view in Figure 6 and Table 1),
accuracy degenerates for both mean joint error and per-
centage of successful frames within the error threshold be-
low 50mm. This baseline shows that our current self-
supervision strategy under a monocular setup is insufﬁcient
to resolve the hand’s self-occlusions – which can be quite
extreme. Yet when comparing with the initial results of
training from synthetic data, there is signiﬁcant improve-
ment. The mean average error decreases from 22.65mm to
17.79mm (see Table 1), which validates the effectiveness of
our single view’s self-supervision terms.
Requiring multi-view setups can limit data capture sce-
narios; they cannot be applied to egocentric views and are
hard to apply when the user moves within a large area. This

510857

Figure 3. Qualitative results. (a) Success cases. 3 right columns: estimated poses rendered from different viewpoints by the differentiable
renderer. (b) Failure cases. 3 right columns: rendered estimaties (top rows) from different views and corresponding depth map)

Figure 4. Synthesized depth maps.(a) Synthesized depth map
from polygonal mesh (Sec. 3.3); (b) Synthesized depth maps aug-
mented with random noise; (c) Estimation of unseen synthesized
depth maps; (d) Estimation of real depth maps. The network is
trained with synthesized data augmented with noise in (c) and (d).

D

n

i

h

t
i

w

r

o

r
r

e

s

t

n

i

o

j

l
l

a

h

t
i

w

s
e

m

a

r
f

f

o

e
g
a

t

n
e
c

r

e

P

100%

90%

80%

70%

60%

50%

40%

30%

20%

10%

0%

0

10

real training set
50% real training set
testing set
traing set + testing set
synth(aug. with noise)
synth
synth (test on synth)

20

30
40
50
Max Allowed distance to GT D(mm)

60

70

80

Figure 5. Impact of training data. Percentage of successful
frames with different error thresholds, trained on different train-
ing set. The “synth (test on synth)” is tested on a 20k synthetic
dataset and the rest methods are all tested on the NYU[45] test set.

begs the question whether we can design a more ﬂexible
learning scheme to use a mixture of both multi-view and
single-view depth maps. To that end, we halve the dataset,
where the ﬁrst half has multiple views and the second half
has only a single view and apply these to two additional
baselines. The ﬁrst baseline is trained only on the ﬁrst half
(50% multi-view) and while the second baseline is trained
on both (50% multi-view + 50% single view). As shown
in Figure 6, the additional single-view training data can im-
prove the percentage of successful frames with threshold of
20mm from 22.5% to 35.0% and decreases the mean aver-
age error from 13.77mm to 13.33mm.

We conclude that multi-view set up is critical in self-
supervision to resolve (self) occlusions. Meanwhile, our
single-view self-supervision terms help improve accuracy,
offering ﬂexibility for setups where it is not possible to cap-
ture data from multiple views.

Impact of prior terms. We study the individual con-
tributions of the three priors by training and testing the net-
work without the Lvae (“without vae loss”), Lcollision (“with-
out collision loss”) and Lbone length (“without bone length
loss”) terms. Figure 7 shows that without regulating the
estimated pose with any of these priors, there is a dramatic
decrease on the percentage of successful frames, especially
in the range of error thresholds form 20mm to 40mm. This
validates each of the priors in enforcing kinematically feasi-
ble pose estimates. The corresponding mean joint position
error increases similarly, as shown in Table 1. Interestingly,
the pose estimation is worse in the absence of Lbone length ,

610858

0

10

20

30
40
50
Max Allowed distance to GT D(mm)

60

70

80

0%

10%

20%

30%

40%

50%

60%

70%

80%

90%

100%

P

e

r

n
e
c

t

e
g
a

o

f

r
f

a

m

s
e

w

t
i

a
h

l
l

j

o

i

n

t

s

e

r
r

o

r

w

t
i

h

i

n

D

100% multi-view
100% single view
50% multi-view + 50% single view
50% multi-view

Figure 6. Impact of multi-view supervision. Percentage of suc-
cessful frames within different error thresholds, trained on differ-
ent training set. All methods are tested on the NYU [45] test set.

even if it is still constrained by Lvae . In theory, the learned
VAE should already encode the bone length constraints as
part of the prior. This reveals that the Lvae term alone is in-
sufﬁcient to ensure kinematic feasibility. We note however,
that the current prior terms are just that – they are priors.
They cannot guarantee that pose estimates will meet strict
kinematic constraints, e.g. the same subject has a constant
bone length, or that joint angles do not exceed a predeﬁned
range. If this is desired, inverse kinematics can be added as
a post processing step [45, 52].

Variation in training data. We investigate how differ-
ent training data inﬂuences the resulting network with two
baselines. First, we train only with the 8252×3 testing sam-
ples to check how well self-supervision might (over-)ﬁt the
network to training data. We then trained with a combina-
tion of both the testing and training samples. Finally, we
compare these two setups with our conventional baseline
of training and testing on the originally designated training
and test splits.

Interestingly enough, training directly on the test sam-
ples alone results in a higher mean joint error than when
training on the training samples (14.53mm vs 12.62mm er-
ror on the test samples, see Table 1). Similarly, the per-
centage of successful frames is only 22.5% as opposed to
40.7% at the 20mm error threshold (see Figure 5). We at-
tribute this to the current model ﬁtting terms and optimiza-
tion with back-propagation; it cannot give rise to highly ac-
curate pose estimates with only small amounts of training
data. However, if the amount of training data increases, then
so does the accuracy. These beneﬁts justify a data-driven
based self-supervision approach over conventional model-
based tracking which optimizes each frame independently.
Sure enough, when training with the combined training and
test set, we further improve (marginally, since we only add
about 10% more data), by decreasing mean joint position
error from 12.62mm to 12.31mm and by increasing the per-
centage of successful frames from 40.7% to 44.3%.

0

10

20

30
40
50
Max Allowed distance to GT D(mm)

60

70

80

0%

10%

20%

30%

40%

50%

60%

70%

80%

90%

100%

P

e

r

n
e
c

t

e
g
a

o

f

r
f

a

m

s
e

w

t
i

a
h

l
l

j

o

i

n

t

s

e

r
r

o

r

w

t
i

h

i

n

D

ours
without vae loss
without bone length loss
without collision loss

Figure 7. Impact of prior terms. The percentage of successful
frames with different error thresholds, trained with different prior
terms. All methods are tested on the NYU [45] test set.

Method
ours (100% multiple view)
100% single view
50% multi-view
50% multi-view + 50% single view

Mean joint error
12.62 mm
17.79 mm
13.77 mm
13.33 mm

without vae loss
without bone length loss
without collision loss

14.17 mm
14.56 mm
13.73 mm

50% multi-view
50% multi-view + 50% single view
synth(aug. with noise)
synth
synt(test on synth)
train on test
train on test + train

13.77 mm
13.33 mm
22.65 mm
27.85 mm
6.76 mm
14.53 mm
12.31 mm

Table 1. Ablation study and self comparison. We report mean
joint error averaged over all joints and frames.

0

10

20

30
40
50
Max Allowed distance to GT D(mm)

60

70

80

0%

10%

20%

30%

40%

50%

60%

70%

80%

90%

100%

P

e

r

n
e
c

t

e
g
a

o

f

r
f

a

m

s
e

w

t
i

a
h

l
l

j

o

i

n

t

s

e

r
r

o

r

w

t
i

h

i

n

D

Ours(wt nyu label)
Ours-s2(wt nyu label)
Refine 3D(wt nyu label) (Dibra et. al.)
Point-to-Point (Ge et. al.)
FeatureMapping (Oberweger et. al.)
DenseReg (Wan et. al.)
V2V (Moon et. al.)
DeepPrior++(Oberweger et. al.)
PoseRen (Chen et. al.)
REN (Guo et. al.)

Figure 8. Comparison to state-of-arts. We report the result on
NYU[45] testing set and plot the percentage of frames in which
all joints are below a threshold.

4.3. Comparison to state(cid:173)of(cid:173)the(cid:173)art

We ﬁrst compare our result with, to the best of our
knowledge, the only other unsupervised method [6]. Sim-
ilar to us, [6] uses a CNN and pretrains their network with

710859

synthesized depth maps. This followed by a model-ﬁtting
based ﬁne-tuning. As can be seen in Figure 8, our network
outperforms [6] by a large margin for the percentage of suc-
cessful frames at error thresholds higher than 25mm. We
achieve a much higher accuracy for two reasons. First, our
method exploits the beneﬁts of an FCN, while [6] directly
regressed joint angles; secondly there are no gradients in
their depth term (Eq. 6 in [6]) associated with unexplained
points from the depth map which we handle with Ld2m .
Next, we compare our results to state-of-the-art [4, 10,
11, 12, 20, 23, 25, 49, 50, 51, 55] which train with ground-
truth annotations. Surprisingly, our result outperforms all
of them in terms of percentage of successful frames when
thresholds are larger than 30mm. This again validates self-
supervision and supports the possibility of learning robust
and accurate pose estimation systems without labels.
One drawback of our current model is that under more
stringent error criteria, e.g. when thresholds are 20mm or
less, our accuracy is no longer as good as state-of-the-art.
We attribute this to two causes. First, approximating the
hand surface with only spheres is insufﬁcient and cannot
capture smaller ﬁtting errors. To improve the accuracy, one
will need to use ﬁner models such as a more personalized
hand mesh model though this comes at greater computa-
tional expense [2, 15]. Secondly, the current prior terms,
because they do not place strict kinematic constraints, likely
give rise to small offsets over the joints.
When comparing in terms of the average joint posi-
tion error, our method falls short of current state-of-the-art.
However, we note that the mean joint error, as a mean, can
be slightly biased, in the sense that certain joints are “easier”
to estimate. The ﬁnger roots, palm center and wrist, are less
sensitive to larger offsets than the ﬁnger tips, even though
the tips are more critical for good user experience in real-
world applications. Actually, our ﬁnger-tip accuracy, with
11.77mm(ours) / 12.39mm(ours stack=2), is higher than the
mean joint error once the roots, palm center and wrist joints
are included (12.26mm(ours) / 12.62mm(ours stack=2)).
Currently, [31] reports the highest accuracy to date
(see Table 2 by using domain adaptation techniques to
leverage synthesized data together with labelled real data.
Such a technique is complementary to our proposed self-
supervision strategy; the two combined together are likely
to lead to even higher accuracies in pose estimates.

5. Conclusion

We present a self-supervision method for 3D hand pose
estimation from single depth maps. Our method does not
require any manual annotation yet can produce highly accu-
rate estimates with results competitive to current supervised
state-of-arts. We formulate the training loss of the pose es-
timation network in terms of a data-ﬁtting error whereby
the hand surface is approximated with a set of spheres. By

Method
ours (stack = 1)
ours (stack = 2)
FeatureMapping [31]
V2V [20]
Point-to-Point[11]
DenseReg [50]
DeepPrior++ [23]
Pose-REN [4]
Ren [12]
3DCNN [10]
Lie-X [51]
CrossingNet [49]
Feedback [25]
DeepModel [55]

mean joint error
12.6 mm
12.3 mm
7.4 mm
8.4 mm
9.0 mm
10.2 mm
12.2 mm
11.8 mm
12.7 mm
14.1 mm
14.5 mm
15.5 mm
15.9 mm
17.0 mm

Table 2. Comparison with state-of-the-art. Mean joint error av-
eraged over all joints and frames. All methods are tested on the
NYU[45] test set.

parameterizing the pose directly as the sphere centers, our
method exploits the beneﬁts of FCNs and avoids the dif-
ﬁculties of direct angular regression.
In addition to data
terms, we place priors, including a data-driven term from a
trained VAE to encourage kinematic feasibility.
Through our model, we are able to jointly beneﬁt from
model-based tracking, which requires no supervision,
and from data-driven approaches, in which the accuracy
steadily improves given more training data, even without
labels. In the future, we look forward to incorporating our
unsupervised approach with domain adaptation methods
to further improve the accuracy with available labelled data.

Acknowledgements The authors gratefully acknowledge
support from ETH Computer Vision Lab’s institutional
funding, the Chinese Scholarship Council, and Singapore
Ministry of Education’s Academic Research Fund Tier 1.

References

[1] S. Baek, K. I. Kim, and T.-K. Kim. Augmented skele-
ton space transfer for depth-based hand pose estimation. In
CVPR, 2018.

[2] L. Ballan, A. Taneja, J. Gall, L. Van Gool, and M. Polle-
feys. Motion capture of hands in action using discriminative
salient points. In ECCV, 2012.

[3] Y. Cai, L. Ge, J. Cai, and J. Yuan. Weakly-supervised
3D hand pose estimation from monocular RGB images. In
ECCV, 2018.

[4] X. Chen, G. Wang, H. Guo, and C. Zhang. Pose guided
structured region ensemble network for cascaded hand pose
estimation. arXiv preprint arXiv:1708.03416, 2017.

[5] X. Chen, G. Wang, C. Zhang, T.-K. Kim, and X. Ji. SHPR-
Net: Deep semantic hand pose regression from point clouds.
IEEE Access, 6:43425–43439, 2018.

810860

[6] E. Dibra, T. Wolf, C. Oztireli, and M. Gross. How to reﬁne
3d hand pose estimation from unlabelled depth data? In 3D
Vision (3DV), 2017.
[7] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In ICCV,
2015.
[8] X. Dong, S.-I. Yu, X. Weng, S.-E. Wei, Y. Yang, and
Y. Sheikh. Supervision-by-registration: An unsupervised ap-
proach to improve the precision of facial landmark detectors.
In CVPR, 2018.
[9] L. Ge, Y. Cai, J. Weng, and J. Yuan. Hand PointNet: 3D
hand pose estimation using point sets. In CVPR, 2018.
[10] L. Ge, H. Liang, J. Yuan, and D. Thalmann. 3D convolu-
tional neural networks for efﬁcient and robust hand pose es-
timation from single depth images. In CVPR, 2017.
[11] L. Ge, Z. Ren, and J. Yuan. Point-to-point regression point-
net for 3d hand pose estimation. ECCV, 2018.
[12] H. Guo, G. Wang, X. Chen, C. Zhang, F. Qiao, and H. Yang.
Region ensemble network:
Improving convolutional net-
work for hand pose estimation. In ICIP, 2017.
[13] S. Han, B. Liu, R. Wang, Y. Ye, C. D. Twigg, and K. Kin.
Online optical marker-based hand tracking with deep labels.
ACM Transactions on Graphics (TOG), 2018.
[14] U. Iqbal, P. Molchanov, T. Breuel, J. Gall, and J. Kautz.
Hand pose estimation via latent 2.5D heatmap regression. In
ECCV, 2018.
[15] D. Joseph Tan, T. Cashman, J. Taylor, A. Fitzgibbon, D. Tar-
low, S. Khamis, S. Izadi, and J. Shotton. Fits like a glove:
Rapid and reliable hand shape personalization.
In CVPR,
2016.
[16] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
to-end recovery of human shape and pose. In CVPR, 2018.
[17] D. P. Kingma and M. Welling. Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114, 2013.
[18] G. Larsson, M. Maire, and G. Shakhnarovich. Colorization
as a proxy task for visual understanding. In CVPR, 2017.
[19] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin,
M. Shaﬁei, H.-P. Seidel, W. Xu, D. Casas, and C. Theobalt.
VNect: real-time 3D human pose estimation with a single
rgb camera. 2017.
[20] G. Moon, J. Y. Chang, and K. M. Lee. V2V-PoseNet: voxel-
to-voxel prediction network for accurate 3D hand and human
pose estimation from a single depth map. In CVPR, 2018.
[21] F. Mueller, F. Bernard, O. Sotnychenko, D. Mehta, S. Srid-
har, D. Casas, and C. Theobalt. GANerated hands for real-
time 3D hand tracking from monocular RGB.
In CVPR,
2018.
[22] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. In ECCV, 2016.
[23] M. Oberweger and V. Lepetit. Deepprior++: Improving fast
and accurate 3d hand pose estimation. In ICCV workshop,
2017.
[24] M. Oberweger, G. Riegler, P. Wohlhart, and V. Lepetit. Ef-
ﬁciently creating 3D training data for ﬁne hand pose estima-
tion. In CVPR, 2016.
[25] M. Oberweger, P. Wohlhart, and V. Lepetit. Training a feed-
back loop for hand pose estimation. In ICCV, 2015.

[26] A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and
A. Torralba. Ambient sound provides supervision for visual
learning. In ECCV, 2016.
[27] D. Pathak, R. B. Girshick, P. Doll ´ar, T. Darrell, and B. Har-
iharan. Learning features by watching objects move.
In
CVPR, 2017.
[28] G. Poier, M. Opitz, D. Schinagl, and H. Bischof. MU-
RAUER: Mapping unlabeled real data for label austerity. In
WACV, 2019.
[29] G. Poier, D. Schinagl, and H. Bischof. Learning pose spe-
ciﬁc representations by predicting different views. In CVPR,
2018.
[30] C. Qian, X. Sun, Y. Wei, X. Tang, and J. Sun. Realtime and
robust hand tracking from depth. In CVPR, 2014.
[31] M. Rad, M. Oberweger, and V. Lepetit. Feature mapping for
learning fast and accurate 3D pose inference from synthetic
images. In CVPR, 2018.
[32] H. Rhodin, J. Sp ¨orri, I. Katircioglu, V. Constantin, F. Meyer,
E. M ¨uller, M. Salzmann, and P. Fua. Learning monocular 3D
human pose estimation from multi-view images. In CVPR,
2018.
[33] T. Sharp, C. Keskin, D. Robertson, J. Taylor, J. Shotton,
D. Kim, C. Rhemann, I. Leichter, A. Vinnikov, Y. Wei, et al.
Accurate, robust, and ﬂexible real-time hand tracking.
In
Proceedings of the 33rd Annual ACM Conference on Human
Factors in Computing Systems, 2015.
[34] A. Shrivastava, T. Pﬁster, O. Tuzel, J. Susskind, W. Wang,
and R. Webb. Learning from simulated and unsupervised
images through adversarial training. In CVPR, 2017.
[35] T. Simon, H. Joo, I. A. Matthews, and Y. Sheikh. Hand key-
point detection in single images using multiview bootstrap-
ping. In CVPR, 2017.
[36] A. Spurr, J. Song, S. Park, and O. Hilliges. Cross-modal deep
variational hand pose estimation. In CVPR, 2018.
[37] S. Sridhar, F. Mueller, A. Oulasvirta, and C. Theobalt. Fast
and robust hand tracking using detection-guided optimiza-
tion. In CVPR, 2015.
[38] S. Sridhar, A. Oulasvirta, and C. Theobalt. Interactive mark-
erless articulated hand motion tracking using RGB and depth
data. In ICCV, 2013.
[39] S. Sridhar, H. Rhodin, H.-P. Seidel, A. Oulasvirta, and
C. Theobalt. Real-time hand tracking using a sum of
anisotropic gaussians model. In 3D Vision (3DV), 2014.
[40] X. Sun, Y. Wei, S. Liang, X. Tang, and J. Sun. Cascaded
hand pose regression. In CVPR, 2015.
[41] X. Sun, B. Xiao, S. Liang, and Y. Wei. Integral human pose
regression. In ECCV, 2018.
[42] S. Suwajanakorn, N. Snavely, J. Tompson, and M. Norouzi.
Discovery of latent 3D keypoints via end-to-end geometric
reasoning. In NIPS, 2018.
[43] D. Tang, J. Taylor, P. Kohli, C. Keskin, T.-K. Kim, and
J. Shotton. Opening the black box: Hierarchical sampling
optimization for estimating human hand pose.
In ICCV,
2015.
[44] J. Taylor, J. Shotton, T. Sharp, and A. Fitzgibbon. The vitru-
vian manifold: Inferring dense correspondences for one-shot
human pose estimation. In CVPR, 2012.

910861

[45] J. Tompson, M. Stein, Y. Lecun, and K. Perlin. Real-time
continuous pose recovery of human hands using convolu-
tional networks. ACM Transactions on Graphics (ToG).
[46] S. Tulsiani, A. A. Efros, and J. Malik. Multi-view consis-
tency as supervisory signal for learning shape and pose pre-
diction. CVPR, 2018.
[47] S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik. Multi-view
supervision for single-view reconstruction via differentiable
ray consistency. In CVPR, 2017.
[48] C. Vondrick, A. Shrivastava, A. Fathi, S. Guadarrama, and
K. Murphy. Tracking emerges by colorizing videos.
In
ECCV, 2018.
[49] C. Wan, T. Probst, L. Van Gool, and A. Yao. Crossing nets:
Combining gans and vaes with a shared latent space for hand
pose estimation. In CVPR, 2017.
[50] C. Wan, T. Probst, L. Van Gool, and A. Yao. Dense 3D
regression for hand pose estimation. In CVPR, 2018.
[51] C. Xu, L. N. Govindarajan, Y. Zhang, and L. Cheng. Lie-X:
Depth image based articulated object pose estimation, track-
ing, and action recognition on lie groups. International Jour-
nal of Computer Vision (IJCV), 123(3):454–478, 2017.
[52] Q. Ye, S. Yuan, and T.-K. Kim. Spatial attention deep net
with partial pso for hierarchical hybrid hand pose estimation.
In ECCV, 2016.
[53] S. Yuan, Q. Ye, B. Stenger, S. Jain, and T.-K. Kim. Big-
Hand2.2M benchmark: Hand pose dataset and state of the
art analysis. In CVPR, 2017.
[54] J. Zhang, J. Jiao, M. Chen, L. Qu, X. Xu, and Q. Yang.
A hand pose tracking benchmark from stereo matching. In
ICIP, 2017.
[55] X. Zhou, Q. Wan, W. Zhang, X. Xue, and Y. Wei. Model-
based deep hand pose estimation. In IJCAI, 2016.

1010862

SelFlow: Self-Supervised Learning of Optical Flow

Pengpeng Liu† ∗, Michael Lyu† , Irwin King† , Jia Xu§
† Chinese University of Hong Kong, § Tencent AI Lab

Abstract

We present a self-supervised learning approach for op-
tical ﬂow. Our method distills reliable ﬂow estimations
from non-occluded pixels, and uses these predictions as
ground truth to learn optical ﬂow for hallucinated occlu-
sions. We further design a simple CNN to utilize tempo-
ral information from multiple frames for better ﬂow estima-
tion. These two principles lead to an approach that yields
the best performance for unsupervised optical ﬂow learn-
ing on the challenging benchmarks including MPI Sintel,
KITTI 2012 and 2015. More notably, our self-supervised
pre-trained model provides an excellent initialization for su-
pervised ﬁne-tuning. Our ﬁne-tuned models achieve state-
of-the-art results on all three datasets. At the time of writ-
ing, we achieve EPE=4.26 on the Sintel benchmark, outper-
forming all submitted methods.

1. Introduction

Optical ﬂow estimation is a core building block for a va-
riety of computer vision systems [30, 8, 39, 4]. Despite
decades of development, accurate ﬂow estimation remains
an open problem due to one key challenge: occlusion. Tra-
ditional approaches minimize an energy function to encour-
age association of visually similar pixels and regularize in-
coherent motion to propagate ﬂow estimation from non-
occluded pixels to occluded pixels [13, 5, 6, 38]. However,
this family of methods is often time-consuming and not ap-
plicable for real-time applications.
Recent studies learn to estimate optical ﬂow end-to-
end from images using convolutional neural networks
(CNNs) [10, 35, 15, 14, 43]. However, training fully su-
pervised CNNs requires a large amount of labeled training
data, which is extremely difﬁcult to obtain for optical ﬂow,
especially when there are occlusions. Considering the re-
cent performance improvements obtained when employing
hundreds of millions of labeled images [40], it is obvious
that the size of training data is a key bottleneck for optical
ﬂow estimation.

∗Work mainly done during an internship at Tencent AI Lab.

In the absence of large-scale real-world annotations,
existing methods turn to pre-train on synthetic labeled
datasets [10, 28] and then ﬁne-tune on small annotated
datasets [15, 14, 43]. However, there usually exists a large
gap between the distribution of synthetic data and natu-
ral scenes.
In order to train a stable model, we have to
carefully follow speciﬁc learning schedules across different
datasets [15, 14, 43].

One promising direction is to develop unsupervised opti-
cal ﬂow learning methods that beneﬁt from unlabeled data.
The basic idea is to warp the target image towards the ref-
erence image according to the estimated optical ﬂow, then
minimize the difference between the reference image and
the warped target image using a photometric loss [20, 37].
Such idea works well for non-occluded pixels but turns to
provide misleading information for occluded pixels. Recent
methods propose to exclude those occluded pixels when
computing the photometric loss or employ additional spa-
tial and temporal smoothness terms to regularize ﬂow es-
timation [29, 46, 18]. Most recently, DDFlow [26] pro-
poses a data distillation approach, which employs random
cropping to create occlusions for self-supervision. Unfortu-
nately, these methods fails to generalize well for all natural
occlusions. As a result, there is still a large performance
gap comparing unsupervised methods with state-of-the-art
fully supervised methods.

Is it possible to effectively learn optical ﬂow with oc-
clusions? In this paper, we show that a self-supervised ap-
proach can learn to estimate optical ﬂow with any form of
occlusions from unlabeled data. Our work is based on dis-
tilling reliable ﬂow estimations from non-occluded pixels,
and using these predictions to guide the optical ﬂow learn-
ing for hallucinated occlusions. Figure 1 illustrates our idea
to create synthetic occlusions by perturbing superpixels. We
further utilize temporal information from multiple frames to
improve ﬂow prediction accuracy within a simple CNN ar-
chitecture. The resulted learning approach yields the high-
est accuracy among all unsupervised optical ﬂow learning
methods on Sintel and KITTI benchmarks.

Surprisingly, our self-supervised pre-trained model pro-
vides an excellent initialization for supervised ﬁne-tuning.
At the time of writing, our ﬁne-tuned model achieves the

14571

Figure 1. A toy example to illustrate our self-supervised learning idea. We ﬁrst train our NOC-model with the classical photometric loss
(measuring the difference between the reference image (a) and the warped target image(d)), guided by the occlusion map (g). Then we
perturbate randomly selected superpixels in the target image (b) to hallucinate occlusions. Finally, we use reliable ﬂow estimations from
our NOC-Model to guide the learning of our OCC-Model for those newly occluded pixels (denoted by self-supervision mask (i), where
value 1 means the pixel is non-occluded in (g) but occluded in (h)). Note the yellow region is part of the moving dog. Our self-supervised
approach learns optical ﬂow for both moving objects and static scenes.

highest reported accuracy (EPE=4.26) on the Sintel bench-
mark. Our approach also signiﬁcantly outperforms all pub-
lished optical ﬂow methods on the KITTI 2012 benchmark,
and achieves highly competitive results on the KITTI 2015
benchmark. To the best of our knowledge, it is the ﬁrst time
that a supervised learning method achieves such remarkable
accuracies without using any external labeled data.

2. Related Work

Classical Optical Flow Estimation. Classical variational
approaches model optical ﬂow estimation as an energy
minimization problem based on brightness constancy and
spatial smoothness [13]. Such methods are effective for
small motion, but tend to fail when displacements are large.
Later works integrate feature matching to initialize sparse
matching, and then interpolate into dense ﬂow maps in
a pyramidal coarse-to-ﬁne manner [6, 47, 38]. Recent
works use convolutional neural networks (CNNs) to im-
prove sparse matching by learning an effective feature em-
bedding [49, 2]. However, these methods are often compu-

tationally expensive and can not be trained end-to-end. One
natural extension to improve robustness and accuracy for
ﬂow estimation is to incorporate temporal information over
multiple frames. A straightforward way is to add temporal
constraints such as constant velocity [19, 22, 41], constant
acceleration [45, 3], low-dimensional linear subspace [16],
or rigid/non-rigid segmentation [48]. While these formu-
lations are elegant and well-motivated, our method is much
simpler and does not rely on any assumption of the data. In-
stead, our approach directly learns optical ﬂow for a much
wider range of challenging cases existing in the data.

Supervised Learning of Optical Flow. One promising di-
rection is to learn optical ﬂow with CNNs. FlowNet [10]
is the ﬁrst end-to-end optical ﬂow learning framework. It
takes two consecutive images as input and outputs a dense
ﬂow map. The following work FlowNet 2.0 [15] stacks
several basic FlowNet models for iterative reﬁnement, and
signiﬁcantly improves the accuracy. SpyNet [35] proposes
to warp images at multiple scales to cope with large dis-
placements, resulting in a compact spatial pyramid network.

24572

Figure 2. Our network architecture at each level (similar to PWC-
˙wl denotes the initial coarse ﬂow of level l and ˆF l de-
Net [43]).
notes the warped feature representation. At each level, we swap
the initial ﬂow and cost volume as input to estimate both for-
ward and backward ﬂow concurrently. Then these estimations are
passed to layer l − 1 to estimate higher-resolution ﬂow.

Recently, PWC-Net [43] and LiteFlowNet [14] propose to
warp features extracted from CNNs and achieve state-of-
the-art results with lightweight framework. However, ob-
taining high accuracy with these CNNs requires pre-training
on multiple synthetic datasets and follows speciﬁc training
schedules [10, 28]. In this paper, we reduce the reliance on
pre-training with synthetic data, and propose an effective
self-supervised training method with unlabeled data.

Unsupervised Learning of Optical Flow. Another inter-
esting line of work is unsupervised optical ﬂow learning.
The basic principles are based on brightness constancy and
spatial smoothness [20, 37]. This leads to the most popular
photometric loss, which measures the difference between
the reference image and the warped image. Unfortunately,
this loss does not hold for occluded pixels. Recent studies
propose to ﬁrst obtain an occlusion map and then exclude
those occluded pixels when computing the photometric dif-
ference [29, 46]. Janai et al. [18] introduces to estimate
optical ﬂow with a multi-frame formulation and more ad-
vanced occlusion reasoning, achieving state-of-the-art un-
supervised results. Very recently, DDFlow [26] proposes
a data distillation approach to learning the optical ﬂow of
occluded pixels, which works particularly well for pixels
near image boundaries. Nonetheless, all these unsupervised
learning methods only handle speciﬁc cases of occluded
pixels. They lack the ability to reason about the optical
ﬂow of all possible occluded pixels. In this work, we ad-
dress this issue by a superpixel-based occlusion hallucina-
tion technique.

Self-Supervised Learning. Our work is closely related to
the family of self-supervised learning methods, where the
supervision signal is purely generated from the data itself. It
is widely used for learning feature representations from un-
labeled data [21]. A pretext task is usually employed, such
as image inpainting [34], image colorization [24], solving

Figure 3. Data ﬂow for self-training with multiple-frame. To esti-
mate occlusion map for three-frame ﬂow learning, we use ﬁve im-
ages as input. This way, we can conduct a forward-backward con-
sistency check to estimate occlusion maps between It and It+1 ,
between It and It−1 respectively.

Jigsaw puzzles [32]. Pathak et al. [33] propose to explore
low-level motion-based cues to learn feature representations
without manual supervision. Doersch et al. [9] combine
multiple self-supervised learning tasks to train a single vi-
sual representation. In this paper, we make use of the do-
main knowledge of optical ﬂow, and take reliable predic-
tions of non-occluded pixels as the self-supervision signal
to guide our optical ﬂow learning of occluded pixels.

3. Method

In this section, we present our self-supervised approach
to learning optical ﬂow from unlabeled data. To this end,
we train two CNNs (NOC-Model and OCC-Model) with
the same network architecture. The former focuses on accu-
rate ﬂow estimation for non-occluded pixels, and the latter
learns to predict optical ﬂow for all pixels. We distill re-
liable non-occluded ﬂow estimations from NOC-Model to
guide the learning of OCC-Model for those occluded pix-
els. Only OCC-Model is needed at testing. We build our
network based on PWC-Net [43] and further extend it to
multi-frame optical ﬂow estimation (Figure 2). Before de-
scribing our approach in detail, we ﬁrst deﬁne our notations.

3.1. Notation

Given three consecutive RGB images It−1 , It , It+1 , our
goal is to estimate the forward optical ﬂow from It to It+1 .
Let wi→j denote the ﬂow from Ii to Ij , e.g., wt→t+1 de-
notes the forward ﬂow from It to It+1 , wt→t−1 denotes
the backward ﬂow from It to It−1 . After obtaining opti-
cal ﬂow, we can backward warp the target image to recon-
struct the reference image using Spatial Transformer Net-
work [17, 46]. Here, we use I w
j→i to denote warping Ij to
Ii with ﬂow wi→j . Similarly, we use Oi→j to denote the
occlusion map from Ii to Ij , where value 1 means the pixel
in Ii is not visible in Ij .
In our self-supervised setting, we create the new target
image eIt+1 by injecting random noise on superpixels for
occlusion generation. We can inject noise to any of three
consecutive frames and even multiple of them as shown in
Figure 1. For brevity, here we choose It+1 as an example.

34573

(a) Reference Image

(b) GT Flow

(c) Our Flow

(d) GT Occlusion

(e) Our Occlusion

Figure 4. Sample unsupervised results on Sintel and KITTI dataset. From top to bottom, we show samples from Sintel Final, KITTI 2012
and KITTI 2015. Our model can estimate both accurate ﬂow and occlusion map. Note that on KITTI datasets, the occlusion maps are
sparse, which only contain pixels moving out of the image boundary.

If we let It−1 , It and eIt+1 as input, then ew, eO , eI w represent
the generated optical ﬂow, occlusion map and warped image
respectively.

next level both in resolution and magnitude.

3.3. Occlusion Estimation

3.2. CNNs for Multi(cid:173)Frame Flow Estimation

In principle, our method can utilize any CNNs. In our
implementation, we build on top of the seminar PWC-
Net [43]. PWC-Net employs pyramidal processing to in-
crease the ﬂow resolution in a coarse-to-ﬁne manner and
utilizes feature warping, cost volume construction to esti-
mate optical ﬂow at each level. Based on these principles,
it has achieved state-of-the-art performance with a compact
model size.
As shown in Figure 2, our three-frame ﬂow estimation
network structure is built upon two-frame PWC-Net with
several modiﬁcations to aggregate temporal information.
First, our network takes three images as input, thus pro-
duces three feature representations Ft−1 , Ft and Ft+1 . Sec-
ond, apart from forward ﬂow wt→t+1 and forward cost vol-
ume, out model also computes backward ﬂow wt→t−1 and
backward cost volume at each level simultaneously. Note
that when estimating forward ﬂow, we also utilize the ini-
tial backward ﬂow and backward cost volume information.
This is because past frame It−1 can provide very valuable
information, especially for those regions that are occluded
in the future frame It+1 but not occluded in It−1 . Our net-
work combines all this information together and therefore
estimates optical ﬂow more accurately. Third, we stack
initial forward ﬂow ˙wl
t→t+1 , minus initial backward ﬂow
t+1→t , feature of reference image F l
t , forward cost vol-
ume and backward cost volume to estimate the forward ﬂow
at each level. For backward ﬂow, we just swap the ﬂow and
cost volume as input. Forward and backward ﬂow estima-
tion networks share the same network structure and weights.
For initial ﬂow at each level, we upscale optical ﬂow of the

− ˙wl

For two-frame optical ﬂow estimation, we can swap two
images as input to generate forward and backward ﬂow,
then the occlusion map can be generated based on the
forward-backward consistency prior [44, 29]. To make this
work under our three-frame setting, we propose to utilize
the adjacent ﬁve frame images as input as shown in Fig-
ure 3. Speciﬁcally, we estimate bi-directional ﬂows be-
tween It and It+1 , namely wt→t+1 and wt+1→t . Similarly,
we also estimate the ﬂows between It and It−1 . Finally,
we conduct a forward and backward consistency check to
reason the occlusion map between two consecutive images.
To check forward-backward consistency, we consider
one pixel as occluded when the mismatch between the for-
ward ﬂow and the reversed forward ﬂow is too large. Take
Ot→t+1 as an example, we can ﬁrst compute the reversed
forward ﬂow as follows,

ˆwt→t+1 = wt+1→t (p + wt→t+1 (p)),

(1)

A pixel is considered occluded whenever it violates the fol-
lowing constraint:

|wt→t+1 + ˆwt→t+1 |2 < α1 (|wt→t+1 |2 + | ˆwt→t+1 |2 ) + α2 ,

(2)
where we set α1 = 0.01, α2 = 0.05 for all our experiments.
Other occlusion maps are computed in the same way.

3.4. Occlusion Hallucination

During our self-supervised training, we hallucinate oc-
clusions by perturbing local regions with random noise. In
a newly generated target image, the pixels corresponding
to noise regions automatically become occluded. There
are many ways to generate such occlusions. The most

44574

d

e

s

i

v

r

e

p
u

s

n

U

d

e

s

i

v

r

e

p
u

S

Method

BackToBasic+ft [20]
DSTFlow+ft [37]
UnFlow-CSS [29]
OccAwareFlow+ft [46]
MultiFrameOccFlow-None+ft [18]
MultiFrameOccFlow-Soft+ft [18]
DDFlow+ft [26]
Ours

FlowNetS+ft [10]
FlowNetC+ft [10]
SpyNet+ft [35]
FlowFieldsCNN+ft [2]
DCFlow+ft [49]
FlowNet2+ft [15]
UnFlow-CSS+ft [29]
LiteFlowNet+ft-CVPR [14]
LiteFlowNet+ft-axXiv [14]
PWC-Net+ft-CVPR [43]
PWC-Net+ft-axXiv [42]
ProFlow+ft [27]
ContinualFlow+ft [31]
MFF+ft [36]
Ours+ft

Sintel Clean

Sintel Final

KITTI 2012

KITTI 2015

train

test

train

test

train

test

test(Fl)

train

test(Fl)

–
(6.16)
–
(4.03)
(6.05)
(3.89)
(2.92)
(2.88)

(3.66)
(3.78)
(3.17)
–
–
(1.45)
–
(1.64)
(1.35)
(2.02)
(1.71)
(1.78)
–
–
(1.68)

–
10.41
–
7.95
–
7.23
6.18
6.56

6.96
6.85
6.64
3.78
3.54
4.16
–
4.86
4.54
4.39
3.45
2.82
3.34
3.42
3.74

–
(6.81)
(7.91)
(5.95)
(7.09)
(5.52)
3.98
(3.87)

(4.44)
(5.28)
(4.32)
–
–
(2.01)
–
(2.23)
(1.78)
(2.08)
(2.34)
–
–
–
(1.77)

–
11.27
10.22
9.15
–
8.81
7.40
6.57

7.76
8.51
8.36
5.36
5.12
5.74
–
6.09
5.38
5.04
4.60
5.02
4.52
4.57
4.26

11.3
10.43
3.29
3.55
–
–
2.35
1.69

7.52
8.79
8.25
–
–
(1.28)
(1.14)
(1.26)
(1.05)
(1.45)
(1.08)
(1.89)
–
–
(0.76)

9.9
12.4
–
4.2
–
–
3.0
2.2

9.1
–
10.1
3.0
–
1.8
1.7
1.7
1.6
1.7
1.5
2.1
–
1.7
1.5

–
–
–
16.79
–
8.10
–
8.88
–
6.65
–
6.59
8.86%
5.72
7.68% 4.84

44.49%
–
–
–
20.97%
–
13.01%
–
–
–
8.8%
(2.3)
8.42% (1.86)
–
(2.16)
7.27% (1.62)
8.10% (2.16)
6.82% (1.45)
7.88% (5.22)
–
–
7.87%
–
6.19% (1.18)

–
39%
23.30%
31.2%
–
22.94%
14.29%
14.19%

–
–
35.07%
18.68 %
14.83%
11.48%
11.11%
10.24%
9.38%
9.60%
7.90%
15.04%
10.03%
7.17%
8.42%

Table 1. Comparison with state-of-the-art learning based optical ﬂow estimation methods. Our method outperforms all unsupervised
optical ﬂow learning approaches on all datasets. Our supervised ﬁne-tuned model achieves the highest accuracy on the Sintel Final dataset
and KITTI 2012 dataset. All numbers are EPE except for the last column of KITTI 2012 and KITTI 2015 testing sets, where we report
percentage of erroneous pixels over all pixels (Fl-all). Missing entries (-) indicate that the results are not reported for the respective method.
Parentheses mean that the training and testing are performed on the same dataset.

straightforward way is to randomly select rectangle regions.
However, rectangle occlusions rarely exist in real-world se-
quences. To address this issue, we propose to ﬁrst gener-
ate superpixels [1], then randomly select several superpix-
els and ﬁll them with noise. There are two main advantages
of using superpixel. First, the shape of a superpixel is usu-
ally random and superpixel edges are often part of object
boundaries. The is consistent with the real-world cases and
makes the noise image more realistic. We can choose sev-
eral superpixels which locate at different locations to cover
more occlusion cases. Second, the pixels within each su-
perpixel usually belong to the same object or have similar
ﬂow ﬁelds. Prior work has found low-level segmentation is
helpful for optical ﬂow estimation [49]. Note that the ran-
dom noise should lie in the pixel value range.

Figure 1 shows a simple example, where only the dog
extracted from the COCO dataset [25] is moving. Initially,
the occlusion map between It and It+1 is (g). After ran-
domly selecting several superpixels from (e) to inject noise,
the occlusion map between It and eIt+1 change to (h). Next,
we describe how to make use of these occlusion maps to

guide our self-training.

3.5. NOC(cid:173)to(cid:173)OCC as Self(cid:173)Supervision

Our self-training idea is built on top of the classical pho-
tometric loss [29, 46, 18], which is highly effective for non-
occluded pixels. Figure 1 illustrates our main idea. Suppose
pixel p1 in image It is not occluded in It+1 , and pixel p′
1 is
its corresponding pixel. If we inject noise to It+1 and let
It−1 , It , eIt+1 as input, p1 then becomes occluded. Good
news is we can still use the ﬂow estimation of NOC-Model
as annotations to guide OCC-Model to learn the ﬂow of p1
from It to eIt+1 . This is also consistent with real-world oc-
clusions, where the ﬂow of occluded pixels can be estimated
based on surrounding non-occluded pixels. In the example
of Figure 1, self-supervision is only employed to (i), which
represents those pixels non-occluded from It to It+1 but be-
come occluded from It to eIt+1 .

3.6. Loss Functions

Similar to previous unsupervised methods, we ﬁrst apply
photometric loss Lp to non-occluded pixels. Photometric

54575

Reference Image (training)

Ground Truth

W/O Occlusion

W/O Self-Supervision

Rectangle

Two-frame Superpixel

Superpixel

Finetune

Reference Image (testing)

Target image

W/O Occlusion

W/O Self-Supervision

Rectangle

Two-frame Superpixel

Superpixel

Finetune

Figure 5. Qualitative comparison of our model under different settings on Sintel Clean training and Sintel Final testing dataset. Occlusion
handling, multi-frame formulation and self-supervision consistently improve the performance.

loss is deﬁned as follows:

Lp = X

i,j

P ψ(Ii − I w
j→i ) ⊙ (1 − Oi )
P (1 − Oi )

(3)

where ψ(x) = (|x|+ ǫ)q is a robust loss function, ⊙ denotes
the element-wise multiplication. We set ǫ = 0.01, q = 0.4
for all our experiments. Only Lp is necessary to train the
NOC-Model.
To train our OCC-Model to estimate optical ﬂow of oc-
cluded pixels, we deﬁne a self-supervision loss Lo for those
synthetic occluded pixels (Figure 1(i)). First, we compute a
self-supervision mask M to represent these pixels,
Mi→j = clip( eOi→j − Oi→j , 0, 1)
Then, we deﬁne our self-supervision loss Lo as,
P ψ(wi→j − ewi→j ) ⊙ Mi→j

Lo = X

P Mi→j

(4)

(5)

i,j

For our OCC-Model, we train with a simple combination of
Lp + Lo for both non-occluded pixels and occluded pixels.
Note our loss functions do not rely on spatial and tempo-
ral consistent assumptions, and they can be used for both
classical two-frame ﬂow estimation and multi-frame ﬂow
estimation.

3.7. Supervised Fine(cid:173)tuning

After pre-training on raw dataset, we use real-world an-
notated data for ﬁne-tuning. Since there are only annota-
tions for forward ﬂow wt→t+1 , we skip backward ﬂow esti-
mation when computing our loss. Suppose that the ground

truth ﬂow is wgt
t→t+1 , and mask V denotes whether the pixel
has a label, where value 1 means that the pixel has a valid
ground truth ﬂow. Then we can obtain the supervised ﬁne-
tuning loss as follows,

Ls = X(ψ(wgt

t→t+1 − wt→t+1 ) ⊙ V )/ X V

(6)

During ﬁne-tuning, We ﬁrst initialize the model with the
pre-trained OCC-Model on each dataset, then optimize it
using Ls .

4. Experiments

We evaluate and compare our methods with state-
of-the-art unsupervised and supervised learning methods
on public optical ﬂow benchmarks including MPI Sin-
tel [7], KITTI 2012 [11] and KITTI 2015 [30].
To
ensure reproducibility and advance further innovations,
we make our code and models publicly available at

https://github.com/ppliuboy/SelFlow.

4.1. Implementation Details

Data Preprocessing. For Sintel, we download the Sintel
movie and extract ∼ 10, 000 images for self-training. We
ﬁrst train our model on this raw data, then add the ofﬁcial
Sintel training data (including both ”ﬁnal” and ”clean” ver-
sions). For KITTI 2012 and KITTI 2015, we use multi-view
extensions of the two datasets for unsupervised pre-training,
similar to [37, 46]. During training, we exclude the image
pairs with ground truth ﬂow and their neighboring frames
(frame number 9-12) to avoid the mixture of training and
testing data.

64576

Reference Image (training)

Ground Truth

W/O Occlusion

W/O Self-Supervision

Rectangle

Two-frame Superpixel

Superpixel

Finetune

Reference Image (testing)

Target image

W/O Occlusion

W/O Self-Supervision

Rectangle

Two-frame Superpixel

Superpixel

Finetune

Figure 7. Qualitative comparison of our model under different settings on KITTI 2015 training and testing dataset. Occlusion handling,
Figure 6. Qualitative comparison of our model under different settings on KITTI 2015 training and testing dataset. Occlusion handling,
multi-frame formulation and self-supervision consistently improve the performance.

We rescale the pixel value from [0, 255] to [0, 1] for
unsupervised training, while normalizing each channel to
be standard normal distribution for supervised ﬁne-tuning.
This is because normalizing image as input is more robust
for luminance changing, which is especially helpful for op-
tical ﬂow estimation. For unsupervised training, we apply
Census Transform [50] to images, which has been proved
robust for optical ﬂow estimation [12, 29].

Training procedure. We train our model with the Adam
optimizer [23] and set batch size to be 4 for all experiments.
For unsupervised training, we set the initial learning rate to
be 10−4 , decay it by half every 50k iterations, and use ran-
dom cropping, random ﬂipping, random channel swapping
during data augmentation. For supervised ﬁne-tuning, we
employ similar data augmentation and learning rate sched-
ule as [10, 15].

For unsupervised pre-training, we ﬁrst train our NOC-
Model with photometric loss for 200k iterations. Then, we
add our occlusion regularization and train for another 500k
iterations. Finally, we initialize the OCC-Model with the
trained weights of NOC-Model and train it with Lp + Lo for
500k iterations.Since training two models simultaneously
will cost more memory and training time, we just gener-
ate the ﬂow and occlusion maps using the NOC-Model in
advance and use them as annotations (just like KITTI with
sparse annotations).

For supervised ﬁne-tuning, we use the pre-trained OCC-
Model as initialization, and train the model using our su-
pervised loss Ls with 500k iterations for KITTI and 1, 000k
iterations for Sintel. Note we do not require pre-training
our model on any labeled synthetic dataset, hence we do
not have to follow the speciﬁc training schedule (Fly-
ingChairs [10]→ FlyingThings3D [28]) as [15, 14, 43].

Evaluation Metrics. We consider two widely-used metrics
to evaluate optical ﬂow estimation: average endpoint error
(EPE), percentage of erroneous pixels (Fl). EPE is the rank-

ing metric on the Sintel benchmark, and Fl is the ranking
metric on KITTI benchmarks.

4.2. Main Results

As shown in Table 1, we achieve state-of-the-art results
for both unsupervised and supervised optical ﬂow learn-
ing on all datasets under all evaluation metrics. Figure 4
shows sample results from Sintel and KITTI. Our method
estimates both accurate optical ﬂow and occlusion maps.

Unsupervised Learning. Our method achieves the high-
est accuracy for unsupervised learning methods on leading
benchmarks. On the Sintel ﬁnal benchmark, we reduce the
previous best EPE from 7.40 [26] to 6.57, with 11.2% rel-
ative improvements. This is even better than several fully
supervised methods including FlowNetS, FlowNetC [10],
and SpyNet [35].
On the KITTI datasets, the improvement is more signif-
icant. For the training dataset, we achieve EPE=1.69 with
28.1% relative improvement on KITTI 2012 and EPE=4.84
with 15.3% relative improvement on KITTI 2015 com-
pared with previous best unsupervised method DDFlow. On
KITTI 2012 testing set, we achieve Fl-all=7.68%, which
is better than state-of-the-art supervised methods includ-
ing FlowNet2 [15], PWC-Net [43], ProFlow [27], and
MFF [36]. On KITTI 2015 testing benchmark, we achieve
Fl-all 14.19%, better than all unsupervised methods. Our
unsupervised results also outperform some fully supervised
methods including DCFlow [49] and ProFlow [27].

Supervised Fine-tuning. We further ﬁne-tune our unsuper-
vised model with the ground truth ﬂow. We achieve state-
of-the-art results on all three datasets, with Fl-all=6.19% on
KITTI 2012 and Fl-all=8.42% on KITTI 2015. Most im-
portantly, our method yields EPE=4.26 on the Sintel ﬁnal
dataset, achieving the highest accuracy on the Sintel bench-
mark among all submitted methods. All these show that
our method reduces the reliance of pre-training with syn-

74577

Occlusion Multiple

Self-Supervision

Self-Supervision

Sintel Clean

Sintel Final

KITTI 2012

KITTI 2015

Handling

Frame

Rectangle

Superpixel

ALL

NOC

OCC

ALL

NOC

OCC

ALL NOC OCC

ALL NOC OCC

✗

✗

✓

✓

✓

✓

✓

✗

✓

✗

✓

✗

✓

✓

✗

✗

✗

✗

✗

✓

✗

✗

✗

✗

✗

✓

✗

✓

(3.85)
(3.67)
(3.35)
(3.20)
(2.96)
(2.91)
(2.88)

(1.53)
(1.54)
(1.37)
(1.35)
(1.33)
(1.37)
(1.30)

(33.48)
(30.80)
(28.70)
(26.63)
(23.78)
(22.58)
(22.06)

(5.28)
(4.98)
(4.50)
(4.33)
(4.06)
(3.99)
(3.87)

(2.81)
(2.68)
(2.37)
(2.32)
(2.25)
(2.27)
(2.24)

(36.83)
(34.42)
(31.81)
(29.80)
(27.19)
(26.01)
(25.42)

7.05
6.52
4.96
3.32
1.97
1.78
1.69

1.31
1.11
0.99
0.94
0.92
0.96
0.91

45.03
42.44
31.29
19.11
8.96
7.47
6.95

13.51
12.13
8.99
7.66
5.85
5.01
4.84

3.71
3.47
3.20
2.47
2.96
2.55
2.40

75.51
66.91
45.68
40.99
24.17
21.86
19.68

Table 2. Ablation study. We report EPE of our unsupervised results under different settings over all pixels (ALL), non-occluded pixels
(NOC) and occluded pixels (OCC). Note that we employ Census Transform when computing photometric loss by default. Without Census
Transform, the performance will drop.

Unsupervised Pre-training

Sintel Clean

Sintel Final KITTI 2012 KITTI 2015

Without
With

1.97
1.50

2.68
2.41

3.93
1.55

3.10
1.86

Table 3. Ablation study. We report EPE of supervised ﬁne-tuning
results on our validation datasets with and without unsupervised
pre-training.

thetic datasets and we do not have to follow speciﬁc training
schedules across different datasets anymore.

4.3. Ablation Study

To demonstrate the usefulness of individual technical
steps, we conduct a rigorous ablation study and show the
quantitative comparison in Table 2. Figure 5 and Figure 6
show the qualitative comparison under different settings,
where “W/O Occlusion” means occlusion handling is not
considered, “W/O Self-Supervision” means occlusion han-
dling is considered but self-supervision is not employed,
“Rectangle” and “Superpixel” represent self-supervision
is employed with rectangle and superpixel noise injec-
tion respectively.
“Two-Frame Superpixel” means self-
supervision is conducted with only two frames as input.

Two-Frame vs Multi-Frame. Comparing row 1 and row
2, row 3 and row 4 row 5 and row 7 in Table 2, we can see
that using multiple frames as input can indeed improve the
performance, especially for occluded pixels. It is because
multiple images provide more information, especially for
those pixels occluded in one direction but non-occluded in
the reverse direction.

Occlusion Handling. Comparing the row 1 and row 3, row
2 and row 4 in Table 2, we can see that occlusion handling
can improve optical ﬂow estimation performance over all
pixels on all datasets. This is due to the fact that brightness
constancy assumption does not hold for occluded pixels.

Self-Supervision. We employ two strategies for our occlu-
sion hallucination: rectangle and superpixel. Both strate-
gies improve the performance signiﬁcantly, especially for
occluded pixels. Take superpixel setting as an example,
EPE-OCC decrease from 26.63 to 22.06 on Sintel Clean,
from 29.80 to 25.42 on Sintel Final, from 19.11 to 6.95
on KITTI 2012, and from 40.99 to 19.68 on KITTI 2015.

Such a big improvement demonstrates the effectiveness of
our self-supervision strategy.
Comparing superpixel noise injection with rectangle
noise injection, superpixel setting has several advantages.
First, the shape of the superpixel is random and edges are
more correlated to motion boundaries. Second, the pixels in
the same superpixel usually have similar motion patterns.
As a result, the superpixel setting achieves slightly better
performance.

Self-Supervised Pre-training. Table 3 compares super-
vised results with and without our self-supervised pre-
training on the validation sets. If we do not employ self-
supervised pre-training and directly train the model using
only the ground truth, the model fails to converge well due
to insufﬁcient training data. However, after utilizing our
self-supervised pre-training, it converges very quickly and
achieves much better results.

5. Conclusion

We have presented a self-supervised approach to learn-
ing accurate optical ﬂow estimation. Our method injects
noise into superpixels to create occlusions, and let one
model guide the another to learn optical ﬂow for occluded
pixels. Our simple CNN effectively aggregates temporal
information from multiple frames to improve ﬂow predic-
tion. Extensive experiments show our method signiﬁcantly
outperforms all existing unsupervised optical ﬂow learning
methods. After ﬁne-tuning with our unsupervised model,
our method achieves state-of-the-art ﬂow estimation accu-
racy on all leading benchmarks. Our results demonstrate it
is possible to completely reduce the reliance of pre-training
on synthetic labeled datasets, and achieve superior perfor-
mance by self-supervised pre-training on unlabeled data.

6. Acknowledgment

This work is supported by the Research Grants Council
of the Hong Kong Special Administrative Region, China
(No. CUHK 14208815 and No. CUHK 14210717 of the
General Research Fund). We thank anonymous reviewers
for their constructive suggestions.

84578

References

[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien
Lucchi, Pascal Fua, Sabine S ¨usstrunk, et al. Slic superpix-
els compared to state-of-the-art superpixel methods.
IEEE
transactions on pattern analysis and machine intelligence,
34(11):2274–2282, 2012.
[2] Christian Bailer, Kiran Varanasi, and Didier Stricker. Cnn-
based patch matching for optical ﬂow with thresholded hinge
embedding loss. In CVPR, 2017.
[3] Michael J Black and Padmanabhan Anandan. Robust dy-
namic motion estimation over time. In CVPR, 1991.
[4] Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, De-
qing Sun, Sylvain Paris, and Hanspeter Pﬁster. Blind video
temporal consistency. ACM Trans. Graph., 34(6):196:1–
196:9, Oct. 2015.
[5] Thomas Brox, Andr ´es Bruhn, Nils Papenberg, and Joachim
Weickert. High accuracy optical ﬂow estimation based on a
theory for warping. In ECCV, 2004.
[6] Thomas Brox and Jitendra Malik. Large displacement opti-
cal ﬂow: descriptor matching in variational motion estima-
tion. TPAMI, 33(3):500–513, 2011.
[7] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A naturalistic open source movie for opti-
cal ﬂow evaluation. In ECCV, 2012.
[8] Abhishek Kumar Chauhan and Prashant Krishan. Moving
object tracking using gaussian mixture model and optical
ﬂow. International Journal of Advanced Research in Com-
puter Science and Software Engineering, 3(4), 2013.
[9] Carl Doersch and Andrew Zisserman. Multi-task self-
supervised visual learning. In ICCV, 2017.
[10] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical ﬂow with convolutional networks. In ICCV,
2015.
[11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In CVPR, 2012.
[12] David Hafner, Oliver Demetz, and Joachim Weickert. Why
is the census transform good for robust optic ﬂow computa-
tion? In International Conference on Scale Space and Vari-
ational Methods in Computer Vision, 2013.
[13] Berthold KP Horn and Brian G Schunck. Determining opti-
cal ﬂow. Artiﬁcial intelligence, 17(1-3):185–203, 1981.
[14] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite-
ﬂownet: A lightweight convolutional neural network for op-
tical ﬂow estimation. In CVPR, 2018.
[15] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-
tion of optical ﬂow estimation with deep networks. In CVPR,
2017.
[16] Michal Irani. Multi-frame optical ﬂow estimation using sub-
space constraints. In ICCV, 1999.
[17] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In NIPS, 2015.
[18] Joel Janai, Fatma G ¨uney, Anurag Ranjan, Michael J. Black,
and Andreas Geiger. Unsupervised learning of multi-frame
optical ﬂow with occlusions. In ECCV, 2018.

[19] Joel Janai, Fatma G ¨uney, Jonas Wulff, Michael J Black, and
Andreas Geiger. Slow ﬂow: Exploiting high-speed cam-
eras for accurate and diverse optical ﬂow reference data. In
CVPR, 2017.
[20] J Yu Jason, Adam W Harley, and Konstantinos G Derpa-
nis. Back to basics: Unsupervised learning of optical ﬂow
via brightness constancy and motion smoothness. In ECCV,
2016.
[21] Longlong Jing and Yingli Tian. Self-supervised visual fea-
ture learning with deep neural networks: A survey. arXiv
preprint arXiv:1902.06162, 2019.
[22] Ryan Kennedy and Camillo J Taylor. Optical ﬂow with geo-
metric occlusion estimation and fusion of multiple frames. In
International Workshop on Energy Minimization Methods in
Computer Vision and Pattern Recognition, pages 364–377.
Springer, 2015.
[23] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014.
[24] Gustav
Larsson, Michael Maire,
and Gregory
Shakhnarovich. Colorization as a proxy task for visual
understanding. In CVPR, 2017.
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
European conference on computer vision, pages 740–755.
Springer, 2014.
[26] Pengpeng Liu, Irwin King, Michael R. Lyu, and Jia Xu.
Ddﬂow: Learning optical ﬂow with unlabeled data distilla-
tion. In AAAI, 2019.
[27] D. Maurer and A. Bruhn. Proﬂow: Learning to predict opti-
cal ﬂow. In BMVC, 2018.
[28] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,
Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A
large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. In CVPR, 2016.
[29] Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un-
supervised learning of optical ﬂow with a bidirectional cen-
sus loss. In AAAI, New Orleans, Louisiana, 2018.
[30] Moritz Menze and Andreas Geiger. Object scene ﬂow for
autonomous vehicles. In CVPR, 2015.
[31] Michal Neoral, Jan ochman, and Ji Matas. Continual occlu-
sions and optical ﬂow estimation. In ACCV, 2018.
[32] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In ECCV,
2016.
[33] Deepak Pathak, Ross Girshick, Piotr Doll ´ar, Trevor Darrell,
and Bharath Hariharan. Learning features by watching ob-
jects move. In CVPR, 2017.
[34] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR, 2016.
[35] Anurag Ranjan and Michael J Black. Optical ﬂow estimation
using a spatial pyramid network. In CVPR, 2017.

94579

[36] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang,
Erik B. Sudderth, and Jan Kautz. A fusion approach for
multi-frame optical ﬂow estimation.
In IEEE Winter Con-
ference on Applications of Computer Vision, 2019.
[37] Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang,
and Hongyuan Zha. Unsupervised deep learning for optical
ﬂow estimation. In AAAI, 2017.
[38] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and
Cordelia Schmid. Epicﬂow: Edge-preserving interpolation
of correspondences for optical ﬂow. In CVPR, 2015.
[39] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos.
In
NIPS, 2014.
[40] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-
nav Gupta. Revisiting unreasonable effectiveness of data in
deep learning era. In ICCV, 2017.
[41] Deqing Sun, Erik B Sudderth, and Michael J Black. Layered
image motion with explicit occlusions, temporal consistency,
and depth ordering. In NIPS, 2010.
[42] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan
Kautz. Models matter, so does training: An empirical
study of cnns for optical ﬂow estimation. arXiv preprint
arXiv:1809.05571, 2018.

[43] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Pwc-net: Cnns for optical ﬂow using pyramid, warping, and
cost volume. In CVPR, 2018.
[44] Narayanan Sundaram, Thomas Brox, and Kurt Keutzer.
Dense point trajectories by gpu-accelerated large displace-
ment optical ﬂow. In ECCV, 2010.
[45] Sebastian Volz, Andres Bruhn, Levi Valgaerts, and Henning
Zimmer. Modeling temporal coherence for optical ﬂow. In
ICCV, 2011.
[46] Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, and Wei
Xu. Occlusion aware unsupervised learning of optical ﬂow.
In CVPR, 2018.
[47] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and
Cordelia Schmid. Deepﬂow: Large displacement optical
ﬂow with deep matching. In ICCV, 2013.
[48] Jonas Wulff, Laura Sevilla-Lara, and Michael J Black. Opti-
cal ﬂow in mostly rigid scenes. In CVPR, 2017.
[49] Jia Xu, Ren ´e Ranftl, and Vladlen Koltun. Accurate Optical
Flow via Direct Cost Volume Processing. In CVPR, 2017.
[50] Ramin Zabih and John Woodﬁll. Non-parametric local trans-
forms for computing visual correspondence. In ECCV, 1994.

104580

2337

2338

2339

2340

Label

Ground Truth

CRN [8]

pix2pixHD [45]

Ours

Figure 5: Visual comparison of semantic image synthesis results on the COCO-Stuff dataset. Our method successfully
synthesizes realistic details from semantic labels.

Label

Ground Truth

CRN [8]

SIMS [40]

pix2pixHD [45]

Ours

Figure 6: Visual comparison of semantic image synthesis results on the ADE20K outdoor and Cityscapes datasets. Our
method produces realistic images while respecting the spatial semantic layout at the same time.

Method
CRN [8]
SIMS [40]
pix2pixHD [45]
Ours

COCO-Stuff
mIoU accu
23.7
40.4
N/A
N/A
14.6
45.8
37.4
67.9

FID
70.4
N/A
111.5
22.6

ADE20K
mIoU accu
22.4
68.8
N/A
N/A
20.3
69.2
38.5
79.9

ADE20K-outdoor
Cityscapes
FID mIoU accu
FID mIoU accu
73.3
16.5
68.6
99.0
52.4
77.1
N/A
13.1
74.7
67.7
47.2
75.5
81.8
17.4
71.6
97.8
58.3
81.4
33.9
30.8
82.9
63.3
62.3
81.9

FID
104.7
49.7
95.0
71.8

Table 1: Our method outperforms current leading methods in semantic segmentation scores (mean IoU and overall pixel
accuracy) and FID [17] on all the benchmark datasets. For mIoU and pixel accuracy, higher is better. For FID, lower is better.

the distance between the distributions of synthesized results
and the distribution of real images.

Baselines. We compare our method with three leading se-
mantic image synthesis models: the pix2pixHD model [45],
the cascaded reﬁnement network model (CRN) [8], and
the semi-parametric image synthesis model (SIMS) [40].
pix2pixHD is the current state-of-the-art GAN-based con-
ditional image synthesis framework. CRN uses a deep net-
work that repeatedly reﬁnes the output from low to high res-
olution, while the SIMS takes a semi-parametric approach

that composites real segments from a training set and reﬁnes
the boundaries. Both the CRN and SIMS are mainly trained
using image reconstruction loss. For a fair comparison, we
train the CRN and pix2pixHD models using the implemen-
tations provided by the authors. As synthesizing an image
using SIMS requires many queries to the training dataset,
it is computationally prohibitive for a large dataset such as
COCO-stuff and the full ADE20K. Therefore, we use the
result images provided by the authors whenever possible.

Quantitative comparisons. As shown in Table 1, our

52341

Figure 7: Semantic image synthesis results on the Flickr Landscapes dataset. The images were generated from semantic
layout of photographs on Flickr.

method outperforms the current state-of-the-art methods by
a large margin in all the datasets. For COCO-Stuff, our
method achieves a mIoU score of 35.2, which is about 1.5
times better than the previous leading method. Our FID
is also 2.2 times better than the previous leading method.
We note that the SIMS model produces a lower FID score
but has poor segmentation performances on the Cityscapes
dataset. This is because the SIMS synthesizes an image by
ﬁrst stitching image patches from the training dataset. As
using the real image patches, the resulting image distribu-
tion can better match the distribution of real images. How-
ever, because there is no guarantee that a perfect query (e.g.,
a person in a particular pose) exists in the dataset, it tends
to copy objects with mismatched segments.

Qualitative results.
In Figures 5 and 6, we provide a
qualitative comparison of the competing methods. We ﬁnd
that our method produces results with much better visual
quality and fewer artifacts, especially for diverse scenes in
the COCO-Stuff and ADE20K dataset. When the training
dataset size is small, the SIMS model also renders images
with good visual quality. However, the depicted content
often deviates from the input segmentation mask (e.g., the
shape of the swimming pool in the second row of Figure 6).
In Figures 7 and 8, we show more example results from
the Flickr Landscape and COCO-Stuff datasets. The pro-
posed method can generate diverse scenes with high image
ﬁdelity. More results are included in the appendix of our

Dataset

COCO-Stuff
ADE20K
ADE20K-outdoor
Cityscapes

Ours vs.
CRN
79.76
76.66
66.04
63.60

Ours vs.
pix2pixHD
86.64
83.74
79.34
53.64

Ours vs.
SIMS
N/A
N/A
85.70
51.52

Table 2: User preference study. The numbers indicate the
percentage of users who favor the results of the proposed
method over the competing method.

arXiv version.

Human evaluation. We use Amazon Mechanical Turk
(AMT) to compare the perceived visual ﬁdelity of our
method against existing approaches. Speciﬁcally, we give
the AMT workers an input segmentation mask and two
synthesis outputs from different methods and ask them to
choose the output image that looks more like a correspond-
ing image of the segmentation mask. The workers are given
unlimited time to make the selection. For each comparison,
we randomly generate 500 questions for each dataset, and
each question is answered by 5 different workers. For qual-
ity control, only workers with a lifetime task approval rate
greater than 98% can participate in our evaluation.
Table 2 shows the evaluation results. We ﬁnd that users
strongly favor our results on all the datasets, especially on
the challenging COCO-Stuff and ADE20K datasets. For the
Cityscapes, even when all the competing methods achieve

62342

Figure 8: Semantic image synthesis results on COCO-Stuff. Our method successfully generates realistic images in diverse
scenes ranging from animals to sports activities.

Method
decoder w/ SPADE (Ours)
compact decoder w/ SPADE
decoder w/ Concat
pix2pixHD++ w/ SPADE
pix2pixHD++ w/ Concat
pix2pixHD++
compact pix2pixHD++
pix2pixHD [45]

#param COCO. ADE. City.
96M
35.2
38.5
62.3
61M
35.2
38.0
62.5
79M
31.9
33.6
61.1
237M
34.4
39.0
62.2
195M
32.9
38.9
57.1
183M
32.7
38.3
58.8
103M
31.6
37.3
57.6
183M
14.6
20.3
58.3

Table 3: mIoU scores are boosted when SPADE lay-
ers are used, for both the decoder architecture (Figure 4)
and encoder-decoder architecture of pix2pixHD++ (our im-
proved baseline over pix2pixHD [45]). On the other hand,
simply concatenating semantic input at every layer fails to
do so. Moreover, our compact model with smaller depth at
all layers outperforms all baselines.

high image ﬁdelity, users still prefer our results.

The effectiveness of SPADE. To study the impor-
tance of SPADE, we introduce a strong baseline called
pix2pixHD++, which combines all the techniques we ﬁnd
useful for enhancing the performance of pix2pixHD except
SPADE. We also train models that receive segmentation
mask input at all the intermediate layers via concatenation
(pix2pixHD++ w/ Concat) in the channel direction. Finally,
the model that combines the strong baseline with SPADE
is denoted as pix2pixHD++ w/ SPADE. Additionally, we
compare models with different capacity by using a different
number of convolutional ﬁlters in the generator.

Method
segmap input
random input
kernelsize 5x5
kernelsize 3x3
kernelsize 1x1
#params 141M
#params 96M
#params 61M
Sync Batch Norm
Batch Norm
Instance Norm

COCO ADE20K Cityscapes
35.2
38.5
62.3
35.3
38.3
61.6
35.0
39.3
61.8
35.2
38.5
62.3
32.7
35.9
59.9
35.3
38.3
62.5
35.2
38.5
62.3
35.2
38.0
62.5
35.0
39.3
61.8
33.7
37.9
61.8
33.9
37.4
58.7

Table 4: The SPADE generator works with different con-
ﬁgurations. We change the input of the generator, the con-
volutional kernel size acting on the segmentation map, the
capacity of the network, and the parameter-free normaliza-
tion method. The settings used in the paper are boldfaced.

As shown in Table 3 the architectures with the pro-
posed SPADE consistently outperforms its counterparts, in
both the decoder-style architecture described in Figure 4
and more traditional encoder-decoder architecture used in
pix2pixHD. We also ﬁnd that concatenating segmentation
masks at all intermediate layers, an intuitive alternative to
SPADE to provide semantic signal, does not achieve the
same performance as SPADE. Furthermore, the decoder-
style SPADE generator achieves better performance than
the strong baselines even when using a smaller number of
parameters.

72343

Figure 9: Our model attains multimodal synthesis capability when trained with the image encoder. During deployment,
by using different random noise, our model synthesizes outputs with diverse appearances but all having the same semantic
layouts depicted in the input mask. For reference, the ground truth image is shown inside the input segmentation mask.

Variations of SPADE generator. Table 4 reports the per-
formance of variations of our generator. First, we compare
two types of the input to the generator: random noise or
downsampled segmentation maps. We ﬁnd that both ren-
der similar performance, and conclude that the modulation
by SPADE alone provides sufﬁcient signal about the input
mask. Second, we vary the type of parameter-free normal-
ization layers before applying the modulation parameters.
We observe that SPADE works reliably across different nor-
malization methods. Next, we vary the convolutional kernel
size acting on the label map, and ﬁnd that kernel size of
1x1 hurts performance, likely because it prohibits utilizing
the context of the label. Lastly, we modify the capacity of
the generator network by changing the number of convolu-
tional ﬁlters. We present more variations and ablations in
the arXiv version for more detailed investigation.

Multi-modal synthesis.
In Figure 9, we show the mul-
timodal image synthesis results on the Flickr Landscape
dataset. For the same input segmentation mask, we sam-
ple different noise inputs to achieve different outputs. More
results are included in the arXiv paper.

Semantic manipulation and guided image synthesis. In
Figure 1, we show an application where a user draws dif-

ferent segmentation masks, and our model renders the cor-
responding landscape images. Moreover, our model allows
users to choose an external style image to control the global
appearances of the output image. We achieve it by replac-
ing the input noise with the embedding vector of the style
image computed by the image encoder.

5. Conclusion

We have proposed the spatially-adaptive normalization,
which utilizes the input semantic layout while performing
the afﬁne transformation in the normalization layers. The
proposed normalization leads to the ﬁrst semantic image
synthesis model that can produce photorealistic outputs for
diverse scenes including indoor, outdoor, landscape, and
street scenes. We further demonstrate its application for
multi-modal synthesis and guided image synthesis.

Acknowledgments We thank Alexei A. Efros and Jan
Kautz for insightful advice. Taesung Park contributed to
the work during his internship at NVIDIA. His Ph.D. is sup-
ported by a Samsung Scholarship.

82344

References

[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-
erative adversarial networks. In International Conference on
Machine Learning (ICML), 2017. 3
[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization.
arXiv preprint arXiv:1607.06450, 2016. 2
[3] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Gold-
man. Patchmatch: A randomized correspondence algorithm
for structural image editing. In ACM SIGGRAPH, 2009. 1
[4] D. Bau, J.-Y. Zhu, H. Strobelt, Z. Bolei, J. B. Tenenbaum,
W. T. Freeman, and A. Torralba. Gan dissection: Visualizing
and understanding generative adversarial networks. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR), 2019. 2
[5] A. Brock, J. Donahue, and K. Simonyan. Large scale gan
training for high ﬁdelity natural image synthesis. In Inter-
national Conference on Learning Representations (ICLR),
2019. 1, 2
[6] H. Caesar, J. Uijlings, and V. Ferrari. Coco-stuff: Thing and
stuff classes in context. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018. 2, 4
[7] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and
A. L. Yuille. Deeplab: Semantic image segmentation with
deep convolutional nets, atrous convolution, and fully con-
nected crfs. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence (TPAMI), 40(4):834–848, 2018. 4
[8] Q. Chen and V. Koltun. Photographic image synthesis with
cascaded reﬁnement networks. In IEEE International Con-
ference on Computer Vision (ICCV), 2017. 1, 2, 4, 5
[9] T. Chen, M. Lucic, N. Houlsby, and S. Gelly. On self mod-
ulation for generative adversarial networks. In International
Conference on Learning Representations, 2019. 2
[10] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
In IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2016. 2, 4
[11] H. De Vries, F. Strub, J. Mary, H. Larochelle, O. Pietquin,
and A. C. Courville. Modulating early visual processing
by language.
In Advances in Neural Information Process-
ing Systems (NeurIPS), 2017. 2
[12] V. Dumoulin, J. Shlens, and M. Kudlur. A learned repre-
sentation for artistic style.
In International Conference on
Learning Representations (ICLR), 2016. 2, 3
[13] I. Goodfellow,
J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-
erative adversarial nets. In Advances in Neural Information
Processing Systems (NeurIPS), 2014. 2
[14] J. Hays and A. A. Efros. Scene completion using millions of
photographs. In ACM SIGGRAPH, 2007. 1
[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition.
In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2016. 3
[16] A. Hertzmann, C. E. Jacobs, N. Oliver, B. Curless, and D. H.
Salesin.
Image analogies.
In Proceedings of the 28th an-
nual conference on Computer graphics and interactive tech-
niques, pages 327–340. ACM, 2001. 2

[17] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and
S. Hochreiter. GANs trained by a two time-scale update rule
converge to a local Nash equilibrium. In Advances in Neural
Information Processing Systems (NeurIPS), 2017. 4, 5
[18] S. Hong, D. Yang, J. Choi, and H. Lee.
Inferring seman-
tic layout for hierarchical text-to-image synthesis. In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 2
[19] X. Huang and S. Belongie. Arbitrary style transfer in real-
time with adaptive instance normalization.
In IEEE Inter-
national Conference on Computer Vision (ICCV), 2017. 2,
3
[20] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz. Multimodal
unsupervised image-to-image translation. European Confer-
ence on Computer Vision (ECCV), 2018. 2, 3
[21] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
In International Conference on Machine Learning (ICML),
2015. 2, 3
[22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
Image-to-
image translation with conditional adversarial networks. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 1, 2, 3
[23] L. Karacan, Z. Akata, A. Erdem, and E. Erdem. Learning
to generate images of outdoor scenes from attributes and se-
mantic layouts. arXiv preprint arXiv:1612.00215, 2016. 2
[24] L. Karacan, Z. Akata, A. Erdem, and E. Erdem. Manipu-
lating attributes of natural scenes via hallucination. arXiv
preprint arXiv:1808.07413, 2018. 2
[25] T. Karras, S. Laine, and T. Aila. A style-based generator
architecture for generative adversarial networks.
In IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 2
[26] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In International Conference on Learning Rep-
resentations (ICLR), 2015. 4
[27] D. P. Kingma and M. Welling. Auto-encoding variational
bayes. In International Conference on Learning Representa-
tions (ICLR), 2014. 2, 4
[28] A. Kolliopoulos,
J. M. Wang,
and A. Hertzmann.
Segmentation-based 3d artistic rendering.
In Rendering
Techniques, pages 361–370, 2006. 2
[29] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. In Ad-
vances in Neural Information Processing Systems (NeurIPS),
2012. 2
[30] J. H. Lim and J. C. Ye. Geometric gan. arXiv preprint
arXiv:1705.02894, 2017. 3
[31] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll ´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European Conference on Com-
puter Vision (ECCV), 2014. 2, 4
[32] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-
image translation networks. In Advances in Neural Informa-
tion Processing Systems (NeurIPS), 2017. 2

92345

[33] X. Mao, Q. Li, H. Xie, Y. R. Lau, Z. Wang, and S. P. Smol-
ley. Least squares generative adversarial networks. In IEEE
International Conference on Computer Vision (ICCV), 2017.
3
[34] L. Mescheder, A. Geiger, and S. Nowozin. Which training
methods for gans do actually converge?
In International
Conference on Machine Learning (ICML), 2018. 2, 3
[35] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spec-
tral normalization for generative adversarial networks. In In-
ternational Conference on Learning Representations (ICLR),
2018. 3, 4
[36] T. Miyato and M. Koyama. cGANs with projection discrim-
inator. In International Conference on Learning Representa-
tions (ICLR), 2018. 2, 3
[37] K. Nakashima. Deeplab-pytorch. https://github.
com/kazuto1011/deeplab-pytorch, 2018. 4
[38] A. Odena, C. Olah, and J. Shlens. Conditional image synthe-
sis with auxiliary classiﬁer GANs. In International Confer-
ence on Machine Learning (ICML), 2017. 2
[39] E. Perez, H. De Vries, F. Strub, V. Dumoulin, and
A. Courville. Learning visual reasoning without strong
priors.
In International Conference on Machine Learning
(ICML), 2017. 2
[40] X. Qi, Q. Chen, J. Jia, and V. Koltun. Semi-parametric im-
age synthesis. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018. 4, 5
[41] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and
H. Lee. Generative adversarial text to image synthesis. In In-
ternational Conference on Machine Learning (ICML), 2016.
2
[42] T. Salimans and D. P. Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In Advances in Neural Information Processing
Systems (NeurIPS), 2016. 2
[43] D. Ulyanov, A. Vedaldi, and V. Lempitsky.
Instance nor-
malization: The missing ingredient for fast stylization. arxiv
2016. arXiv preprint arXiv:1607.08022, 2016. 2, 3
[44] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz,
and B. Catanzaro. Video-to-video synthesis. In Advances in
Neural Information Processing Systems (NeurIPS), 2018. 1,
4
[45] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional gans. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018. 1,
2, 3, 4, 5, 7

[46] X. Wang, K. Yu, C. Dong, and C. Change Loy. Recover-
ing realistic texture in image super-resolution by deep spatial
feature transform. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 606–615,
2018. 2
[47] Y. Wu and K. He. Group normalization. In European Con-
ference on Computer Vision (ECCV), 2018. 2
[48] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun. Uniﬁed per-
ceptual parsing for scene understanding. In European Con-
ference on Computer Vision (ECCV), 2018. 4
[49] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
X. He. Attngan: Fine-grained text to image generation with
attentional generative adversarial networks. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2018. 2
[50] F. Yu, V. Koltun, and T. Funkhouser. Dilated residual net-
works. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017. 4
[51] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-
attention generative adversarial networks. arXiv preprint
arXiv:1805.08318, 2018. 1, 2, 3
[52] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and
D. Metaxas. Stackgan: Text to photo-realistic image synthe-
sis with stacked generative adversarial networks.
In IEEE
International Conference on Computer Vision (ICCV), 2017.
1, 2
[53] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang,
and D. Metaxas. Stackgan++: Realistic image synthesis
with stacked generative adversarial networks. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence (TPAMI),
2018. 1
[54] B. Zhao, L. Meng, W. Yin, and L. Sigal. Image generation
from layout. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019. 2
[55] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and
A. Torralba. Scene parsing through ade20k dataset.
In
IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), 2017. 2, 4
[56] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-
to-image translation using cycle-consistent adversarial net-
works. In IEEE International Conference on Computer Vi-
sion (ICCV), 2017. 2
[57] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros,
O. Wang, and E. Shechtman. Toward multimodal image-to-
image translation. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), 2017. 2, 3

102346

Shapes and Context: In-the-Wild Image Synthesis & Manipulation

Aayush Bansal
Yaser Sheikh
Deva Ramanan
Carnegie Mellon University

{aayushb,yaser,deva}@cs.cmu.edu

input	

output	

input	

output	

input	

output	

Figure 1. Our approach synthesizes images from a label mask by non-parametric matching of shapes, parts, and pixels. We show
example results for diverse “in-the-wild” scenes containing large amounts of variation in object composition and deformation.

Abstract

We introduce a data-driven model for interactively syn-
thesizing in-the-wild images from semantic label
input
masks. Our approach is dramatically different from re-
cent work in this space, in that we make use of no learn-
ing.
Instead, our approach uses simple but classic tools
for matching scene context, shapes, and parts to a stored
library of exemplars. Though simple, this approach has
several notable advantages over recent work: (1) because
nothing is learned, it is not limited to speciﬁc training data
distributions (such as cityscapes, facades, or faces); (2) it
can synthesize arbitrarily high-resolution images, limited
only by the resolution of the exemplar library; (3) by ap-
propriately composing shapes and parts, it can generate
an exponentially large set of viable candidate output im-
ages (that can say, be interactively searched by a user). We
present results on the diverse COCO dataset, signiﬁcantly
outperforming learning-based approaches on standard im-
age synthesis metrics. Finally, we explore user-interaction
and user-controllability, demonstrating that our system can
be used as a platform for user-driven content creation.

1. Introduction

We introduce a data-driven model for interactively syn-
thesizing diverse images from semantic label input masks.
Speciﬁcally, we seek to design a system for in-the-wild im-
age synthesis that is controllable and interpretable. While
content creation is a compelling task in of itself (a classic

goal of computer graphics), image synthesis is also useful
for generating data that can be used to train discriminative
visual recognition systems [29]. Synthesized data can be
used to explore scenarios that are difﬁcult or too dangerous
to sample directly (e.g., training an autonomous perception
system on unsafe urban scenes [31]). Figure 1 shows im-
ages synthesized using our approach, where the input is a
semantic label image.
Parametric vs Nonparametric: Current approaches for
image synthesis and editing can be broadly classiﬁed into
three categories. The ﬁrst category uses parametric machine
learning models. The current state-of-the-art [10, 33, 55]
relies on deep neural networks [39] trained with adversar-
ial losses (GANs) [21] or perceptual losses [34] to create
images. These approaches work remarkably well when
trained on datasets with somewhat limited diversity, such
as cityscapes [11], faces [2, 52], or facades [54]. But it
is unclear how to extend such approaches for “in-the-wild”
image synthesis or editing: parametric models trained on
one data distribution (e.g. cityscapes) do not seem to gen-
eralize to others (e.g. facades), a problem widely known as
dataset bias [53]. The second category of work [1, 14, 15,
27, 38, 48] uses non-parametric nearest neighbors to create
content.These approaches have been demonstrated on inter-
active image editing tasks such as object insertion [38] or
scene completion [25]. Though a large inspiration for our
own work, such approaches have interestingly fallen out of
favor in recent history.
Does more data help? A peculiar property of many
parametric synthesis methods is that they do better with less
data [2, 21, 33, 45, 58, 62]. The culprit seems to be that such

12317

p
a

m

l

e
b
a

l

e
g
a

r

e
v
a

Cityscapes	

COCO	

s

e
p
a
c
s

y

t

i

C

pix2pix	
ours	
an	example	of	label	map	similar	to	average	label	map	

input	

pix2pix	

ours	

original	

(a).	constrained	vs.	in-the-wild	data	distribu>on	

(b).	simple	examples	with	varying	foreground/background	

input	

ours	(global	shapes)	

ours	(full)	

input	

ours	(global	shapes)	

ours	(full)	

(c).	global	shapes	are	not	suﬃcient	

Figure 2. Limitations of current approaches for images synthesis: (a) Current image synthesis models tend to be trained on datasets with
somewhat limited diversity, such as cityscapes [11], faces [52], or facades [54]. For example, the average label mask for Cityscapes [11]
clearly reveals redundant structure such as a car hood, road, and foliage. In contrast, the average image for COCO [40] is much less
structured, suggesting it is a more varied dataset. (b) Indeed, we train state-of-the-art neural architectures [33, 55] on COCO and observe
poor convergence (even after a month of training!) resulting in a mode collapsed and averaged outputs. (c) In contrast, our simple matching-
based approach is able to synthesize realistic image content by matching to exemplar shapes. In order to generate high-quality images, we
ﬁnd it crucial to encode scene context and part deformation in the matching process - i.e., matching global shapes alone will produce poor
images with missing regions due to shape mismatches.

methods don’t do well on diverse training sets, and in prac-
tice larger training sets tend to be diverse. This is in contrast
with truly non-parametric methods that do better with more
data [25]. Figure 2-(a) highlights the differences between
limited and diverse datasets, using illustrative examples of
Cityscapes [11] and COCO [40]. While parametric meth-
ods do well on limited data distributions, they struggle to
perform on diverse datasets. Recent works [9, 43] have at-
tempted to overcome this challenge by using enormously
large model sizes and crazy big compute.

Composition by parts: In this work, we make three ob-
servations that inﬂuence our ﬁnal approach; (1) humans can
imagine multiple plausible output images given a particular
input label mask. We see this rich space of potential out-
puts as a vital part of the human capacity to imagine and
generate. Most parametric networks tend to formulate syn-
thesis as a one-to-one mapping problem, and so struggle to
provide diverse outputs (a phenomena also known as mode
collapse). Important exceptions include [3, 10, 20, 63] that
generated multiple outputs by employing various modiﬁ-
cations.
(2) visual scenes are exponentially complex due

to many possible compositions of constituent objects and
parts.
It is tempting to combine both observations, and
generate multiple outputs by composing scene elements to-
gether. But these compositions cannot be arbitrary - one
cannot freely swap out a face with a wheel, or place a
elephant on a baseball ﬁeld. To ensure consistency, our
matching process makes use of implicit contextual seman-
tics present in the library of exemplar label masks. (3) given
an exemplar set of sufﬁcient size, nearest neighbor meth-
ods may still perform well with simple features that are not
learned (e.g., pixel values). We combine our observations to
construct an image synthesis system that exemplar shapes
and parts using simple pixel features.

Our Contributions: (1) we study the problem of visual
content creation and manipulation for in-the-wild settings,
and observe that reliance on parametric models lead to aver-
aged or mode-collapsed outputs; (2) we present an approach
that utilize shapes and context to generate images consist-
ing of rigid and non-rigid objects in varied backgrounds,
and different environmental and illumination conditions; (3)
we demonstrate the controllable and interpretable aspects of

2318

our approach that enables a user to inﬂuence the generation
and select examples from many outputs.

2. Background

Our work combines various ideas on shapes, deformable
parts, context, and non-parametric approaches developed in
last two decades. We position each separately and the spe-
ciﬁc insights for their particular usage.
Shapes & Non-parametric approaches: Shapes [37, 41,
46] emerge naturally in our world due to its compositional
structure. If we had an inﬁnite data-source with all potential
shapes for all the objects, then our world could be repre-
sented by a linear combination of different shapes [23, 46].
In this work, we aim to generate images from seman-
tic and instance label maps as input. Meaningful shapes
and contours [5, 24] makes for an obvious interpretation
for such input.
In an unconstrained in-the-wild data dis-
tribution consisting of both rigid and non-rigid objects,
it become non-trivial to model such shapes for a one-to-
many mappings. We, therefore, want to leverage the shape
information explicitly from the training data in our for-
mulation by simple copy-pasting. Non-parametric meth-
ods [14, 15, 16, 19, 27, 35] ﬁnd their use for various com-
puter vision tasks such as texture-synthesis [15, 16], im-
age super-resolution [19], action recognition [14], or scene
completion [25].
Our work draws similarity to idea of scene compos-
ites [32, 44, 49, 48]. Russell et al [48] use shapes or scene
composites to query matches for semantic segmentation us-
ing LabelMe dataset [50]. In an another work, Russell et
al [49] use similar idea of composites for object discov-
ery.
Isola and Liu [32] use this idea of composites for
scene parsing and collaging. Recently, Qi et al [44] used
shapes in a semi-parametric form to synthesize images from
a semantic label map. These different approaches [44, 48]
on scene composites or shapes are however constrained to
rigid and non-deformable objects from a constrained data-
distribution such as road-side scenarios from LabelMe [50],
or Cityscapes [11]. Our work extends the prior work to
non-rigid and deformable shapes from an unconstrained in-
the-wild data distribution. Figure 2-(c) shows how global
shapes are insufﬁcient and one needs to consider local in-
formation about parts and pixels.
Deformable Objects & Parts: The global shape ﬁtting can
be reliably estimated for non-deformable objects but local
shapes or parts [7, 18, 22] are required when considering
non-rigid objects. The prior work on local components [7],
regions [22], or parts [18, 51, 57] has largely been focused
on recognition. On the other hand, our work draws insight
from ideas on compositional matching [8, 17] and we use
the parts, components, and regions for synthesizing images.
In this work, we generate parts from various global shapes
to do image synthesis. This enables us to consider local

information without any explicit part labels.
Context as a major cue: Context is a natural and power-
ful tool to put things in perspective [6, 30]. There is a wide
literature on the use of context in computer vision commu-
nity [13, 42] and is beyond the scope of this work to illus-
trate them completely. In this work, we use contextual infor-
mation at both global and local level to do better and faster
matching of global shapes, parts, and pixels. The contextual
information, while itself remaining in background, enables
us to do an effective non-parametric matching.
User Controllable Content Synthesis & Manipulation:
Multiple works [3, 4, 38, 61, 59, 55] in computer graph-
ics and vision literature have demonstrated the importance
of user-controlled image operations. Grab-cut [47] enables
user-based segmentation of a given scene. Lalonde et al [38]
use a non-parametric approach to insert objects in a given
image. Kholgade et al [36] demonstrate a user-controlled
3D object manipulation. In this work, we demonstrate how
shapes can be used naturally and intuitively for a user con-
trollable content creation and manipulation.

3. Method

Given a semantic and an instance label map, X , our
goal is to synthesize a new image, Y . Our formulation is a
hierarchical non-parametric matching ensuring the follow-
ing stages in order: (1) global scene context; (2) instance
shape consistency; (3) local part consistency; and ﬁnally
(4) minute pixel-level consistency.
Global Scene Context: In a big data settings with hundred
thousands and million examples, doing nearest neighbors
could be a time-consuming process. We make this pro-
cess faster by using global scene context to prune the list
of training examples from which the shapes should be ex-
tracted. Only those examples are considered if they fall in
one of three categories: (1) their global image has same la-
bels as input; (2). the labels in input is its subset; (3). the
labels in input is its superset. This reduces the search space
from hundred thousand shapes to a few hundreds. We fur-
ther prune them to top-N images for searching shapes by
computing a global coverage and a pixel coverage score.
A global coverage score is computed to ensure the top-
N label maps in the training set have similar distribution of
labels as are in a given query label map. We compute the
normalized histogram of labels (both query and training),
and compute a l2 distance between query and training label
map. A pixel coverage score is computed to ensure we se-
lect the images with maximum pixel-to-pixel overlap. This
score is computed by aligning a query label map and an
example from training set, followed by the hamming dis-
tance between them. To make it faster, we resize the im-
ages to 100×100 and then compute the normalized ham-
ming distance between the respective labels. We sum both
global coverage and pixel coverage scores, and choose N

2319

input	

top-3	global	shape	matching	by	shape	&	context	feature	

*	

*	

=	

shapes	are	resized	to	256	x	256		

original	shape	

retrieved	shape	

each	shape	is	binned	into	16	x16	bins	

48x48		
feature	
vector	

extrac?ng	features	
for	a	16x16	bin	

search	space	of	
112x112	for	each	bin	

shapes	

shapes+parts	

shapes+parts+pixels	

shape	consistency	

part	consistency	

hierarchical	composi0on	

Figure 3. Three stages of non-parametric matching: (1) Shape Consistency - Given an input label mask, we extract various shapes. We
extract shapes from the training set for a query shape by using a shape-and-context feature. We show examples of top-3 retrieved shapes
for a query shape on left. The image information from the retrieved shapes is then extracted by considering the mask of query shape and
retrieved shape; (2) Part Consistency - We observe that global shape retrieved in last stage is missing information about the hands and
legs of the query shape (human in this case). We deﬁne a local shape matching approach that looks in the neighborhood to synthesize
parts. The query and top-k shapes are resized to 256×256, and binned into 16×16 bins with each bin being a 16×16 patch. Each patch
is represented by label information contained in it, and an additional 8 neighboring patches. This provides contextual information about
the surroundings. The parts are looked in an adjacent 112×112 region and the ones with minimum hamming distance is considered. (3)
Hierarchical Composition: Given an input label mask (top-left), we show the outputs of three stages of our non-parametric matching
approach. The ﬁrst column shows the output of composition of global shapes extracted using our shape consistency algorithm. The second
column shows the improved output by introducing local part consistency to previous output. Finally, minute pixel level holes are ﬁlled by
our pixel-consistency algorithm. See section 3 for more details.

images in the training set with the lowest scores. This use
of global scene context drastically reduces the search space
for our non-parametric approach, and enables to do synthe-
sis with a humble compute power (single core CPUs instead
of GPUs).

Shape Consistency: We seek shapes as the ﬁrst step
to deﬁne different components in an image. We repre-
sent the shapes in an instance and semantic label mask as
{x1 , x2 , ..., xN } where N is the total number of shapes for
a given input. Each shape has an associated semantic label
l : l ∈ {1, 2, ..., L} where L is the number of unique la-
bels. We then make a tight bounding box around this shape
xi so that it could be used as a rectangular convolutional
ﬁlter (wi ) to retrieve similar shapes from the training data.
We represent this ﬁlter using: (1) a simple logical opera-
tor: the part of a shape (xi ) in the ﬁlter (wi ) is set to 1,
and the remaining part is set to −1. This forces the ﬁlter to
search the composites with boundaries and details; and (2).
a contextual operator: we extract the labels from the input
label mask for this ﬁlter. This information is to force our
matching function to extract the shapes which have similar
context.

We use the logical operator (w l ) and contextual operator
(wc ) to rank the remaining shapes for our query component
using the scoring function:

Sshape (wi , wj ) = w l

i

∗ w l

j +

Ns

X

k=1

I (wc

i,k

− wc
j,k ),

(1)

where I is an indicator function, and Ns is the total num-
ber of pixels in a given query component. Since we have
ﬁxed the size to 50×50 in our formulation, Ns = 2500. We
use this scoring function (Eq. 1) to score different shapes in
our pruned list for a given shape. The RGB component for
an extracted shape is its intersection with the query shape,
i.e. only pixels active in both extracted and query shape are
considered. Figure 3-left shows the part of our algorithm to
compute shape-consistency score.
We ignore the shapes if the ratio of their aspect-ratio to
that of query component is either less than 0.5 or greater
than 2. Finally, we make this convolution processing faster
by using a ﬁxed size ﬁlters and low-res label masks of
50×50. This also helps us to generate composite of arbitrar-
ily high resolution without any extra computational cost.
Part Consistency: Modeling occlusions and deformable
aspects of non-rigid objects in real world are extremely
hard. The problem is even aggravated with noisy shape in-
puts. The insufﬁcient shape data and non-rigid objects in
real world leads to parts and local regions [7]. We seek
parts from top-k global shapes.
Importantly, the part in-

2320

input	

output	

input	

output	

input	

output	

Figure 4. Non-parametric matching: Our approach for generating images from a label mask by non-parametric matching of global
shapes, local parts, and pixel-consistency. The above examples contain varying background, cluttered environment, varying weather and
illumination conditions, and multiple rigid and non-rigid objects in various shapes and forms.

formation is required when a global shape is not able to
capture. We extract the knowledge of parts from the global
shapes in a spirit similar to non-parametric texture synthe-
sis [16]. The shape components are resized to 256×256, so
that local information can be well searched. We extract a
16×16 patches from the resized global shape template. Lo-
cal contextual information (similar to HOG [12], or Group-
Normalization [56]) is used by considering the neighboring
8 patches. The parts are scored using:

Spart (wp

i , wp

j ) =

Np

X

k=1

I (wp

i,k

− wp
j,k )

(2)

where I is an indicator function, and each patch (wp ),
is represented by a Np (256×9) dimensional vector con-
taining the label information in the patch. Importantly, we
do not need to look in a larger window for part matching
as we have weakly aligned global shapes. Therefore, we
restrict the patches to look in a surrounding 5×5 patch win-
dow. This corresponds to 112×112 pixel window in a re-
sized global shape template. To copy the RGB component,
we take an average of top-3 retrieved patch windows. Fig-
ure 3-middle shows the part of our algorithm to compute
part-consistency score.
Pixel Consistency: The shapes and parts have accounted
for most of the non-parametric image synthesis. How-

ever, they does not ensure pixel-level consistency and of-
ten ends up with minor holes in an image. We enforce a
pixel-level consistency in this process to account for the re-
maining holes in synthesized image. This process is simi-
lar to our part consistency algorithm, except that it is done
on every pixel. Each pixel is represented by a surround-
ing 11×11 window. We use the criterion in Eq. 2 to com-
pute similarity between two feature vectors. To expedite
this matching, we compute features for a low-res input la-
bel map (128×128) as pixel consistency is ensured to ﬁll
minor holes alone. Finally, we look in surrounding region
of 5×5 from a 128×128 image to ﬁll in the information as
global and local consistency have already been accounted
by shape and part consistency.

Hierarchical Composition: We combine the information
hierarchically from shapes, parts, and pixels to generate a
full image. Figure 3-right shows the composition starting
from an input label mask. Firstly, we use the global shape
component to ﬁll the major chunk of images. The miss-
ing information is then ﬁlled using the local part consis-
tency. Finally, the minor holes are ﬁlled using the pixel-
level consistency. The combination of these three stages
enable us to generate a image from an input label mask by
simple non-parametric matching. Figure 4 shows outputs
generated by our approach for varying background, clut-
tered environment, varying weather and illumination con-
ditions, and multiple rigid and non-rigid objects in various

2321

input	

pix2pix	

ours	(global	shape)	

ours	(full)	

original	

rand-top-5-NN	

Figure 5. Parametric vs. Non-parametric: We generate images from an input label mask (left). The second column shows the output
of Pix2Pix [33] speciﬁcally trained on Coco training set. The third and fourth column are the output of our approach with global shape
matching, and considering parts and pixel consistency respectively. The ﬁfth column contains the original image. Finally, the last column
shows an image for randomly selected one of top-5 nearest neighbors for input label mask.

shapes and forms.

4. Experiments

Generating Multiple-Outputs: A salient aspect of con-
sidering shapes and parts in a non-parametric matching pro-
vides multiple outputs for free. We use the extracted shapes
and parts, and can combine them in exponential ways with-
out any extra overhead. We show multiple examples syn-
thesized for a given label mask using our approach in Fig-
ure 6. Generating these multiple outputs is not trivial when
using parametric approaches [10, 33], and a substantial re-
search [20, 63] has been conducted for this process. How-
ever, it is just trivial for non-parametric matching.

User Controllable Content Creation: We ﬁnally demon-
strate the applicability of our approach for a user control-
lable content creation in Figure 7. Note how our approach
can be easily used to edit the label mask by inserting shapes
to generate a new output. More importantly, synthesis and
manipulation aspects go hand-in-hand for our approach. A
human user can clearly interpret and inﬂuence any stage of
synthesis, and can easily generate a different output by vary-
ing a shape. Manipulation naturally emerges in our non-
parametric approach without any additional efforts. This is
not true for prior parametric approaches that require a spe-
cialized machinery for this task.

Dataset: We use semantic and instance label mask from
COCO [40] to study the problem of in-the-wild image syn-
thesis and manipulation. This dataset consists of 134 differ-
ent objects and stuff categories making it the most diverse
and varied publicly available dataset. There are 118, 287
images in the training set (40× more than cityscapes [11]),
and 5, 000 images in the validation set (100× more than
cityscapes). We use the paired data of labels and images
from training set to extract global shapes and synthesize
parts and pixels. The images are synthesized using seman-
tic and instance label masks in validation set. Our approach
does not require any training, and therefore can use the la-
bels and image component from anywhere. For the sake
of fair comparison with parametric approaches, we restrict
ourselves to COCO training data.

Baselines: To the best of our knowledge, there does not ex-
ist a non-parametric approach that has attempted the prob-
lem of the in-the-wild image synthesis from label masks.
We, therefore, compare our approach with parametric ap-
proaches: (1). Pix2Pix [33]; and (2). Pix2Pix-HD [55],
using their publicly available codes. The complexity, di-
versity, and size of this dataset makes it a computational
challenge for a generative parametric approach to deal with.
Training a simple Pix2Pix model took 20 days on a single

2322

input	

outputs	

input	

outputs	

Figure 6. Multiple Outputs: Our approach can easily generate exponentially large number of outputs by changing shapes and parts. We
show four outputs generated for each input label mask.

Nvidia Titan-X GPU. On the same compute, we trained a
Pix2Pix-HD model for a month but did not observe any con-
vergence. It may be possible that a reasonable Pix2Pix-HD
model be trained if we let the training go longer for an ex-
tra month or two, or use advanced computational resources.
It may also be due to design of architecture and hyper-
parameters speciﬁcally suited for Cityscapes, and that ef-
forts are required to tune hyper-parameters to make it work
for a large and diverse dataset as COCO. For the sake of
fair comparison, we additionally use Cityscapes to contrast
our approach with prior works [10, 33, 44, 55] even when
it comes at the cost of performance for our own approach
due to limited data. Additionally, we resize our generated
outputs to 256 × 256 just to make a fair comparison with
Pix2Pix on COCO. However, we can generate outputs hav-
ing same resolution as that of input label masks without any
increase in compute.

FID Scores: We compute FID scores [28] using the im-
ages generated from different approaches. Lower FID val-
ues suggest more realism. Table 1 contrast FID scores
computed on generated images (COCO) with Pix2Pix and
Pix2Pix-HD (resized to 256×256 and 64×64 resolution).
Without using any oracle, the top-1 example generated from
our approach signiﬁcantly outperforms the prior work. Ad-
ditionally, note the performance improvement due to each
stage in our hierarchical composition.

Mask-RCNN Scores:
We use a pre-trained Mask-
RCNN [26] to study the quality of synthesis on COCO [40]
for Pix2Pix and our approach. This model is trained for
80 object categories of COCO dataset. While it is trained
for instance segmentation, we use its output and convert it
to semantic labels for consistency in evaluation. Our goal
is to observe if we can get the same class labels from the
synthesized images as one would expect from a real image.
We, therefore, run it on original images from the validation
set and use these pseudo semantic labels as ground truth for

Method

#examples Oracle

FID score

FID score

Pix2Pix [33]

Pix2Pix-HD [55]

Ours (shapes)

Ours (shapes+parts)

Ours (shapes+parts+pixels)

1

1

1

1

1

(256×256)

(64×64)

7

7

7

7

7

70.43

157.13

37.26

32.62

31.63

41.45

109.49

23.22

18.02

16.61

Table 1. FID Scores on COCO: We compute FID score [28] to
contrast the realism in outputs produced by different approaches.
Lower FID values suggest more realism. We observe that our
approach outperforms prior approaches signiﬁcantly. We also
demonstrate as how different stages in our hierarchical composi-
tion leads to better outputs.

evaluation. Next, we run it on synthesized images and con-
trast it with the labels from original image. To measure the
performance, we use three criterion: (1) mean pixel accu-
racy (PC); (2) mean class accuracy (AC); (3) mean inter-
section over union (IoU). Higher the score for each of the
criterion, better is the quality of synthesis. Table 2 con-
trasts the performance of our approach with Pix2Pix and
demonstrates substantially better results. Our performance
improves when an oracle is used to select the best from ﬁve
outputs. Note that top-100 examples from the training set
are used for global shape matching in our approach for this
experiment.
Human Studies: We did human studies on a randomly
selected 500 images. We show the outputs of Pix2Pix,
Pix2Pix-HD, and our approach (randomly picked one out-
put from multiple) to human subjects for as much time as
they need to make a decision. We asked them to pick one
that looks close to a real image. The users were advised to
use ‘none of these’ if all approaches are consistently bad.
51.2% times user picked an output generated from our ap-
proach, 7.8% times the outputs from Pix2Pix, and preferred
‘none of these’ 41% times. The human studies suggest that
while our approach is most likable but there are still many

2323

original	label	

synthesized	output	

add	shape	

modiﬁed	label	

new	RGB	component	

manipulated	output	

Figure 7. User-Intervention & Image Manipulation: The ﬁrst two columns shows an original label mask and a synthesized output for it
using our approach. A user can add a shape to the label mask, and a new output will be generated by matching the corresponding shape.

Method

#examples Oracle

PC AC IoU

Method

#examples Oracle

PC AC IoU

Parametric

Pix2Pix [33]

Non-Parametric

Ours

Ours

1

1

5

7

7

17.9

8.9

4.9

44.5 31.0 20.9

X 58.2 41.2 31.4

Table 2. Mask-RCNN Scores on COCO: We use a pre-trained
Mask-RCNN model [26] to study the quality of image synthesis.
We run it on synthesized images and contrast it with the labels
from original image. To measure the performance, we use three
criterion: (1) mean pixel accuracy (PC); (2) mean class accuracy
(AC); (3) mean intersection over union (IoU). Higher the score
for each of the criterion, better is the quality of synthesis. We
outperform Pix2Pix. The performance further improves signiﬁ-
cantly when an oracle is used to select from ﬁve examples.

situations where our approach produced undesirable out-
puts.
Cityscapes: Table 3 contrasts the performance of our ap-
proach with prior approaches [10, 33, 44, 55] that have
speciﬁcally demonstrated on Cityscapes. Except Pix2Pix,
we used publicly available results for this evaluation. Our
approach performs second to Pix2Pix-HD and better than
prior parametric and semi-parametric approaches with just
25 images to extract shapes and parts to compose a new im-
age from semantic labels. The performance improves when
using an oracle to select best amongst the 5 generated out-
puts. Our performance may further improve as we increase
the number of global images to do shape and part extraction.

5. Discussion & Future Work

We present an exceedingly simple non-parametric ap-
proach for image synthesis and manipulation in-the-wild.
While the diverse data-distribution and large datasets make
it challenging for parametric approaches to operate on, it

Parametric

Pix2Pix [33]

CRN [10]

Pix2Pix-HD [55]

Semi-Parametric

SIMS [44]

Non-Parametric

Ours (top-25)

Ours (top-25)

1

1

1

1

1

5

7

7

7

7

7

72.5 29.5 24.6

49.0 22.5 18.2

79.0 43.3 37.8

68.6 35.1 28.1

67.1 38.0 30.5

X 71.3 39.6 32.4

Table 3. PSP-Net Scores on Cityscapes: We use a pre-trained
PSP-Net model [60] to evaluate the quality of synthesized images
(1024×2048). This model is trained for semantic segmentation on
cityscapes. We run the synthesized images through this model, and
generate a semantic label map for each image. The semantic label
map from the synthesized images is contrasted with the semantic
label map from the original image. We compute three statistics
for each approach: (1) Mean Pixel Accuracy (PC); (2) Mean Class
Accuracy (AC); (3) Mean intersection over union (IoU). For each
of these criterion- higher the score, better is the quality of syn-
thesis. With just 25 global nearest neighbors to extract shapes and
parts, our non-parametric approach is competitive to the paramet-
ric models and semi-parametric models.

enables simple matching of shapes and parts to work well.
The non-parametric matching enables us to generate expo-
nentially large number of outputs by varying shapes and
parts. Importantly, shapes and parts are intuitive to a nor-
mal human user as well. This makes our approach inter-
pretable and suitable for user-controllable content creation
and editing. The future work in this direction may address
smarter ways of combining shapes and parts information,
and explore spatiotemporal consistency to do in-the-wild
video synthesis and manipulation.

2324

References

[1] Mathieu Aubry, Daniel Maturana, Alexei A. Efros, Bryan C.
Russell, and Josef Sivic. Seeing 3D chairs: Exemplar part-
based 2D-3D alignment using a large dataset of CAD mod-
els. In CVPR, 2014. 1
[2] Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser
Sheikh. Recycle-gan: Unsupervised video retargeting.
In
ECCV, 2018. 1
[3] Aayush Bansal, Yaser Sheikh, and Deva Ramanan. PixelNN:
Example-based image synthesis. In ICLR, 2018. 2, 3
[4] Connelly Barnes, Eli Shechtman, Adam Finkelstein, and
Dan B Goldman. Patchmatch: A randomized correspon-
dence algorithm for structural image editing. ACM Trans.
Graph., 2009. 3
[5] Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape
matching and object recognition using shape contexts. IEEE
Trans. Pattern Anal. Mach. Intell., 2002. 3
[6] I. Biederman. On the Semantics of a Glance at a Scene. 3
[7] I. Biederman. Recognition by components: a theory of hu-
man image interpretation. Pyschological review, 94:115–
147, 1987. 3, 4
[8] Oren Boiman and Michal Irani. Similarity by composition.
In NIPS. 2006. 3
[9] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high ﬁdelity natural image synthesis.
In ICLR, 2019. 2
[10] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reﬁnement networks. In ICCV, 2017. 1,
2, 6, 7, 8
[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapes
dataset for semantic urban scene understanding. In CVPR,
2016. 1, 2, 3, 6
[12] Navneet Dalal and Bill Triggs. Histograms of oriented gra-
dients for human detection. In CVPR, 2005. 5
[13] Santosh Kumar Divvala, Derek Hoiem, James H. Hays,
Alexei A. Efros, and Martial Hebert. An empirical study
of context in object detection. In CVPR, 2009. 3
[14] Alexei A. Efros, Alexander C. Berg, Greg Mori, and Jitendra
Malik. Recognizing action at a distance. In ICCV, 2003. 1,
3
[15] Alexei A. Efros and William T. Freeman. Image quilting for
texture synthesis and transfer. 2001. 1, 3
[16] Alexei A. Efros and Thomas K. Leung. Texture synthesis by
non-parametric sampling. In ICCV, 1999. 3, 5
[17] A. Faktor and M. Irani. Co-segmentation by composition. In
ICCV, 2013. 3
[18] Pedro Felzenszwalb, David McAllester, and Deva Ra-
manan. A discriminatively trained, multiscale, deformable
part model. In CVPR, 2008. 3
[19] William T. Freeman, Thouis R. Jones, and Egon C Pasz-
tor. Example-based super-resolution. IEEE Comput. Graph.
Appl., 2002. 3
[20] Arnab Ghosh, Viveka Kulharia, Vinay P. Namboodiri, Philip
H. S. Torr, and Puneet Kumar Dokania. Multi-agent diverse
generative adversarial networks. In CVPR, 2018. 2, 6

[21] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
and Yoshua Bengio. Generative adversarial networks.
In
NIPS, 2014. 1
[22] Chunhui Gu, Joseph J Lim, Pablo Arbel ´aez, and Jitendra Ma-
lik. Recognition using regions. In CVPR, 2009. 3
[23] A. Gupta, A. Efros, and M. Hebert. Blocks world revis-
ited: Image understanding using qualitative geometry and
mechanics. In ECCV, 2010. 3
[24] B. Hariharan, P. Arbelez, L. Bourdev, S. Maji, and J. Malik.
Semantic contours from inverse detectors. In ICCV, 2011. 3
[25] James Hays and Alexei A Efros. Scene completion using
millions of photographs. ACM Trans. Graph., 2007. 1, 2, 3
[26] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask R-CNN. In ICCV, 2017. 7, 8
[27] Aaron Hertzmann, Charles E. Jacobs, Nuria Oliver, Brian
Curless, and David H. Salesin. Image analogies. ACM Trans.
Graph., 2001. 1, 3
[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NIPS. 2017. 7
[29] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,
Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Dar-
rell. Cycada: Cycle-consistent adversarial domain adapta-
tion. In ICML, 2018. 1
[30] D. Hoiem, A.A. Efros, and M. Hebert. Putting objects in
perspective. In CVPR, 2006. 3
[31] Shiyu Huang and Deva Ramanan. Expecting the unexpected:
Training detectors for unusual pedestrians with adversarial
imposters. In CVPR, 2017. 1
[32] Phillip Isola and Ce Liu. Scene collaging: Analysis and syn-
thesis of natural images with semantic layers. In ICCV, 2013.
3
[33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros.
Image-to-image translation with conditional adver-
sarial networks. In CVPR, 2017. 1, 2, 6, 7, 8
[34] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual
losses for real-time style transfer and super-resolution.
In
ECCV, 2016. 1
[35] Micah K. Johnson, Kevin Dale, Shai Avidan, Hanspeter Pﬁs-
ter, William T. Freeman, and Wojciech Matusik. CG2Real:
Improving the realism of computer generated images using a
large collection of photographs. In IEEE Trans. Visualization
and Computer Graphics, 2011. 3
[36] Natasha Kholgade, Tomas Simon, Alexei Efros, and Yaser
Sheikh. 3d object manipulation in a single photograph using
stock 3d models. ACM Trans. Graph., 2014. 3
[37] Jan J. Koenderink. Solid Shape. MIT Press, Cambridge, MA,
USA, 1990. 3
[38] Jean-Franc¸ ois Lalonde, Derek Hoiem, Alexei A. Efros,
Carsten Rother, John Winn, and Antonio Criminisi. Photo
clip art. ACM Trans. Graph., 2007. 1, 3
[39] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep
learning. Nature, 521(7553):436–444, 2015. 1
[40] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
Bourdev, Ross B Girshick, James Hays, Pietro Perona, Deva

2325

[59] Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng,
Angela S Lin, Tianhe Yu, and Alexei A Efros. Real-time
user-guided image colorization with learned deep priors.
ACM Trans. Graph., 2017. 3
[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network.
In
CVPR, 2017. 8
[61] Jun-Yan Zhu, Philipp Kr ¨ahenb ¨uhl, Eli Shechtman, and
Alexei A. Efros. Generative visual manipulation on the nat-
ural image manifold. In ECCV, 2016. 3
[62] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017. 1
[63] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A Efros, Oliver Wang, and Eli Shechtman. To-
ward multimodal image-to-image translation. In NIPS, 2017.
2, 6

Ramanan, Piotr Doll ´ar, and C. Lawrence Zitnick. Microsoft
COCO: common objects in context. In ECCV, 2014. 2, 6, 7
[41] David Marr. Vision: A computational investigation into the
human representation and processing of visual information.
1982. 3
[42] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and se-
mantic segmentation in the wild. In CVPR, 2014. 3
[43] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan
Zhu. Semantic image synthesis with spatially-adaptive nor-
malization. In CVPR, 2019. 2
[44] Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun.
Semi-parametric image synthesis. In CVPR, 2018. 3, 7, 8
[45] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-
vised representation learning with deep convolutional gen-
erative adversarial networks. CoRR, abs/1511.06434, 2015.
1
[46] L. Roberts. Machine perception of 3-D solids. PhD. Thesis,
1965. 3
[47] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake.
Grabcut:
Interactive foreground extraction using iterated
graph cuts. ACM Trans. Graph., 2004. 3
[48] Bryan Russell, Alexei A Efros, Josef Sivic, William T Free-
man, and Andrew Zisserman. Segmenting scenes by match-
ing image composites. In NIPS, 2009. 1, 3
[49] Bryan Russell, William T Freeman, Alexei A Efros, Josef
Sivic, and Andrew Zisserman. Using multiple segmentations
to discover objects and their extent in image collections. In
CVPR, 2006. 3
[50] Bryan Russell, Antonio Torralba, Kevin P Murphy, and
William T Freeman. Labelme: a database and web-based
tool for image annotation. IJCV, 2008. 3
[51] Saurabh Singh, Abhinav Gupta, and Alexei A. Efros. Un-
supervised discovery of mid-level discriminative patches. In
ECCV, 2012. 3
[52] Brandon M. Smith, Li Zhang, Jonathan Brandt, Zhe L. Lin,
and Jianchao Yang. Exemplar-based face parsing. In CVPR,
2013. 1, 2
[53] A. Torralba and A. A. Efros. Unbiased look at dataset bias.
In CVPR, 2011. 1
[54] Radim Tyle \377cek and Radim \377S ´ara. Spatial pattern templates
for recognition of objects with regular structure. In GCPR,
2013. 1, 2
[55] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,
Jan Kautz, and Bryan Catanzaro. High-resolution image syn-
thesis and semantic manipulation with conditional gans. In
CVPR, 2018. 1, 2, 3, 6, 7, 8
[56] Yuxin Wu and Kaiming He. Group normalization. In ECCV,
2018. 5
[57] Yi Yang and Deva Ramanan. Articulated human detection
with ﬂexible mixtures of parts.
IEEE Trans. Pattern Anal.
Mach. Intell. 3
[58] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. In ICCV, 2017. 1

2326

Shifting More Attention to Video Salient Object Detection

Deng-Ping Fan1

Wenguan Wang2

Ming-Ming Cheng1 ∗

Jianbing Shen2

3

,

1 TKLNDST, CS, Nankai University

2 Inception Institute of Artiﬁcial Intelligence

3 Beijing Institute of Technology

http://mmcheng.net/DAVSOD/

Abstract

The last decade has witnessed a growing interest in
video salient object detection (VSOD). However, the re-
search community long-term lacked a well-established
VSOD dataset representative of real dynamic scenes with
high-quality annotations. To address this issue, we elabo-
rately collected a visual-attention-consistent Densely Anno-
tated VSOD (DAVSOD) dataset, which contains 226 videos
with 23,938 frames that cover diverse realistic-scenes, ob-
jects, instances and motions. With corresponding real hu-
man eye-ﬁxation data, we obtain precise ground-truths.
This is the ﬁrst work that explicitly emphasizes the chal-
lenge of saliency shift, i.e., the video salient object(s) may
dynamically change. To further contribute the community a
complete benchmark, we systematically assess 17 represen-
tative VSOD algorithms over seven existing VSOD datasets
and our DAVSOD with totally ∼84K frames (largest-scale).
Utilizing three famous metrics, we then present a compre-
hensive and insightful performance analysis. Furthermore,
we propose a baseline model. It is equipped with a saliency-
shift-aware convLSTM, which can efﬁciently capture video
saliency dynamics through learning human attention-shift
behavior. Extensive experiments1 open up promising future
directions for model development and comparison.

1. Introduction

Salient object detection (SOD) targets at extracting the
most attention-grabbing objects from still images [17] or
dynamic videos. This task originates from the cognitive
studies of human visual attention behavior, i.e., the aston-
ishing ability of the human visual system (HVS) to quick-
ly orient attention to the most informative parts of visual
scenes. Previous studies [6, 45] quantitatively conﬁrmed
that there exists a strong correlation between such explic-
it, object-level saliency judgment (object-saliency) and the
implicit visual attention allocation behavior (visual atten-
tion mechanism).

∗M.M. Cheng (cmm@nankai.edu.cn) is the corresponding author.
1Dataset and code are available at: http://dpfan.net/DAVSOD/

Figure 1: Annotation examples of our DAVSOD dataset. The
rich annotations, including saliency shift, object-/instance-level
ground-truths (GT), salient object numbers, scene/object cate-
gories, and camera/object motions, provide a solid foundation for
VSOD task and beneﬁt a wide range of potential applications.

Video salient object detection (VSOD) is thus signiﬁ-
cantly essential for understanding the underlying mecha-
nism behind HVS during free-viewing in general and in-
strumental to a wide range of real-world applications, e.g.,
video segmentation [74, 83], video captioning [57], video
compression [27, 29], autonomous driving [91], robotic in-
teraction [82], weakly supervised attention [95]. Besides its
academic value and practical signiﬁcance, VSOD presents
great difﬁculties due to the challenges carried by video da-
ta (diverse motion patterns, occlusions, blur, large object-
deformations, etc.) and the inherent complexity of human
visual attention behavior (i.e., selective attention allocation,
attention shift [5, 37, 60]) during dynamic scenes. Thus it
invoked dramatically increasing research interest over the
past few years [7, 25, 31, 36, 38, 39, 61] (Table 2).
However, in striking contrast with the ﬂourishing de-
velopment of VSOD modeling, the effort on a standard
representative VSOD benchmark still lags behind serious-
ly. Although several datasets [35, 40, 43, 52, 56, 59, 75]
are proposed for VSOD, they suffered from the following
shortages. First, during dynamic-viewing, the allocation
of attentional resources is not only selective but also dy-
namically varied among different parts of inputs, with the
changing of video content. Nevertheless, previous datasets
are annotated via static frames, without a dynamic human
eye-ﬁxation guided annotation methodology, and thus do
not reveal real human attention behavior during dynamic-

8554

Figure 2: Sample video sequences from our DAVSOD dataset, with instance-level GT and ﬁxations overlaid.

viewing. Second, they are typically limited in their scala-
bility, coverage, diversity and difﬁculty. Thus, these limita-
tions of existing datasets inhibit the further development of
this branch.

This paper presents two contributions. First, we collect
a large-scale DAVSOD (Densely Annotated Video Salient
Object Detection) dataset speciﬁcally designed for VSOD.

• It contains 226 video sequences, which were strict-
ly annotated according to real human ﬁxation record-
s (Fig. 2). More importantly, two essential dynam-
ic human attention characteristics, i.e., selective at-
tention and attention shift are both considered.
In
DAVSOD, the salient object(s) may change at differ-
ent time (Fig. 1), which is more realistic and requires a
complete video content understanding. Above efforts
result in a visual-attention-consistent VSOD dataset.

• Besides, the videos were carefully selected to cov-
er diverse scene/object categories, motion patterns,
and densely annotated with per-frame pixel-accurate
ground-truths (GT).

• Another discriminative feature of DAVSOD is the
availability of both object- and instance-level annota-
tions, beneﬁting broader potential research direction-
s, such as instance-level VSOD, video salient object
subitizing, saliency-aware video captioning, etc.

Second, with the established DAVSOD dataset and previ-
ous 7 VSOD datasets [35, 40, 43, 52, 56, 59, 75], we present a
comprehensive evaluation of 17 state-of-the-art models [8,
11, 35, 41, 44, 52, 53, 62, 67, 68, 70, 74–76, 81, 87, 92], making
it the most complete VSOD benchmark. Additionally, we
also propose a baseline model, named SSAV (Saliency-Shift
Aware VSOD). It learns to predict video saliency by using
a saliency-shift-aware convLSTM module, which explicit-
ly models human visual attention-shift behavior in dynamic
scenes. The promising results on above benchmark clearly
demonstrate its effectiveness.

Our two contributions represent a complete benchmark
suite with the necessary tools for a complementary evalua-
tion, bring a more insightful glimpse into the task of VSOD
and boost more research efforts towards this direction.

Dataset
Year
SegV2 [40]
2013
FBMS [56]
2014
MCL [35]
2015
ViSal [75]
2015
DAVIS [59]
2016
UVSD [52]
2017
VOS [43]
2018
DAVSOD 2019

#Vi.
#AF. DL AS FP EF IL
14
1,065 X
59
720
9
463
17
193
50
3,455 X
18
3,262 X
200
7,467
226 23,938 X X X X X

X

Table 1: Statistics of previous VSOD datasets and the pro-
posed DAVSOD dataset, showing DAVSOD provides much richer
annotations. #Vi.: number of videos. #AF.: number of annotated
frames. DL: whether provide densely (per-frame) labeling. AS:
whether consider attention shift. FP: whether annotate salient ob-
jects according to eye ﬁxation records. EF: whether offer the eye
ﬁxation records for annotated salient object(s). IL: whether pro-
vide instance-level annotation.

2. Related Work

VSOD Datasets. Over the past few years, several datasets
(Table 1) have been created or introduced into VSOD.
Speciﬁcally, SegV2 [40] and FBMS [56] are two early
adopted datasets. Since they are designed for their spe-
ciﬁc purposes, they are not very suitable for VSOD task.
Another dataset MCL [35] only has 9 simple video ex-
amples. ViSal [75] is the ﬁrst specially designed VSOD
dataset, while only containing 17 video sequences with ob-
vious objects. More recently, Wang et al. [76] introduced
DAVIS [59], a famous video segmentation dataset with 50
challenging scenes, for VSOD. Although above datasets
advanced the ﬁeld of VSOD to various degrees, they are
severely limited to small scales (only dozens of videos). In
addition, those datasets do not consider real human atten-
tion during dynamic scenes instead arbitrarily and manual-
ly identify the salient objects by only a few annotators. The
annotation is performed over each frame individually, failed
in accounting temporal characteristics in complex dynam-
ic scenes. A recent larger scale VOS [43] dataset partially
remedied above limitations. But its diversity and generality
are quite limited as it contains many simple indoor, stable-
camera scenarios.
Overall, our DAVSOD signiﬁcantly discriminate from
above datasets: i) Through in-depth analyzing real human
dynamic attention behavior, we observe visual attention-
shift phenomenon, and thus, for the ﬁrst time, emphasize
the shift of salient objects in dynamic scenes and provide

8555

No.

Model

Year Pub.

#Training

Training Set

Basic

Type OF SP

S-measure

PCT

Code

1
SIVM [62]
2
DCSM [36]
3
RDCM [47]
4
SPVM [53]
5
CDVM [20]
6
TIMP [92]
7
STUW [21]
8
EBSG [55]
9
SAGM [74]
10
ETPM [64]
11 RWRV [35]
12 GFVM [75]
13 MB+M [87]
14 MSTM [70]
15
SGSP [52]
16
SFLR [8]
17
STBP [81]
18
VSOP [28]
19
DSR3 [38]
20
VQCU [3]
21 CSGM [77]
22
STUM [2]
23
SAVM [78]
24
bMRF [7]
25
LESR [93]
26
TVPI [61]
27
SDVM [4]
28
SCOM [11]
29
STCR [39]
30
DLVS [76]
31
SCNN [68]
32
FGRN [41]
33
SCOV [33]
34 MBNM [44]
35
PDBM [67]
36 UVOS [31]
37 SSAV (Ours)

2010 ECCV
2011 TCSVT
2013 TCSVT
2014 TCSVT
2014 TCSVT
2014 CVPR
2014 TIP
2015 CVPR
2015 CVPR
2015 CVPR
2015 TIP
2015 TIP
2015 ICCV
2016 CVPR
2017 TCSVT
2017 TIP
2017 TIP
2017 TYCB
2017 BMVC 44 (6+8+30) clips
2018 TMM
2018 TCSVT
2018 TIP
2018 TPAMI
2018 TMM
2018 TMM
2018 TIP
2018 TIP
2018 TIP
∼10K frame pairs
2018 TIP
44 (6+8+30) clips
2018 TIP
∼18K frame pairs
2018 TCSVT ∼11K frame pairs
2018 CVPR ∼10K frame pairs
2018 ECCV
2018 ECCV ∼13K frame pairs Voc12 + Coco [49] + DV
2018 ECCV ∼18K frame pairs
MK+DO+DV
2018 ECCV
2019 CVPR ∼13K frame pairs DAVSOD val + DO +DV

MK
10C+S2+DV
MK+DO+S2+FS
MK+S2+FS
S2+FS+DV

10C+S2+DV

0.481∼0.606

0.539∼0.667

X 0.470∼0.724

CRF, statistic
SORM distance
gabor, region contrast
SP, histogram
compressed domain
time-mapping
uncertainty weighting
gestalt principle
geodesic distance
eye tracking prior
random walk
gradient ﬂow
minimum barrier distance
minimum spanning tree
histogram, graph
low-rank coherency
background priors
object proposals
RCL [48]
spectral, graph structure
joint video co-saliency
local spatiotemporal neighborhood cues T
geodesic distance
T X X 0.615∼0.749
MRF
localized estimation, spatiotemporal
geodesic distance, CRF
spatiotemporal decomposition
DCL [42]
CRF
FCN [54]
VGGNet [66]
LSTM
BOW [22], proposal, FCIS [46]
motion based, DeepLab [9]
DC [85]
standard edge detector
SSLSTM, PDC [67]

T
72.4* M&C++
T
0.023*
C++
T X
9.8*
N/A
T
56.1* M&C++
T
1.73*
M
T X
69.2* M&C++
T X
50.7*
M
T X
N/A
T X X 0.615∼0.749
45.4* M&C++
T X
N/A
T
0.330∼0.595
18.3*
M
T X X 0.613∼0.757
53.7* M&C++
T
0.552∼0.726
0.02* M&C++
T
0.540∼ 0.657
0.02* M&C++
T X X 0.557∼0.706
51.7* M&C++
T X X 0.470∼0.724 119.4* M&C++
T
X 0.533∼0.752 49.49* M&C++
M&C++
Py&Ca
0.78*
M
3.86* M&C++
N.A.
45.4* M&C++
2.63*
N/A
5.93*
N/A
2.78* M&C
N/A
N/A
N/A
Py&Ca
N/A
Py&Ca
N/A
N/A
Py&Ca
N/A
Py&Ca

T
T
D X X 0.555∼0.832
D
D X X 0.682∼0.881
D X X 0.674∼0.794
D X
0.693∼0.861

T X X
T X X
X

0.637∼0.898
0.698∼0.907

38.8

0.47
38.5
0.09
3.44
2.63
0.05

0.724∼0.941

0.05

T X X

D
T

X
T X X

X

T X X

D X
D

D X X

D

Table 2: Summarizing of 36 previous representative VSOD methods and the proposed SSAV model. Training Set: 10C = 10-
Clips [24]. S2 = SegV2 [40]. DV = DAVIS [59]. DO = DUT-OMRON [84]. MK = MSRA10K [12]. MB = MSRA-B [51]. FS = FBMS [56].
Voc12= PASCAL VOC2012 [16]. Basic: CRF = Conditional Random Field. SP = superpixel. SORM = self-ordinal resemblance measure.
MRF = Markov Random Field. Type: T = Traditional. D = Deep learning. OF: Whether use optical ﬂow. SP: Whether use superpixel
over-segmentation. S-measure [18]: The range of scores over the 8 datasets in Table 4. PCT: Per-frames Computation Time (second).
Since [3, 7, 11, 33, 44, 47, 68, 93] did not release implementations, corresponding PCTs are borrowed from their papers or provided by
authors. Code: M = Matlab. Py = Python. Ca= Caffe. N/A = Not Available in the literature. “*” indicates CPU time.

the unique annotations of visual-attention-consistent prop-
erty. ii) Its diversity, large-scale dense annotation, as well
as comprehensive object-/instance-level salient object an-
notations, rich attribute annotations (e.g., object numbers,
motion patterns, scene/object categories), altogether make
a solid and unique foundation for VSOD.

VSOD Models. Early VSOD models [8, 26, 28, 35, 52, 53,
62, 63, 74, 75] are built upon hand-crafted features (color,
motion, etc.), and largely rely on classic heuristics in im-
age salient object detection area (e.g., center-surround con-
trast [12], background prior [79]) and cognitive theories of
visual attention (e.g., feature integration theory [69], guid-
ed search [80]). They also explored the way of integrat-
ing spatial and temporal saliency features through different
computational mechanisms, such as gradient ﬂow ﬁeld [75],
geodesic distance [74], restarted random walk [35], and
spectral graph structure [3]. Traditional VSOD models are
bound to signiﬁcant feature engineering and limited expres-
sion ability of hand-features. See Table 2 for more details.

More recently, deep learning based VSOD models [31,

38, 39, 41, 67, 68, 76] have gained more attention inspired
by the success of applying deep neural networks on im-
age saliency detection [13–15, 32, 50, 71, 72, 86, 88–90, 94].
More speciﬁcally, the work of Wang et al. [76] represents
an early attempt that trains a fully convolutional neural net-
work for VSOD. Another concurrent work [38] uses a 3D
ﬁlter to incorporate both spatial and temporal information
in a spatiotemporal CRF framework. Later, spatiotemporal
deep feature [39], RNN [41], pyramid dilated convLSTM
[67] are proposed for better capturing spatial and temporal
saliency characteristics. These deep VSOD models gener-
ally achieved better performance due to the strong learning
ability of neural network. However, these models ignored
the saliency shift phenomenon which is quite important for
understanding the human visual attention mechanism.
In
contrast, our SSAV model utilizes the saliency shift cue ex-
plicitly, yielding a competitive VSOD model.

In this work, we systematically benchmark 17 state-of-
the-art VSOD models on seven previous datasetsand the
proposed DAVSOD dataset, which represents the largest

8556

(a)

(b)

(c)

(d)

s
e
c
n
a

t

s

n

i

#

s
e

m

a

r
f

#

y
c
n
e
u
q
e

r
f

Annotated salient object instances in each video

video

video

Image frames in each video

Object/Instance size

(e)

ratio

Figure 3: Statistics of the proposed DAVSOD dataset. (a) Scene/object categories. (b, c) Distribution of annotated instances and image
frames, respectively. (d) Ratio distribution of the objects/instances. (e) Mutual dependencies among scene categories in (a).

performance evaluation in VSOD area so far. With our ex-
tensively quantitative results, we present deep insights into
VSOD and point out some promising research directions.

3. Proposed Dataset

Some example frames can be found in Fig. 1 and
Fig. 2. See our website for details. We will show details
of DAVSOD from the following 4 key aspects.

3.1. Stimuli Collection

The stimuli of DAVSOD come from DHF1K [73], which
is the current largest-scale dynamic eye-tracking dataset.
There are several advantages of using DHF1K create our
dataset. DHF1K 2 is collected from Youtube and covers di-
verse realistic-scenes, different object appearances and mo-
tion patterns, various object categories, and large span of
major challenges in dynamic scenarios, providing us a solid
basis to build a large-scale and representative benchmark.
More essentially, the companied visual ﬁxation record al-
lows us to produce reasonable and biologically-inspired
object-level saliency annotations. We manually trim the
videos into shot clips (Fig. 3(c)) and remove dark-screen
transitions.
In this way, we ﬁnally reach a large-scale
dataset, containing 226 video sequences with totally 23, 938
frames and 798 seconds duration.

3.2. Data Annotation

Saliency Shift Annotation. Human attention behavior is
more complex during realistic, dynamic scenes [37, 60], i.e.,
selective attention allocation and overt attention shift (due
to abrupt onsets, new dynamic events, etc.) may both hap-
pening. With the eye-tracking record of DHF1K, we also
observe stimulus-driven attention-shifts [23] are ubiquitous,
as shown in Fig. 1. However, none of the previous work in
the VSOD area explicitly emphasizes such essential visu-
al attention behavior. In DAVSOD, we annotate the salient

2Download: https://github.com/wenguanwang/DHF1K

objects according to real human ﬁxations, and the temporal
location at which attention shift occurs, for the ﬁrst time,
emphasizing the challenge of saliency shift3 in this ﬁeld.
Scene/Object Category Labeling. Consistent with [73],
each video is manually labeled with a category (i.e., Ani-
mal, Vehicle, Artifact, Human Activity). Human Activity has
four sub-classes: Sports, Daily-, Social-, and Art-Activity.
For object class, following MSCOCO [49], only “thing”
categories instead of “stuff ” are included. Then we built
a list of about 70 most frequently present scenes/objects. In
Fig. 3(a)&(e), we show the scene/object categories and their
mutual dependencies, respectively. Five annotators were
asked to annotate the object labels.
Instance-/Object-Level Salient Object Annotation.
Twenty human annotators, who were pre-trained with ten
video examples, are instructed to select up to ﬁve objects
per-frame according to the corresponding ﬁxation records
and carefully annotate them (by tracing boundaries instead
of rough polygons). They are also asked to differentiate
instances and annotate them individually,
resulting in
totally 23,938 object-level ground-truth masks and 39,498
instance-level salient object annotations.

3.3. Dataset Features and Statistics

To offer deeper insights into the proposed DAVSOD, we
discuss its several important characteristics.
Sufﬁcient Salient Object Diversity. The salient objects in
DAVSOD span a large set of classes (Fig. 3 (a)) such as an-
imals (e.g., lion, bird), vehicles (e.g., car, bicycle), artifacts
(e.g., box, building), and humans in various activities (e.g.,
dancer, rider), enabling a comprehensive understanding of
object-level saliency in dynamic scenes.

3 Notion of saliency shift. The saliency shift is not just represented
as a binary signal, w.r.t., whether it happens in a certain frame. Since we
focus on an object-level task, we change the saliency values of different
objects according to the shift of human attention.

8557

DAVSOD

Camera Mo.

Object Mo.

# Object Instances

slow fast

stable

slow fast

1

2

3 ≥ 4

# videos

102

124

117

72

37

134 125 46

33

Table 3: Statistics regarding camera/object motions and
salient object instance numbers in DAVSOD dataset.

Amount of Salient Object Instances. Existing datasets
fall in short of limited numbers of salient object instances
(Table 1). However, previous studies [34] showed human
could accurately enumerate up to ﬁve objects at a glance
without counting.
In Table 3, DAVSOD is therefore de-
signed to contain more salient objects (≤ 5 salient object
instances per-frame, avg.: 1.65). The distribution of anno-
tated instances in each video can be found in Fig. 3(b).
Size of Salient Objects. The size of object-level salien-
t object is deﬁned as the proportion of foreground objec-
t pixels to the image.
In Fig. 3(d), the ratio distribution
in DAVSOD are 0.29% ∼ 91.3% (avg.: 11.5%), yielding a
broader range.
Varied Camera Motion Patterns. DAVSOD contains di-
verse camera motions (summarized in Table 3). Algorithms
trained on such data could potentially handle realistic dy-
namic scenes better and thus are more practical.
Diverse Object Motion Patterns. DAVSOD inherits the
advantage of DHF1K that covers diverse (Table 3) realistic
dynamic scenes (e.g., object motion from stable to fast). It
is crucial to avoid over-ﬁtting and benchmark algorithms
objectively and precisely.
Center Bias. To depict the degree of center bias, we
compute the average saliency map over all frames for
each dataset. The center bias of DAVSOD and existing
datasets [35, 40, 43, 52, 56, 59, 75] are presented in Fig. 4.

3.4. Dataset Splits

Existing datasets do not maintain a preserved test set,
easily leading to model over-ﬁtting. Thus, our videos are s-
plit into separate training, validation and test sets in the ratio
of 4:2:4. Following random selection, we arrive at a unique
split containing 90 training and 46 validation videos with
released annotations, and 90 test videos with preserved an-
notations for benchmarking. The test set is further divided
into 35 easy, 30 normal, and 25 difﬁcult subsets according
to the degree of difﬁculty of the VSOD task.

4. Proposed Approach

4.1. Saliency(cid:173)Shift(cid:173)Aware VSOD Model

Overview of Model. The proposed SSAV model has two
essential components: pyramid dilated convolution (PD-
C) [67], and saliency-shift-aware convLSTM (SSLSTM).
The former is for robust static saliency representation learn-
ing. The latter one extends traditional convLSTM [65]
with saliency-shift-aware attention (SSAA) mechanism. It
takes the static feature sequence from PDC module as input

SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]
SegV2 [40]

FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]
FBMS [56]

ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]
ViSal [75]

MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]
MCL [35]

DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]
DAVIS [59]

UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]
UVSD [52]

VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]
VOS [43]

DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD
DAVSOD

Figure 4: Center bias of DAVSOD and existing VSOD datasets.

and produces corresponding VSOD results with considering
temporal dynamics and saliency-shift simultaneously.
Pyramid Dilated Convolution (PDC) Module. Recent ad-
vance [10, 67] in semantic segmentation and VSOD showed
that stacking a set of parallel dilated convolution layer with
sampling rates can bring better performance, due to the ex-
ploit of multi-scale information and the preservation of s-
patial details. We use the PDC module [67] as our static
feature extractor. Formally, let Q ∈ RW×H×C denote a 3D
feature tensor of an input frame I ∈ Rw×h×3 . A dilated conv
layer Dd with the dilated rate d > 1 can be applied to Q
to obtain an output feature P ∈ RW×H×C ′
, which maintains
original spatial resolution while considering a larger recep-
tive ﬁeld (with sampling step d). The PDC is achieved by
arranging a set of K dilated conv layers {Ddk }K
k=1 with d-
ifferent dilated rates {dk }K
k=1 in parallel:

X = [Q, P1 , . . . , Pk , . . . , PK ],

(1)

where X ∈ RW×H×(C+KC ′ ) and Pk = Ddk(Q). [., .] indicates
the concatenation operation. The PDC-enhanced feature X
is a more robust representation (by leveraging multi-scale
information) and preserves original information Q (through
residual connection).
Saliency-Shift-Aware convLSTM (SSLSTM). We pro-
pose a saliency-shift-aware convLSTM, which equips con-
vLSTM [65] with a saliency-shift-aware attention mecha-
nism. It is a powerful recurrent model that not only captures
temporal dynamics but also discriminates salient objects
from the background as well as encodes attention-shift in-
formation. More speciﬁcally, through the PDC module, we
obtain the static representations {Xt }T
t=1 of an input video
with T frames. At time step t, given Xt , the saliency-shift-
aware convLSTM outputs the corresponding salient object
mask St ∈ [0, 1]W×H :

Hidden state: Ht = convLSTM(Xt , Ht−1 ),
Saliency-shift-aware attention: At = F A ({X1 , · · · , Xt }),
Attention-enhanced feature: Gm,t = At ⊙ Hm,t ,
St = σ(wS ⊗ Gt ),

Salient object prediction:

(2)

where H ∈ RW×H×M indicates the 3D-tensor hidden s-
tate. The attention map A ∈ [0, 1]W×H is computed from
a saliency-shift-aware attention network F A , which takes
previous frames into account. Gt ∈ RW×H×M indicates the
attention-enhanced feature in time t. Gm,t ∈ RW×H indi-
cates the 2D feature slice of Gt in the m-th channel (m ∈
[1, M ]). ⊙ is element-wise multiplication. wS ∈ R1×1×M ,
a 1 × 1 conv kernel, is adopted as a salient object readout

8558

Figure 5: Overall architecture of the proposed SSAV model. SSAV consists of two components: pyramid dilated convolution (PDC)
module and saliency-shift-aware convLSTM (SSLSTM) module. The former is for efﬁcient static saliency learning, and the latter captures
temporal dynamics and saliency-shift simultaneously. See § 4 for details.

function, ⊗ indicates conv operation and σ is the sigmoid
activation function.
The key component of above module is the saliency-
shift-aware attention networkF A . Clearly, it acts as a neural
attention mechanism since it is utilized to weight the output
feature H of the convLSTM. Besides, it is desired to be ef-
fective enough to model the human attention-shift behavior.
Considering such task is also different, a small convLSTM
is introduced to build F A , generating a convLSTM in con-
vLSTM structure:

Saliency-shift-aware attention: At = F A ({X1 , · · · , Xt }),
Attention feature extraction: HA
t = convLSTMA (Xt , HA
Attention mapping: At = σ(wA ⊗ HA
t ),

t−1 ),

(3)

note that the ﬁrst equation is formulated by the last two
equations. Where wA ∈ R1×1×M indicates a 1 × 1 con-
v kernel that maps the attention feature HA as a signif-
icance matrix and sigmoid σ maps the signiﬁcance value
into [0, 1]. Then the attention At is employed to enhance
the salient object segmentation feature H in Eq. 2. Due
to the apply of convLSTMA , our attention module gain-
s strong learning ability, which provides a solid founda-
tion for learning attention-shift in both explicit and implicit
manners. Let {It∈ Rw×h×3 }T
t=1denote a training video with
T frames, {Ft ∈ [0, 1]W×H }T
t=1 human eye-tracking annota-
tion sequence and {Mt ∈ {0, 1}W×H }T
t=1 video salient ob-
ject ground-truth, we adopt a loss deﬁned over the output
t=1 of the attention model and the ﬁnal
video salient object estimation {St ∈ {0, 1}W×H }T
L = XT
where LAtt and LVSOD are both cross entropy loss. ℓ(·) ∈
{0,1} indicates whether the attention annotation is avail-
able (since most current VSOD datasets lack eye-ﬁxation

t=1 (cid:16)ℓ(It ) · LAtt (At , Ft ) + LVSOD (St , Mt )(cid:17),

{At ∈ {0, 1}W×H}T

t=1 :

(4)

record, see Table 1). When the corresponding attention an-
notation is missing, the error cannot be propagated back.
More importantly, when ℓ(·) = 0, the saliency-shift-aware
attention model F A in Eq. 3 is trained implicitly, which can
be viewed as a typical neural attention mechanism. When
the ground-truth attention is available (ℓ(·) = 1), F A is
trained in an explicit way. With the convLSTM structure,
F A is powerful enough to accurately shift the attention of
our VSOD model to the important objects (see Fig. 6).

4.2. Implementation Details

The base CNN network of PDC model is borrowed from
the conv blocks from ResNet-50 [30] and the conv strides
of the last two blocks are changed to 1. All the input frame
images are resized into 473×473 spatial resolution, and Q ∈
R60×60×2048 . Following [67], we set K = 4, C = 512 and
dk = 2k (k ∈ {1, · · ·, 4}). For the convLSTM in Eq. 2, we
use a 3 × 3 × 32 conv kernel. The convLSTMA in Eq. 3
utilizes a 3 × 3 × 16 conv kernel. For training protocol, we
follow the same settings in [67] (exclude MSRA-10k [12]
dataset). In addition, we further exploit the validation set of
DAVSOD to train the saliency-shift-aware attention module
explicitly.

5. Benchmark Evaluation Results

5.1. Experimental Settings

Evaluation Metrics. To quantitatively assess the model
performance, we adopt 2 popular evaluation metrics: Mean
Absolute Error (MAE) M [58], F-measure F [1], and the
recent released structure measure S-measure S [18].
Benchmark Models. We benchmark 17 models in total (11
traditional methods, 6 deep learning based models). These
models were selected based on the two criteria: i) having
released implementations, and ii) being representative.
Benchmark Protocols.
To provide a comprehensive

8559

2010-2015
2016-2017
2018
SIVM TIMP SPVM RWRV MB+M SAGM GFVM MSTM STBP SGSP SFLR SCOM SCNN DLVS FGRN MBNM PDBM SSAV†
[62]
[92]
[53]
[35]
[87]
[74]
[75]
[70]
[81]
[52]
[8]
[11]†
[68]†
[76]†
[41]†
[44]†
[67]†

Metric

V

i

a
S

l max F ↑

.522
.606
.197

.479
.612
.170

.700
.724
.133

.440
.595
.188

.692
.726
.129

.688
.749
.105

.683
.757
.107

.673
.749
.095

.622
.629
.163

.677
.706
.165

.779
.814
.062

.831
.762
.122

.831
.847
.071

.852
.881
.048

.848
.861
.045

.883
.898
.020

.888
.907
.032

.939
.943
.020

S ↑
M ↓

B
F

M

S

-

T max F ↑

.426
.545
.236

.456
.576
.192

.330
.515
.209

.336
.521
.242

.487
.609
.206

.564
.659
.161

.571
.651
.160

.500
.613
.177

.595
.627
.152

.630
.661
.172

.660
.699
.117

.797
.794
.079

.762
.794
.095

.759
.794
.091

.767
.809
.088

.816
.857
.047

.821
.851
.064

.865
.879
.040

S ↑
M ↓

D

V
A

I

S

-

T max F ↑

.450
.557
.212

.488
.593
.172

.390
.592
.146

.345
.556
.199

.470
.597
.177

.515
.676
.103

.569
.687
.103

.429
.583
.165

.544
.677
.096

.655
.692
.138

.727
.790
.056

.783
.832
.048

.714
.783
.064

.708
.794
.061

.783
.838
.043

.861
.887
.031

.855
.882
.028

.861
.893
.028

S ↑
M ↓

g
e
S

V

2

max F ↑

.581
.605
.251

.573
.644
.116

.618
.668
.108

.438
.583
.162

.554
.618
.146

.634
.719
.081

.592
.699
.091

.526
.643
.114

.640
.735
.061

.673
.681
.124

.745
.804
.037

.764
.815
.030

**
**
**

**
**
**

**
**
**

.716
.809
.026

.800
.864
.024

.801
.851
.023

S ↑
M ↓

U

V

S

D max F ↑

.293
.481
.260

.338
.537
.178

.404
.581
.146

.281
.536
.180

.339
.563
.169

.414
.629
.111

.426
.628
.106

.336
.551
.145

.403
.614
.105

.544
.601
.165

.562
.713
.059

.420
.555
.206

.550
.712
.075

.564
.721
.060

.630
.745
.042

.550
.698
.079

.863
.901
.018

.801
.861
.025

S ↑
M ↓

M

C

L max F ↑

.420
.548
.185

.598
.642
.113

.595
.665
.105

.446
.577
.167

.261
.539
.178

.422
.615
.136

.406
.613
.132

.313
.540
.171

.607
.700
.078

.645
.679
.100

.669
.734
.054

.422
.569
.204

.628
.730
.054

.551
.682
.060

.625
.709
.044

.698
.755
.119

.798
.856
.021

.774
.819
.027

S ↑
M ↓

V

O

S

-

T max F ↑

.439
.558
.217

.401
.575
.215

.351
.511
.223

.422
.552
.211

.562
.661
.158

.482
.619
.172

.506
.615
.162

.567
.657
.144

.526
.576
.163

.426
.557
.236

.546
.624
.145

.690
.712
.162

.609
.704
.109

.675
.760
.099

.669
.715
.097

.670
.742
.099

.742
.818
.078

.742
.819
.073

S ↑
M ↓

D

V
A

S

D
O

-

T max F ↑

.298
.486
.288

.395
.563
.195

.358
.538
.202

.283
.504
.245

.342
.538
.228

.370
.565
.184

.334
.553
.167

.344
.532
.211

.410
.568
.160

.426
.577
.207

.478
.624
.132

.464
.599
.220

.532
.674
.128

.521
.657
.129

.573
.693
.098

.520
.637
.159

.572
.698
.116

.603
.724
.092

S ↑
M ↓

Table 4: Benchmarking results of 17 state-of-the-art VSOD models on 7 datasets: SegV2 [40], FBMS [56], ViSal [75], MCL [35],
DAVIS [59], UVSD [52], VOS [43] and the proposed DAVSOD (35 easy test set). Note that TIMP was only tested on 9 short sequences of
VOS because it cannot handle long videos. “**” indicates the model has been trained on this dataset. “-T” indicates the results on the test
set of this dataset. “†” indicates deep learning model. Darker color indicates better performance. The best scores are marked in bold.

benchmark, we evaluate 17 representative methods on ex-
isting 7 datasets and the proposed DAVSOD dataset. The
test sets of FBMS [56] (30 clips), DAVIS [59] (20 clips),
DAVSOD (35 easy clips) datasets, and the whole ViSal [75]
(17 clips), MCL [35] (9 clips), SegV2 [40] (13 clips), UVS-
D [52] (18 clips) datasets are used for testing. For VOS [43]
dataset, we randomly select 40 sequences as test set. There
are total 182 videos with 848,340 (47,130×18) frames.

5.2. Performance Comparison and Data Analysis

In this section, we provide some interesting ﬁndings
which would beneﬁt the further research.
Performance of Traditional Models. Based on the differ-
ent metrics in Table 4, we conclude that: “SFLR [8], S-
GSP [52], and STBP [81] are the top 3 non-deep learning
models for VSOD.” Both SFLR and SGSP explicitly con-
sider the optical ﬂow strategy to extract the motion features.
However, the computational cost is usually expensive (see
Table 2). One noteworthy ﬁnding is that all these models u-
tilize the superpixel technology to integrate spatiotemporal
features on region level.
Performance of Deep Models. The top 3 models in this
benchmark (i.e., SSAV, PDBM [67], MBNM [44]) are al-
l based on deep learning technique, which demonstrates
the strong learning power of neural networks. For ViSal

dataset (the ﬁrst speciﬁcally-designed dataset for VSOD),
their average performance (e.g., max E-measure [19], max
F-measure, or S-measure) is even higher than 0.9.

Traditional vs Deep VSOD Models.
In Table 4, almost
all of the deep models outperform traditional algorithms,
as more powerful saliency representations can be extract-
ed from networks. Another interesting ﬁnding is the clas-
sic leading method (SFLR [8]) performs better than some
deep models (e.g., SCOM [11]) on MCL, UVSD, ViSal, and
DAVSOD datasets. It indicates that investigating more ef-
fective deep learning architectures with the exploit of hu-
man prior knowledge for VSOD is a promising direction.

Dataset Analysis. We mark the scores with gray color in
Table 4. Darker colors mean better performance for speciﬁc
metrics (e.g., max F , S , and M). We ﬁnd that ViSal and
UVSD datasets are relatively easy, since the top 2 models:
SSAV and PDBM [67] gained very high performance (e.g.,
S > 0.9). However, for more challenging datasets like
DAVSOD, the performance of VOSD models decrease dra-
matically (S < 0.73). It reveals that both the overall and
individual performance of VOSD models leave abundant
room for future research.

Runtime Analysis. Table 2 reports the computation time of
previous VSOD methods and the proposed SSAV approach

8560

(1)

(2)

(3)

(4)

(5)

(a) Frame

(b) Fixation

(c) GT

(d) SSAV

(e) MBNM [44] (f) FGRN [41]

(g) PDBM [67]

(h) SFLR [8]

(i) SAGM [74]

Figure 6: Visual comparisons with top 3 deep (MBNM [44], FGRN [41], PDBM [67]) models and 2 traditional classical (SFLR [8],
SAGM [74]) models on the proposed DAVSOD dataset. Our SSAV model captures the saliency shift phenomenon successfully.

(in PCT column). For the models with released codes, the
timings are tested on the same platform: Intel Xeon(R) E5-
2676v3 @2.4GHz×24 and GTX TITAN X. The rest of the
timings are borrowed from their papers. Note that the pro-
posed model does not apply any pre-/post-processing (e.g.,
CRF), thus the processing speed only takes about 0.05s.

5.3. Ablation Study

Implicit vs Explicit Saliency-Shift-Aware Attention
Mechanism. To study the inﬂuence of different training
strategies of the proposed SSAA module, we derive 2 base-
lines: explicit and implicit, refer to the proposed SSAV
model trained explicitly or implicitly. We obtain the im-
plicit baseline by only using VSOD annotations (exclude
DAVSOD). We observe that SSAV with explicit attention is
better than the one with implicit attention, according to the
statistics in Table 5. It demonstrates that utilizing ﬁxation
data can help our model to better capture saliency shift and
thus further boost ﬁnal VSOD performance.
Effectiveness of Saliency-Shift-Aware convLSTM. To s-
tudy the effectiveness of SSLSTM (§ 4), we provide another
baseline: w/o SSLSTM, which excludes SSLSTM module
from the proposed SSAV model. From Table 5, we observe
a performance decrease (e.g., S : 0.724 → 0.667), which
conﬁrms that the proposed SSLSTM module is effective to
learn both selective attention allocation and attention shift
cues from the challenging data.
Comparison with State-of-the-Arts. In Table 4, we com-
pare the proposed SSAV model with current 17 state-of-the-
art VSOD algorithms. The proposed baseline method per-
forms better against other competitors over most existing
datasets. More speciﬁcally, our model obtains signiﬁcant
performance improvements on ViSal and FBMS datasets. It
also obtains comparable performance on VOS, SegV2 and
DAVIS datasets.

5.4. Analysis for the saliency shift challenge

For the proposed challenging DAVSOD dataset,
the
SSAV model also gains the best performance. We attribute

Type

Baseline
S ↑ max F ↑ M ↓
explicit
0.724
0.603
0.092
implicit
0.684
0.593
0.103
SSLSTM w/o SSLSTM 0.667
0.541
0.132

SSAA

Table 5: Ablation studies of the SSAV on DAVSOD dataset.

the promising performance to the introduce of SSLSTM,
which efﬁciently captures saliency allocations in dynam-
ic scenes and guides our model to accurately attend to
those visually important regions. Fig. 6 shows that the pro-
posed SSAV approach obtains more visually favorable re-
sults than other top competitors. Our SSAV model captures
the saliency shift successfully (from frame-1 to frame-5:
cat → [cat, box] → cat → box → [cat, box]). However, the
other top-performance VSOD models either do not high-
light the whole salient objects (e.g., SFLR, SAGM) or on-
ly capture the moving cat (e.g., MBNM). We envision our
SSAV model would open up promising future directions for
model development.

6. Conclusion

We have presented a comprehensive study on VSOD by
creating a new visual-attention-consistent DAVSOD dataset,
building up the largest-scale benchmark, and proposing a
SSAV baseline model. Compared with other competing tra-
ditional or deep learning models, the proposed SSAV model
achieves superior performance and produces more visually
favorable results. Extensive experiments veriﬁed that even
considering top performing models, VSOD remain seems
far from being solved. The above contributions and in-depth
analyses would beneﬁt the develop of this area and be help-
ful to stimulate broader potential research, e.g., saliency-
aware video captioning, video salient object subitizing and
instance-level VSOD.

Acknowledgements. This

research was
supported by NSFC
(61620106008, 61572264), the national youth talent support program,
the Fundamental Research Funds for the Central Universities (Nankai
University, NO. 63191501) and Tianjin Natural Science Foundation
(17JCJQJC43700, 18ZXZNGX00110).

8561

References

[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada,
and Sabine Susstrunk. Frequency-tuned salient region detec-
tion. In IEEE CVPR, pages 1597–1604, 2009. 6
[2] Tariq Alshawi, Zhiling Long, and Ghassan AlRegib. Unsu-
pervised uncertainty estimation using spatiotemporal cues in
video saliency detection. IEEE TIP, pages 2818–2827, 2018.
3
[3] C¸ a ˘glar Aytekin, Horst Possegger, Thomas Mauthner, Serkan
Kiranyaz, Horst Bischof, and Moncef Gabbouj. Spatiotem-
poral saliency estimation by spectral foreground detection.
IEEE TMM, 20(1):82–95, 2018. 3
[4] Saumik Bhattacharya, K Subramanian Venkatesh, and
Sumana Gupta. Visual saliency detection using spatiotem-
poral decomposition. IEEE TIP, 27(4):1665–1675, 2018. 3
[5] Marc D. Binder, Nobutaka Hirokawa, and Uwe Windhorst,
editors. Gaze Shift, pages 1676–1676. Springer Berlin Hei-
delberg, Berlin, Heidelberg, 2009. 1
[6] Ali Borji, Dicky N Sihite, and Laurent Itti. What stands out
in a scene? A study of human explicit saliency judgment.
Vision Research, 91:62–77, 2013. 1
[7] Chenglizhao Chen, Shuai Li, Hong Qin, Zhenkuan Pan, and
Guowei Yang. Bi-level feature learning for video saliency
detection. IEEE TMM, 2018. 1, 3
[8] Chenglizhao Chen, Shuai Li, Yongguang Wang, Hong Qin,
and Aimin Hao. Video saliency detection via spatial-
temporal fusion and low-rank coherency diffusion.
IEEE
TIP, 26(7):3156–3170, 2017. 2, 3, 7, 8
[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE TPAMI, 40(4):834–848,
2018. 3
[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets, atrous convolu-
tion, and fully connected crfs. IEEE TPAMI, 40(4):834–848,
2018. 5
[11] Yuhuan Chen, Wenbin Zou, Yi Tang, Xia Li, Chen Xu, and
Nikos Komodakis. Scom: Spatiotemporal constrained opti-
mization for salient object detection. IEEE TIP, 27(7):3345–
3357, 2018. 2, 3, 7
[12] Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip
H. S. Torr, and Shi-Min Hu. Global contrast based salient
region detection. IEEE TPAMI, 37(3):569–582, 2015. 3, 6
[13] Runmin Cong, Jianjun Lei, Huazhu Fu, Ming-Ming Cheng,
Weisi Lin, and Qingming Huang. Review of visual saliency
detection with comprehensive information.
IEEE TCSVT,
PP(99):1–19, 2018. 3
[14] Runmin Cong, Jianjun Lei, Huazhu Fu, Qingming Huang,
Xiaochun Cao, and Chunping Hou. Co-saliency detection for
rgbd images based on multi-constraint feature matching and
cross label propagation. IEEE TIP, 27(2):568–579, 2018. 3
[15] Runmin Cong, Jianjun Lei, Huazhu Fu, Weisi Lin, Qing-
ming Huang, Xiaochun Cao, and Chunping Hou. An iter-
ative co-saliency framework for rgbd images. IEEE TYCB,
49(1):233–246, 2019. 3

[16] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. IJCV,
111(1):98–136, 2015. 3

[17] Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang-
Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clut-
ter: Bringing salient object detection to the foreground. In
ECCV. Springer, 2018. 1

[18] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and
Ali Borji. Structure-measure: A New Way to Evaluate Fore-
ground Maps. In IEEE ICCV, pages 4548–4557, 2017. 3,
6

[19] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-
Ming Cheng, and Ali Borji. Enhanced-alignment Measure
for Binary Foreground Map Evaluation.
In IJCAI, pages
698–704, 2018. 7

[20] Yuming Fang, Weisi Lin, Zhenzhong Chen, Chia-Ming T-
sai, and Chia-Wen Lin. A video saliency detection model in
compressed domain. IEEE TCSVT, 24(1):27–38, 2014. 3

[21] Yuming Fang, Zhou Wang, Weisi Lin, and Zhijun Fang.
Video saliency incorporating spatiotemporal cues and uncer-
tainty weighting. IEEE TIP, 23(9):3910–3921, 2014. 3

[22] Li Fei-Fei and Pietro Perona. A bayesian hierarchical model
for learning natural scene categories. In IEEE CVPR, pages
524–531, 2005. 3

[23] Steven L Franconeri, Daniel J Simons, and Justin A Junge.
Searching for stimulus-driven shifts of attention. Psycho-
nomic Bulletin & Review, 11(5):876–881, 2004. 4

[24] Ken Fukuchi, Kouji Miyazato, Akisato Kimura, Shigeru
Takagi, and Junji Yamato. Saliency-based video segmen-
tation with graph cuts and sequentially updated priors.
In
ICME, pages 638–641, 2009. 3

[25] Siavash Gorji and James J Clark. Going From Image to
Video Saliency: Augmenting Image Salience With Dynamic
Attentional Push. In IEEE CVPR, pages 7501–7511, 2018.
1

[26] Chenlei Guo, Qi Ma, and Liming Zhang. Spatio-temporal
saliency detection using phase spectrum of quaternion fouri-
er transform. In IEEE CVPR, pages 1–8, 2008. 3

[27] Chenlei Guo and Liming Zhang. A novel multiresolution
spatiotemporal saliency detection model and its applications
in image and video compression. IEEE TIP, 19(1):185–198,
2010. 1

[28] Fang Guo, Wenguan Wang, Jianbing Shen, Ling Shao, Jian
Yang, Dacheng Tao, and Yuan Yan Tang. Video saliency
detection using object proposals. IEEE Transactions on Cy-
bernetics, 2017. 3

[29] Hadi Hadizadeh and Ivan V Bajic. Saliency-aware video
compression. IEEE TIP, 23(1):19–33, 2014. 1

[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In IEEE
CVPR, pages 770–778, 2016. 6

[31] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G. Schwing.
Unsupervised video object segmentation using motion
saliency-guided spatio-temporal propagation.
In ECCV.
Springer, 2018. 1, 3

8562

[32] Md Amirul Islam, Mahmoud Kalash, and Neil DB Bruce.
Revisiting salient object detection: Simultaneous detection,
ranking, and subitizing of multiple salient objects. In IEEE
CVPR, pages 7142–7150, 2018. 3
[33] Yeong Jun Koh, Young-Yoon Lee, and Chang-Su Kim. Se-
quential clique optimization for video object segmentation.
In ECCV. Springer, 2018. 3
[34] Edna L Kaufman, Miles W Lord, Thomas Whelan Reese,
and John Volkmann. The discrimination of visual number.
The American Journal of Psychology, 62(4):498–525, 1949.
5
[35] Hansang Kim, Youngbae Kim, Jae-Young Sim, and Chang-
Su Kim. Spatiotemporal saliency detection for video se-
quences based on random walk with restart.
IEEE TIP,
24(8):2552–2564, 2015. 1, 2, 3, 5, 7
[36] Wonjun Kim, Chanho Jung, and Changick Kim. Spatiotem-
poral saliency detection and its applications in static and dy-
namic scenes. IEEE TCSVT, 21(4):446–456, 2011. 1, 3
[37] Christof Koch and Shimon Ullman. Shifts in selective vi-
sual attention: Towards the underlying neural circuitry.
In
Matters of Intelligence, pages 115–141. 1987. 1, 4
[38] Trung-Nghia Le and Akihiro Sugimoto. Deeply supervised
3d recurrent fcn for salient object detection in videos.
In
BMVC, 2017. 1, 3
[39] Trung-Nghia Le and Akihiro Sugimoto. Video salient objec-
t detection using spatiotemporal deep features.
IEEE TIP,
27(10):5002–5015, 2018. 1, 3
[40] Fuxin Li, Taeyoung Kim, Ahmad Humayun, David Tsai, and
James M Rehg. Video segmentation by tracking many ﬁgure-
ground segments. In IEEE ICCV, pages 2192–2199, 2013.
1, 2, 3, 5, 7
[41] Guanbin Li, Yuan Xie, Tianhao Wei, Keze Wang, and Liang
Lin. Flow guided recurrent neural encoder for video salient
object detection. In IEEE CVPR, pages 3243–3252, 2018. 2,
3, 7, 8
[42] Guanbin Li and Yizhou Yu. Deep contrast learning for salient
object detection. In IEEE CVPR, pages 478–487, 2016. 3
[43] Jia Li, Changqun Xia, and Xiaowu Chen. A benchmark
dataset and saliency-guided stacked autoencoders for video-
based salient object detection.
IEEE TIP, 27(1):349–364,
2018. 1, 2, 5, 7
[44] Siyang Li, Bryan Seybold, Alexey Vorobyov, Xuejing Lei,
and C.-C. Jay Kuo. Unsupervised video object segmentation
with motion-based bilateral networks.
In ECCV. Springer,
2018. 2, 3, 7, 8
[45] Yin Li, Xiaodi Hou, Christof Koch, James M Rehg, and
Alan L Yuille. The secrets of salient object segmentation.
In IEEE CVPR, pages 280–287, 2014. 1
[46] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.
Fully convolutional instance-aware semantic segmentation.
In IEEE CVPR, pages 2359–2367, 2017. 3
[47] Yong Li, Bin Sheng, Lizhuang Ma, Wen Wu, and Zhifeng
Xie. Temporally coherent video saliency using regional dy-
namic contrast. IEEE TCSVT, 23(12):2067–2076, 2013. 3
[48] Ming Liang and Xiaolin Hu. Recurrent convolutional neural
network for object recognition. In IEEE CVPR, pages 3367–
3375, 2015. 3

[49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context.
In
ECCV, pages 740–755. Springer, 2014. 3, 4
[50] Nian Liu, Junwei Han, and Ming-Hsuan Yang. PiCANet:
Learning pixel-wise contextual attention for saliency detec-
tion. In IEEE CVPR, pages 3089–3098, 2018. 3
[51] Tie Liu, Jian Sun, Nan-Ning Zheng, Xiaoou Tang, and
Heung-Yeung Shum. Learning to Detect A Salient Object.
In IEEE CVPR, pages 1–8, 2007. 3
[52] Zhi Liu, Junhao Li, Linwei Ye, Guangling Sun, and Li-
quan Shen. Saliency detection for unconstrained videos us-
ing superpixel-level graph and spatiotemporal propagation.
IEEE TCSVT, 27(12):2527–2542, 2017. 1, 2, 3, 5, 7
[53] Zhi Liu, Xiang Zhang, Shuhua Luo, and Olivier Le Meur.
Superpixel-based spatiotemporal saliency detection.
IEEE
TCSVT, 24(9):1522–1540, 2014. 2, 3, 7
[54] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In IEEE
CVPR, pages 3431–3440, 2015. 3
[55] Thomas Mauthner, Horst Possegger, Georg Waltner, and
Horst Bischof. Encoding based saliency detection for videos
and images. In IEEE CVPR, pages 2494–2502, 2015. 3
[56] Peter Ochs, Jitendra Malik, and Thomas Brox. Segmentation
of moving objects by long term video analysis. IEEE TPAMI,
36(6):1187–1200, 2014. 1, 2, 3, 5, 7
[57] Yingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. Video
captioning with transferred semantic attributes.
In CVPR,
pages 6504–6512, 2017. 1
[58] Federico Perazzi, Philipp Kr ¨ahenb ¨uhl, Yael Pritch, and
Alexander Hornung. Saliency ﬁlters: Contrast based ﬁltering
for salient region detection. In CVPR, pages 733–740, 2012.
6
[59] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc
Van Gool, Markus Gross, and Alexander Sorkine-Hornung.
A benchmark dataset and evaluation methodology for video
object segmentation. In IEEE CVPR, pages 724–732, 2016.
1, 2, 3, 5, 7
[60] Matthew S Peterson, Arthur F Kramer, and David E Irwin.
Covert shifts of attention precede involuntary eye move-
ments. Perception & Psychophysics, 66(3):398–405, 2004.
1, 4
[61] Wenliang Qiu, Xinbo Gao, and Bing Han. Eye ﬁxation as-
sisted video saliency detection via total variation-based pair-
wise interaction. IEEE TIP, pages 4724–4739, 2018. 1, 3
[62] Esa Rahtu, Juho Kannala, Mikko Salo, and Janne Heikkil ¨a.
Segmenting salient objects from images and videos. In EC-
CV, pages 366–379. Springer, 2010. 2, 3, 7
[63] Hae Jong Seo and Peyman Milanfar. Static and space-time
visual saliency detection by self-resemblance. Journal of Vi-
sion, 9(12):15–15, 2009. 3
[64] Karthikeyan Shanmuga Vadivel, Thuyen Ngo, Miguel Eck-
stein, and BS Manjunath. Eye tracking assisted extraction of
attentionally important objects from videos. In CVPR, pages
3241–3250, 2015. 3
[65] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Kin Wong, and Wang-Chun Woo. Convolutional LST-

8563

[82] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and
Thomas S Huang. Deep interactive object selection. In IEEE
CVPR, pages 373–381, 2016. 1
[83] Ning Xu, Linjie Yang, Yuchen Fan, Jianchao Yang, D-
ingcheng Yue, Yuchen Liang, Brian Price, Scott Cohen, and
Thomas Huang. Youtube-vos: Sequence-to-sequence video
object segmentation. In ECCV, pages 585–601, 2018. 1
[84] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and
Ming-Hsuan Yang. Saliency detection via graph-based man-
ifold ranking. In IEEE CVPR, pages 3166–3173, 2013. 3
[85] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-
tion by dilated convolutions. In ICLR, 2016. 3
[86] Yu Zeng, Huchuan Lu, Lihe Zhang, Mengyang Feng, and
Ali Borji. Learning to promote saliency detectors. In IEEE
CVPR, pages 1644–1653, 2018. 3
[87] Jianming Zhang, Stan Sclaroff, Zhe Lin, Xiaohui Shen, Bri-
an Price, and Radomir Mech. Minimum barrier salient object
detection at 80 fps. In IEEE ICCV, pages 1404–1412, 2015.
2, 3, 7
[88] Jing Zhang, Tong Zhang, Yuchao Dai, Mehrtash Harandi,
and Richard Hartley. Deep unsupervised saliency detection:
A multiple noisy labeling perspective. In IEEE CVPR, pages
9029–9038, 2018. 3
[89] Lu Zhang, Ju Dai, Huchuan Lu, You He, and Gang Wang. A
bi-directional message passing model for salient object de-
tection. In IEEE CVPR, pages 1741–1750, 2018. 3
[90] Xiaoning Zhang, Tiantian Wang, Jinqing Qi, Huchuan Lu,
and Gang Wang. Progressive attention guided recurrent net-
work for salient object detection. In IEEE CVPR, pages 714–
722, 2018. 3
[91] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun.
Instance-
level segmentation for autonomous driving with deep dense-
ly connected mrfs.
In IEEE CVPR, pages 669–677, 2016.
1
[92] Feng Zhou, Sing Bing Kang, and Michael F Cohen. Time-
mapping using space-time saliency. In IEEE CVPR, pages
3358–3365, 2014. 2, 3, 7
[93] Xiaofei Zhou, Zhi Liu, Chen Gong, and Wei Liu. Improv-
ing video saliency detection via localized estimation and s-
patiotemporal reﬁnement.
IEEE TMM, pages 2993–3007,
2018. 3
[94] Yizhe Zhu, Mohamed Elhoseiny, Bingchen Liu, Xi Peng,
and Ahmed Elgammal. A generative adversarial approach
for zero-shot learning from noisy texts. In CVPR, 2018. 3
[95] Yizhe Zhu, Jianwen Xie, Zhiqiang Tang, Xi Peng, and
Ahmed Elgammal. Learning where to look: Semantic-
guided multi-attention localization for zero-shot learning.
arXiv preprint arXiv:1903.00502, 2019. 1

M network: A machine learning approach for precipitation
nowcasting. In NIPS, 2015. 5
[66] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In ICLR,
2015. 3
[67] Hongmei Song, Wenguan Wang, Sanyuan Zhao, Jianbing
Sheng, and Kin-Man Lam. Pyramid dilated deeper convL-
STM for video salient object detection. In ECCV. Springer,
2018. 2, 3, 5, 6, 7, 8
[68] Yi Tang, Wenbin Zou, Zhi Jin, Yuhuan Chen, Yang Hua, and
Xia Li. Weakly supervised salient object detection with spa-
tiotemporal cascade neural networks. IEEE TCSVT, 2018. 2,
3, 7
[69] Anne M Treisman and Garry Gelade. A feature-integration
theory of attention. Cognitive Psychology, 12(1):97–136,
1980. 3
[70] Wei-Chih Tu, Shengfeng He, Qingxiong Yang, and Shao-Yi
Chien. Real-time salient object detection with a minimum
spanning tree. In IEEE CVPR, pages 2334–2342, 2016. 2, 3,
7
[71] Tiantian Wang, Lihe Zhang, Shuo Wang, Huchuan Lu, Gang
Yang, Xiang Ruan, and Ali Borji. Detect globally, reﬁne
locally: A novel approach to saliency detection.
In IEEE
CVPR, pages 3127–3135, 2018. 3
[72] Wenguan Wang, Jianbing Shen, Xingping Dong, and Al-
i Borji. Salient object detection driven by ﬁxation prediction.
In IEEE CVPR, pages 1711–1720, 2018. 3
[73] Wenguan Wang, Jianbing Shen, Fang Guo, Ming-Ming
Cheng, and Ali Borji. Revisiting video saliency: A large-
scale benchmark and a new model. In IEEE CVPR, pages
4894–4903, 2018. 4
[74] Wenguan Wang, Jianbing Shen, and Fatih Porikli. Saliency-
aware geodesic video object segmentation. In IEEE CVPR,
pages 3395–3402, 2015. 1, 2, 3, 7, 8
[75] Wenguan Wang, Jianbing Shen, and Ling Shao. Consisten-
t video saliency using local gradient ﬂow optimization and
global reﬁnement. IEEE TIP, 24(11):4185–4196, 2015. 1, 2,
3, 5, 7
[76] Wenguan Wang, Jianbing Shen, and Ling Shao. Video salient
object detection via fully convolutional networks. IEEE TIP,
27(1):38–49, 2018. 2, 3, 7
[77] Wenguan Wang, Jianbing Shen, Hanqiu Sun, and Ling Shao.
Video co-saliency guided co-segmentation.
IEEE TCSVT,
28(8):1727–1736, 2018. 3
[78] Wenguan Wang, Jianbing Shen, Ruigang Yang, and Fatih
Porikli. Saliency-aware video object segmentation.
IEEE
TPAMI, pages 20–33, 2018. 3
[79] Yichen Wei, Fang Wen, Wangjiang Zhu, and Jian Sun.
Geodesic saliency using background priors. In ECCV, pages
29–42. Springer, 2012. 3
[80] Jeremy M Wolfe, Kyle R Cave, and Susan L Franzel. Guided
search: An alternative to the feature integration model for
visual search. Journal of Experimental Psychology: Human
Perception and Performance, 15(3):419, 1989. 3
[81] Tao Xi, Wei Zhao, Han Wang, and Weisi Lin. Salient object
detection with spatiotemporal background priors for video.
IEEE TIP, 26(7):3425–3436, 2017. 2, 3, 7

8564

SiCloPe: Silhouette-Based Clothed People

Ryota Natsume1,3 ∗
Shunsuke Saito1,2 ∗
Chongyang Ma4
Hao Li1,2,5

Zeng Huang1,2
Weikai Chen1
Shigeo Morishima3

1USC Institute for Creative Technologies
2University of Southern California
3Waseda University
4Snap Inc.
5Pinscreen

Abstract

We introduce a new silhouette-based representation
for modeling clothed human bodies using deep generative
models. Our method can reconstruct a complete and
textured 3D model of a person wearing clothes from a single
input picture. Inspired by the visual hull algorithm, our
implicit representation uses 2D silhouettes and 3D joints
of a body pose to describe the immense shape complexity
and variations of clothed people. Given a segmented 2D
silhouette of a person and its inferred 3D joints from the
input picture, we ﬁrst synthesize consistent silhouettes from
novel view points around the subject. The synthesized
silhouettes which are the most consistent with the input
segmentation are fed into a deep visual hull algorithm
for robust 3D shape prediction. We then infer the texture
of the subject’s back view using the frontal image and
segmentation mask as input to a conditional generative
adversarial network. Our experiments demonstrate that
our silhouette-based model is an effective representation
and the appearance of the back view can be predicted
reliably using an image-to-image translation network. While
classic methods based on parametric models often fail for
single-view images of subjects with challenging clothing,
our approach can still produce successful results, which are
comparable to those obtained from multi-view input.

1. Introduction

The ability to digitize and predict a complete and fully
textured 3D model of a clothed subject from a single view
can open the door to endless applications, ranging from
virtual and augmented reality, gaming, virtual try-on, to
3D printing. A system that could generate a full-body
3D avatar of a person by simply taking a picture as input
would signiﬁcantly impact the scalability of producing
virtual humans for immersive content creation, as well as its
attainability by the general population. Such single-view

∗ Joint ﬁrst authors

input image

fully textured 3D mesh

Figure 1: Given a single image of a person from the frontal
view, we can automatically reconstruct a complete and
textured 3D clothed body shape.

inference is extremely difﬁcult due to the vast range of
possible shapes and appearances that clothed human bodies
can take in natural conditions. Furthermore, only a 2D
projection of the real world is available and the entire back
view of the subject is missing.
While 3D range sensing [26, 34] and photogramme-
try [39] are popular ways of obtaining complete 3D models,
they are restricted to a tedious scanning process or require
specialized equipment. The modeling of humans from a
single view, on the other hand, has been facilitated by the
availability of large 3D human model repositories [3, 28],
where a parametric model of human shapes is used to guide
the reconstruction process [6]. However, these parametric
models only represent naked bodies and do not describe the
clothing geometry nor the texture. Another option is to use a
pre-captured template of the subject in order to handle new
poses [54], but such an approach is limited to the recording
of one particular person.
In this work, we propose a deep learning based non-
parametric approach for generating the geometry and texture
of clothed 3D human bodies from a single frontal-view
image. Our method can predict ﬁne-level geometric details
of clothes and generalizes well to new subjects different
from those being used during training (See Figure 1).
While directly estimating 3D volumetric geometry from a
single view is notoriously challenging and likely to require a
large amount of training data as well as extensive parameter
tuning, two cutting-edge deep learning techniques have

14480

shown that impressive results can be obtained using 2D
silhouettes from very sparse views [20, 44]. Inspired by
these approaches based on visual hull, we propose the ﬁrst
algorithm to predict 2D silhouettes of the subject from
multiple views given an input segmentation, which implicitly
encodes 3D body shapes. We also show that a sparse 3D
pose estimated from the 2D input [6, 38] can help reduce
the dimensionality of the shape deformation and guide the
synthesis of consistent silhouettes from novel views.
We then reconstruct the ﬁnal 3D geometry from multiple
silhouettes using a deep learning based visual hull technique
by incorporating a clothed human shape prior. Since
silhouettes from arbitrary views can be generated, we further
improve the reconstruction result by greedily choosing view
points that will lead to improved silhouette consistency. To
fully texture the reconstructed geometry, we propose to train
an image-to-image translation framework to infer the color
texture of the back view given the input image from the
frontal view.
We demonstrate the effectiveness of our method on a
variety of input data, including both synthetic and real ones.
We also evaluate major design decisions using ablation
studies and compare our approach with state of the art single-
view as well as multi-view reconstruction techniques.
In summary, our contributions include:

• The ﬁrst non-parametric solution for reconstructing
fully textured and clothed 3D humans from a single-
view input image.

• An effective two-stage 3D shape reconstruction pipeline
that consists of predicting multi-view 2D silhouettes
from a single input segmentation and a novel deep
visual hull based mesh reconstruction technique with
view sampling optimization.

• An image-to-image translation framework to recon-
struct the texture of a full body from a single photo.

2. Related Work

Multi-view reconstruction. Due to the geometric com-
plexity introduced by garment deformation and self occlu-
sions, reconstructing clothed human bodies usually requires
images captured from multiple viewpoints. Early attempts
in this direction have extensively explored visual hull based
approaches [31, 45, 15, 13, 9, 14] due to its efﬁciency
and robustness to approximate the underlying 3D geometry.
However, a visual hull based representation cannot handle
concave regions nor generate good approximations of ﬁne-
scale details especially when the number of input views
is limited. To address this issue, detailed geometry are
often captured using techniques based on multi-view stereo
constraints [41, 63, 50, 39, 46, 16, 53]. A number of
techniques [56, 36, 58] exploit motion cues as additional
priors for a more accurate digitization of body shapes.

Some more recent research have focused on monocular
input capture, with the goal of making human modeling
more accessible to end users [54, 2, 1]. With the recent
advancement of deep learning, an active research direction is
to encode shape prior in a deep neural network in order
to model the complexity of human body and garment
deformations. To this end, Huang et al. [20] and Gilbert et
al. [17] have presented techniques that can synthesize
clothed humans in a volumetric form from highly sparse
views. Although the number of input views are reduced, both
methods still require a carefully calibrated capture system.
In this work, we push the envelop by reducing the input to a
single unconstrained input photograph.

Single-view reconstruction. To reduce the immense solu-
tion space of human body shapes, several 3D body model
repositories, e.g. SCAPE [3] and SMPL [28], have been
introduced, which have made the single-view reconstruction
of human bodies more tractable.
In particular, a 3D
parametric model is built from such database, which uses
pose and shape parameters of the 3D body to best match
an input image [5, 18, 6, 24]. As the mapping between
the body geometry and the parameters of the deformable
model is highly non-linear, alternative approaches based
on deep learning have become increasingly popular. The
seminal work of Dibra et al. [10, 11] introduces deep neural
networks to estimate the shape parameters from a single
input silhouette. More recent works predict body parameters
of the popular SMPL model [6] by either minimizing
the silhouette matching error [42], joint error based on
the silhouette and 2D joints [43], or an adversarial loss
that can distinguish unrealistic reconstruction output [23].
Concurrent to our work, Weng et al. [52] present a method
to animate a person in 3D from a single image based on the
SMPL model and 2D warping.
Although deformable models offer a low-dimensional em-
bedding of complex non-rigid human body shapes, they are
not suitable for modeling of ﬁne-scale clothing details. To
address this issue, additional information such as 2D [51, 8]
and 3D body pose [32, 57, 19] has been incorporated to help
recover clothed body geometry without relying on a template
mesh. BodyNet [44] for instance, estimates volumetric
body shapes from a single image based on an intermediate
inference of 2D pose, 2D part segmentation, as well as 3D
pose. The latest advances in novel view synthesis of human
pose [29, 4] and 3D shape [62, 61, 37] have demonstrated the
ability of obtaining multi-view inference from a single image.
In this work, we introduce an approach that combines 3D
poses estimation with the inference of silhouettes from novel
views for predicting high-ﬁdelity clothed 3D human shapes
from a single photograph. We show that our method can
achieve reasonably accurate reconstructions automatically
without any template model.

24481

input image

2D silhouette

back-view image

front-to-back
texture inference
(§ 3.3)

3D pose

2D pose
(input view)

2D pose 
(target views)

multi-view
silhouette synthesis
(§ 3.1)

deep
visual hull
(§ 3.2)

ﬁnal reconstruction

2D silhouettes
(target views)

reconstructed geometry

Figure 2: Overview of our framework.

3. Method

Our goal is to reconstruct a wide range of 3D clothed
human body shapes with a complete texture from a single
image of a person in frontal view. Figure 2 illustrates an
overview of our system. Given an input image, we ﬁrst
extract the 2D silhouette and 3D joint locations, which are
fed into a silhouette synthesis network to generate plausible
2D silhouettes from novel viewpoints (Sec. 3.1). The
network produces multiple silhouettes with known camera
projections, which are used as input for 3D reconstruction
via visual hull algorithms [45]. However, due to possible
inconsistency between the synthesized silhouettes,
the
subtraction operation of visual hull tends to excessively
erode the reconstructed mesh. To further improve the output
quality, we adopt a deep visual hull algorithm similar to
Huang et al. [20] with a greedy view sampling strategy so
that the reconstruction results account for domain-speciﬁc
clothed human body priors (Sec. 3.2). Finally, we inpaint
the non-visible body texture on the reconstructed mesh by
inferring the back view of the input image using an image-
to-image translation network (Sec. 3.3).

3.1. Multi(cid:173)View Silhouette Synthesis

We seek an effective human shape representation that can
handle the shape complexity due to different clothing types
and deformations. Inspired by visual hull algorithms [31]
and recent advances in conditional image generation [12,
30, 60, 59, 25], we propose to train a generative network
for synthesizing 2D silhouettes from viewpoints other than
the input image (see Figure 3). We use these silhouettes
as an intermediate implicit representation for the 3D shape
inference.
Speciﬁcally, given the subject’s 3D pose, estimated from
the input image as a set of 3D joint locations, we project the
3D pose onto the input image and a target image plane to get

2D silhouette

2D pose
(input view)

2D pose
(target view)

LBCE

generator

D

Real/Fake

Ladv

Figure 3: Illustration of our silhouette synthesis network.

the 2D pose Ps in the source view and the pose Pt in the
target view, respectively. Our silhouette synthesis network
Gs takes the input silhouette Ss together with Ps and Pt as
input, and predicts the 2D silhouette in the target view Pt :

St = Gs (cid:0)Ss , Ps , Pt (cid:1).

(1)

Our loss function for training the network Gs consists
of reconstruction errors of the inferred silhouettes using
a binary cross entropy loss LBCE and a patch-based
adversarial loss Ladv [22]. The total objective function is
given by

L = λBCE · LBCE + Ladv ,

(2)

where the relative weight λBCE is set to 750. In particular,
the adversarial loss turns out to be critical for synthesizing
sharp and detailed silhouettes. Figure 4 shows that the loss
function with the adversarial term generate much sharper
silhouettes, while without an adversarial loss would lead to
blurry synthesis output.

34482

silhouette+2D pose
(input view)

2D pose
(target view)

synthesis
w/ GAN

synthesis
w/o GAN

Figure 4: GAN helps generate clean silhouettes in presence
of ambiguity in silhouette synthesis from a single view.

Discussions. The advantages of using silhouettes to guide
the 3D reconstruction are two-fold. First, since silhouettes
are binary masks, the synthesis can be formulated as a
pixel-wise classiﬁcation problem, which can be trained
more robustly without the need of complex loss functions
or extensive hyper parameter tuning in contrast to novel
view image synthesis [29, 4]. Second, the network can
predict much a higher spatial resolution since it does not
store 3D voxel information explicitly, as with volumetric
representations [44], which are bounded by the limited
output resolution.

3.2. Deep Visual Hull Prediction

Although our silhouette synthesis algorithm generates
sharp prediction of novel-view silhouettes, the estimated
results may not be perfectly consistent as the conditioned
3D joints may fail to fully disambiguate the details in
the corresponding silhouettes (e.g., ﬁngers, wrinkles of
garments). Therefore, naively applying conventional visual
hull algorithms is prone to excessive erosion in the recon-
struction, since the visual hull is designed to subtract the
inconsistent silhouettes in each view. To address this issue,
we propose a deep visual hull network that reconstructs
a plausible 3D shape of clothed body without requiring
perfectly view-consistent silhouettes by leveraging the shape
prior of clothed human bodies.
In particular, we use a network structure based on [20].
At a high level, Huang et al. [20] propose to map 2D
images to a 3D volumetric ﬁeld through a multi-view
convolutional neural network. The 3D ﬁeld encodes the
probabilistic distribution of 3D points on the captured
surface. By querying the resulting ﬁeld, one can instan-
tiate the geometry of clothed human body at an arbitrary
resolution. However, unlike their approach which takes
carefully calibrated color images from ﬁxed views as input,
our network only consumes the probability maps of novel-
view silhouettes, which can be inconsistent across different
views. Although arbitrary number of novel-view silhouettes
can be generated, it remains challenging to properly select
optimal input views to maximize the network performance.
Therefore, we introduce several improvements to increase
the reconstruction accuracy.

Greedy view sampling. We propose a greedy view sam-
pling strategy to choose proper views that can lead to better
reconstruction quality. Our key idea is to generate a pool of
candidate silhouettes and then select the views that are most
consistent in a greedy manner. In particular, the candidate
silhouettes are rendered from 12 view bins {Bi }: the main
orientations of the bins are obtained by uniformly sampling
12 angles in the yaw axis. The ﬁrst bin only contains the
input view and thus has to be aligned with the orientation
of the input viewpoint. Each of the other bins consists of 5
candidate viewpoints, which are distributed along the pitch
axis with angles sampled from {0◦ , 15◦ , 30◦ , 45◦ , 60◦ }. In
the end, we obtain 55 candidate viewpoints {Vi } to cover
most parts of the 3D body.
To select the views with maximal consistency, we ﬁrst
compute an initial bounding volume of the target model
based on the input 3D joints. We then carve the bounding
volume using the silhouette of the input image and obtain
a coarse visual hull H1 . The bins with remaining views
are iterated in a clockwise order, i.e., only one candidate
view will be sampled from each bin at the end of the
sampling process. Starting from the second bin B2 , the
previously computed visual hull H1 is projected to its
enclosed views. The candidate silhouette that has the
maximum 2D intersection over union (IoU) with H1 ’s
projection will be selected as the next input silhouette for
our deep visual hull algorithm. After the best silhouette
ˆV2 is sampled from B2 , H1 is further carved by ˆV2 and the
updated visual hull H2 is passed to the next iteration. We
iterated until all the view bins have been sampled.
The selected input silhouettes generated by our greedy
view sampling algorithm are then fed into a deep visual
hull network. The choice of our network design is similar
to that of [20]. The main difference lies in the format of
inputs. Speciﬁcally, in addition to multi-view silhouettes,
our network also takes the 2D projection of the 3D pose
as additional channel concatenated with the corresponding
silhouette. This change helps to regularize the body part
generation by passing the semantic supervision to the
network and thus improves robustness. Moreover, we also
reduce some layers of the network of [20] to achieve a more
compact model and to prevent overﬁtting. The detailed
architecture is provided in our supplementary materials.

3.3. Front(cid:173)to(cid:173)Back Texture Synthesis

When capturing the subject from a single viewpoint, only
one side of the texture is visible and therefore predicting the
other side of the texture appearance is required to reconstruct
a fully textured 3D body shape. Our key observation is
that the frontal view and the back view of a person are
spatially aligned by sharing the same contour and many
visual features. This fact has inspired us to solve the
problem of back-view texture prediction using an image-

44483

input 
frontal image

2D silhouette
(input view)

Real/Fake

Ladv

D

LVGG

LFM

generator

back-view image

Figure 5: Illustration of our front-to-back synthesis network.

to-image translation framework based conditional generative
adversarial network. Speciﬁcally, we train a generator Gt to
predict the back-view texture ˆIb from the frontal-view input
image If and the corresponding silhouette Sf :

ˆIb = Gt (If , Sf ).

(3)

We train the generator Gt in a supervised manner by
leveraging textured 3D human shape repositories to generate
a dataset that sufﬁces for our training objective (Sec. 3.4).
Adopted from a high-resolution image-to-image translation
network [48], our loss function consists of a feature matching
loss LF M that minimizes the discrepancy of intermediate
layer activation of the discriminator D , a perceptual loss
LV GG using a VGG19 model pre-trained for image classi-
ﬁcation task [40], and an adversarial loss Ladv conditioned
by the input frontal image (see Figure 5). The total objective
is deﬁned as:

L = λF M · LF M + λV GG · LV GG + Ladv ,

(4)

where we set the relative weights as λF M = λV GG = 10.0
in our experiments.
The resulting back-view image is used to complete the
per-vertex color texture of the reconstructed 3D mesh. If
the dot product between the surface normal n in the input
camera space and the camera ray c is negative (i.e., surface
is facing towards the camera), the vertex color is sampled
from the input view image at the corresponding screen
coordinate. Likewise, if the dot product is positive (i.e.,
surface is facing in the opposite direction), the vertex color
is sampled from the synthesized back-view image. When the
surface is perpendicular to the camera ray (i.e., |n · c| ≤ ǫ =
1.0 × 10−4 ), we blend the colors from the front and back
views so that there are no visible seams across the boundary.

3.4. Implementation Details

Body mesh datasets. We have collected 73 rigged meshes
with full textures from aXYZ1 and 194 meshes from
Renderpeople2 . We randomly split the dataset into a training

set and a test set of 247 and 20 meshes, respectively. We
apply 48 animation sequences (such as walking, waving,
and Samba dancing) from Mixamo3 to each mesh from
Renderpeople to collect body meshes of different poses.
Similarly, the meshes from aXYZ have been animated into
11 different sequences. To render synthetic training data, we
have also obtained 163 second-order spherical harmonics of
indoor environment maps from HDRI Haven4 and they are
randomly rotated around the yaw axis.

Camera settings for synthetic data. We place the pro-
jective camera so that the pelvis joint is aligned with the
image center and relative body size in the screen space
remains unchanged. Since our silhouette synthesis network
takes an unconstrained silhouette as input and generate a
new silhouette in predeﬁned view points, we separate the
data generation for the source silhouettes and the target
silhouettes. We render our data images at the resolution of
256×256. For the source silhouettes a yaw angle is randomly
sampled from 360◦ and a pitch angle between −10◦ and 60◦ ,
whereas for the target silhouettes, a yaw angle is sampled
from every 7.5◦ and a pitch angle from 10, 15, 30, 45, 60◦ .
The camera has a randomly sampled 35mm ﬁlm equivalent
focal length ranged between 40 and 135mm for the source
silhouettes and a ﬁxed focal length of 800mm for the target
silhouettes. For the front-to-back image synthesis, we set
the yaw angle to be frontal and sample the pitch angle from
0, 7.5, 15◦ with a focal length of 800mm. Given the camera
projection, we project 13 joint locations that are compatible
with MPII [33] onto each view point.

Front-to-back rendering. Figure 6 illustrates how we
generate a pair of front and back view images. Given
a camera ray, normal rendering of 3D mesh sorts the
depth of triangles per pixel and display the rasterization
results assigned from the closest triangle. To obtain the
corresponding image from the other side, we instead takes
that of the furthest triangle. Note that most common graphics
libraries (e.g., OpenGL, DirectX) support this function,
allowing us to generate training samples within a reasonable
amount of time.

Network architectures. Both our silhouette synthesis
network and the front-to-back synthesis network follow the
U-Net network architecture in [22, 55, 21, 49, 47] with an
input channel size of 7 and 4, respectively. All the weights in
these networks are initialized based on Gaussian distribution.
We use the Adam optimizer with learning rates of 2.0× 10−4 ,
1.0 × 10−4 , and 2.0 × 10−4 , batch size of 30, 1, and 1, the
number of iterations of 250, 000, 160, 000, and 50, 000, and

1 https://secure.axyz- design.com/
2 https://renderpeople.com/3d- people/

3 https://www.mixamo.com/
4 https://hdrihaven.com/

54484

w

e
v

i

-
t

n
o

r
f

w

e
v

i

-

k
c
a
b

camera ray

front-view 
sampled point

back-view 
sampled point

Figure 6: Illustration of our back-view rendering approach.

no weight decay for the silhouette synthesis, deep visual hull,
and front-to-back synthesis, respectively. The deep visual
hull is trained with the output of our silhouette synthesis
network so that the distribution gap between the output of
silhouette synthesis and the input for the deep visual hull
algorithm is minimized.

Additional networks. Although 2D silhouette segmenta-
tion and 3D pose estimation are not our major contributions
and in practice one can use any existing methods, we
train two additional networks to automatically process
the input image with consistent segmentation and joint
conﬁgurations. For the silhouette segmentation, we adopt a
stacked hourglass network [35] with three stacks. Given an
input image of resolution 256× 256× 3, the network predicts
a probability map of resolution 64 × 64 × 1 for silhouettes.
We further apply a deconvolution layer with a kernel size
of 4 to obtain sharper silhouettes, after concatenating 2×
upsampled probability and the latent features after the ﬁrst
convolution in the hourglass network. The network is
trained with the mean-squared error between the predicted
probability map and the ground truth of UP dataset [24].
For 3D pose estimation, we adopt a state-of-the-art 3D face
alignment network [7] without modiﬁcation. We train the
pose estimation network using our synthetically rendered
body images of resolution 256 × 256 together with the
corresponding 3D joints. We use the RMSProp optimizer
with a learning rate of 2.0 × 10−5 , a batch size of 8 and no
weight decay for training both the silhouette segmentation
and pose estimation networks.

4. Experimental Results

Figure 7 shows our reconstruction results of 3D clothed
human bodies with full textures on different single-view
input images from the DeepFashion dataset [27]. For each
input, we show the back-view texture synthesis result, the
reconstructed 3D geometry rendered with plain shading, as
well as the ﬁnal textured geometry. Our method can robustly
handle a variety of realistic test photos of different poses,

input image

back-view 
synthesis

reconstruction 
(geometry)

reconstruction 
(textured)

Figure 7: Our 3D reconstruction results of clothed human
body using test images from the DeepFashion dataset [27].

body shapes, and cloth styles, although we train the networks
using synthetically rendered images only.

4.1. Evaluations

Silhouette Representation. We verify the effectiveness
of our silhouette-based representation by comparing it with

64485

Input

Method

Inferred
silhouettes

GT silhouettes
GT images

visual hull (random)
visual hull (optimized)
deep v-hull (random)
deep v-hull (optimized)
visual hull (8 views)
Huang et al. [20] (4 views)

CD

2.12
1.37
1.41

1.34

0.67
0.98

EMD

6.95
6.93
3.79

3.66

3.19
4.09

Table 2: Evaluation of our greedy sampling method to
compute deep visual hull.

It is evident that direct voxel prediction will lead to poor
accuracy when matching the side view in 2D and aligning
with the ground-truth geometry in 3D, as compared to our
silhouette-based representation. Figure 8 shows qualitative
comparisons demonstrating the advantages of our silhouette-
based representation.

Visual hull reconstruction.
In Table 2 and Figure 9, we
compare our deep visual hull algorithm (Sec. 3.2) with a
naive visual hull method. We also evaluate our greedy
view sampling strategy by comparing it with random view
selection. We use 12 inferred silhouettes as input for
different methods and evaluate the reconstruction errors
using Chamfer distances. For random view selection, we
repeat the process 100 times and compute the average error.
As additional references, we also provide the corresponding
results using the naive visual hull method with 8 ground-
truth silhouettes, as well as the method in [20] using 4
ground-truth images. As shown in Table 2, our deep visual
algorithm outperforms a naive approach and our greedy view
sampling strategy can signiﬁcantly improve the results in
terms of reconstruction errors.
In addition, for the deep
visual hull algorithm, our view sampling strategy is better
than 69% random selected views, while for the naive visual
hull method, our approach always outperforms random view
selection. Figure 9 demonstrates that our deep visual hull
method helps ﬁx some artifacts and missing parts especially
in concave regions, which are caused by inconsistency
among multi-view silhouettes synthesis results.

4.2. Comparisons

ground truth

ours
(silhouette + dvh)

ours
error map

voxel regression

voxel
error map

Figure 8: Qualitative evaluation of our silhouette-based
shape representation as compared to direct voxel prediction.

Input

Output

IoU (2D)

CD

EMD

RGB+2D Pose
Silhouette+2D Pose
RGB+3D Pose
Silhouette+3D Pose

Silhouette
Silhouette
Voxel
Voxel

0.826

0.886

0.471
0.462

1.66

1.36

2.49
2.77

4.38

3.69

5.67
6.23

Table 1: Evaluation of our silhouette-based representation
compared to direct voxel prediction.
The errors are
measured using Chamfer Distance(CD) and Earth Mover’s
Distance(EDM) between the reconstructed meshes and the
ground-truth.

ground truth

naive visual hull
(random 12 views)

naive visual hull
(optimized 12 views)

deep visual hull
(random 12 views)

deep visual hull
(optimized 12 views)

Figure 9: Comparisons between our deep visual hull method
with a native visual hull algorithm, using both random view
selection and our greedy view sampling strategy.

several alternative approaches based on the Renderpeople
dataset, including direct voxel prediction from 3D pose and
using RGB image to replace 2D silhouette as input for deep
visual hull algorithm. Please refer to our supplementary
materials for implementation details of our baseline methods
used for comparisons. For all the methods, we report (1)
the 2D Intersection over Union (IoU) for the synthetically
generated side view and (2) the 3D reconstruction error
based on Chamfer distances between the reconstructed
meshes and the ground-truth (in centimeter) in Table 1.

In Figure 10, we compare our method using single-view
input with a native visual hull algorithm using 8 input views
as well as Huang et al. [20] using four input views. For
each result, we show both the plain shaded 3D geometry and
the color-coded 3D reconstruction error. Although using a
single image as input each time, we can still generate results
that are visually comparable to those from methods based
on multi-view input.
In Figure 11, we qualitatively compare our results
with state-of-the-art single-view human reconstruction tech-

74486

input

HMR

BodyNet

Ours

Figure 11: We qualitatively compare our method with two
state-of-the-art single view human reconstruction techniques,
HMR [23] and BodyNet [44].

input

visual hull
(8 views)

[Huang et al.]
(4 views)

ours
(1 view)

Figure 10: Comparison with multiview visual hull algo-
rithms. Despite the single view input, our method produces
comparable reconstruction results. Note that input image in
red color is the single-view input for our method and the top
four views are used for Huang et al. [20].

niques [23, 44]. Since existing methods focus on body shape
only using parametric models, our approach can generate
more faithful results in cases of complex clothed geometry.

5. Discussion and Future Work

In this paper, we present a framework for monocular 3D
human reconstruction using deep neural networks. From
a single input image of the subject, we can predict the
3D textured geometry of clothed body shape, without
any requirement of a parametric model or a pre-captured
template. To this end, we propose a novel-view silhouette
synthesis network based on adversarial training, an improved
deep visual hull algorithm with a greedy view selection
strategy, as well as a front-to-back texture synthesis network.
One major limitation of our current implementation is that
our synthetic training data is very limited and may be biased
from real images. See Figure 12 for a few typical failure
cases, in which the 3D pose estimation may fail or there are
some additional accessories not covered by our training data.
It would be helpful to add realistic training data which may
be tedious and costly to acquire. The output mesh using our

incorrect 
pose estimation

poor 
back-view inference

failed 
segmentation

Figure 12: Failure cases.

method is not rigged and thus cannot be directly used for
animation. Also we do not explicitly separate the geometry
of cloth and human body. In the future, we plan to extend
our method to predict output with high frequency details and
semantic labels. Finally, it is interesting to infer relightable
textures such as diffuse and specular albedo maps.

Acknowledgements

Shigeo Morishima is supported by the JST ACCEL Grant Num-
ber JPMJAC1602, JSPS KAKENHI Grant Number JP17H06101,
the Waseda Research Institute for Science and Engineering. Hao
Li is afﬁliated with the University of Southern California, the USC
Institute for Creative Technologies, and Pinscreen. This research
was conducted at USC and was funded by in part by the ONR YIP
grant N00014-17-S-FO14, the CONIX Research Center, one of six
centers in JUMP, a Semiconductor Research Corporation (SRC)
program sponsored by DARPA, the Andrew and Erna Viterbi Early
Career Chair, the U.S. Army Research Laboratory (ARL) under
contract number W911NF-14-D-0005, Adobe, and Sony. This
project was not funded by Pinscreen, nor has it been conducted at
Pinscreen or by anyone else afﬁliated with Pinscreen. The content
of the information does not necessarily reﬂect the position or the
policy of the Government, and no ofﬁcial endorsement should be
inferred.

84487

References

[1] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-
Moll. Detailed human avatars from monocular video. In
International Conference on 3D Vision, pages 98–109, 2018.
[2] T. Alldieck, M. A. Magnor, W. Xu, C. Theobalt, and
G. Pons-Moll. Video based reconstruction of 3d people
models. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 8387–8397, 2018.
[3] D. Anguelov, P. Srinivasan, D. Koller, S. Thrun, J. Rodgers,
and J. Davis. SCAPE: shape completion and animation of
people. ACM Transactions on Graphics, 24(3):408–416,
2005.
[4] G. Balakrishnan, A. Zhao, A. V. Dalca, F. Durand, and
J. Guttag.
Synthesizing images of humans in unseen
poses. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 8340–8348, 2018.
[5] A. O. Balan, L. Sigal, M. J. Black, J. E. Davis, and
H. W. Haussecker. Detailed human shape and pose from
images. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 1–8, 2007.
[6] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and
M. J. Black. Keep it SMPL: Automatic estimation of 3D
human pose and shape from a single image. In European
Conference on Computer Vision, pages 561–578, 2016.
[7] A. Bulat and G. Tzimiropoulos. How far are we from solving
the 2d & 3d face alignment problem?(and a dataset of 230,000
3d facial landmarks). In IEEE International Conference on
Computer Vision, pages 1021–1030, 2017.
[8] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-
person 2D pose estimation using part afﬁnity ﬁelds. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 7291–7299, 2017.
[9] G. K. Cheung, S. Baker, and T. Kanade. Visual hull alignment
and reﬁnement across time: A 3D reconstruction algorithm
combining shape-from-silhouette with stereo.
In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 375–382, 2003.
[10] E. Dibra, H. Jain, C. ¨Oztireli, R. Ziegler, and M. Gross. Hs-
nets: Estimating human body shape from silhouettes with
convolutional neural networks. In International Conference
on 3D Vision, pages 108–117, 2016.
[11] E. Dibra, H. Jain, C. Oztireli, R. Ziegler, and M. Gross. Hu-
man shape from silhouettes using generative hks descriptors
and cross-modal neural networks. In IEEE Conference on
Computer Vision and Pattern Recognition, pages 4826–4836,
2017.
[12] P. Esser, E. Sutter, and B. Ommer. A variational u-net
for conditional appearance and shape generation. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 8857–8866, 2018.
[13] C. H. Esteban and F. Schmitt. Silhouette and stereo fusion
for 3D object modeling. Computer Vision and Image
Understanding, 96(3):367–392, 2004.
[14] J.-S. Franco, M. Lapierre, and E. Boyer. Visual shapes of
silhouette sets.
In International Symposium on 3D Data
Processing, Visualization, and Transmission, pages 397–404,
2006.

[15] Y. Furukawa and J. Ponce. Carved visual hulls for image-
based modeling. In European Conference on Computer Vision,
pages 564–577, 2006.

[16] Y. Furukawa and J. Ponce. Accurate, dense, and robust
multiview stereopsis. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 32(8):1362–1376, 2010.

[17] A. Gilbert, M. Volino, J. Collomosse, and A. Hilton. Volu-
metric performance capture from minimal camera viewpoints.
In European Conference on Computer Vision, pages 566–581,
2018.

[18] P. Guan, A. Weiss, A. O. Balan, and M. J. Black. Estimating
human shape and pose from a single image.
In IEEE
International Conference on Computer Vision, pages 1381–
1388, 2009.

[19] R. A. G ¨uler, N. Neverova, and I. Kokkinos. Densepose: Dense
human pose estimation in the wild. In IEEE Conference on
Computer Vision and Pattern Recognition, pages 7297–7306,
2018.

[20] Z. Huang, T. Li, W. Chen, Y. Zhao, J. Xing, C. LeGendre,
L. Luo, C. Ma, and H. Li. Deep volumetric video from
very sparse multi-view performance capture. In European
Conference on Computer Vision, pages 336–354, 2018.

[21] L. Huynh, W. Chen, S. Saito, J. Xing, K. Nagano, A. Jones,
P. Debevec, and H. Li. Mesoscopic facial geometry inference
using deep neural networks.
In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
pages 8407–8416, 2018.

[22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
translation with conditional adversarial networks. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 1125–1134, 2017.

[23] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-to-
end recovery of human shape and pose. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 7122–
7131, 2018.

[24] C. Lassner, J. Romero, M. Kiefel, F. Bogo, M. J. Black, and
P. V. Gehler. Unite the people: Closing the loop between
3d and 2d human representations. In IEEE Conference on
Computer Vision and Pattern Recognition, pages 6050–6059,
2017.

[25] H. Li, G. Li, L. Lin, H. Yu, and Y. Yu. Context-aware semantic
inpainting. IEEE Transactions on Cybernetics, 2018.

[26] H. Li, E. Vouga, A. Gudym, L. Luo, J. T. Barron, and
G. Gusev. 3D self-portraits. ACM Transactions on Graphics,
32(6):187:1–187:9, 2013.

[27] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Deepfashion:
Powering robust clothes recognition and retrieval with rich
annotations. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 1096–1104, 2016.

[28] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J.
Black. SMPL: A skinned multi-person linear model. ACM
Transactions on Graphics, 34(6):248:1–248:16, 2015.

[29] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and
L. Van Gool. Pose guided person image generation.
In
Advances in Neural Information Processing Systems, pages
406–416, 2017.

94488

[30] L. Ma, Q. Sun, S. Georgoulis, L. Van Gool, B. Schiele, and
M. Fritz. Disentangled person image generation. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 99–108, 2018.
[31] W. Matusik, C. Buehler, R. Raskar, S. J. Gortler, and
L. McMillan. Image-based visual hulls. In ACM SIGGRAPH,
pages 369–374, 2000.
[32] D. Mehta, S. Sridhar, O. Sotnychenko, H. Rhodin, M. Shaﬁei,
H.-P. Seidel, W. Xu, D. Casas, and C. Theobalt. VNect: Real-
time 3D Human Pose Estimation with a Single RGB Camera.
ACM Transactions on Graphics, 36(4):44:1–44:14, 2017.
[33] A. Mykhaylo, P. Leonid, G. Peter, and B. Schiele.
2d
human pose estimation: New benchmark and state of the
art analysis. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 3686–3693, 2014.
[34] R. A. Newcombe, D. Fox, and S. M. Seitz. DynamicFusion:
Reconstruction and tracking of non-rigid scenes in real-
time. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 343–352, 2015.
[35] A. Newell, K. Yang, and J. Deng. Stacked hourglass networks
for human pose estimation.
In European Conference on
Computer Vision, pages 483–499, 2016.
[36] G. Pons-Moll, S. Pujades, S. Hu, and M. J. Black. Cloth-
cap: Seamless 4d clothing capture and retargeting. ACM
Transactions on Graphics, 36(4):73:1–73:15, 2017.
[37] K. Rematas, C. H. Nguyen, T. Ritschel, M. Fritz, and T. Tuyte-
laars. Novel views of objects from a single image. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
39(8):1576–1590, 2017.
[38] G. Rogez, P. Weinzaepfel, and C. Schmid. LCR-Net++: Multi-
person 2D and 3D Pose Detection in Natural Images. arXiv
preprint arXiv:1803.00455, 2018.
[39] S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and
R. Szeliski. A comparison and evaluation of multi-view stereo
reconstruction algorithms. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 519–528, 2006.
[40] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556, 2014.
[41] J. Starck and A. Hilton. Surface capture for performance-
based animation. IEEE Computer Graphics and Applications,
27(3):21–31, 2007.
[42] J. Tan, I. Budvytis, and R. Cipolla. Indirect deep structured
learning for 3d human body shape and pose prediction. In
British Machine Vision Conference, pages 6.1–6.11, 2017.
[43] H.-Y. Tung, H.-W. Tung, E. Yumer, and K. Fragkiadaki. Self-
supervised learning of motion capture. In Advances in Neural
Information Processing Systems, pages 5236–5246, 2017.
[44] G. Varol, D. Ceylan, B. Russell, J. Yang, E. Yumer, I. Laptev,
and C. Schmid. BodyNet: Volumetric inference of 3D human
body shapes. In European Conference on Computer Vision,
pages 20–36, 2018.
[45] D. Vlasic, I. Baran, W. Matusik, and J. Popovi ´c. Articulated
mesh animation from multi-view silhouettes. ACM Transac-
tions on Graphics, 27(3):97:1–97:9, 2008.
[46] D. Vlasic, P. Peers, I. Baran, P. Debevec, J. Popovi ´c,
S. Rusinkiewicz, and W. Matusik. Dynamic shape capture

using multi-view photometric stereo. ACM Transactions on
Graphics, 28(5):174:1–174:11, 2009.

[47] C. Wang, H. Huang, X. Han, and J. Wang. Video inpainting by
jointly learning temporal structure and spatial details. arXiv
preprint arXiv:1806.08482, 2018.

[48] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and
B. Catanzaro. High-resolution image synthesis and semantic
manipulation with conditional gans. In IEEE Conference on
Computer Vision and Pattern Recognition, pages 8798–8807,
2018.

[49] Y. Wang, H. Huang, C. Wang, T. He, J. Wang, and M. Hoai.
Gif2video: Color dequantization and temporal interpolation
of gif images. arXiv preprint arXiv:1901.02840, 2019.

[50] M. Waschb ¨usch, S. W ¨urmlin, D. Cotting, F. Sadlo, and
M. Gross. Scalable 3D video of dynamic scenes. The Visual
Computer, 21(8):629–638, 2005.

[51] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh.
Convolutional pose machines.
In IEEE Conference on
Computer Vision and Pattern Recognition, pages 4724–4732,
2016.

[52] C.-Y. Weng, B. Curless, and I. Kemelmacher-Shlizerman.
Photo wake-up: 3d character animation from a single photo.
arXiv preprint arXiv:1812.02246, 2018.

[53] C. Wu, K. Varanasi, Y. Liu, H.-P. Seidel, and C. Theobalt.
Shading-based dynamic shape reﬁnement from multi-view
video under general illumination.
In IEEE International
Conference on Computer Vision, pages 1108–1115, 2011.

[54] W. Xu, A. Chatterjee, M. Zollh ¨ofer, H. Rhodin, D. Mehta,
H.-P. Seidel, and C. Theobalt. Monoperfcap: Human perfor-
mance capture from monocular video. ACM Transactions on
Graphics, 37(2):27:1–27:15, 2018.

[55] S. Yamaguchi, S. Saito, K. Nagano, Y. Zhao, W. Chen,
K. Olszewski, S. Morishima, and H. Li. High-ﬁdelity facial
reﬂectance and geometry inference from an unconstrained
image. ACM Transactions on Graphics, 37(4):162, 2018.

[56] J. Yang, J.-S. Franco, F. H ´etroy-Wheeler, and S. Wuhrer.
Estimation of human body shape in motion with wide clothing.
In European Conference on Computer Vision, pages 439–454,
2016.

[57] W. Yang, W. Ouyang, X. Wang, J. Ren, H. Li, and X. Wang.
3D human pose estimation in the wild by adversarial learn-
ing. In IEEE Conference on Computer Vision and Pattern
Recognition, pages 5255–5264, 2018.

[58] C. Zhang, S. Pujades, M. Black, and G. Pons-Moll. Detailed,
accurate, human shape estimation from clothed 3D scan
sequences.
In IEEE Conference on Computer Vision and
Pattern Recognition, pages 4191–4200, 2017.

[59] H. Zhang and V. M. Patel. Densely connected pyramid
dehazing network. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 3194–3203, 2018.

[60] H. Zhang, V. Sindagi, and V. M. Patel. Image de-raining using
a conditional generative adversarial network. arXiv preprint
arXiv:1701.05957, 2017.

[61] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View
synthesis by appearance ﬂow. In European conference on
computer vision, pages 286–301, 2016.

104489

[62] H. Zhu, H. Su, P. Wang, X. Cao, and R. Yang. View
extrapolation of human body from a single image. In IEEE
Conference on Computer Vision and Pattern Recognition,
pages 4450–4459, 2018.
[63] C. L. Zitnick, S. B. Kang, M. Uyttendaele, S. Winder, and
R. Szeliski. High-quality video view interpolation using
a layered representation. ACM Transactions on Graphics,
23(3):600–608, 2004.

114490

Taking a Deeper Look at the Inverse Compositional Algorithm

,

2

Zhaoyang Lv1
Frank Dellaert1
James M. Rehg1 Andreas Geiger2
1Georgia Institute of Technology, Atlanta, United States
2Autonomous Vision Group, MPI-IS and University of T ¨ubingen, Germany

{zhaoyang.lv, rehg}@gatech.edu

frank.dellaert@cc.gatech.edu

andreas.geiger@tue.mpg.de

Abstract

denote the warped template and image, respectively1 , the
Lucas-Kanade objective can be stated as follows

In this paper, we provide a modern synthesis of the clas-
sic inverse compositional algorithm for dense image align-
ment. We ﬁrst discuss the assumptions made by this well-
established technique, and subsequently propose to relax
these assumptions by incorporating data-driven priors into
this model. More speciﬁcally, we unroll a robust version of
the inverse compositional algorithm and replace multiple
components of this algorithm using more expressive models
whose parameters we train in an end-to-end fashion from
data. Our experiments on several challenging 3D rigid mo-
tion estimation tasks demonstrate the advantages of com-
bining optimization with learning-based techniques, out-
performing the classic inverse compositional algorithm as
well as data-driven image-to-pose regression approaches.

1. Introduction

Since the seminal work by Lucas and Kanade [32], dense
image alignment has become an ubiquitous tool in computer
vision with many applications including stereo reconstruc-
tion [41], tracking [5, 42, 50], image registration [8, 29, 43],
super-resolution [22] and SLAM [13, 16, 17].
In this pa-
per, we provide a learning-based perspective on the Inverse
Compositional algorithm, an efﬁcient variant of the origi-
nal Lucas-Kanade image registration technique. In partic-
ular, we lift some of the restrictive assumptions by param-
eterizing several components of the algorithm using neural
networks and training the entire optimization process end-
to-end. In order to put contributions into context, we will
now brieﬂy review the Lucas-Kanade algorithm, the Inverse
Compositional algorithm, as well as the robust M-Estimator
which form the basis for our model. More details can be
found in the comprehensive reviews of Baker et al. [2, 3].

min

ξ

kI(ξ) − T(0)k2

2

(1)

where I(ξ) denotes image I transformed using warp param-
eters ξ and T(0) = T denotes the original template.
Minimizing (1) is a non-linear optimization task as the
image I depends non-linearly on the warp parameters ξ .
The Lucas-Kanade algorithm therefore iteratively solves for
the warp parameters ξk+1 = ξk ◦ ∆ξ . At every iteration k ,
the warp increment ∆ξ is obtained by linearizing

kI(ξk + ∆ξ) − T(0)k2

2

min

∆ξ

using ﬁrst-order Taylor expansion

(2)

(3)

2

2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

min

∆ξ

I(ξk ) +

∂ I(ξk )
∂ ξ

∆ξ − T(0)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Note that the “steepest descent image” ∂ I(ξk )/∂ ξ needs to
be recomputed at every iteration as it depends on ξk .

Inverse Compositional Algorithm: The inverse compo-
sitional (IC) algorithm [3] avoids this by applying the warp
increments ∆ξ to the template instead of the image

kI(ξk ) − T(∆ξ)k2

2

min

∆ξ

(4)

using the warp parameter update ξk+1 = ξk ◦ (∆ξ)−1 . In
the corresponding linearized equation

min

∆ξ

I(ξk ) − T(0) −

∂T(0)
∂ ξ

2

2

(5)

∆ξ(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

∂T(0)/∂ ξ does not depend on ξk and can thus be pre-
computed, resulting in a more efﬁcient algorithm.

Lucas-Kanade Algorithm: The Lucas-Kanade algorithm
minimizes the photometric error between a template and an
image. Letting T : Ξ → RW ×H and I : Ξ → RW ×H

1 The warping function Wξ : R2 → R2 might represent translation,
afﬁne 2D motion or (if depth is available) rigid or non-rigid 3D motion. To
avoid clutter in the notation, we do not make Wξ explicit in our equations.

14581

Robust M-Estimation: To handle outliers or ambiguities
(e.g., multiple motions), robust estimation [19, 54] can be
used. The robust version of the IC algorithm [2] has the
following objective function

min

∆ξ

rk (∆ξ)T W rk (∆ξ)

(6)

where rk (∆ξ) = I(ξk ) − T(∆ξ) is the residual between
image I and template T at the k’th iteration, and W is a
diagonal weight matrix that depends on the residual2 and is
chosen based on the desired robust loss function ρ [54].

Optimization: The minimizer of (6) after linearization is
obtained as the Gauss-Newton update step [6]

(JT WJ)∆ξ = JT W rk (0)

(7)

where J = ∂T(0)/∂ ξ is the Jacobian of the template T(0)
with respect to the warp parameters ξ . As the approximate
Hessian JT WJ easily becomes ill-conditioned, a damping
term is added in practice. This results in the popular Leven-
berg–Marquardt (trust-region) update equation [35]:

∆ξ = (JT WJ + λ diag(JT WJ))

−1

JT W rk (0)

(8)

For different values of λ, the parameter update ∆ξ varies
between the Gauss-Newton direction and gradient descent.
In practice, λ is chosen based on simple heuristics.

Limitations: Despite its widespread utility, the IC method
suffers from a number of important limitations. First, it as-
sumes that the linearized residual leads to an update which
iteratively reaches a good local optimum. However, this as-
sumption is invalid in the presence of high-frequency textu-
ral information or noise in I or T. Second, choosing a good
robust loss function ρ is difﬁcult as the true data/residual
distribution is often unknown. Moreover, Equation (6) does
not capture correlations or higher-order statistics in the in-
puts I and T as the residuals operate directly on the pixel
values and the weight matrix W is diagonal. Finally, damp-
ing heuristics do not fully exploit the information available
during optimization and thus lead to suboptimal solutions.

Contributions:
In this paper, we propose to combine the
best of both (optimization and learning-based) worlds by
unrolling the robust IC algorithm into a more general pa-
rameterized feed-forward model which is trained end-to-
end from data. In contrast to generic neural network esti-
mators, this allows our algorithm to incorporate knowledge
about the structure of the problem (e.g., family of warp-
ing functions, 3D geometry) as well as the advantages of
a robust iterative estimation framework. At the same time,
our approach relaxes the restrictive assumptions made in the
original IC formulation [3] by incorporating trainable mod-
ules and learning the entire model end-to-end.
More speciﬁcally, we make the following contributions:

(A) We propose a Two-View Feature Encoder which re-
places I, T with feature representations Iθ , Tθ that
jointly encode information about the input image I and
the template T. This allows our model to exploit spa-
tial as well as temporal correlations in the data.

(B) We propose a Convolutional M-Estimator that re-
places W in (6) with a learned weight matrix Wθ
which encodes information about I, T and rk in a way
such that the unrolled optimization algorithm ignores
irrelevant or ambiguous information as well as outliers.

(C) We propose a Trust Region Network which replaces
the damping matrix λ diag(JT WJ) in (8) with a
learned damping matrix diag(λθ ) whose diagonal en-
tries λθ are estimated from “residual volumes” which
comprise residuals of a Levenberg-Marquardt update
when applying a range of hypothetical λ values.

We demonstrate the advantages of combining the classical
IC method with deep learning on the task of 3D rigid mo-
tion estimation using several challenging RGB-D datasets.
We also provide an extensive ablation study about the rel-
evance of each model component that we propose. Results
on traditional afﬁne 2D image alignment tasks are provided
in the supplementary material. Our implementation is pub-
licly accessible.3

2. Related Work

We are not the ﬁrst to inject deep learning into an opti-
mization pipeline. In this section, we ﬁrst review classical
methods, followed by direct pose regression techniques and
related work on learning-based optimization.

Classical Methods: Direct methods [18, 21] that align im-
ages using the sum-of-square error objective (1) are prone to
outliers and varying illuminations. Classical approaches ad-
dress this problem by exploiting more robust objective func-
tions [32, 37], heuristically chosen patch- [47] or gradient-
based [29] features, and photometric calibration as a pre-
processing step [16]. The most common approach is to use
robust estimation (6) as in [7]. However, the selection of a
good robust function ρ is challenging and traditional formu-
lations assume that the same function applies to all pixels,
ignoring correlations in the inputs. Moreover, the inversion
of the linearized system (7) may still be ill-conditioned [1].
To overcome this problem, soft constraints in the form of
damping terms (8) can be added to the objective [2,3]. How-
ever, this may bias the system to sub-optimal solutions.
This paper addresses these problems by relaxing the
main assumptions of the Inverse Compositional (IC) algo-
rithm [2, 3] using data-driven learning. More speciﬁcally,
we propose to learn the feature representation (A), robust

2We omit this dependency to avoid clutter in the notation.

3 https://github.com/lvzhaoyang/DeeperInverseCompositionalAlgorithm

24582

estimator (B) and damping (C) jointly to replace the tradi-
tional heuristic rules of classical algorithms.

Direct Pose Regression: A notably different approach to
classical optimization techniques is to directly learn the en-
tire mapping from the input to the warping parameters ξ
from large amounts of data, spanning from early work using
linear hyperplane approximation [23] to recent work using
deep neural networks [9, 49, 51, 53, 58]. Prominent exam-
ples include single image to camera pose regression [25,26],
image-based 3D object pose estimation [34, 46] and relative
pose prediction from two views [9, 49]. However, learning
a direct mapping requires high-capacity models and large
amounts of training data. Furthermore, obtaining pixel-
accurate registrations remains difﬁcult and the learned rep-
resentations do not generalize well to new domains.
To improve accuracy, recent methods adopt cascaded
networks [20, 52] and iterative feedback [24]. Lin et al. [31]
combines the multi-step iterative spatial transformer net-
work (STN) with the classical IC algorithm [2, 3] for align-
ing 2D images. Variants of this approach have recently been
applied to various 3D tasks: Li et al. [30] proposes to itera-
tively align a 3D CAD model to an image. Zhou et al. [56]
jointly train for depth, pose and optical ﬂow.
Different from [31] and its variants which approximate
the pseudo-inverse of the Jacobian implicitly using stacked
convolutional layers, we exploit the structure of the opti-
mization problem and explicitly solve the original robust
objective (6) with learned modules using few parameters.

Learning-based Optimization: Recently, several meth-
ods have exploited the differentiable nature of iterative op-
timization algorithms by unrolling for a ﬁxed number of it-
erations. Each iteration is treated as a layer in a neural net-
work [12, 28, 36, 39, 40, 55]. In this section, we focus on
the most related work which also tackles the least-squares
optimization problem [13, 38, 48, 51]. We remark that most
of these techniques can be considered special cases of our
more general deep IC framework.
Wang et al. [51] address the 2D image tracking prob-
lem by learning an input representation using a two-stream
Siamese network for the IC setting. In contrast to us, they
exploit only spatial but not temporal correlations in the in-
puts (A), leverage a formulation which is not robust (B) and
do not exploit trust-region optimization (C).
Clark et al. [13] propose to jointly learn depth and pose
estimation by minimizing photometric error using the for-
mulation in (4). In contrast to us, they do not learn feature
representations (A) and neither employ a robust formulation
(B) nor trust-region optimization (C).
Ranftl et al. [38] propose to learn robust weights for
sparse feature correspondences and apply their model to
fundamental matrix estimation. As they do not target direct
image alignment, their problem setup is different from ours.

Besides, they neither learn input features (A) nor leverage
trust-region optimization (C). Instead, they solve their opti-
mization problem using singular value decomposition.
Concurrent to our work, Tang and Tan [48] propose
a photometric Bundle Adjustment network by learning to
align feature spaces for monocular reconstruction of a static
scene. Different from us, they did not exploit temporal cor-
relation in the inputs (A) and do not employ a robust for-
mulation (B). While they propose to learn the damping pa-
rameters (C), in contrast to our trust-region volume formu-
lation, they regress the damping parameters from the global
average pooled residuals.

3. Method

This section describes our model. A high-level overview
over the proposed unrolled inverse-compositional algorithm
is given in Fig. 1. Using the same notation as in Section 1,
our goal is to minimize the error by warping the image to-
wards the template, similar to (6):

min

ξ

r(ξ)T Wθ r(ξ)

(9)

The difference to (6) is that in our formulation, the weight
matrix Wθ as well as the template Tθ (ξ) and the image
Iθ (ξ) (and thus also the residual r(ξ) = Iθ (ξ) − Tθ (0))
depend on the parameters of a learned model θ .
We exploit the inverse compositional algorithm to solve
the non-linear optimization problem in (9), i.e., we linearize
the objective and iteratively update the warp parameters ξ :

ξk+1 = ξk ◦ (∆ξ)−1
∆ξ = (JT Wθ J + diag(λθ ))
rk = Iθ (ξk ) − Tθ (0)

J = ∂Tθ (0)/∂ ξ

−1

JT Wθ rk

(10)

(11)

(12)

(13)

starting from ξ0 = 0.
The most notable change from (8) is that the image fea-
tures (Iθ , Tθ ), the weight matrix (Wθ ) and the damping
factors λθ are predicted by learned functions which have
been collectively parameterized by θ = {θI , θW , θλ }. Note
that also the residual rk as well as the Jacobian J implicitly
depend on the parameters θ , though we have omitted this
dependence here for notational clarity.
We will now provide details about these mappings.

(A) Two-View Feature Encoder: We use a fully convo-
lutional neural network φθ to extract feature maps from the
image I and the template T:

Iθ = φθ ([I, T])
Tθ = φθ ([T, I])

(14)

(15)

Here, the operator [·, ·] indicates concatenation along the
feature dimension.
Instead of I and T we then feed Iθ

34583

[", $]

[$, "]

Feature Pyramid

(A) Two-V iew 
Feature Encoder

(A) Two-V iew 
Feature Encoder

(A) Two-V iew 
Feature Encoder

(A) Two-V iew 
Feature Encoder

"'

∑

"(

∑

"(

∑

"'

∑

K	Inverse-
Composition

K	Inverse-
Composition

K	Inverse-
Composition

K	Inverse-
Composition

$'

$'

$'

$'

Pre-computation

'

Inverse-composition
at ;th iteration

Residual maps from damping proposals:

("# (*)
(,

%0

−

"#

$#

(B) Convolutional 
M-estimator

−

%2

-( .) → Δ1(. ) → %2+1
(. )

, . ∈ {1 … N}

(C) Trust Region 
Network

,:

,2+1 = ,2 ∘ Δ, −>

Δ,

-? 

Figure 1: High-level Overview of our Deep Inverse Compositional (IC) Algorithm. We stack [I, T] and [T, I] as inputs
to our (A) Two-View Feature Encoder pyramid which extracts 1-channel feature maps Tθ and Iθ at multiple scales using
channel-wise summation. We then perform K IC steps at each scale using Tθ and Iθ as input. At the beginning of each scale,
we pre-compute W using our (B) Convolutional M-estimator. For each of the K IC iterations, we compute the warped
image I(ξk ) and rk . Subsequently, we sample N damping proposals λ(i) and compute the proposed residual maps r
k+1 . Our
(C) Trust Region Network takes these residual maps as input and predicts λθ for the trust region update step.

(i)

and Tθ to the residuals in (12) and to the Jacobian in (13).
Note that we use the notation Iθ (ξk ) in (12) to denote that
the feature map Iθ is warped by a warping function that is
parameterized via ξ . More formally, this can be stated as
Iθ (ξ) = Iθ (Wξ (x)), where x ∈ R2 denote pixel locations
and Wξ : R2 → R2 is a warping function that maps a pixel
location to another pixel location. For instance, Wξ may
represent the space of 2D translations or afﬁne transforma-
tions. In our experiments, we will focus on challenging 3D
rigid body motions, using RGB-D inputs and representing
ξ as an element of the Euclidean group ξ ∈ se(3).
Note that compared to directly using the image I and T
as input, our features capture high-order spatial correlations
in the data, depending on the receptive ﬁeld size of the con-
volutional network. Moreover, they also capture temporal
information as they operate on both I and T as input.

(B) Convolutional M-Estimator: We parameterize the
weight matrix Wθ as a diagonal matrix whose elements are

determined by a fully convolutional network ψθ that oper-
ates on the feature maps and the residual:

Wθ = diag(ψθ (Iθ (ξk ), Tθ (0), rk ))

(16)

as is illustrated in Fig. 1. Note that this enables our al-
gorithm to reason about relevant image information while
capturing spatial-temporal correlations in the inputs which
is not possible with classical M-Estimators. Furthermore,
we do not restrict the implicit robust function ρ to a partic-
ular error model, but instead condition ρ itself on the input.
This allows for learning more expressive noise models.

(C) Trust Region Network: For estimating the damping
λθ we use a fully-connected network as illustrated in Fig. 1.
We ﬁrst sample a set of scalar damping proposals λi on
a logarithmic scale and compute the resulting Levenberg-
Marquardt update step as

∆ξ i = (JT WJ + λi diag(JT WJ))

−1

JT W rk (0) (17)

44584

We stack the resulting N residual maps

r

(i)

k+1 = Iθ (ξk ◦ (∆ξ i )−1 ) − Tθ (0)

(18)

into a single feature map, ﬂatten it, and pass it to a fully
connected neural network νθ that outputs the damping pa-
rameters λθ :

λθ = νθ (cid:16)JT WJ, hJT Wr

(1)

k+1 , . . . , JT Wr

(N )

k+1 i(cid:17) (19)

The intuition behind our trust region networks is that the
residuals predicted using the Levenberg-Marquardt update
comprise valuable information about the damping parame-
ter itself. This is empirically conﬁrmed by our experiments.

Coarse-to-Fine Estimation: To handle large motions, it is
common practice to apply direct methods in a coarse-to-ﬁne
fashion. We apply our algorithm at four pyramid levels with
three iterations each. Our entire model including coarse-to-
ﬁne estimation is illustrated in Fig. 1. We extract features at
all four scales using a single convolutional neural network
with spatial average pooling between pyramid levels. We
start with ξ = 0 at the coarsest pyramid level, perform 3
iterations of our deep IC algorithm, and proceed with the
next level until we reach the original image resolution.

Training and Inference: For training and inference, we
unroll the iterative algorithm in equations (10)-(13). We ob-
tain the gradients of the resulting computation graph using
auto-differentiation. Details about the network architectures
which we use can be found in the supplementary material.

4. Experiments

We perform our experiments on the challenging task
of 3D rigid body motion estimation from RGB-D inputs4 .
Apart from the simple scenario of purely static scenes where
only the camera is moving, we also consider scenes where
both the camera as well as objects are in motion, hence re-
sulting in strong ambiguities. We cast this as a supervised
learning problem: using the ground truth motion, we train
the models to resolve these ambiguities by learning to focus
either on the foreground or the background.

Warping Function: Given pixel x ∈ R2 , camera intrin-
sics K and depth D(x), we deﬁne the warping Wξ (x) in-
duced by rigid body transform Tξ with ξ ∈ se(3) as

Wξ (x) = K Tξ D(x)K−1 x

(20)

Using the warped coordinates, compute Iθ (ξ) via bilinear
sampling from Iθ and set the warped feature value to zero
for all occluded areas (estimated via z-buffering).

Training Objective: To balance the inﬂuences of trans-
lation and rotation we follow [30] and exploit the 3D End-
Point-Error (EPE) as loss function. Let p = D(x)K−1x
denote the 3D point corresponding to pixel x in image I
and let P denote the set of all such 3D points. We minimize
the following loss function

L =

1
|P | X

l∈L

X

p∈P

kTgt p − T(ξ l ) pk2

2

(21)

where L denotes the set of coarse-to-ﬁne pyramid levels (we
apply our loss at the ﬁnal iteration of every pyramid level)
and Tgt is the ground truth transformation.

Implementation: We use the Sobel operator to compute
the gradients in T and analytically derive J. We calculate
the matrix inverse on the CPU since we observed that invert-
ing a small dense matrices H ∈ R6×6 is signiﬁcantly faster
on the CPU than on the GPU. In all our experiments we
use N = 10 damping proposals for our Trust Region Net-
work, sampled uniformly in logscale between [10−5 , 105 ].
We use four coarse-to-ﬁne pyramid levels with three itera-
tions each. We implemented our model and the baselines in
PyTorch. All experiments start with a ﬁxed learning rate of
0.0005 using ADAM [27]. We train a total of 30 epochs,
reducing the learning rate at epoch [5,10,15].

4.1. Datasets

We systematically train and evaluate our method on four
datasets which we now brieﬂy describe.

MovingObjects3D: For the purpose of systematically eval-
uating highly varying object motions, we downloaded six
categories of 3D models from ShapeNet [10]. For each ob-
ject category, we rendered 200 video sequences with 100
frames in each sequence using Blender. We use data ren-
dered from the categories ’boat’ and ’motorbike’ as test set
and data from categories ’aeroplane’, ’bicycle’, ’bus’, ’car’
as training set. From the training set we use the ﬁrst 95%
of the videos for training and the remaining 5% for valida-
tion. In total, we obtain 75K images for training, 5K images
for validation, and 25K for testing. We further subsample
the sequences using sampling intervals {1, 2, 4} in order to
obtain small, medium and large motion subsets.
For each rendered sequence, we randomly select one 3D
object model within the chosen category and stage it in a
static 3D cuboid room with random wall textures and four
point light sources. We randomly choose the camera view-
point, point light source position and object trajectory, see
supplementary material for details. This ensures diversity in
object motions, textures and illumination. The videos also
contain frames where the object is only partially visible. We
exclude all frames where the entire object is not visible.

4Additional experiments on classical afﬁne 2D motion estimation tasks
can be found in the supplementary material.

BundleFusion: To evaluate camera motion estimation in
a static environment, we use the eight publicly released

54585

scenes from BundleFusion5 [14] which provide fully syn-
chronized RGB-D sequences. We hold out ’copyroom’ and
’ofﬁce2’ scenes for test and split the remaining scenes into
training (ﬁrst 95% of each trajectory) and validation (last
5%). We use the released camera trajectories as ground
truth. We subsampled frames at intervals {2, 4, 8} to in-
crease motion magnitudes and hence the level of difﬁculty.

DynamicBundleFusion: To further evaluate camera mo-
tion estimation under heavy occlusion and motion ambigu-
ity, we use the DynamicBundleFusion dataset [33] which
augments the scenes from BundleFusion with non-rigidly
moving human subjects as distractors. We use the same
training, validation and test split as above. We train and
evaluate frames subsampled at intervals {1, 2, 5} due to the
increased difﬁculty of this task.

TUM RGB-D SLAM: We evaluate our camera motion es-
timates on the TUM RGB-D SLAM dataset [45]. We hold
out ’fr1/360’, ’fr1/desk’, ’fr2/desk’ and ’fr2/pioneer 360’
for testing and split the remaining trajectories into training
(ﬁrst 95% of each trajectory) and validation (last 5%). We
randomly sample frame intervals from {1,2,4,8}. All im-
ages are resized to 160×120 pixels and depth values outside
the range [0.5m, 5.0m] are considered invalid.

4.2. Baselines

We implemented the following baselines.

ICP: We use classical Point-to-Plane ICP [11] and Point-
to-Point-ICP [4] implemented in Open3D [57]. To examine
the effect of ambiguity between the foreground and back-
ground in the object motion estimation task, we also evalu-
ate a version for which we provide the ground truth instance
segmentation to both methods. Note that this is an upper
bound to the performance achievable by ICP methods. We
thus call these the Oracle ICP methods.

RGB-D Visual Odometry: We compare to the RGB-D
visual odometry method [44] implemented in Open3D [57]
on TUM RGBD SLAM datasets for visual odometry.

Direct Pose Regression: We compare three different vari-
ants that directly predict the mapping f : I, T → ξ . All
three networks use Conv1-6 encoder layers from FlowNet-
Simple [15] as two-view regression backbones. We use spa-
tial average pooling after the last feature layer followed by a
fully-connected layer to regress ξ . All three CNN baselines
are trained using the loss in (21).
• PoseCNN: A feed-forward CNN that directly predicts ξ .
• IC-PoseCNN: A PoseCNN with iterative reﬁnement us-
ing the IC algorithm, similar to [31] and [30]. We noticed
that training becomes unstable and performance saturates

5 http://graphics.stanford.edu/projects/bundlefusion/

when increasing the number of iterations. For all our ex-
periments, we thus used three iterations.
• Cascaded-PoseCNN: A cascaded network with three it-
erations, similar to IC-PoseCNN but with independent
weights for each iteration.

Learning-based Optimization: We implemented the fol-
lowing related algorithms within our deep IC framework.
For all methods, we use the same number of iterations,
training loss and learning rate as used for our method.
• DeepLK-6DoF: We implemented a variant of DeepLK
[50] which predicts the 3D transformation ξ ∈ se(3) in-
stead of translation and scale prediction in their original
2D task. We use Gauss-Newton as the default optimiza-
tion for this approach and no Convolutional M-Estimator.
A comparison of this approach with our method when
using only the two-view feature network (A) shows the
beneﬁts of our two-view feature encoder.
• IC-FC-LS-Net: We also implemented LS-Net [13]
within our IC framework with the following differences
to the original paper. First, we do not estimate or re-
ﬁne depth. Second, we do not use a separate network
to provide an initial pose estimation. Third, we replace
their LSTM layers with three fully connected (FC) layers
which take the ﬂattened JT WJ and JT Wrk as input.

Ablation Study: We use (A), (B), (C) to refer to our contri-
butions in Sec.1. We set W to the identity matrix when the
Convolutional M-Estimator (B) is not used and use Gauss-
Newton optimization in the absence of the Trust Region
Network (C). We consider the following conﬁgurations:
• Ours (A)+(B)+(C): Our proposed method with shared
weights. We perform coarse-to-ﬁne iterations on four
pyramid levels with three IC iterations at each level. We
use shared weights for all iterations in (B) and (C).
• Ours (A)+(B)+(C) (No WS): A version of our method
without shared weights. All settings are the same as
above except that the network for (B) and (C) have inde-
pendent weight parameters at each coarse-to-ﬁne scale.
• Ours (A)+(B)+(C) (K iterations/scale): The same net-
work as the default setting with shared weights, except
that we change the inner iteration number K .
• No Learning: Vanilla coarse-to-ﬁne IC alignment mini-
mizing photometric error (4) without learned modules.

4.3. Results and Discussion

Table 1 and Table 2 summarize our main results. For
each dataset, we evaluate the method separately for three
different motion magnitudes {Small, Medium, Large}. In
Table 1, {Small, Medium, Large} correspond to frames
sampled from the original videos at intervals {1, 2, 4}. In
Table 2, [Small, Medium, Large] correspond to frame in-
tervals {2, 4, 8} on BundleFusion and {1, 2, 5} on Dynam-
icBundleFusion. We show the following metrics/statistics:

64586

Model Descriptions

3D EPE (cm) ↓ on Validation/Test

Θ (Deg) ↓ / t (cm) ↓ / (t < 5 (cm) & Θ < 5◦ ) ↑ on Test

Small

Medium

Large

Small

Medium

Large

I

C

P

Point-Plane ICP [11]
Point-Point ICP [4]
Oracle Point-Plane ICP [11]
Oracle Point-Point ICP [4]

4.88/4.28
5.02/4.38
3.91/3.31
4.34/3.99

10.13/8.74
10.33/9.06
10.68/9.63
11.83/10.29

20.24/17.43
20.43/17.68
22.53/19.98
21.27/26.13

4.32/10.54/66.23%
4.04/10.51/70.0%
2.74/9.75/78.6%
3.81/10.11/75.0%

8.29/20.90/33.4%
15.40/40.45/11.2%
7.89/20.15/33.8%
15.33/40.52/11.1%
8.31/19.72/40.3% 16.64/41.40/16.4%
9.30/26.41/39.1%
19.53/62.30/14.1%

D

P

R

PoseCNN
IC-PoseCNN
Cascaded-PoseCNN

5.18/4.60
5.14/4.56
5.24/4.68

10.43/9.20
10.40/9.13
10.43/9.21

20.08/17.74
19.80/17.31
20.32/17.30

3.91/10.51/69.8%
3.93/10.49/70.1%
3.90/10.50/70.0%

7.89/21.10/34.7%
7.90/21.01/34.8%
7.90/21.11/34.6%

15.34/40.54/11.0%
15.31/40.51/11.1%
15.32/40.60/10.8%

L

a
e

r

n

i

g
n

O

p

i
t

m

No learning
IC-FC-LS-Net, adapted from [13]
DeepLK-6DoF, adapted from [50]
Ours: (A)
Ours: (A)+(B)
Ours: (A)+(B)+(C)
Ours: (A)+(B)+(C) (No WS)
Ours: (A)+(B)+(C) (K = 1)
Ours: (A)+(B)+(C) (K = 5)

11.66/11.26
4.96/4.62
4.41/3.75
4.35/3.66
4.33/3.26
3.58/2.91
3.62/2.89
4.12/3.37
3.60/2.92

21.85/22.95
10.49/9.21
9.05/7.54
8.80/7.23
8.84/7.30
7.30/5.94
7.54/6.08
8.64/7.08
7.49/6.09

37.01/38.88
20.31/17.34
18.46/15.33
18.28/15.06
18.14/15.04
15.48/12.96
16.00/12.98
17.67/14.92
16.06/13.01

4.29/15.90/66.8%
3.94/10.49/70.0%
4.03/10.25/68.1%
4.09/10.19/68.6%
4.02/10.11/69.2%
3.74/9.73/74.5%
3.69/9.73/74.9%
3.85/9.95/71.2%
3.68/9.77/74.4%

8.33/31.80/32.2%
7.90/21.20/34.5%
7.96/20.34/33.6%
8.00/20.28/32.9%
7.96/20.26/33.4%
7.41/19.60/38.2%
7.37/19.74/38.4%
7.80/20.13/35.5%
7.46/19.69/37.8%

15.78/55.70/10.44%
15.33/40.63/11.2%
15.43/39.56/10.6%
15.37/39.49/10.9%
15.35/39.56/10.9%
14.71/38.39/12.9%
14.65/38.69/12.8%
15.21/39.30/11.6%
14.76/38.65/12.4%

Table 1: Quantitative Evaluation on MovingObjects3D. We evaluate the average 3D EPE, angular error in Θ (Euler angles),
translation error t and success ratios t < 5 & Θ < 5◦ for three different motion magnitudes {Small, Medium, Large} which
correspond to frames sampled from the original videos using frame intervals {1, 2, 4}.

Model Descriptions

3D EPE (cm) ↓ Validation/Test
on BundleFusion [14]

3D EPE (cm) ↓ Validation/Test
on DynamicBundleFusion [33]

Model
Size (K)

Inference
Time (ms)

Small

Medium

Large

Small

Medium

Large

I

C

P

Point-Plane ICP [11]
Point-Point ICP [4]

2.81/2.01
3.62/2.48

6.85/4.52
8.17/5.72

16.20/11.11
17.23/12.58

1.26/0.97
1.42/1.08

2.64/2.09
3.40/2.42

10.38/4.89
13.09/7.43

-
-

310
402

D

P

R

PoseCNN
IC-PoseCNN
Cascaded-PoseCNN

4.76/3.41
4.32/3.26
4.46/3.41

9.78/6.85
8.98/6.52
9.38/6.81

17.90/13.31
16.30/12.81
16.50/13.02

2.20/1.65
2.21/1.66
2.20/1.60

4.22/3.19
4.24/3.18
4.13/3.15

13.90/8.24
12.99/8.05
12.97/8.15

19544
19544
58632

5.7
14.1
14.1

L

a
e

r

n

i

g
n

O

p

i
t

m

No learning
IC-FC-LS-Net, adapted from [13]
DeepLK-6DoF, adapted from [50]
Ours: (A)
Ours: (A)+(B)
Ours: (A)+(B)+(C)
Ours: (A)+(B)+(C) (No WS)

4.52/3.35
4.04/3.03
4.09/2.99
3.59/2.65
2.42/1.75
2.27/1.48
2.14/1.52

8.64/6.30
9.06/6.85
8.14/5.84
7.68/5.46
5.39/3.47
5.11/3.09
5.15/3.10

17.06/12.51
17.15/13.32
16.81/12.27
16.56/11.92
12.59/8.40
12.16/7.84
12.26/7.81

4.89/3.39
2.33/1.80
2.15/1.72
2.01/1.65
2.22/1.70
1.09/0.74
0.93/0.61

5.64/4.69
4.43/3.45
3.78/3.12
3.68/2.96
3.61/2.97
2.15/1.54
1.86/1.32

13.88/8.58
13.84/8.35
12.73/7.22
12.68/7.11
12.57/6.88
9.78/4.64
8.88/3.82

-
674
596
597
622
662
883

1.5
1.7
2.2
2.2
2.4
7.6
7.6

Table 2: Quantitative Evaluation on BundleFusion and DynamicBundleFusion. In BundleFusion [14], the motion mag-
nitudes {Small, Medium, Large} correspond to frame intervals {2, 4, 8}. In DynamicBundleFusion [33], the motion magni-
tudes {Small, Medium, Large} correspond to frame intervals {1, 2, 5} (we reduce the intervals due to the increased difﬁculty).

mRPE: θ (Deg) ↓ / t (cm) ↓
KF 2
KF 4

KF 1

KF 8

RGBD VO [44]
Ours: (A)
Ours: (A)+(B)
Ours: (A)+(B)+(C)

0.55/1.03
0.53/1.17
0.51/1.14
0.45/0.69

1.39/2.81
0.97/2.63
0.87/2.44
0.63/1.14

3.99/5.95
2.87/6.89
2.60/6.56
1.10/2.09

9.20/13.83
7.63/12.16
7.30/11.21
3.76/5.88

Table 3: Results on TUM RGB-D Dataset [45]. This table
shows the mean relative pose error (mRPE) on our test split
of the TUM RGB-D Dataset [45]. KF denotes the size of
the key frame intervals. Please refer to the supplementary
materials for a detailed evaluation of individual trajectories.

• 3D End-Point-Error (3D EPE): This metric is deﬁned
in (21). We only evaluate errors on the rigidly moving
objects, i.e., the moving objects in MovingObjects3D and
the rigid background mask in DynamicBundleFusion.
• Object rotation and translation: We evaluate 3D rota-

tion using the norm of Euler angles Θ, translation t in cm
and the success ratio (t < 5 (cm) & Θ < 5◦ ), in Table 1.
• Relative Pose Error: We follow the TUM RGBD metric
[45] using relative axis angle θ and translation t in cm.
• Model Weight: The number of learnable parameters.
• Inference speed: The forward execution time for an
image-pair of size 160 × 120, using GTX 1080 Ti.

Comparison to baseline methods: Compared to all base-
line methods (Table 1 row 1-10 and Table 2 row 1-8), our
full model ((A)+(B)+(C)) achieves the overall best per-
formance across different motion magnitudes and datasets
while maintaining fast inference speed. Compared to ICP
methods (ICP rows in Table 1 and Table 2) and classical
method (No Learning in Table 1 and Table 2), our method
achieves better performance without instance information
at runtime. Compared to RGB-D visual odometry [44],
our method works particularly well in unseen scenarios and

74587

T

I

I(ξGT )

I(ξ⋆ )

Figure 2: Qualitative Results on MovingObjects3D. Visualization of the warped image I(ξ) using the ground truth object
motion ξGT (third row) and the object motion ξ⋆ estimated using our method (last row) on the MovingObjects3D validation
(left two) and test sets (right three). In I, we plot the instance boundary in red and the instance boundary of T in green
as comparison. Note the difﬁculty of the task (truncation, independent background object) and the high quality of our
alignments. Black regions in the warped image are due to truncation or occlusion.

in the presence of large motions (Table 3). Besides, note
that our model can achieve better performance with a sig-
niﬁcantly smaller number of weight parameters compared
to direct image-to-pose regression (DRP rows in Table 1
and Table 2). Fig. 2 shows a qualitative comparison of
our method on the MovingObject3D dataset by visualizing
I(ξ). Note that our method achieves excellent two-view im-
age alignments, despite large motion, heavy occlusion and
varying illumination of the scene.

Ablation Discussion: Across all ablation variants and
datasets, our model achieves the best performance by com-
bining all three proposed modules ((A)+(B)+(C)). This
demonstrates that all components are relevant for robust
learning-based optimization. Note that the inﬂuence of the
proposed modules may vary according to the properties of
the speciﬁc task and dataset. For example, in the presence
of noisy depth estimates from real data, learning a robust M-
estimator (B) (Table 2 row 10 in BundleFusion) provide sig-
niﬁcant improvements. In the presence of heavy occlusions
and motion ambiguities, learning the trust region step (C)
helps to ﬁnd better local optima which results in large im-
provements in our experiments, observed when estimating
object motion (Table 1 row 13) and when estimating cam-

era motion in dynamic scenes (Table 2 row 11 in Dynam-
icBundleFusion). We ﬁnd our trust region network (C) to be
highly effective for learning accurate relative motion in the
presence of large variations in motion magnitude(Table 3).

5. Conclusion

We have taken a deeper look at the inverse compositional
algorithm by rephrasing it as a neural network with three
trainable submodules which allows for relaxing some of the
most signiﬁcant restrictions of the original formulation. Ex-
periments on the challenging task of relative rigid motion
estimation from two RGB-D frames demonstrate that our
method achieves more accurate results compared to both
classical IC algorithms as well as data hungry direct image-
to-pose regression techniques. Although our data-driven IC
method can better handle challenges in large object mo-
tions, heavy occlusions and varying illuminations, solving
those challenges in the wild remains a subject for future re-
search. To extend the current method to real-world envi-
ronments with complex entangled motions, possible future
directions include exploring multiple motion hypotheses,
multi-view constraints and various motion models which
will capture a broader family of computer vision tasks.

84588

References

[1] P. Anandan. A computational framework and an algorithm
for the measurement of visual motion. International Journal
of Computer Vision (IJCV), 2(3):283–310, 1989. 2

[2] S. Baker, R. Gross, I. Matthews, and T. Ishikawa. Lucas-
kanade 20 years on: A unifying framework: Part 2. Technical
report, Carnegie Mellon University, 2003. 1, 2, 3

[3] S. Baker and I. Matthews. Lucas-kanade 20 years on: A uni-
fying framework: Part 1. Technical report, Carnegie Mellon
University, Pittsburgh, PA, July 2002. 1, 2, 3

[4] P. J. Besl and N. D. McKay. A method for registration
of 3-d shapes.
IEEE Trans. Pattern Anal. Machine Intell.,
14(2):239–256, Feb. 1992. 6, 7

[5] A. Bibi, M. Mueller, and B. Ghanem. Target response adap-
tation for correlation ﬁlter tracking. In Proc. of the European
Conf. on Computer Vision (ECCV), 2016. 1

[6] A. Bj ¨orck. Numerical Methods for Least Squares Problems.
Siam Philadelphia, 1996. 2

[7] M. J. Black and P. Anandan. The robust estimation of
multiple motions: Parametric and piecewise-smooth ﬂow
ﬁelds. Computer Vision and Image Understanding (CVIU),
63(1):75–104, 1996. 2

[8] Brown, Matthew, Lowe, and David. Automatic panoramic
image stitching using invariant features. International Jour-
nal of Computer Vision (IJCV), 74(1):59–73, August 2007.
1

[9] A. Byravan and D. Fox. SE3-Nets: Learning rigid body mo-
tion using deep neural networks. In Proc. IEEE International
Conf. on Robotics and Automation (ICRA), pages 173–180.
IEEE, 2017. 3

[10] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan,
Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su,
J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich
3D Model Repository. Technical Report arXiv:1512.03012
[cs.GR], Stanford University — Princeton University —
Toyota Technological Institute at Chicago, 2015. 5

[11] Y. Chen and G. Medioni. Object modelling by registration
of multiple range images.
Image and Vision Computing,
10(3):145–155, 1992. 6, 7

[12] I. Cherabier, J. Sch ¨onberger, M. Oswald, M. Pollefeys, and
A. Geiger. Learning priors for semantic 3d reconstruction.
In Proc. of the European Conf. on Computer Vision (ECCV),
2018. 3

[13] R. Clark, M. Bloesch, J. Czarnowski, S. Leutenegger, and
A. J. Davison. Learning to solve nonlinear least squares for
monocular stereo.
In European Conf. on Computer Vision
(ECCV), Sept. 2018. 1, 3, 6, 7

[14] A. Dai, M. Nießner, M. Zoll ¨ofer, S. Izadi, and C. Theobalt.
BundleFusion: real-time globally consistent 3D reconstruc-
tion using on-the-ﬂy surface re-integration. ACM Transac-
tions on Graphics 2017 (TOG), 2017. 6, 7

[15] A. Dosovitskiy, P. Fischer, E. Ilg, P. Haeusser, C. Hazirbas,
V. Golkov, P. v.d. Smagt, D. Cremers, and T. Brox. Flownet:
Learning optical ﬂow with convolutional networks. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV),
2015. 6

[16] J. Engel, V. Koltun, and D. Cremers. Direct sparse odome-
try. IEEE Trans. Pattern Anal. Machine Intell., 40:611–625,
2018. 1, 2
[17] J. Engel, T. Sch ¨ops, and D. Cremers. LSD-SLAM: Large-
scale direct monocular SLAM. In European Conf. on Com-
puter Vision (ECCV), September 2014. 1
[18] B. K. Horn and E. Weldon. Direct methods for recovering
motion. Intl. J. of Computer Vision, 2(1):51–76, 1988. 2
[19] P. J. Huber. Robust estimation of a location parameter. The
Annals of Mathematical Statistics, 35(1):73–101, 03 1964. 2
[20] E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and
T. Brox. FlowNet 2.0: evolution of optical ﬂow estimation
with deep networks. In IEEE Conf. on Computer Vision and
Pattern Recognition (CVPR), 2017. 3
[21] M. Irani and P. Anandan. About direct methods. In Proc.
of the IEEE International Conf. on Computer Vision (ICCV)
Workshops, 2000. 2
[22] M. Irani and S. Peleg. Improving resolution by image reg-
istration. Graphical Model and Image Processing (CVGIP),
53(3):231–239, 1991. 1
[23] F. Jurie and M. Dhome. Hyperplane approximation for tem-
plate matching. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 24(7):996–1000, 2002. 3
[24] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik. End-
to-end recovery of human shape and pose.
In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
2018. 3
[25] A. Kendall, M. Grimes, and R. Cipolla. Posenet: A convolu-
tional network for real-time 6-dof camera relocalization. In
Proc. of the IEEE International Conf. on Computer Vision
(ICCV), 2015. 3
[26] A. Kendall, H. Martirosyan, S. Dasgupta, and P. Henry. End-
to-end learning of geometry and context for deep stereo re-
gression. In Proc. of the IEEE International Conf. on Com-
puter Vision (ICCV), 2017. 3
[27] D. P. Kingma and J. Ba. Adam: A method for stochastic
optimization. In Proc. of the International Conf. on Learning
Representations (ICLR), 2015. 5
[28] E. Kobler, T. Klatzer, K. Hammernik, and T. Pock. Varia-
tional networks: Connecting variational methods and deep
learning.
In Proc. of the German Conference on Pattern
Recognition (GCPR), 2017. 3
[29] A. Levin, A. Zomet, S. Peleg, and Y. Weiss. Seamless image
stitching in the gradient domain. In Proc. of the European
Conf. on Computer Vision (ECCV), 2004. 1, 2
[30] Y. Li, G. Wang, X. Ji, Y. Xiang, and D. Fox. Deepim: Deep
iterative matching for 6d pose estimation.
In Proc. of the
European Conf. on Computer Vision (ECCV), 2018. 3, 5, 6
[31] C.-H. Lin and S. Lucey. Inverse compositional spatial trans-
former networks. In Proc. IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), 2017. 3, 6
[32] B. D. Lucas and T. Kanade. An iterative image registration
technique with an application to stereo vision. In Proc. of the
International Joint Conf. on Artiﬁcial Intelligence (IJCAI),
1981. 1, 2
[33] Z. Lv, K. Kim, A. Troccoli, D. Sun, J. Rehg, and J. Kautz.
Learning rigidity in dynamic scenes with a moving camera

94589

network for learning monocular stereo. In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2017.
3
[50] C. Wang, H. K. Galoogahi, C.-H. Lin, and S. Lucey. Deep-lk
for efﬁcient adaptive object tracking. In IEEE Intl. Conf. on
Robotics and Automation (ICRA), 2018. 1, 6, 7
[51] C. Wang, J. Miguel Buenaposada, R. Zhu, and S. Lucey.
Learning depth from monocular videos using direct meth-
ods. In Proc. IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), June 2018. 3
[52] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
volutional pose machines. In IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR), pages 4724–4732, 2016. 3
[53] Z. Yin and J. Shi. GeoNet: Unsupervised Learning of Dense
Depth, Optical Flow and Camera Pose. In Proc. IEEE Conf.
on Computer Vision and Pattern Recognition (CVPR), 2018.
3
[54] Z. Zhang, P. Robotique, and P. Robotvis. Parameter estima-
tion techniques: a tutorial with application to conic ﬁtting.
Image and Vision Computing (IVC), 15:59–76, 1997. 2
[55] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet,
Z. Su, D. Du, C. Huang, and P. H. S. Torr. Conditional ran-
dom ﬁelds as recurrent neural networks. In Proc. of the IEEE
International Conf. on Computer Vision (ICCV), 2015. 3
[56] H. Zhou, B. Ummenhofer, and T. Brox. Deeptam: Deep
tracking and mapping.
In Proc. of the European Conf. on
Computer Vision (ECCV), 2018. 3
[57] Q.-Y. Zhou, J. Park, and V. Koltun. Open3D: A modern li-
brary for 3D data processing. arXiv:1801.09847, 2018. 6
[58] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsu-
pervised learning of depth and ego-motion from video.
In
Proc. IEEE Conf. on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 3

for 3d motion ﬁeld estimation. In European Conf. on Com-
puter Vision (ECCV), 2018. 6, 7
[34] F. Manhardt, W. Kehl, N. Navab, and F. Tombari. Deep
model-based 6d pose reﬁnement in rgb. In Proc. of the Eu-
ropean Conf. on Computer Vision (ECCV), September 2018.
3
[35] D. W. Marquardt. An algorithm for least-squares estimation
of nonlinear parameters. Journal of the Society for Industrial
and Applied Mathematics (SIAM), 11(2):431–441, 1963. 2
[36] T. Meinhardt, M. M ¨oller, C. Hazirbas, and D. Cremers.
Learning proximal operators: Using denoising networks for
regularizing inverse imaging problems. In Proc. of the IEEE
International Conf. on Computer Vision (ICCV), 2017. 3
[37] R. A. Newcombe, S. Lovegrove, and A. J. Davison. DTAM:
dense tracking and mapping in real-time.
In Proc. of the
IEEE International Conf. on Computer Vision (ICCV), 2011.
2
[38] R. Ranftl and V. Koltun. Deep fundamental matrix estima-
tion.
In Proc. of the European Conf. on Computer Vision
(ECCV), 2018. 3
[39] R. Ranftl and T. Pock. A deep variational model for image
segmentation. In Proc. of the German Conference on Pattern
Recognition (GCPR), 2014. 3
[40] G. Riegler, M. R ¨uther, and H. Bischof. Atgv-net: Accurate
depth super-resolution.
In Proc. of the European Conf. on
Computer Vision (ECCV), 2016. 3
[41] D. Scharstein and R. Szeliski. A taxonomy and evaluation
of dense two-frame stereo correspondence algorithms. Inter-
national Journal of Computer Vision (IJCV), 47:7–42, 2002.
1
[42] J. Shi and C. Tomasi. Good features to track. In Proc. IEEE
Conf. on Computer Vision and Pattern Recognition (CVPR),
1994. 1
[43] H.-Y. Shum and R. Szeliski. Systems and experiment paper:
Construction of panoramic image mosaics with global and
local alignment. International Journal of Computer Vision
(IJCV), 36(2):101–130, February 2000. 1
[44] F. Steinbr ¨ucker, J. Sturm, and D. Cremers. Real-time vi-
sual odometry from dense rgb-d images.
In Computer Vi-
sion Workshops (ICCV Workshops), 2011 IEEE Interna-
tional Conference on, pages 719–722. IEEE, 2011. 6, 7
[45] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre-
mers. A benchmark for the evaluation of rgb-d slam systems.
In IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems
(IROS), Oct. 2012. 6, 7
[46] M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and
R. Triebel. Implicit 3d orientation learning for 6d object de-
tection from rgb images. In Proc. of the European Conf. on
Computer Vision (ECCV), September 2018. 3
[47] R. Szeliski.
Image alignment and stitching: A tutorial.
Foundations and Trends in Computer Graphics and Vision,
2(1):1–104, 2007. 2
[48] C. Tang and P. Tan. Ba-net: Dense bundle adjustment net-
work.
In Intl. Conf. on Learning Representations (ICLR),
2019. 3
[49] B. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg,
A. Dosovitskiy, and T. Brox. Demon: Depth and motion

104590

Text2Scene: Generating Compositional Scenes from Textual Descriptions

Fuwen Tan1
Song Feng2 Vicente Ordonez1
1University of Virginia, 2 IBM Thomas J. Watson Research Center.

fuwen.tan@virginia.edu, sfeng@us.ibm.com, vicente@virginia.edu

Abstract

In this paper, we propose Text2Scene, a model that gen-
erates various forms of compositional scene representations
from natural language descriptions. Unlike recent works,
our method does NOT use Generative Adversarial Networks
(GANs). Text2Scene instead learns to sequentially generate
objects and their attributes (location, size, appearance, etc)
at every time step by attending to different parts of the in-
put text and the current status of the generated scene. We
show that under minor modiﬁcations, the proposed frame-
work can handle the generation of different forms of scene
representations, including cartoon-like scenes, object lay-
outs corresponding to real images, and synthetic images.
Our method is not only competitive when compared with
state-of-the-art GAN-based methods using automatic met-
rics and superior based on human judgments but also has
the advantage of producing interpretable results.

1. Introduction

Generating images from textual descriptions has recently
become an active research topic [26, 37, 38, 13, 33, 11].
This interest has been partially fueled by the adoption of
Generative Adversarial Networks (GANs) [8] which have
demonstrated impressive results on a number of image syn-
thesis tasks. Synthesizing images from text requires a level
of language and visual understanding which could lead to
applications in image retrieval through natural language
queries, representation learning for text, and automated
computer graphics and image editing applications.
In this work, we introduce Text2Scene, a model to in-
terpret visually descriptive language in order to generate
compositional scene representations. We speciﬁcally fo-
cus on generating a scene representation consisting of a
list of objects, along with their attributes (e.g.
location,
size, aspect ratio, pose, appearance). We adapt and train
models to generate three types of scenes as shown in Fig-
ure 1, (1) Cartoon-like scenes as depicted in the Abstract
Scenes dataset [41], (2) Object layouts corresponding to im-
age scenes from the COCO dataset [19], and (3) Synthetic

Figure 1. Sample inputs (left) and outputs of our Text2Scene
model (middle), along with ground truth reference scenes (right)
for the generation of abstract scenes (top), object layouts (middle),
and synthetic image composites (bottom).

scenes corresponding to images in the COCO dataset [19].
We propose a uniﬁed framework to handle these three seem-
ingly different tasks with unique challenges. Our method,
unlike recent approaches, does not rely on Generative Ad-
versarial Networks (GANs) [8]. Instead, we produce an in-
terpretable model that iteratively generates a scene by pre-
dicting and adding new objects at each time step. Our
method is superior to the best result reported in Abstract
Scenes [41], and provides near state-of-the-art performance
on COCO [19] under automatic evaluation metrics, and
state-of-the-art results when evaluated by humans.
Generating rich textual representations for scene gen-
eration is a challenging task. For instance, input textual
descriptions could hint only indirectly at the presence of
attributes – e.g. in the ﬁrst example in Fig. 1 the input
text “Mike is surprised” should change the facial attribute
on the generated object “Mike”. Textual descriptions often
have complex information about relative spatial conﬁgura-

6710

Figure 2. Overview of Text2Scene. Our general framework consists of (A) a Text Encoder that produces a sequential representation of
the input, (B) an Image Encoder that encodes the current state of the generated scene, (C) a Convolutional Recurrent Module that tracks,
for each spatial location, the history of what have been generated so far, (D-F) two attention-based predictors that sequentially focus on
different parts of the input text, ﬁrst to decide what object to place, then to decide what attributes to be assigned to the object, and (G) an
optional foreground embedding step that learns an appearance vector for patch retrieval in the synthetic image generation task.

tions – e.g. in the ﬁrst example in Fig. 1 the input text “Jenny
is running towards Mike and the duck” makes the orienta-
tion of “Jenny” dependent on the positions of both “Mike”,
and “duck”. In the last example in Fig. 1 the text “elephants
walking together in a line” also implies certain overall spa-
tial conﬁguration of the objects in the scene. We model this
text-to-scene task using a sequence-to-sequence approach
where objects are placed sequentially on an initially empty
canvas (see an overview in Fig 2). Generally, Text2Scene,
consists of a text encoder (Fig 2 (A)) that maps the input
sentence to a set of latent representations, an image encoder
(Fig 2 (B)) which encodes the current generated canvas, a
convolutional recurrent module (Fig 2 (C)), which passes
the current state to the next step, attention modules (Fig 2
(D)) which focus on different parts of the input text, an ob-
ject decoder (Fig 2 (E)) that predicts the next object condi-
tioned on the current scene state and attended input text, and
an attribute decoder (Fig 2 (F)) that assigns attributes to the
predicted object. To the best of our knowledge, Text2Scene
is the ﬁrst model demonstrating its capacities on both ab-
stract and real images, thus opening the possibility for fu-
ture work on transfer learning across domains.

Our main contributions can be summarized as follows:

• We propose Text2Scene, a framework to generate
compositional scene representations from natural lan-
guage descriptions.

• We show that Text2Scene can be used to generate, un-
der minor modiﬁcations, different forms of scene rep-
resentations, including cartoon-like scenes, semantic
layouts corresponding to real images, and synthetic
image composites.

• We conduct extensive experiments on the tasks of
abstract
image generation for the Abstract Scenes
dataset [41], semantic layout and synthetic image gen-
erations for the COCO dataset [19].

2. Related Work

Most research on visually descriptive language has fo-
cused on generating captions from images [5, 21, 16, 14, 31,
32, 22, 2]. Recently, there is work in the opposite direction
of text-to-image synthesis [25, 26, 37, 13, 38, 33, 11]. Most
of the recent approaches have leveraged conditional Gen-
erative Adversarial Networks (GANs). While these works
have managed to generate results of increasing quality, there
are major challenges when attempting to synthesize images
for complex scenes with multiple interacting objects with-
out explicitly deﬁning such interactions [34]. Inspired by
the principle of compositionality [39], our model does not
use GANs but produces a scene by sequentially generat-
ing objects (e.g. in the forms of clip-arts, bounding boxes,
or segmented object patches) containing the semantic ele-
ments that compose the scene.
Our work is also related to prior research on using ab-
stract scenes to mirror and analyze complex situations in
the real world [40, 41, 7, 30]. In [41], a graphical model
was introduced to generate an abstract scene from textual
descriptions. Unlike this previous work, our method does
not use a semantic parser but is trained end-to-end from
input sentences. Our work is also related to recent re-
search on generating images from pixel-wise semantic la-
bels [12, 4, 24], especially [24] which proposed a retrieval-
based semi-parametric method for image synthesis given
the spatial semantic map. Our synthetic image genera-
tion model optionally uses the cascaded reﬁnement module
in [24] as a post-processing step. Unlike these works, our
method is not given the spatial layout of the objects in the
scene but learns to predict a layout indirectly from text.
Most closely related to our approach are [13, 9, 11],
and [15], as these works also attempt to predict explicit
2D layout representations. Johnson et al [13] proposed a
graph-convolutional model to generate images from struc-

6711

M ike is holding a hotdog.         Jenny is sitting in the sandbox.        Jenny is holding the shovel.

object attn: 
si tting sandbox ho ld ing
attribute attn: 
jenny <eos> jenny

object attn: 
sandbox si tting mike
attribute attn: 
sandbox <eos> jenny

object attn: 
mike jenny si tting
attribute attn: 
ho ld ing hotdog mike

object attn: 
jenny jenny mike
attribute attn: 
si tting jenny ho ld ing

object attn: 
hotdog shove l ho ld ing
attribute attn: 
mike hotdog ho ld ing

object attn: 
shove l ho ld ing sandbox
attribute attn: 
shove l ho ld ing <eos>

Figure 3. Step-by-step generation of an abstract scene, showing the top-3 attended words for the object prediction and attribute prediction
at each time step. Notice how except for predicting the sun at the ﬁrst time step, the top-1 attended words in the object decoder are
almost one-to-one mappings with the predicted objects. The attended words by the attribute decoder also correspond semantically to useful
information for predicting either pose or location, e.g. to predict the location of the hotdog at the ﬁfth time step, the model attends to mike
and holding.

tured scene graphs. The presented objects and their re-
lationships were provided as inputs in the scene graphs,
while in our work, the presence of objects is inferred from
text. Hong et al [11] targeted image synthesis using con-
ditional GANs but unlike prior works, it generated lay-
outs as intermediate representations in a separably trained
module. Our work also attempts to predict layouts for
photographic image synthesis but unlike [11], we generate
pixel-level outputs using a semi-parametric retrieval mod-
ule without adversarial training and demonstrate superior
results. Kim et al [15] performed pictorial generation from
chat logs, while our work uses text which is considerably
more underspeciﬁed. Gupta et al [9] proposed a semi-
parametric method to generate cartoon-like pictures. How-
ever the presented objects were also provided as inputs to
the model, and the predictions of layouts, foregrounds and
backgrounds were performed by separably trained modules.
Our method is trained end-to-end and goes beyond cartoon-
like scenes. To the best of our knowledge, our model is
the ﬁrst work targeting various types of scenes (e.g. ab-
stract scenes, semantic layouts and composite images) un-
der a uniﬁed framework.

3. Model

Text2Scene adopts a Seq-to-Seq framework [28] and in-
troduces key designs for spatial and sequential reasoning.
Speciﬁcally, at each time step, the model modiﬁes a back-
ground canvas in three steps: (1) the model attends to the
input text to decide what is the next object to add, or decide
whether the generation should end; (2) if the decision is to
add a new object, the model zooms in the language context
of the object to decide its attributes (e.g. pose, size) and re-
lations with its surroundings (e.g. location, interactions with
other objects); (3) the model refers back to the canvas and
grounds (places) the extracted textual attributes into their
corresponding visual representations.

To model this procedure, Text2Scene consists of a text
encoder, which takes as input a sequence of M words wi
(section 3.1), an object decoder, which predicts sequen-
tially T objects ot , and an attribute decoder that predicts
for each ot their locations lt and a set of k attributes {Rk
(section 3.2). The scene generation starts from an initially
empty canvas B0 that is updated at each time step. In the
synthetic image generation task, we also jointly train a fore-
ground patch embedding network (section 3.3) and treat the
embedded vector as a target attribute. Figure 3 shows a step-
by-step generation of an abstract scene.

t }

3.1. Text Encoder

Our text encoder is a bidirectional recurrent network
with Gated Recurrent Units (GRUs). For a given sentence,
we compute for each word wi :

hE
i = BiGRU(xi , hE

i−1 , hE
i+1 ),

(1)

Here BiGRU is a bidirectional GRU cell, xi is a word em-
bedding vector corresponding to the i-th word wi , and hE
is
a hidden vector encoding the current word and its context.
We use the pairs [hE
i ; xi ], the concatenation of hE
i and xi ,
as the encoded text feature.

i

3.2. Object and Attribute Decoders

At each step t, our model predicts the next object ot from
an object vocabulary V and its k attributes {Rk
t }, using text
feature {[hE
i ; xi ]} and the current canvas Bt as input. For
this part, we use a convolutional network (CNN) Ω to en-
code Bt into a C × H × W feature map, representing the
current scene state. We model the history of the scene states
{hD
t } with a convolutional GRU (ConvGRU):

hD
t = ConvGRU(Ω(Bt ), hD

t−1 ),

(2)

The initial hidden state is created by spatially replicating
the last hidden state of the text encoder. Here hD
t provides

6712

an informative representation of the temporal dynamics of
each spatial (grid) location in the scene. Since this repre-
sentation might fail to capture small objects, a one-hot vec-
tor of the object predicted at the previous step ot−1 is also
provided as input to the downstream decoders. The initial
object is set as a special start-of-scene token.
Attention-based Object Decoder: Our object decoder is
an attention-based model that outputs the likelihood scores
of all possible objects in an object vocabulary V . It takes as
input the recurrent scene state hD
t , text features {[hE
and the previously predicted object ot−1 :

i ; xi ]}

uo
t = AvgPooling(Ψo (hD
co
t ; ot−1 ], {[hE

t = Φo ([uo
p(ot ) ∝ Θo ([uo

t )),
i ; xi ]}),
t ]),

t ; ot−1 ; co

(3)

(4)

(5)

i ; xi ]}

here Ψo is a convolutional network with spatial attention on
hD
t , similar as in [32]. The goal of Ψo is to collect the spa-
tial contexts necessary for the object prediction, e.g. what
objects have already been added. The attended spatial fea-
tures are then fused into a vector uo
t by average pooling. Φo
is the text-based attention module, similar as in [20], which
uses [uo
t ; ot−1 ] to attend to the language context {[hE
and collect the context vector co
t . Ideally, co
t encodes infor-
mation about all the described objects that have not been
added to the scene thus far. Θo is a two-layer perceptron
predicting the likelihood of the next object p(ot ) from the
concatenation of uo
t , ot−1 , and co
t , using a softmax function.
Attention-based Attribute Decoder The attribute set cor-
responding to the object ot can be predicted similarly. We
use another attention module Φa to “zoom in” the language
context of ot , extracting a new context vector ca
t . Compared
with co
t which may contain information of all the objects
that have not been added yet, ca
t focuses more speciﬁcally
on contents related to the current object ot . For each spatial
location in hD
t , the model predicts a location likelihood lt ,
and a set of attribute likelihoods {Rk
t }. Here, possible loca-
tions are discretized into the same spatial resolution of hD
t .
In summary, we have:

ca
t = Φa (ot , {[hE
ua
t ; ca
p(lt , {Rk
t ; ot ; ca

i ; xi ]})
t = Ψa ([hD
t ])
t }) = Θa ([ua

t ]),

(6)

(7)

(8)

Φa is a text-based attention module aligning ot with the lan-
guage context {[hE
i ; xi ]}. Ψa is an image-based attention
module aiming to ﬁnd an affordable location to add ot . Here
ca
t is spatially replicated before concatenating with hD
t . The
ﬁnal likelihood map p(lt , {Rk
t }) is predicted by a convolu-
tional network Θa , followed by softmax classiﬁers for lt
and discrete {Rk
t }. For continuous attributes {Rk
t } such as
the appearance vector Qt for patch retrieval (next section),
we normalize the output using an ℓ2 -norm.

3.3. Foreground Patch Embedding

We predict a particular attribute: an appearance vector
Qt , only for the model trained to generate synthetic image
composites (i.e. images composed of patches retrieved from
other images). As with other attributes, Qt is predicted for
every location in the output feature map which is used at
test time to retrieve similar patches from a pre-computed
collection of object segments from other images. We train a
patch embedding network using a CNN which reduces the
foreground patch in the target image into a 1D vector Ft .
The goal is to minimize the ℓ2 -distance between Qt and Ft
using a triplet embedding loss [6] that encourages a small
distance ||Qt , Ft ||2 but a larger distance with other patches
||Qt , Fk ||2 . Here Fk is the feature of a ”negative” patch,
which is randomly selected from the same category of Ft :

Ltriplet (Qt , Ft ) = max{||Qt , Ft ||2 − ||Qt , Fk ||2 + α, 0}
(9)

α is a margin hyper-parameter.

3.4. Objective

The loss function for a given example in our model with
reference values (ot , lt , {Rk
t }, Ft ) is:
log p(ot ) − wl X

L = − wo X

log p(lt )

t

t

− X

wk X

log p(Rk

t ) + we X

Ltriplet (Qt , Ft )

k
t
a LO
attn + wA
a LA
attn ,

+ wO

t

attn

attn = Pi [1 −

where the ﬁrst three terms are negative log-likelihood losses
corresponding to the object, location, and discrete attribute
softmax classiﬁers. Ltriplet is a triplet embedding loss op-
tionally used for the synthetic image generation task. L∗
are regularization terms inspired by the doubly stochastic
attention module proposed in [32]. Here L∗
Pt α∗
ti ]2 where {αo
ti } and {αa
ti } are the attention weights
from Φo and Φa respectively. These regularization terms
encourage the model to distribute the attention across all
the words in the input sentence so that it will not miss any
described objects. Finally, wo , wl , {wk }, we , wO
a , and wA
are hyperparameters controlling the relative contribution of
each loss.
Details for different scene generation tasks In the Ab-
stract Scenes generation task, Bt is represented directly as
an RGB image. In the layout generation task, Bt is a 3D
tensor with a shape of (V , H, W ), where each spatial loca-
tion contains a one-hot vector indicating the semantic la-
bel of the object at that location. Similarly, in the synthetic
image generation task, Bt is a 3D tensor with a shape of
(3V , H, W ), where every three channels of this tensor en-
code the color patches of a speciﬁc category from the back-
ground canvas image. For the foreground embedding mod-
ule, we adopt the canvas representation in [24] to encode

a

6713

Methods

Zitnick et al. [41]
Text2Scene (w/o attention)
Text2Scene (w object attention)
Text2Scene (w both attentions)
Text2Scene (full)

U-obj
Recall
0.655
0.605
0.671
0.685
0.698

Prec
0.722
0.665
0.731
0.749
0.760

B-obj
Recall
0.265
0.186
0.261
0.272
0.301

Prec
0.280
0.228
0.312
0.327
0.348

Pose

Expr

0.407
0.305
0.365
0.408
0.418

0.370
0.323
0.368
0.374
0.375

U-obj
B-obj
Coord Coord
0.449
0.416
0.395
0.338
0.406
0.427
0.402
0.467
0.409
0.483

Table 1. Quantitative evaluation on the Abstract Scenes dataset. Our full model performs better in all metrics except U-obj Coord which
measures exact object coordinates. It also shows that our sequential attention approach is effective.

Methods

Scores ≥ 1 ≥ 2

Obj-Single

Obj-Pair

Location Expression

sub-pred

sub-pred-obj

pred:loc

pred:expr

Reference
Zitnick et al. [41]
Text2Scene (w/o attention)
Text2Scene (full)

0.919
0.555
0.455
0.644

1.0
0.92
0.75
0.94

0.97
0.49
0.42
0.62

0.905
0.53
0.431
0.628

0.88
0.44
0.36
0.48

0.933
0.667
0.6
0.667

0.875
0.625
0.583
0.708

Table 2. Human evaluation on Abstract Scenes. Each scene is generated from three textual statements. Users are asked to rate if the
generated scene validates each input statement. Our method generates scenes that abide by at least one of the statements 94% of the times,
and by at least two statements 64%, and is superior in all types of statements except Location.

the foreground patch for simplicity. As the composite im-
ages may exhibit gaps between patches, we also leverage
the stitching network in [24] for post-processing. Note that
the missing region may also be ﬁlled by any other inpaint-
ing approaches. Full details about the implementation of
our model can be found in the supplementary material. Our
code and data are publicly available1 .

Task (I): Clip-art Generation on Abstract Scenes We use
the dataset introduced by [41], which contains over 1,000
sets of 10 semantically similar scenes of children playing
outside. The scenes are composed with 58 clip-art objects.
The attributes we consider for each clip-art object are the
location, size (|Rsize | = 3), and the direction the object
is facing (|Rdirection | = 2). For the person objects, we
also explicitly model the pose (|Rpose | = 7) and expression
(|Rexpression | = 5). There are three sentences describing
different aspects of a scene. After ﬁltering empty scenes,
we obtain 9997 samples. Following [41], we reserve 1000
samples as the test set and 497 samples for validation.

Figure 4. Evaluation metrics for the abstract scene generation task
(best viewed in color): the green dots show the common U-obj
between the reference (B) and the generated abstract scene (A),
the blue dots show the missing and mispredicted objects. Sim-
ilarly, the yellow lines show the common B-obj and the red
lines show the missing and mispredicted B-obj. The U-obj
precision/recall for this example is 0.667/0.667, the B-obj pre-
cision/recall is 1.0/0.5.

4. Experiments

We conduct experiments on three text-to-scene tasks:
(I) constructing abstract scenes of clip-arts in the Abstract
Scenes [41] dataset; (II) predicting semantic object layouts
of real images in the COCO [19] dataset; and (III) generat-
ing synthetic image composites in the COCO [19] dataset.

1 https://github.com/uvavision/Text2Scene

Task (II): Semantic Layout Generation on COCO The
semantic layouts contain bounding boxes of the objects
from 80 object categories deﬁned in the COCO [19] dataset.
We use the val2017 split as our test set and use 5000 sam-
ples from the train2017 split for validation. We normal-
ize the bounding boxes and order the objects from bot-
tom to top as the y-coordinates typically indicate the dis-
tances between the objects and the camera. We further or-
der the objects with the same y-coordinate based on their
x-coordinates (from left to right) and categorical indices.
The attributes we consider are location, size (|Rsize | = 17),
and aspect ratio (|Raspect ratio | = 17). For the size at-
tribute, we discretize the normalized size range evenly into
17 scales. We also use 17 aspect ratio scales, which are
1 }.
Task (III): Synthetic Image Generation on COCO We
demonstrate our approach by generating synthetic image
composites given input captions in COCO [19].
For
fair comparisons with alternative approaches, we use the
val2014 split as our test set and use 5000 samples from the

9 , 1
8 , 1
7 , 1
6 , 1
5 , 1
4 , 1
3 , 1
2 , 1
1 , 2
1 , 3
1 , 4
1 , 5
1 , 6
1 , 7
1 , 8
1 , 9

{ 1

6714

Methods
Captioning from True Layout [36]
Text2Scene (w/o attention)
Text2Scene (w object attention)
Text2Scene (w both attentions)
Text2Scene (full)

B1
0.678
0.591
0.591
0.600
0.615

B2
0.492
0.391
0.391
0.401
0.415

B3
0.348
0.254
0.256
0.263
0.275

B4
0.248
0.169
0.171
0.175
0.185

METEOR ROUGE CIDEr
0.227
0.495
0.838
0.179
0.430
0.531
0.179
0.430
0.524
0.182
0.436
0.555
0.189
0.446
0.601

SPICE
0.160
0.110
0.110
0.114
0.123

Table 3. Quantitative evaluation on the layout generation task. Our full model generates more accurate captions from the generated layouts
than the baselines. We also include caption generation results from ground truth layouts as an upper bound on this task.

Methods
Real image
GAN-INT-CLS [26]
SG2IM* [13]
StackGAN [37]
HDGAN [38]
Hong et al [11]
AttnGan [33]
Text2Scene (w/o inpaint.)
Text2Scene (w inpaint.)

IS
36.00±0.7
7.88±0.07
6.7±0.1
10.62±0.19
11.86±0.18
11.46±0.09
25.89±0.47
22.33±1.58
24.77±1.59

B1
0.730
0.470
0.504
0.486
0.489
0.541
0.640
0.602
0.614

B2
0.563
0.253
0.294
0.278
0.284
0.332
0.455
0.412
0.426

B3
0.428
0.136
0.178
0.166
0.173
0.199
0.324
0.288
0.300

B4
0.327
0.077
0.116
0.106
0.112
0.122
0.235
0.207
0.218

METEOR ROUGE CIDEr
0.262
0.545
1.012
0.122
–
0.160
0.141
0.373
0.289
0.130
0.360
0.216
0.132
0.363
0.225
0.154
–
0.367
0.213
0.474
0.693
0.196
0.448
0.624
0.201
0.457
0.656

SPICE
0.188
–
0.070
0.057
0.060
–
0.141
0.126
0.130

Table 4. Quantitative evaluation on the synthetic image generation task. Our model is superior on automated metrics than all competing
approaches except AttnGan, even without post-processing. *The result of SG2IM is evaluated on the validation set deﬁned in [13], which
is a subset of the COCO val2014 split.

Text2Scene > SG2IM [13]
Text2Scene > HDGAN [38]
Text2Scene > AttnGAN [33]

Ratio
0.7672
0.8692
0.7588

Table 5. Two-alternative forced-choiced evaluation on the syn-
thetic image generation task against competing methods.

train2014 split for validation. We collect segmented object
and stuff patches from the training split. The stuff segments
are extracted from the training images by taking connected
components in corresponding semantic label maps from the
COCO-Stuff annotations [10]. For object segments, we use
all 80 categories deﬁned in COCO. For stuff segments, we
use the 15 super-categories deﬁned in [10] as the class la-
bels, which results in 95 categories in total. We order the
patches as in the layout generation task but when compos-
ing the patches, we always render the object patches in front
of the stuff patches. In our experiment, Qt and Ft have a di-
mension of 128.

4.1. Evaluation

Automatic Metrics Our tasks pose new challenges on
evaluating the models as (1) the three types of scene repre-
sentations are quite different from each other; and (2) there
is no absolute one-to-one correspondence between a sen-
tence and a scene. For the abstract scene generation task,
we draw inspiration from the evaluation metrics applied in
machine translation [17] but we aim at aligning multimodal
visual-linguistic data instead. To this end, we propose to

compute the following metrics: precision/recall on single
objects (U-obj), “bigram” object pairs (B-obj); classi-
ﬁcation accuracies for poses, expressions; Euclidean dis-
tances (deﬁned as a Gaussian function with a kernel size
of 0.2) for normalized coordinates of U-obj and B-obj.
A “bigram” object pair is deﬁned as a pair of objects with
overlapping bounding boxes as illustrated in Figure 4.

In the layout generation experiment, it is harder to deﬁne
evaluation metrics given the complexity of real world object
layouts. Inspired by [11], we employ caption generation as
an extrinsic evaluation. We generate captions from the se-
mantic layouts using [36] and compare them back to the
original captions used to generate the scenes. We use com-
monly used metrics for captioning such as BLEU [23], ME-
TEOR [17], ROUGE L [18], CIDEr [29] and SPICE [1].

For synthetic image generation, we adopt the Inception
Score (IS) metric [27] which is commonly used in recent
text to image generation methods. However, as IS does not
evaluate correspondence between images and captions, we
also employ an extrinsic evaluation using image captioning
using the Show-and-Tell caption generator [31] as in [11].

Baselines For abstract scene generation, we run compar-
isons with [41]. We also consider variants of our full model:
(1) Text2Scene (w/o attention): a model without any atten-
tion module.
In particular, we replace Eq. 3 with a pure
average pooling operation on hD
t , discard co
t in Eq. 5, dis-
card ca
t and replace ua
t with hD
in Eq. 8. (2) Text2Scene (w
object attention): a model with attention modules for object
prediction but no dedicated attention for attribute predic-

t

6715

Figure 5. Examples of generated abstract scenes. The ﬁrst column
shows the input text, and the last column shows the reference true
scene from the dataset.

Figure 6. Generated layouts from input captions and generated
captions from the predicted layouts (best viewed in color). Our
model successfully predicts the presence (purple text) and number
of objects (blue text), and their spatial relations (red text).

tion. Speciﬁcally, we replace (ua
t , ca
t ) with (hD
t , co
t ) in Eq. 8.
(3) Text2Scene (w both attentions): a model with dedicated
attention modules for both object and attribute predictions
but no regularization.

Human Evaluations We also conduct human evaluations
using crowdsourcing on 100 groups of clip-art scenes gen-
erated for the Abstract Scene dataset using random captions
from the test split. Human annotators are asked to determine
whether an input text is a true statement given the generated
scene (entailment). Each scene in this dataset is associated
with three sentences that are used as the statements. Each
sentence-scene pair is reviewed by three annotators to de-
termine if the entailment is true, false or uncertain.
Ignoring uncertain responses, we use the ratio of the
sentence-scene pairs marked as true for evaluation.

We also perform predicate-argument semantic frame
analysis [3] on our results. Using the semantic parser
from [41], we subdivide the sentences as: sub-pred
corresponding to sentences referring to only one object,
sub-pred-obj corresponding to sentences referring to
object pairs with semantic relations, pred:loc corre-
sponding to sentences referring to locations, and pred:pa
corresponding to sentences mentioning facial expressions.

For synthetic image generation we use a similar human
evaluation as in [24]. We compare our method against
SG2IM [13], HDGAN [38] and AttnGAN [33]. We resize
our generated images to the same resolutions as in these
alternative methods, 64 × 64 for SG2IM [13], 256 × 256
for HDGAN [38] and AttnGAN [33]. For each sentence
randomly selected from the test set, we present images gen-
erated by our method and a competing method and allow
the user to choose the one which better represents the text.
We collect results for 500 sentences. For each sentence, we
collect responses from 5 different annotators.

4.2. Results and Discussion

Abstract Scenes and Semantic Layouts: Table 1 shows
quantitative results on Abstract Scenes. Text2Scene im-
proves over [41] and our variants on all metrics ex-
cept U-obj Coord. Human evaluation results on Ta-
ble 2 conﬁrm the quality of our outputs, where Scores
are the percentage of sentence-scene pairs with a true
entailment;
(≥ 1) (≥ 2) indicate if our method pro-
duces scenes that entailed at least one (or two) out of
three statements. Text2Scene also shows better results
on statements with speciﬁc semantic information such as

Obj-single, Obj-pair, and Expression, and is

comparable on Location statements. As a sanity check,
we also test reference true scenes provided in the Abstract
Scenes dataset (ﬁrst row). Results show that it is more chal-
lenging to generate the semantically related object pairs.
Overall, the results also suggest that our proposed metrics
correlate with human judgment on the task.
Figure 5 shows qualitative examples of our models on
Abstract Scenes in comparison with baseline approaches
and the reference scenes. These examples illustrate that
Text2Scene is able to capture semantic nuances such as the
spatial relation between two objects (e.g., the bucket and the
shovel are correctly placed in Jenny’s hands in the last row)
and object locations (e.g., Mike is on the ground near the
swing set in the last row).
Table 3 shows an extrinsic evaluation on the layout gen-
eration task. We perform this evaluation by generating cap-
tions from our predicted layouts. Results show our full
method generates the captions that are closest to the cap-
tions generated from true layouts. Qualitative results in Fig-
ure 6 also show that our model learns important visual con-
cepts such as presence and number of object instances, and
their spatial relations.

6716

Figure 7. Qualitative examples of synthetic image generation (best viewed in color). The ﬁrst column shows input captions with manually
highlighted objects (purple), counts (blue) and relations (red). The second columns shows the true images. Columns in the middle show
competing approaches. The last two columns show the outputs of our model before and after pre-processing.

Synthetic Image Composites: Table 4 shows evaluation
of synthetic scenes using automatic metrics. Text2Scene
without any post-processing already outperforms all previ-
ous methods by large margins except AttnGAN [33]. As
our model adopts a composite image generation framework
without adversarial training, gaps between adjacent patches
may result in unnaturally shaded areas. We observe that, af-
ter performing a regression-based inpainting [24], the com-
posite outputs achieve consistent improvements on all auto-
matic metrics. We posit that our model can be further im-
proved by incorporating more robust post-processing or in
combination with GAN-based methods. On the other hand,
human evaluations show that our method signiﬁcantly out-
performs alternative approaches including AttnGAN [33],
demonstrating the potential of leveraging realistic image
patches for text-to-image generation. It is important to note
that SG2IM [13] and Hong et al [11] also use segment and
bounding box supervision – as does our method, and At-
tnGan [33] uses an Imagenet (ILSVRC) pretrained Incep-
tionv3 network. In addition, as our model contains a patch
retrieval module, it is important that the model does not gen-
erate a synthetic image by simply retrieving patches from a
single training image. On average, each composite image
generated from our model contains 8.15 patches from 7.38

different source images, demonstrating that the model does
not simply learn a global image retrieval. Fig. 7 shows qual-
itative examples of the synthetic image composites, We in-
clude examples of generated images along with their corre-
sponding source images from which patch segments are re-
trieved, and more extensive qualitative results in the supple-
mental material. Since our model learns about objects and
relations separately, we also observed that it is often able to
generalize to uncommon situations (as deﬁned in [35]).

5. Conclusions

This work presents a novel sequence-to-sequence model
for generating compositional scene representations from vi-
sually descriptive language. We provide extensive quantita-
tive and qualitative analysis of our model for different scene
generation tasks on datasets from two different domains:
Abstract Scenes [41] and COCO [19]. Experimental results
demonstrate the capacity of our model to capture ﬁner se-
mantic concepts from visually descriptive text and generate
complex scenes.

Acknowledgements: This work was partially supported by
an IBM Faculty Award to V.O, and gift funding from SAP
Research.

6717

References

[1] Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. Spice: Semantic propositional image cap-
tion evaluation. In European Conference on Computer Vision
(ECCV), pages 382–398, 2016.
[2] Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. Guided open vocabulary image captioning
with constrained beam search. In Empirical Methods in Nat-
ural Language Processing (EMNLP), 2017.
[3] Xavier Carreras and Llu´ıs M `arquez.
Introduction to
the conll-2005 shared task: Semantic role labeling.
In
Conference on Computational Natural Language Learning
(CoNLL), pages 152–164, 2005.
[4] Qifeng Chen and Vladlen Koltun. Photographic image syn-
thesis with cascaded reﬁnement networks. In IEEE Interna-
tional Conference on Computer Vision (ICCV), 2017.
[5] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Pe-
ter Young, Cyrus Rashtchian, Julia Hockenmaier, and David
Forsyth. Every picture tells a story: Generating sentences
from images. In European Conference on Computer Vision
(ECCV), pages 15–29, 2010.
[6] James Philbin Florian Schroff, Dmitry Kalenichenko.
Facenet: A uniﬁed embedding for face recognition and clus-
tering. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015.
[7] David F Fouhey and C Lawrence Zitnick. Predicting object
dynamics in scenes. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2014.
[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
Neural Information Processing Systems (NeurIPS), 2014.
[9] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem,
and Aniruddha Kembhavi. Imagine this! scripts to composi-
tions to videos. In European Conference on Computer Vision
(ECCV), 2018.
[10] Jasper Uijlings Holger Caesar and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018.
[11] Seunghoon Hong, Dingdong Yang, Jongwook Choi, and
Honglak Lee. Inferring semantic layout for hierarchical text-
to-image synthesis. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018.
[12] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros.
Image-to-image translation with conditional adver-
sarial networks.
In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.
[13] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image gener-
ation from scene graphs. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018.
[14] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-
ments for generating image descriptions. In IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 3128–3137, 2015.
[15] Jin-Hwa Kim, Devi Parikh, Dhruv Batra, Byoung-Tak
Zhang, and Yuandong Tian. Codraw: Visual dialog for col-
laborative drawing. arXiv preprint arXiv:1712.05558, 2017.

[16] Polina Kuznetsova, Vicente Ordonez, Tamara Berg, and
Yejin Choi. Treetalk: Composition and compression of trees
for image descriptions. Transactions of the Association of
Computational Linguistics, 2(1):351–362, 2014.
[17] Alon Lavie and Abhaya Agarwal. Meteor: An automatic
metric for mt evaluation with high levels of correlation with
human judgments. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 228–231, 2007.
[18] Chin-Yew Lin. Rouge: A package for automatic evaluation
of summaries. Text Summarization Branches Out, 2004.
[19] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll ´ar, and C. Lawrence Zitnick. Microsoft
COCO: Common objects in context. European Conference
on Computer Vision (ECCV), 2014.
[20] Minh-Thang Luong, Hieu Pham, and Christopher D. Man-
ning. Effective approaches to attention-based neural machine
translation. In Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1412–1421, 2015.
[21] Rebecca Mason and Eugene Charniak. Nonparametric
method for data-driven image captioning. In Annual Meet-
ing of the Association for Computational Linguistics (ACL),
volume 2, pages 592–598, 2014.
[22] Vicente Ordonez, Xufeng Han, Polina Kuznetsova, Girish
Kulkarni, Margaret Mitchell, Kota Yamaguchi, Karl Stratos,
Amit Goyal, Jesse Dodge, Alyssa Mensch, et al. Large scale
retrieval and generation of image descriptions. International
Journal of Computer Vision (IJCV), 119(1):46–59, 2016.
[23] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 311–318, 2002.
[24] Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun.
Semi-parametric image synthesis.
In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018.
[25] Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka,
Bernt Schiele, and Honglak Lee. Learning what and where to
draw. In Advances in Neural Information Processing Systems
(NeurIPS), 2016.
[26] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In International Conference
on Learning Representations (ICLR), 2016.
[27] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), 2016.
[28] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence
to sequence learning with neural networks. In Advances in
Neural Information Processing Systems (NeurIPS), 2014.
[29] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description evalua-
tion. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 4566–4575, 2015.
[30] Ramakrishna Vedantam, Xiao Lin, Tanmay Batra, C
Lawrence Zitnick, and Devi Parikh. Learning common sense
through visual abstraction.
In IEEE International Confer-
ence on Computer Vision (ICCV), 2015.

6718

[31] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-
mitru Erhan. Show and tell: A neural image caption gen-
erator. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2015.
[32] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International Conference on
Machine Learning (ICML), volume 37, pages 2048–2057,
2015.
[33] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2018.
[34] Mark Yatskar, Vicente Ordonez, and Ali Farhadi. Stating
the obvious: Extracting visual common sense knowledge. In
Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 193–198, 2016.
[35] Mark Yatskar, Vicente Ordonez, Luke Zettlemoyer, and Ali
Farhadi. Commonly uncommon: Semantic sparsity in situ-
ation recognition. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 7196–7205, 2017.
[36] Xuwang Yin and Vicente Ordonez. Obj2text: Generating vi-
sually descriptive language from object layouts. In Empirical
Methods in Natural Language Processing (EMNLP), 2017.
[37] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei
Huang, Xiaogang Wang, and Dimitris Metaxas. Stackgan:
Text to photo-realistic image synthesis with stacked genera-
tive adversarial networks. In IEEE International Conference
on Computer Vision (ICCV), 2017.
[38] Zizhao Zhang, Yuanpu Xie, and Lin Yang. Photographic
text-to-image synthesis with a hierarchically-nested adver-
sarial network. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018.
[39] Xiaodan Zhu and Edward Grefenstette. Deep learning for
semantic composition. In ACL tutorial, 2017.
[40] C. Lawrence Zitnick and Devi Parikh. Bringing semantics
into focus using visual abstraction. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2013.
[41] C. Lawrence Zitnick, Devi Parikh, and Lucy Vanderwende.
Learning the visual interpretation of sentences. In IEEE In-
ternational Conference on Computer Vision (ICCV), 2013.

6719

Unsupervised Part-Based Disentangling of Object Shape and Appearance

Dominik Lorenz
Leonard Bereska
Timo Milbich
Bj ¨orn Ommer
Heidelberg Collaboratory for Image Processing / IWR, Heidelberg University

Abstract

Large intra-class variation is the result of changes in
multiple object characteristics. Images, however, only show
the superposition of different variable factors such as ap-
pearance or shape. Therefore, learning to disentangle and
represent these different characteristics poses a great chal-
lenge, especially in the unsupervised case. Moreover, large
object articulation calls for a ﬂexible part-based model.
We present an unsupervised approach for disentangling ap-
pearance and shape by learning parts consistently over all
instances of a category. Our model for learning an ob-
ject representation is trained by simultaneously exploiting
invariance and equivariance constraints between synthet-
ically transformed images. Since no part annotation or
prior information on an object class is required, the ap-
proach is applicable to arbitrary classes. We evaluate our
approach on a wide range of object categories and diverse
tasks including pose prediction, disentangled image synthe-
sis, and video-to-video translation. The approach outper-
forms the state-of-the-art on unsupervised keypoint predic-
tion and compares favorably even against supervised ap-
proaches on the task of shape and appearance transfer.

1. Introduction

A grand goal of computer vision is to automatically,
without supervision information, learn about the character-
istics of an object in the world. Typically, images show
the interplay of multiple such factors of variation. We
want to disentangle [9, 2, 5, 17, 10] the effects of these
different characteristics and imagine, i.e., synthesize, new
images where they are altered individually. For instance,
after observing a number of different unlabeled instances
of an object category, we want to learn their variations in
shape (such as pose relative to the viewer and body articu-
lation) and appearance, e.g., texture and color differences in
fur/clothing or skin color. Disentangling shape and appear-
ance is particularly challenging because object deformation
typically leads to complicated “recoloring” of image pixels

1 https://compvis.github.io/unsupervised-disentangling/

Figure 1: Our unsupervised learning of a disentangled part-
based shape and appearance enables numerous tasks rang-
ing from unsupervised pose estimation to image synthesis
and retargeting. For more results visit the project page 1

[40, 12]: moving a limb may change the color of former
background pixels into foreground and vice versa.
To address the disentangling problem for shape and ap-
pearance, several supervised methods have been proposed
lately [29, 28, 7, 12, 41, 1]. By conditioning generative
models on a pre-speciﬁed shape representation, they are
able to successfully explain away appearance. However,
they are limited to object categories, for which pose labels
are readily available such as human bodies and faces, but
they cannot be applied to the vast amounts of unlabelled
data of arbitrary object classes.
For unsupervised learning, instead of taking a known
shape to capture all non-shape factors, both shape and ap-
pearance need to be learned simultaneously. Recently some
unsupervised approaches have been proposed to disentan-
gle these factors [40, 52]. However, these works have only
shown results for rather rigid objects, like human faces or
require several instances of the same person [8].
Object variation can be global, such as difference in

10955

viewpoint, but it is oftentimes local (animal tilting its head,
person with/without jacket), thus calling for a local, disen-
tangled object representation. The traditional answer are
compositions of rigid parts [15, 14, 13].
In the context
of recent unsupervised shape learning an instantiation of
this are landmarks [45, 58, 21]. In this paper, we propose
the ﬁrst approach to learn a part-based disentangled rep-
resentation of shape and appearance for articulated object
classes without supervision and from scratch. In the spirit
of analysis-by-synthesis [54], we learn the factors by a gen-
erative process. We formulate explicit equivariance and in-
variance constraints an object representation should fulﬁll
and incorporate them in a fully differentiable autoencoding
framework.
Our approach yields signiﬁcant improvements upon the
state-of-the-art in unsupervised object shape learning, eval-
uated on the task of landmark regression. We compare to
competitors on a wide range of diverse datasets both for
rigid and articulated objects, with particularly large gains
for strong articulations. Furthermore, our disentangled rep-
resentation of shape and appearance competes favorably
even against state-of-the-art supervised results. We also
show disentangling results on the task of video-to-video
translation, where ﬁne-grained articulation is smoothly and
consistently translated on a frame-to-frame level. Lastly,
since our representation captures appearance locally, it is
also possible to transfer appearance on the level of indi-
vidual object parts. An overview of the scope of possible
applications is given in Fig. 1.

2. Related Work

Disentangling shape and appearance. Factorizing an
object representation into shape and appearance is a pop-
ular ansatz for representation learning. Recently, a lot of
progress has been made in this direction by conditioning
generative models on shape information [12, 29, 7, 28, 41,
1]. While most of them explain the object holistically, only
few also introduce a factorization into parts [41, 1]. In con-
trast to these shape-supervised approaches, we learn both
shape and appearance without any supervision.
For unsupervised disentangling,
several generative
frameworks have been proposed [17, 5, 24, 8, 40, 52].
However, these works use holistic models and show results
on rather rigid objects and simple datasets, while we explic-
itly tackle strong articulation with a part-based formulation.
Part-based representation learning. Describing an
object as an assembly of parts is a classical paradigm
for learning an object representation in computer vision
[38, 32, 6, 11]. What constitutes a part, is the deﬁning ques-
tion in this scheme. Deﬁning parts by visual and semantic
features or by geometric shape and its behavior under view-
point changes and object articulation in general leads to a
different partition of the object. Recently, part learning has

been mostly employed for discriminative tasks, such as in
[13, 33, 42, 30, 53, 22]. To solve a discriminative task, parts
will encode their semantic connection to the object and can
ignore the spatial arrangement and articulation. In contrast,
our method is driven by an image modelling task. Hence,
parts have to encode both spatial structure and visual ap-
pearance accurately.
Landmark learning. There is an extensive literature on
landmarks as compact representations of object structure.
Most approaches, however, make use of manual landmark
annotations as supervision signal [50, 36, 55, 60, 61, 59, 47,
34, 18, 46, 35, 49, 31, 25, 3].
To tackle the problem without supervision, Thewlis et
al. [45] proposed enforcing equivariance of landmark loca-
tions under artiﬁcial transformations of images. The equiv-
ariance idea had been formulated in earlier work [23] and
has since been extended to learn a dense object-centric co-
ordinate frame [44]. However, enforcing only equivariance
encourages consistent landmarks at discriminable object lo-
cations, but disregards an explanatory coverage of the ob-
ject.
Zhang et al. [58] addresses this issue: the equivariance
task is supplemented by a reconstruction task in an autoen-
coder framework, which gives visual meaning to the land-
marks. However, in contrast to our work, he does not disen-
tangle shape and appearance of the object. Furthermore, his
approach relies on a separation constraint in order to avoid
the collapse of landmarks. This constraint results in an ar-
tiﬁcial, rather grid-like layout of landmarks, that does not
scale to complex articulations.
Jakab et al. [21] proposes conditioning the generation on
a landmark representation from another image. A global
feature representation of one image is combined with the
landmark positions of another image to reconstruct the lat-
ter.
Instead of considering landmarks which only form a
representation for spatial object structure, we factorize an
object into local parts, each with its own shape and appear-
ance description. Thus, parts are learned which meaning-
fully capture the variance of an object class in shape as well
as in appearance.
Additionally, and in contrast to all these works ([45, 58,
21]) we take the extend of parts into account, when formu-
lating our equivariance constraint. Furthermore, we explic-
itly address the goal of disentangling shape and appearance
on a part-based level by introducing invariance constraints.

3. Approach

Let x : Λ → R be an image portraying an object and
background clutter. Λ ⊂ N2 is the space of image coordi-
nates. Now consider an image x′ : Λ → R showing another
instance of the same object category. Despite drastic differ-
ences of their image pixels, you can recognize both to be

10956

Shape

Stream

Decoder

appearance

approximate

transformation

Eq. 7



rec

Eq. 2

E

σ

part

shapes

σ

(a(x))

i

a(x)

σ ̃ 

(a(x))

i



equiv

Eq. 4

reconstruction

x ̂ 

x

spatial

transformation

extract

project

Eq. 6

Eq. 8

⋮

⋮

part

appearances

α

(x∘s)

i

E

σ

σ

(x∘s)

i

E

α

local features

f

x

x∘s

x∘s

f

Appearance

Stream

Figure 2: Two-stream autoencoding architecture for unsupervised learning of object shape and appearance.

related. What renders both images similar although no two
pixels are identical? What are the characteristic, salient dif-
ferences? And how can we obtain a representation φ that
maps images to vectors φ(x) which retain both, these simi-
larities and also the characteristic differences?

3.1. Part(cid:173)based Representation

Numerous causes may have led x to be changed into x′
(change in articulation, viewpoint, object color or clothing,
lighting conditions, etc.). But we can approximate and sum-
marize their effects as a combination of a change in appear-
ance and a change in shape. The effect of a change in object
shape on an image x can be expressed in terms of a spatial
image transformation s : Λ → Λ acting on the underlying
image coordinates, such that the image x◦s depicts the ob-
ject with altered shape. Similarly, we denote the effect of
a change in object appearance on an image x as an image
transformation a such that the image a(x) depicts the object
with altered appearance.
Note that many of the image changes are local in nature,
affecting only part of the image. For instance, animals may
only move an individual body part. Similarly, only part of
their appearance may vary, e.g., by switching a shirt but not
the pants. This motivates a part-based factorization of the
representation, φ(x) := (φ1 (x), φ2 (x), . . . )⊤ , so that local
changes in appearance and shape stay local and do not alter
the overall representation. Nevertheless, global changes can
also be accounted for by representing them as as a compo-

sition of changes in the individual part representations φi .

3.2. Invariance and Equivariance

Let us now carefully observe differences between im-
ages x and x′ to derive constraints for the representation
φ that is to be learned.
i) Changes in the appearance of
an object (e.g. in its color or texture), should not impact
its shape. ii) Similarly, changes in shape (e.g. through ar-
ticulation), should not alter the appearance. Therefore, the
representation needs to separate appearance and shape of
the object, so that both can vary individually, i.e. the rep-
resentation of a part is disentangled into two components
φi (x) = (αi (x), σi (x)). Part appearance is modeled as an
n-dimensional feature vector αi (x) ∈ Rn . Whereas part
shape is modeled as a part activation map σi (x) : Λ → R+ .
We visualize these maps as colored images (cf. Fig. 2, Fig.
3), where each color denotes a single part activation map.

The invariance of our representation under changes in
object appearance and shape can be summarized by the in-
variance constraints i) αi (x◦s) = αi (x) and ii) σi (a(x)) =
σi (x). In addition, changes in shape should obviously be
captured by the shape representation. Thus, for spatial
transformations s we obtain the equivariance constraint iii)
σi (x◦s) = σi (x)◦s. The equivariance constraint simply
states that the part activation maps have to consistently track
the object part they represent (cf. σi (a(x)) and σi (x◦s) in
Fig. 2).

10957

3.3. Objective Function for Learning

Learning of the representation φ is driven by integrat-
ing invariance and equivariance constraints from the previ-
ous section into a reconstruction task. The invariance con-
straints i) and ii) imply

φi (x) = [αi (x), σi (x)]

!

= [αi (x◦s), σi (a(x))].

(1)

Let D([φi (x)]i=1,..., ) be a reconstruction of the orig-
image x from the encoded part
inal
representations
φ1 (x), φ2 (x), ... using a decoder D . We seek to reconstruct
x and, simultaneously, demand the representation to obey
the invariance constraints summarized in (1),

Lrec = (cid:13)(cid:13)(cid:13)

x − D (cid:16)(cid:2)αi (x◦s), σi (a(x))(cid:3)i=1,...(cid:17)(cid:13)(cid:13)(cid:13)1

.

(2)

Moreover, the representation of part shape σi (x) should be
equivariant under deformations. However, simply minimiz-
ing equivariance on the scale of pixels, i.e.

X

i

X

u∈Λ

(cid:13)(cid:13)(cid:13)

σi (x◦s)[u] − σi (x)[s(u)](cid:13)(cid:13)(cid:13)

,

(3)

is unstable in practice and favors to the trivial solution of
uniform part activations. Therefore, we establish an equiv-
ariance loss

Lequiv = X
i

λµ kµ[σi (x◦s)] − µ[σi (a(x))◦s]k2

(4)

+ λΣ kΣ[σi (x◦s)] − Σ[σi (a(x))◦s]k1 ,

where µ[σi (x)] and Σ[σi (x)] denote the mean and covari-
ance over coordinates of σi (x)/ Pu∈Λ σi (x)[u]. Note that
we have employed invariance ii) so that we can use the same
shape encoding σi (a(x)) as in (2). The overall training ob-
jective of our model is to minimize the reconstruction and
equivariance loss,

L = Lrec + Lequiv .

(5)

Note that object parts a priori unknown, but in order to
reconstruct the object, the representations φi automatically
learn to structure it into meaningful parts which capture the
variance in shape and appearance. In particular, we do not
need to introduce artiﬁcial prior assumptions about the rela-
tions between parts, such as the separation constraints em-
ployed in [58, 45]. Instead, the local modelling of the part
representation (cf. sec. 3.4) as disentangled components of
shape and appearance drives our representation to meaning-
fully structure the object and learn natural relations between
parts.

3.4. Unsupervised Learning of Part(cid:173)based Shape
and Appearance

Subsequently, we discuss the network architecture in
Fig. 2 for unsupervised learning of an appearance and shape

representation using the reconstruction (2) and equivariance
loss (4). Learning considers image pairs x◦s and a(x). The
leading design principle of our architecture is to model the
local interplay between part shape and part appearance. In
a fully differentiable procedure equivariance of part activa-
tion maps is used to extract part appearances from x◦s and
assign them to corresponding image regions in x.
Part Shape. In a shape stream (cf. Fig. 2), an hourglass
network [31] Eσ learns to localize parts i by means of part
activation maps σi (a(x)) ∈ Rh×w . The hourglass model
nicely suits this task, since it preserves pixel-wise locality
and integrates information from multiple scales [31]. Multi-
scale context is essential to learn the relations between parts
and consistently assign them to an object.
Part Appearance. Let us now localize the parts by
detecting σi (x◦s) in a spatially transformed image x◦s
using the same network Eσ (cf.
Fig.
2 Appearance
Stream).
To learn representations of part appearance
αi (x◦s), we ﬁrst stack all normalized part activations
σi (x◦s)/ Pu∈Λ σi (x◦s)[u] and an image encoding, i.e., the
output of the ﬁrst convolution ﬁlters of the network Eσ ap-
plied to x◦s. A second hourglass network Eα takes this
stack as input and maps it onto a localized image appear-
ance encoding fx◦s ∈ Rh×w×n . To obtain local part ap-
pearances, we average pool these features at all locations
where part i has positive activation

αi (x◦s) = Pu∈Λ fx◦s [u]σi (x◦s)[u]
Pu∈Λ σi (x◦s)[u]

.

(6)

Reconstructing the Original Image. Next we recon-
struct x from part appearances αi (x◦s) and part activations
σi (a(x)) using a U-Net [37] (cf. Fig. 2). The encoder
of the U-Net is simply a set of ﬁxed downsampling layers.
Only its decoder is learned. We approximate part activa-
tions σi (a(x)) by their ﬁrst two moments,

˜σi (a(x))[u] =

1
1 + (u − µi )T Σ−1

i

,

(7)

(u − µi )

where µi and Σi denote the mean and covariance of the nor-
malized part activation maps σi (a(x))/ Pu∈Λ σi (a(x))[u].
Thus, extra information present in part activations is ne-
glected, forcing the shape encoder Eσ to concentrate on an
unambiguous part localization (or else reconstruction loss
would increase). The second input to the decoder D in
Eq. 2 are part appearances αi (x◦s). Note that αi (x◦s) are
feature vectors devoid of localization. We exploit the fact
that the corresponding part activations ˜σi (a(x)) designate
the regions of parts i in image x (cf. Fig 2) to project the
part appearances onto a localized appearance encoding fx :

fx [u] = X

i

αi (x◦s) · ˜σi (a(x))[u]
1 + Pj ˜σj (a(x))[u]

.

(8)

10958

Table 1: Difﬁculties of datasets: articulation, intra-class
variance, background clutter and viewpoint variation.

Dataset
CelebA
Cat Head
CUB-200-2011
Human3.6M
BBC Pose
Dogs Run
Penn Action

Articul. Var. Backgr. Viewp.

X

X

X

X

X

X

X

X

X

X

X

X

X

X

To reconstruct x, the U-Net can then exploit the local cor-
respondence between fx , ˜σi (a(x)) and x.

3.5. Implementation Details

For appearance transformation a we apply changes in
brightness, contrast, and hue. For image datasets, s are thin
plate spline (TPS) transformations. On video datasets, in
addition to applying synthetic TPS transformations we ran-
domly sample another frame from the same video sequence
which acts as x◦s. Selecting the number of parts is un-
critical, since our model is robust for different numbers of
parts, Tab. 2. For image synthesis in Sect. 4.3 we train
the decoder D with an adversarial loss [20]. We refer to the
supplementary for further details on the architecture and the
experimental setup.

4. Experiments

In this section we evaluate our unsupervised approach
for learning disentangled representation of appearance and
shape. Sect. 4.2 evaluates and visualizes the shape repre-
sentation on the task of unsupervised landmark discovery.
Sect. 4.3 investigates the disentangling of our representa-
tion. On the task of conditional image generation, we com-
pare our unsupervised shape/appearance disentanglement
performance against a state-of-the-art disentangling method
that utilizes groundtruth shape annotations. Moreover, on
the task of frame-to-frame video translation we show the ro-
bustness of our representation across multiple frames. Ad-
ditionally, we evaluate the ability of our method to disen-
tangle parts and their local appearance and shape using a
part-wise appearance transfer.

4.1. Datasets

CelebA [27] contains ca. 200k celebrity faces of 10k
identities. We resize all images to 128 × 128 and exclude
the training and test set of the MAFL subset, following [45].
As [45, 58], we train the regression (to 5 ground truth land-
marks) on the MAFL training set (19k images) and test on
the MAFL test set (1k images).
Cat Head [56] has nearly 9k images of cat heads. We

(a)

(b)

Figure 3: Learned shape representation on Penn Action. For
visualization, 13 of 16 part activation maps are plotted in
one image. (a) Different instances, showing intra-class con-
sistency and (b) video sequence, showing consistency and
smoothness under motion, although each frame is processed
individually.

use the train-test split of [58] for training (7,747 images)
and testing (1,257 images). We regress 5 of the 7 (same
as [58]) annotated landmarks. The images are cropped by
bounding boxes constructed around the mean of the ground
truth landmark coordinates and resized to 128 × 128.
CUB-200-2011 [48] comprises ca. 12k images of birds
in the wild from 200 bird species. We excluded bird species
of seabirds, roughly cropped using the provided landmarks
as bounding box information and resized to 128 × 128. We
aligned the parity with the information about the visibility
of the eye landmark. For comparing with [58] we used their
published code.
BBC Pose [4] contains videos of sign-language signers
with varied appearance in front of a changing background.
Like [21] we loosely crop around the signers. The test set
includes 1000 frames and the test set signers did not ap-
pear in the train set. For evaluation, as [21], we utilized the
provided evaluation script, which measures the PCK around
d = 6 pixels in the original image resolution.
Human3.6M [19] features human activity videos. We
adopt the training and evaluation procedure of [58]. For
proper comparison to [58] we also removed the background
using the off-the-shelf unsupervised background subtrac-
tion method provided in the dataset.
Penn Action [57] contains 2326 video sequences of 15
different sports categories. For this experiment we use 6 cat-
egories (tennis serve, tennis forehand, baseball pitch, base-
ball swing, jumping jacks, golf swing). We roughly cropped

10959

Figure 5: Comparing discovered keypoints against [58] on
CUB-200-2011. We improve on object coverage and land-
mark consistency. Note our ﬂexible part placement com-
pared to a rather rigid placement of [58] due to their part
separation bias.

4.2. Evaluating Unsupervised Learning of Shape

Fig. 3 visualizes the learned shape representation. To
quantitatively evaluate the shape estimation, we measure
how well groundtruth landmarks (only during testing) are
predicted from it. The part means µ[σi (x)] (cf. (4)) serve
as our landmark estimates and we measure the error when
linearly regressing the human-annotated groundtruth land-
marks from our estimates. For this, we follow the protocol
of Thewlis et al. [45], ﬁxing the network weights after train-
ing the model, extracting unsupervised landmarks and train-
ing a single linear layer without bias. The performance is
quantiﬁed on a test set by the mean error and the percentage
of correct landmarks (PCK). We extensively evaluate our
model on a diverse set of datasets, each with speciﬁc chal-
lenges. An overview over the challenges implied by each
dataset is given in Tab. 1. On all datasets we outperform the
state-of-the-art by a signiﬁcant margin.
Diverse Object Classes. On the object classes of hu-
man faces, cat faces, and birds (datasets CelebA, Cat Head,
and CUB-200-2011) our model predicts landmarks consis-
tently across different instances, cf. Fig. 4. Tab. 2 com-
pares against the state-of-the-art. Due to different breeds
and species the Cat Head, CUB-200-2011 exhibit large vari-
ations between instances. Especially on these challenging
datasets we outperform competing methods by a large mar-
gin. Fig. 5 also provides a direct visual comparison to [58]
on CUB-200-2011. It becomes evident that our predicted
landmarks track the object much more closely. In contrast,
[58] have learned a slightly deformable, but still rather rigid
grid. This is due to their separation constraint, which forces
landmarks to be mutually distant. We do not need such a
problematic bias in our approach, since the localized, part-
based representation and reconstruction guides the shape
learning and captures the object and its articulations more
closely.
Articulated Object Pose. Object articulation makes
consistent landmark discovery challenging. Fig. 4 shows
that our model exhibits strong landmark consistency un-
der articulation and covers the full human body meaning-

10960

Figure 4: Unsupervised discovery of landmarks on diverse
object classes such as human or cat faces and birds and for
highly articulated human bodies and running dogs.

Table 2: Error of unsupervised methods for landmark pre-
diction on the Cat Head, MAFL (subset of CelebA), and
CUB-200-2011 testing sets. The error is in % of inter-ocular
distance for Cat Head and MAFL and in % of edge length
of the image for CUB-200-2011.

Dataset
# Landmarks
Thewlis [45]
Jakab [21]
Zhang [58]
Ours

Cat Head
10
26.76
-
15.35
9.88

MAFL CUB
10
10
6.32
-
4.69
-
3.46
5.36
3.24
3.91

20
26.94
-
14.84
9.30

the images around the person, using the provided bounding
boxes, then resized to 128 × 128.

Dogs Run is made from dog videos from YouTube to-
taling in 1250 images under similar conditions as in Penn
Action. The dogs are running in one direction in front of
varying backgrounds. The 17 different dog breeds exhibit
widely varying appearances.

Deep Fashion [26] consists of ca. 53k in-shop clothes
images in high-resolution of 256 × 256. We selected the
images which are showing a full body (all keypoints visible,
measured by [3]) and used the provided train-test split. For
comparison with Esser et al. [12] we used their published
code.

Table 3: Performance of landmark prediction on BBC Pose
test set. As upper bound, we also report the performance
of supervised methods. The metric is % of points within 6
pixels of groundtruth location.

BBC Pose
supervised

unsupervised

Charles [4]
Pﬁster [35]
Jakab [21]
Ours

Accuracy
79.9%
88.0%
68.4%
74.5%

Table 4: Comparing against supervised, semi-supervised
and unsupervised methods for landmark prediction on the
Human3.6M test set. The error is in % of the edge length of
the image. All methods predict 16 landmarks.

Human3.6M
supervised
Newell [31]
semi-supervised Zhang [58]
unsupervised
Thewlis [45]
Zhang [58]
Ours

Error w.r.t. image size
2.16
4.14
7.51
4.91
2.79

fully. Even ﬁne-grained parts such as the arms are tracked
across heavy body articulations, which are frequent in the
Human3.6M and Penn Action datasets. Despite further
complications such as viewpoint variations or blurred limbs
our model can detect landmarks on Penn Action of simi-
lar quality as in the more constrained Human3.6M dataset.
Additionally, complex background clutter as in BBC Pose
and Penn Action, does not hinder ﬁnding the object. Ex-
periments on the Dogs Run dataset underlines that even
completely dissimilar dog breeds can be related via seman-
tic parts. Tab. 3 and Tab. 4 summarize the quantitative
evaluations: we outperform other unsupervised and semi-
supervised methods by a large margin on both datasets. On
Human3.6M, our approach achieves a large performance
gain even compared to methods that utilize optical ﬂow su-
pervision. On BBC Pose, we outperform [21] by 6.1%, re-
ducing the performance gap to supervised methods signiﬁ-
cantly.

4.3. Disentangling Shape and Appearance

Disentangled representations of object shape and appear-
ance allow to alter both properties individually to synthesize
new images. The ability to ﬂexibly control the generator al-
lows, for instance, to change the pose of a person or their
clothing. In contrast to previous work [12, 8, 28, 29, 7, 21],
we achieve this ability without requiring supervision and
using a ﬂexible part-based model instead of a holistic rep-
resentation. This allows to explicitly control the parts of
an object that are to be altered. We quantitatively compare

Figure 6: Transferring shape and appearance on Deep Fash-
ion. Without annotation the model estimates shape, 2nd col-
umn. Target appearance is extracted from images in top row
to synthesize images. Note that we trained without image
pairs only using synthetic transformations. All images are
from the test set.

against supervised state-of-the-art disentangled synthesis of
human ﬁgures. Also we qualitatively evaluate our model on
unsupervised synthesis of still images, video-to-video trans-
lation, and local editing for appearance transfer.
Conditional Image Generation. On Deep Fashion
[27, 26], a benchmark dataset for supervised disentangling
methods, the task is to separate person ID (appearance)
from body pose (shape) and then synthesize new images for
previously unseen persons from the test set in eight differ-
ent poses. We randomly sample the target pose and appear-
ance conditioning from the test set. Fig. 6 shows qualita-
tive results. We quantitatively compare against supervised
state-of-the-art disentangling [12] by evaluating i) invari-
ance of appearance against variation in shape by the re-
identiﬁcation error and ii) invariance of shape against vari-
ation in appearance by the distance in pose between gener-
ated and pose target image.
i) To evaluate appearance we ﬁne-tune an ImageNet-
pretrained [39] Inception-Net [43] with a re-identiﬁcation
(ReID) algorithm [51] via a triplet loss [16] to the Deep
Fashion training set. On the generated images we evalu-
ate the standard metrics for ReID, mean average precision
(mAP) and rank-1, -5, and -10 accuracy in Tab. 5. Although
our approach is unsupervised it is competitive compared to
the supervised VU-Net [12].

10961

Table 5: Mean average precision (mAP) and rank-n accu-
racy for person re-identiﬁcation on synthesized images af-
ter performing shape/appearance swap. Input images from
Deep Fashion test set. Note [12] is supervised w.r.t. shape.

VU-Net [12]
Ours

mAP
rank-1
rank-5
88.7% 87.5% 98.7%
90.3% 89.4% 98.2%

rank-10
99.5%
99.2%

Table 6: Percentage of Correct Keypoints (PCK) for pose
estimation on shape/appearance swapped generations. α is
pixel distance divided by image diagonal. Note that [12]
serves as upper bound, as it uses the groundtruth shape es-
timates.

α

2.5%

5%

7.5%

10%

VU-Net [12]
Ours

95.2% 98.4% 98.9% 99.1%
85.6% 94.2% 96.5% 97.4%

Figure 7: Video-to-video translation on BBC Pose. Top-
row: target appearances, left: target pose. Note that even
ﬁne details in shape are accurately captured. Visit the
project page for the video.

ii) To evaluate shape, we extract keypoints using the pose
estimator [3]. Tab. 6 reports the difference between gen-
erated and pose target in percentage of correct keypoints
(PCK). As would be expected, VU-Net performs better,
since it is trained with exactly the keypoints of [3]. Still our
approach achieves an impressive PCK without supervision
underlining the disentanglement of appearance and shape.
Video-to-Video Translation. To evaluate the robust-
ness of our disentangled representation, we synthesize a
video sequence frame-by-frame without temporal consis-
tency constraints. On BBC Pose [4], one video provides
a sequence of target poses, another video a sequence of
source appearances to then perform retargeting, Fig. 7.
Although there is no temporal coupling, the generated se-
quences are smooth and pose estimation is robust. Sec-
ondly, the training on the natural spatial deformations in
video data enables the model to encapsulate realistic tran-
sitions such as out-of-plane rotation and complex 3D artic-

(a)

(b)

(c)

(d)

Figure 8: Swapping part appearance on Deep Fashion. Ap-
pearances can be exchanged for parts individually and with-
out altering shape. We show part-wise swaps for (a) head
(b) torso (c) legs, (d) shoes. All images are from the test
set.

ulation of hands and even ﬁngers. Due to the local nature of
the part based representation, the model is robust to varia-
tions in the background and focuses on the object whilst the
background is only roughly reconstructed.
Part Appearance Transfer. The ﬂexible part-based rep-
resentation allows to explicitly control local appearance.
Fig. 8 shows swaps of appearance for shirt, pants, etc. In
contrast to holistic representations [12, 21, 28, 29, 7], we
can guarantee the transfer to be focused on selected object
parts.

5. Conclusion

We have presented an unsupervised approach to learning
the compositional part structure of objects by disentangling
shape from appearance. We incorporate invariance and
equivariance constraints in a generative framework. The
model discovers consistent parts without requiring prior as-
sumptions. Experiments show our approach signiﬁcantly
improves upon previous unsupervised methods.

This work has been supported in part by DFG grant OM81/1-1 and a
hardware donation from NVIDIA Corporation.

10962

References

[1] G. Balakrishnan, A. Zhao, A. V. Dalca, F. Durand, and J. V.
Guttag. Synthesizing images of humans in unseen poses.
arXiv preprint arXiv:1804.07739, 2018. 1, 2
[2] Y. Bengio, A. Courville, and P. Vincent. Representation
learning: A review and new perspectives. TPAMI, 2013. 1
[3] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multi-
person 2d pose estimation using part afﬁnity ﬁelds. In CVPR,
2017. 2, 6, 8
[4] J. Charles, T. Pﬁster, D. R. Magee, D. C. Hogg, and A. Zis-
serman. Domain adaptation for upper body pose tracking in
signed tv broadcasts. In BMVC, 2013. 5, 7, 8
[5] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever,
and P. Abbeel. Infogan: Interpretable representation learn-
ing by information maximizing generative adversarial nets.
NIPS, 2016. 1, 2
[6] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appear-
ance models. In ECCV, 1998. 2
[7] R. de Bem, A. Ghosh, T. Ajanthan, O. Miksik, N. Siddharth,
and P. H. S. Torr. Dgpose: Disentangled semi-supervised
deep generative models for human body analysis. arXiv
preprint arXiv:1804.06364, 2018. 1, 2, 7, 8
[8] E. L. Denton and V. Birodkar. Unsupervised learning of dis-
entangled representations from video. In NIPS, 2017. 1, 2,
7
[9] G. Desjardins, A. Courville, and Y. Bengio. Disentangling
factors of variation via generative entangling. arXiv preprint
arXiv:1210.5474, 2012. 1
[10] C. Eastwood and C. K. Williams. A framework for the quan-
titative evaluation of disentangled representations.
ICLR,
2018. 1
[11] A. Eigenstetter, M. Takami, and B. Ommer. Randomized
max-margin compositions for visual recognition.
In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3590–3597. IEEE, IEEE, 2014.
2
[12] P. Esser, E. Sutter, and B. Ommer. A variational u-net for
conditional appearance and shape generation. CVPR, 2018.
1, 2, 6, 7, 8
[13] P. F. Felzenszwalb, R. B. Girshick, D. A. McAllester, and
D. Ramanan. Object detection with discriminatively trained
part-based models. TPAMI, 2010. 2
[14] R. Fergus, P. Perona, and A. Zisserman. Object class recog-
nition by unsupervised scale-invariant learning.
In CVPR,
2003. 2
[15] M. A. Fischler and R. A. Elschlager. The representation and
matching of pictorial structures. IEEE Transactions on com-
puters, 1973. 2
[16] A. Hermans, L. Beyer, and B. Leibe.
triplet
loss for person re-identiﬁcation.
arXiv:1703.07737, 2017. 7
[17] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,
M. Botvinick, S. Mohamed, and A. Lerchner. Beta-vae:
Learning basic visual concepts with a constrained variational
framework. ICLR, 2017. 1, 2
[18] C. Ionescu, F. Li, and C. Sminchisescu. Latent structured
models for human pose estimation. In ICCV, 2011. 2

In defense of the
arXiv preprint

[19] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Hu-
man3.6m: Large scale datasets and predictive methods for
3d human sensing in natural environments. TPAMI, 2014. 5
[20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
Image-
to-image translation with conditional adversarial networks.
arXiv preprint, 2017. 5
[21] T. Jakab, A. Gupta, H. Bilen, and A. Vedaldi. Conditional
image generation for learning the structure of visual objects.
NIPS, 2018. 2, 5, 6, 7, 8
[22] M. Lam, B. Mahasseni, and S. Todorovic. Fine-grained
recognition as hsnet search for informative image parts. In
CVPR, 2017. 2
[23] K. Lenc and A. Vedaldi. Learning covariant feature detec-
tors. In ECCV Workshops, 2016. 2
[24] Z. Li, Y. Tang, and Y. He. Unsupervised disentangled repre-
sentation learning with analogical relations. In IJCAI, 2018.
2
[25] J. Lim, Y. Yoo, B. Heo, and J. Y. Choi. Pose transform-
ing network: Learning to disentangle human posture in vari-
ational auto-encoded latent space. Pattern Recognit. Lett.,
2018. 2
[26] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. Deepfashion:
Powering robust clothes recognition and retrieval with rich
annotations. In CVPR, 2016. 6, 7
[27] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face
attributes in the wild. In ICCV, 2015. 5, 7
[28] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. V.
Gool. Pose guided person image generation. In NIPS, 2017.
1, 2, 7, 8
[29] L. Ma, Q. Sun, S. Georgoulis, L. V. Gool, B. Schiele, and
M. Fritz. Disentangled person image generation. CVPR,
2017. 1, 2, 7, 8
[30] G. Mesnil, A. Bordes, J. Weston, G. Chechik, and Y. Bengio.
Learning semantic representations of objects and their parts.
Mach Learn, 2013. 2
[31] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-
works for human pose estimation. ECCV, 2016. 2, 4, 7
[32] T. D. Nguyen, T. Tran, D. Q. Phung, and S. Venkatesh.
Learning parts-based representations with nonnegative re-
stricted boltzmann machine. In ACML, 2013. 2
[33] D. Novotny, D. Larlus, and A. Vedaldi. Anchornet: A weakly
supervised network to learn geometry-sensitive features for
semantic matching. In CVPR, 2017. 2
[34] M. Pedersoli, R. Timofte, T. Tuytelaars, and L. J. V. Gool.
Using a deformation ﬁeld model for localizing faces and fa-
cial points under weak supervision. In CVPR, 2014. 2
[35] T. Pﬁster, J. Charles, and A. Zisserman. Flowing convnets
for human pose estimation in videos. In ICCV, 2015. 2, 7
[36] R. Ranjan, V. M. Patel, and R. Chellappa. Hyperface: A
deep multi-task learning framework for face detection, land-
mark localization, pose estimation, and gender recognition.
TPAMI, 2017. 2
[37] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu-
tional networks for biomedical image segmentation. In MIC-
CAI, 2015. 4
[38] D. A. Ross and R. S. Zemel. Learning parts-based represen-
tations of data. JMLR, 2006. 2

10963

[58] Y. Zhang, Y. Guo, Y. Jin, Y. Luo, Z. He, and H. Lee. Un-
supervised discovery of object landmarks as structural repre-
sentations. In CVPR, 2018. 2, 4, 5, 6, 7
[59] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Facial landmark
detection by deep multi-task learning. In ECCV, 2014. 2
[60] Z. Zhang, P. Luo, C. C. Loy, and X. Tang. Learning deep
representation for face alignment with auxiliary attributes.
TPAMI, 2016. 2
[61] S. Zhu, C. Li, C. C. Loy, and X. Tang. Face alignment by
coarse-to-ﬁne shape searching. In CVPR, 2015. 2

[39] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al.
Imagenet large scale visual recognition challenge.
ICCV, 2015. 7
[40] Z. Shu, M. Sahasrabudhe, R. A. G ¨uler, D. Samaras, N. Para-
gios, and I. Kokkinos. Deforming autoencoders: Unsuper-
vised disentangling of shape and appearance.
In ECCV,
2018. 1, 2
[41] A. Siarohin, E. Sangineto, S. Lathuili `ere, and N. Sebe.
Deformable gans for pose-based human image generation.
CVPR, 2018. 1, 2
[42] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery
of mid-level discriminative patches. In ECCV, 2012. 2
[43] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 7
[44] J. Thewlis, H. Bilen, and A. Vedaldi. Unsupervised learning
of object frames by dense equivariant image labelling.
In
NIPS, 2017. 2
[45] J. Thewlis, H. Bilen, and A. Vedaldi. Unsupervised learning
of object landmarks by factorized spatial embeddings.
In
ICCV, 2017. 2, 4, 5, 6, 7
[46] A. Toshev and C. Szegedy. Deeppose: Human pose estima-
tion via deep neural networks. In CVPR, 2014. 2
[47] N. Ufer and B. Ommer. Deep semantic feature matching.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 6914–6923, 2017. 2
[48] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
The caltech-ucsd birds-200-2011 dataset. Technical report,
California Institute of Technology, 2011. 5
[49] S.-E. Wei, V. Ramakrishna, T. Kanade, and Y. Sheikh. Con-
volutional pose machines. In CVPR, 2016. 2
[50] Y. Wu and Q. Ji. Robust facial landmark detection under
signiﬁcant head poses and occlusion. CVPR, 2015. 2
[51] T. Xiao, S. Li, B. Wang, L. Lin, and X. Wang. Joint detec-
tion and identiﬁcation feature learning for person search. In
CVPR. IEEE, 2017. 7
[52] X. Xing, R. Gao, T. Han, S.-C. Zhu, and Y. N. Wu.
Deformable generator network: Unsupervised disentan-
glement of appearance and geometry.
arXiv preprint
arXiv:1806.06298, 2018. 1, 2
[53] W. Yang, W. Ouyang, H. Li, and X. Wang. End-to-end learn-
ing of deformable mixture of parts and deep convolutional
neural networks for human pose estimation. In CVPR, 2016.
2
[54] I. Yildirim, T. D. Kulkarni, W. Freiwald, and J. B. Tenen-
baum. Efﬁcient analysis-by-synthesis in vision: A compu-
tational framework, behavioral tests, and modeling neuronal
representations. In CogSci, 2015. 2
[55] X. Yu, F. Zhou, and M. Chandraker. Deep deformation net-
work for object landmark localization. In ECCV, 2016. 2
[56] W. Zhang, J. Sun, and X. Tang. Cat head detection - how
to effectively exploit shape and texture features. In ECCV,
2008. 5
[57] W. Zhang, M. Zhu, and K. G. Derpanis. From actemes to
action: A strongly-supervised representation for detailed ac-
tion understanding. In ICCV, 2013. 5

10964

